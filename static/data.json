[{"author": "zsz", "index_original": 2, "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data", "pub_year": 2020, "domain": "Healthcare", "requirement": {"requirement_text": "T1: Cell Type Discovery and Calling. The task most frequently mentioned by all experts is identifying and analyzing speci\ufb01c types and states of cells based on the intensity and pattern of staining with speci\ufb01c antibodies (O1, O2, P1, P2, CB1-6). Challenges: Challenges lie in processing, displaying, and faceting the large and high-dimensional data, as well as in mutual support of manual and automated analysis. A lack of adequate tools makes this task very time-consuming at present.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.", "data_code": {"clusters_and_sets_and_lists": 1, "tables": 1, "media": 1, "geometry": 1}}, "solution": [{"solution_text": "Classification and clustering can be triggered in different views to support the hierarchical discovery of cell types.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 3, "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data", "pub_year": 2020, "domain": "Healthcare", "requirement": {"requirement_text": "T1: Cell Type Discovery and Calling. The task most frequently mentioned by all experts is identifying and analyzing speci\ufb01c types and states of cells based on the intensity and pattern of staining with speci\ufb01c antibodies (O1, O2, P1, P2, CB1-6). Challenges: Challenges lie in processing, displaying, and faceting the large and high-dimensional data, as well as in mutual support of manual and automated analysis. A lack of adequate tools makes this task very time-consuming at present.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.", "data_code": {"clusters_and_sets_and_lists": 1, "tables": 1, "media": 1, "geometry": 1}}, "solution": [{"solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 4, "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data", "pub_year": 2020, "domain": "Healthcare", "requirement": {"requirement_text": "T2: Overview-Detail Exploration of Multi-Channel Image Data. A crucial task for oncologists and pathologists is rapid navigation and visualization of multi-channel images (O1, O2, P1, P2). Pathologists are accustomed to moving slides back and forth physically on a micro-scope stage and switching between high and low power views. They rely on a seamless visual experience to make a diagnosis. Challenges: Image analysis must not only support seamless pan and zoom, but also switching between groups of channels. Current tools do not scale beyond 4-5 channels and lack on-demand rendering, blending of channels, and means to emphasize (and recall) regions or individual cells of interest.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.", "data_code": {"clusters_and_sets_and_lists": 1, "tables": 1, "media": 1, "geometry": 1}}, "solution": [{"solution_text": "Facetto\u2019s image viewer allows users to navigate and explore large multi- channel image data using a scalable multi-resolution visualization approach. The visualization supports interactive and seamless zooming and panning, on-demand rendering of multi-channel information, man- ual selection of cells, and highlighting of classification and clustering results as well as faceting operations. The image viewer also provides details on demand when hovering over individual cells.", "solution_category": "interaction", "solution_axial": "Filtering,OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Filtering"], "componenet_code": ["overview_and_explore", "filtering"]}]}, {"author": "zsz", "index_original": 5, "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data", "pub_year": 2020, "domain": "Healthcare", "requirement": {"requirement_text": "T2: Overview-Detail Exploration of Multi-Channel Image Data. A crucial task for oncologists and pathologists is rapid navigation and visualization of multi-channel images (O1, O2, P1, P2). Pathologists are accustomed to moving slides back and forth physically on a micro-scope stage and switching between high and low power views. They rely on a seamless visual experience to make a diagnosis. Challenges: Image analysis must not only support seamless pan and zoom, but also switching between groups of channels. Current tools do not scale beyond 4-5 channels and lack on-demand rendering, blending of channels, and means to emphasize (and recall) regions or individual cells of interest.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.", "data_code": {"clusters_and_sets_and_lists": 1, "tables": 1, "media": 1, "geometry": 1}}, "solution": [{"solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Support dynamic selection of cells and their visual display in image space and dynamically adjust the render mode based on the cur-rent pixel\u2019s cell ID. ", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "image+area", "axial_code": [], "componenet_code": ["image", "area"]}, {"solution_text": "Facetto supports different render modes, depending on whether the user is currently focusing on the entire dataset, a subset(i.e., a node in the hierarchical phenotype tree), or a user selection (i.e.,based either on manual selection or a clustering/classification result).Facetto can then display individual cells in their original grayscale in-tensity, apply a color and opacity transfer function, or show color overlays for cluster/class membership.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 6, "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data", "pub_year": 2020, "domain": "Healthcare", "requirement": {"requirement_text": "T3: Data Filtering and (Sub-)Structuring. Another task frequently performed by pathologists is gating, which refers to manual \ufb01ltering of selected image channels based on the channel\u2019s intensity value range (often visualized as a frequency-intensity plot), or speci\ufb01c spatial features or regions of interest (P2, O2, CB2). Challenges: Analysis steps such as gating are often applied in an iterative manner in which the data is hierarchically faceted into subsets. These subsets can then be further analyzed, used in benchmarks, or exported for presentation or reuse with other samples. Thus, tracking the evolution and provenance of gates and gated data is important.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.", "data_code": {"clusters_and_sets_and_lists": 1, "media": 1, "tables": 1}}, "solution": [{"solution_text": "Experts usually start with a region of interest (ROI) at high resolution (so thatindividuals cells are visible) that represents a subset of the completemulti-channel image stack and then define spatial and image featuresto create data subsets of interest. The results obtained on this ROI are then applied to the entire specimen", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "tree+circle", "axial_code": [], "componenet_code": ["tree", "circle"]}, {"solution_text": "Experts usually start with a region of interest (ROI) at high resolution (so thatindividuals cells are visible) that represents a subset of the completemulti-channel image stack and then define spatial and image featuresto create data subsets of interest. The results obtained on this ROI are then applied to the entire specimen", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "zsz", "index_original": 7, "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data", "pub_year": 2020, "domain": "Healthcare", "requirement": {"requirement_text": "T3: Data Filtering and (Sub-)Structuring. Another task frequently performed by pathologists is gating, which refers to manual \ufb01ltering of selected image channels based on the channel\u2019s intensity value range (often visualized as a frequency-intensity plot), or speci\ufb01c spatial features or regions of interest (P2, O2, CB2). Challenges: Analysis steps such as gating are often applied in an iterative manner in which the data is hierarchically faceted into subsets. These subsets can then be further analyzed, used in benchmarks, or exported for presentation or reuse with other samples. Thus, tracking the evolution and provenance of gates and gated data is important.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.", "data_code": {"clusters_and_sets_and_lists": 1, "tables": 1, "media": 1, "geometry": 1}}, "solution": [{"solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Visually encode features of interest and subsets of the data. To make spatial distributions and correlations among multiple channels pre-attentively visible, we allow the user to blend the data from different channels into a single image.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "image+area", "axial_code": [], "componenet_code": ["image", "area"]}, {"solution_text": "Users select the appropriate channels for their current tasks based on domain knowledge. For each channel, users can set and modify a linear color and opacity transfer function by specifying the respective intensity range (i.e., lower and upper bound) as well as the colors of the transfer function.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 8, "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data", "pub_year": 2020, "domain": "Healthcare", "requirement": {"requirement_text": "T3: Data Filtering and (Sub-)Structuring. Another task frequently performed by pathologists is gating, which refers to manual \ufb01ltering of selected image channels based on the channel\u2019s intensity value range (often visualized as a frequency-intensity plot), or speci\ufb01c spatial features or regions of interest (P2, O2, CB2). Challenges: Analysis steps such as gating are often applied in an iterative manner in which the data is hierarchically faceted into subsets. These subsets can then be further analyzed, used in benchmarks, or exported for presentation or reuse with other samples. Thus, tracking the evolution and provenance of gates and gated data is important.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.", "data_code": {"clusters_and_sets_and_lists": 1, "tables": 1, "media": 1, "geometry": 1}}, "solution": [{"solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Support dynamic selection of cells and their visual display in image space and dynamically adjust the render mode based on the cur-rent pixel\u2019s cell ID. ", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "image+area", "axial_code": [], "componenet_code": ["image", "area"]}, {"solution_text": "Facetto supports different render modes, depending on whether the user is currently focusing on the entire dataset, a subset(i.e., a node in the hierarchical phenotype tree), or a user selection (i.e.,based either on manual selection or a clustering/classification result).Facetto can then display individual cells in their original grayscale in-tensity, apply a color and opacity transfer function, or show color overlays for cluster/class membership.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 9, "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data", "pub_year": 2020, "domain": "Healthcare", "requirement": {"requirement_text": "T3: Data Filtering and (Sub-)Structuring. Another task frequently performed by pathologists is gating, which refers to manual \ufb01ltering of selected image channels based on the channel\u2019s intensity value range (often visualized as a frequency-intensity plot), or speci\ufb01c spatial features or regions of interest (P2, O2, CB2). Challenges: Analysis steps such as gating are often applied in an iterative manner in which the data is hierarchically faceted into subsets. These subsets can then be further analyzed, used in benchmarks, or exported for presentation or reuse with other samples. Thus, tracking the evolution and provenance of gates and gated data is important.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.", "data_code": {"clusters_and_sets_and_lists": 1, "tables": 1, "media": 1, "geometry": 1}}, "solution": [{"solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Each ridge is equipped with range sliders, allowing to filter theunderlying data with visual feedback at the level of the distribution.The selection range is used directly for specifying the color transferfunction applied in the image viewer. To allow the exploration of fea-ture distributions for subsets of the image or individually selected cells,we overlay the parallel x-axes in the ridgeplot with vertical polylines(see orange paths), encoding each cell\u2019s features as a connectedpath, similar to parallel coordinate plots [36]. To reduce clutter, wedecrease opacity as the number of selected cells grows, an approachthat provides an indication of the correlations and distribution rangesof a selection, while focusing less on individual cells. Alternatively, asingle polyline can be used to represent the mean values of a cluster.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "image+parallelcoordinates", "axial_code": [], "componenet_code": ["image", "parallelcoordinates"]}, {"solution_text": "Each ridge is equipped with range sliders, allowing to filter theunderlying data with visual feedback at the level of the distribution.The selection range is used directly for specifying the color transferfunction applied in the image viewer. To allow the exploration of fea-ture distributions for subsets of the image or individually selected cells,we overlay the parallel x-axes in the ridgeplot with vertical polylines(see orange paths), encoding each cell\u2019s features as a connectedpath, similar to parallel coordinate plots [36]. To reduce clutter, wedecrease opacity as the number of selected cells grows, an approachthat provides an indication of the correlations and distribution rangesof a selection, while focusing less on individual cells. Alternatively, asingle polyline can be used to represent the mean values of a cluster.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 10, "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data", "pub_year": 2020, "domain": "Healthcare", "requirement": {"requirement_text": "T4: Proofreading and Analyzing Results in Spatial Context. Many algorithms operate on features computed from images following segmentation; these include mean intensity value per cell and channel. Feature extraction and segmentation from tissues, in which cells of different sizes and shapes are crowded together, are challenging tasks for which software tools are still being developed. As a result, it is essential that the results of feature extraction are checked and corrected prior to downstream data processing (CB1, CB3). This requires effective means to link feature and image space (P1, O1). Challenges: Currently, such linking is only supported by HistoCat [60], and generally requires domain experts to continuously switch between tools (CB2).", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.", "data_code": {"clusters_and_sets_and_lists": 1, "tables": 1, "media": 1, "geometry": 1}}, "solution": [{"solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Visually encode features of interest and subsets of the data. To make spatial distributions and correlations among multiple channels pre-attentively visible, we allow the user to blend the data from different channels into a single image.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "image+area", "axial_code": [], "componenet_code": ["image", "area"]}, {"solution_text": "Users select the appropriate channels for their current tasks based on domain knowledge. For each channel, users can set and modify a linear color and opacity transfer function by specifying the respective intensity range (i.e., lower and upper bound) as well as the colors of the transfer function.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 11, "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data", "pub_year": 2020, "domain": "Healthcare", "requirement": {"requirement_text": "T4: Proofreading and Analyzing Results in Spatial Context. Many algorithms operate on features computed from images following segmentation; these include mean intensity value per cell and channel. Feature extraction and segmentation from tissues, in which cells of different sizes and shapes are crowded together, are challenging tasks for which software tools are still being developed. As a result, it is essential that the results of feature extraction are checked and corrected prior to downstream data processing (CB1, CB3). This requires effective means to link feature and image space (P1, O1). Challenges: Currently, such linking is only supported by HistoCat [60], and generally requires domain experts to continuously switch between tools (CB2).", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.", "data_code": {"clusters_and_sets_and_lists": 1, "media": 1, "tables": 1}}, "solution": [{"solution_text": "Experts can sort, inspect, select,and manipulate individual values for each feature of a cell using theinteractive visual tabular display. The main goal of thetabular view is a) detailed analysis and direct manipulation of individualcells, and b) allowing users access to the original data table. We encodethe extracted intensity values in the tabular view as numbers, as wellas by using small multiples of bars. The tabular view is also colorencoded, with each color indicating a distinct phenotypic class. Allviews in Facetto are connected via brushing and linking so that userscan analyze a selection from different perspectives. In this way, userscan mark (and edit) individual cells with certain features in the tabularview and inspect spatial context in the image view or vice versa.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+bar", "axial_code": [], "componenet_code": ["bar", "table"]}, {"solution_text": "Experts can sort, inspect, select,and manipulate individual values for each feature of a cell using theinteractive visual tabular display. The main goal of thetabular view is a) detailed analysis and direct manipulation of individualcells, and b) allowing users access to the original data table. We encodethe extracted intensity values in the tabular view as numbers, as wellas by using small multiples of bars. The tabular view is also colorencoded, with each color indicating a distinct phenotypic class. Allviews in Facetto are connected via brushing and linking so that userscan analyze a selection from different perspectives. In this way, userscan mark (and edit) individual cells with certain features in the tabularview and inspect spatial context in the image view or vice versa.", "solution_category": "interaction", "solution_axial": "Selecting,Reconfigure,Participation/Collaboration", "solution_compoent": "", "axial_code": ["Selecting", "Reconfigure", "Participation/Collaboration"], "componenet_code": ["selecting", "reconfigure", "participation_collaboration"]}]}, {"author": "zsz", "index_original": 12, "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data", "pub_year": 2020, "domain": "Healthcare", "requirement": {"requirement_text": "T5: Deriving Pro\ufb01les for (Sub)regions and Classes. Once a type/region is detected, it is important to identify, annotate, and extract a pro\ufb01le of typical marker distributions within an area of interest (O2, P2). The pro\ufb01le includes statistical measures and distributions of cell features and can be used to present the outcome of an analysis session, diagnosis, or as a starting point for further analysis. Challenges: The variables used to construct pro\ufb01les, and the ways in which these variables are displayed, are not standardized and can only be developed by human-machine interaction.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.", "data_code": {"clusters_and_sets_and_lists": 1, "media": 1, "tables": 1}}, "solution": [{"solution_text": "Users can build up a hierarchy of different data subsets, and we automatically display this ongoing analysis in a hierarchical phenotype tree view. This allows users to track their progress, and to maintain an overview of their data faceting and analysis steps they have performed.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "tree+circle", "axial_code": [], "componenet_code": ["tree", "circle"]}, {"solution_text": "Users can build up a hierarchy of different data subsets, and we automatically display this ongoing analysis in a hierarchical phenotype tree view. This allows users to track their progress, and to maintain an overview of their data faceting and analysis steps they have performed.", "solution_category": "interaction", "solution_axial": "Selecting,Abstract/Elaborate", "solution_compoent": "", "axial_code": ["Selecting", "Abstract/Elaborate"], "componenet_code": ["selecting", "abstract_elaborate"]}]}, {"author": "zsz", "index_original": 13, "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data", "pub_year": 2020, "domain": "Healthcare", "requirement": {"requirement_text": "T5: Deriving Pro\ufb01les for (Sub)regions and Classes. Once a type/region is detected, it is important to identify, annotate, and extract a pro\ufb01le of typical marker distributions within an area of interest (O2, P2). The pro\ufb01le includes statistical measures and distributions of cell features and can be used to present the outcome of an analysis session, diagnosis, or as a starting point for further analysis. Challenges: The variables used to construct pro\ufb01les, and the ways in which these variables are displayed, are not standardized and can only be developed by human-machine interaction.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.", "data_code": {"clusters_and_sets_and_lists": 1, "tables": 1, "media": 1, "geometry": 1}}, "solution": [{"solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "To reveal contextual feature information for anindividual cell or for a selection, users can click on a cell in the im-age viewer and show a visual profile card in which data statistics aresummarized. The card shows a boxplot of the feature space, thephenotype labels, and a short summary that includes any previous userannotation, making it possible for information to be acquired sequen-tially over a number of sessions involving multiple users.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "To reveal contextual feature information for anindividual cell or for a selection, users can click on a cell in the im-age viewer and show a visual profile card in which data statistics aresummarized. The card shows a boxplot of the feature space, thephenotype labels, and a short summary that includes any previous userannotation, making it possible for information to be acquired sequen-tially over a number of sessions involving multiple users.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 14, "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data", "pub_year": 2020, "domain": "Healthcare", "requirement": {"requirement_text": "T5: Deriving Pro\ufb01les for (Sub)regions and Classes. Once a type/region is detected, it is important to identify, annotate, and extract a pro\ufb01le of typical marker distributions within an area of interest (O2, P2). The pro\ufb01le includes statistical measures and distributions of cell features and can be used to present the outcome of an analysis session, diagnosis, or as a starting point for further analysis. Challenges: The variables used to construct pro\ufb01les, and the ways in which these variables are displayed, are not standardized and can only be developed by human-machine interaction.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.", "data_code": {"clusters_and_sets_and_lists": 1, "tables": 1, "media": 1, "geometry": 1}}, "solution": [{"solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "For the exploration of each chan-nel\u2019s distribution, we integrated a ridgeplot that comprisesmultiple areas alongside relevant information about the variablebeing examined. Each chart represents a feature\u2019s value distribu-tion (a ridge). Typically, the distribution of features that are derivedfrom a channel\u2019s intensity values is skewed, having a few distinct peaksand some outliers.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "image+parallelcoordinates", "axial_code": [], "componenet_code": ["image", "parallelcoordinates"]}]}, {"author": "zsz", "index_original": 15, "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data", "pub_year": 2020, "domain": "Healthcare", "requirement": {"requirement_text": "T5: Deriving Pro\ufb01les for (Sub)regions and Classes. Once a type/region is detected, it is important to identify, annotate, and extract a pro\ufb01le of typical marker distributions within an area of interest (O2, P2). The pro\ufb01le includes statistical measures and distributions of cell features and can be used to present the outcome of an analysis session, diagnosis, or as a starting point for further analysis. Challenges: The variables used to construct pro\ufb01les, and the ways in which these variables are displayed, are not standardized and can only be developed by human-machine interaction.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.", "data_code": {"clusters_and_sets_and_lists": 1, "tables": 1, "media": 1, "geometry": 1}}, "solution": [{"solution_text": "We visualize higher-order similarities and differences between data subsets using dimensionality reduction techniques and subsequent displayin a 2D scatterplot. We use UMAP (uniform manifold approximationand projection for dimension reduction), a recently developed ma-chine learning technique, to display features of the current data subset.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "We visualize higher-order similarities and differences between data subsets using dimensionality reduction techniques and subsequent displayin a 2D scatterplot. We use UMAP (uniform manifold approximationand projection for dimension reduction), a recently developed ma-chine learning technique, to display features of the current data subset.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}]}, {"author": "zsz", "index_original": 18, "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos", "pub_year": 2021, "domain": "Emotion", "requirement": {"requirement_text": "Obtain the emotion status of all the people in a video. Given a specific video, users have a great interest in gaining a quick overview of the video content. For example, what is the overall emotion trend as the video progresses? What kind of emotion dominates the video? Compared with checking the original video back and forth, a visual overview would greatly reduce the browsing burden.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "It is important to provide users with an overview of the emotion evolution of individuals (R1). Thus, we design a summary view to provide users with both a static and a dynamic summary of the emotions: the emotion archives (Fig. 3a) to visualize the emotion distribution of individuals (static summary), and the emotion \ufb02ow (Fig. 3b) to show the dynamic evolution of these emotions (dynamic summary).", "solution_category": "data_manipulation", "solution_axial": "Sampling,Modeling", "solution_compoent": "Videosampling,categoricalmodels", "axial_code": ["Modeling", "Sampling"], "componenet_code": ["modeling", "sampling"]}]}, {"author": "zsz", "index_original": 19, "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos", "pub_year": 2021, "domain": "Emotion", "requirement": {"requirement_text": "Obtain the emotion status of all the people in a video. Given a specific video, users have a great interest in gaining a quick overview of the video content. For example, what is the overall emotion trend as the video progresses? What kind of emotion dominates the video? Compared with checking the original video back and forth, a visual overview would greatly reduce the browsing burden.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "It is important to provide users with an overview of the emotion evolution of individuals. Thus, we design a summary view to provide users with both a static and a dynamic summary of the emotions: the emotion flow to show the dynamic evolution of these emotions (dynamic summary).", "solution_category": "data_manipulation", "solution_axial": "Sampling,Modeling", "solution_compoent": "Videosampling,categoricalmodels", "axial_code": ["Modeling", "Sampling"], "componenet_code": ["modeling", "sampling"]}]}, {"author": "zsz", "index_original": 20, "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos", "pub_year": 2021, "domain": "Emotion", "requirement": {"requirement_text": "Uncover emotion patterns of an individual in a video. After gaining an overview of the given video, users concentrate on an individual of interest. For example, most parents are concerned about their own children, and they are likely to explore individuals in a video. What is the emotion pattern of a selected person in this video? How do his/her emotions evolve over time?", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "We use lines to connect the emotion archives to the corresponding flows. Each line represents a person, starting from a person selected in the emotion archives, and then connects to his/her emotion flow. Therefore, users can easily track personal emotion evolution.", "solution_category": "data_manipulation", "solution_axial": "Sampling,Modeling", "solution_compoent": "Videosampling,categoricalmodels", "axial_code": ["Modeling", "Sampling"], "componenet_code": ["modeling", "sampling"]}]}, {"author": "zsz", "index_original": 21, "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos", "pub_year": 2021, "domain": "Emotion", "requirement": {"requirement_text": "Uncover emotion patterns of an individual in a video. After gaining an overview of the given video, users concentrate on an individual of interest. For example, most parents are concerned about their own children, and they are likely to explore individuals in a video. What is the emotion pattern of a selected person in this video? How do his/her emotions evolve over time?", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "The character view visualizes the emotion status of a selected person with a portrait glyph. We adopt a tailored donut chart in this design. Each annular sector on the outer part represents an emotion. The area of each annular sector illustrates the amount of the corresponding emotion which appears in the video.", "solution_category": "data_manipulation", "solution_axial": "Sampling,Modeling", "solution_compoent": "Videosampling,categoricalmodels", "axial_code": ["Modeling", "Sampling"], "componenet_code": ["modeling", "sampling"]}]}, {"author": "zsz", "index_original": 22, "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos", "pub_year": 2021, "domain": "Emotion", "requirement": {"requirement_text": "Compare emotion portraits of different people. Users would like to explore a person of interest, especially to obtain his/her relative status in a video. Further comparisons between different people empower users to identify abnormal patterns. For example, teachers may worry about a special student in the class, and parents are curious about whether their children behave differently compared to others. Therefore, comparing different people\u2019s emotion patterns and measuring their similarity and difference are very valuable for users.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "We use lines to connect the emotion archives to the corresponding flows. Each line represents a person, starting from a person selected in the emotion archives, and then connects to his/her emotion flow. Therefore, users can easily compare the emotion evolution of different people.", "solution_category": "data_manipulation", "solution_axial": "Sampling,Modeling", "solution_compoent": "Videosampling,categoricalmodels", "axial_code": ["Modeling", "Sampling"], "componenet_code": ["modeling", "sampling"]}]}, {"author": "zsz", "index_original": 23, "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos", "pub_year": 2021, "domain": "Emotion", "requirement": {"requirement_text": "Compare emotion portraits of different people. Users would like to explore a person of interest, especially to obtain his/her relative status in a video. Further comparisons between different people empower users to identify abnormal patterns. For example, teachers may worry about a special student in the class, and parents are curious about whether their children behave differently compared to others. Therefore, comparing different people\u2019s emotion patterns and measuring their similarity and difference are very valuable for users.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "Comparison between different emotion portraits enables users to identify and compare the characteristics of different people. We adopt a tailored donut chart in this design. Each annular sector on the outer part represents an emotion. The area of each annular sector illustrates the amount of the corresponding emotion which appears in the video.", "solution_category": "data_manipulation", "solution_axial": "Sampling,Modeling", "solution_compoent": "Videosampling,categoricalmodels", "axial_code": ["Modeling", "Sampling"], "componenet_code": ["modeling", "sampling"]}]}, {"author": "zsz", "index_original": 24, "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos", "pub_year": 2021, "domain": "Emotion", "requirement": {"requirement_text": "Compare emotion portraits of different people. Users would like to explore a person of interest, especially to obtain his/her relative status in a video. Further comparisons between different people empower users to identify abnormal patterns. For example, teachers may worry about a special student in the class, and parents are curious about whether their children behave differently compared to others. Therefore, comparing different people\u2019s emotion patterns and measuring their similarity and difference are very valuable for users.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "The comparison of the basic emotion information between different people become easy with the screen snapshot function. If users want to explore details, they can click the snapshot of interest for further exploration. Snapshot examples are demonstrated on the left-hand side of the character view.", "solution_category": "data_manipulation", "solution_axial": "Sampling,Modeling", "solution_compoent": "Videosampling,categoricalmodels", "axial_code": ["Modeling", "Sampling"], "componenet_code": ["modeling", "sampling"]}]}, {"author": "zsz", "index_original": 25, "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos", "pub_year": 2021, "domain": "Emotion", "requirement": {"requirement_text": "Reveal model uncertainty with influencing factors. Emotion recognition algorithms are not perfect and the accuracy is influenced by multiple factors. Leveraging these factors properly can provide useful cues for inferring underlying patterns. For example, the accuracy of emotion recognition probably decreases, when the algorithm processes a child face image with a small face size in the video or occluded by others. It would also be better to allow users to investigate model accuracy and correct corresponding errors if needed.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "The influencing factor bar chart mainly shows the aggregation information of face size and occlusion. We use a bar to encode different influencing factors foreach time span. The height of the bar indicates the face size detected in the video (the higher, the larger face size), whereas the black shading area of the bar represents the occlusion degree.", "solution_category": "data_manipulation", "solution_axial": "Sampling", "solution_compoent": "Videosampling", "axial_code": ["Sampling"], "componenet_code": ["sampling"]}]}, {"author": "zsz", "index_original": 26, "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos", "pub_year": 2021, "domain": "Emotion", "requirement": {"requirement_text": "Reveal model uncertainty with influencing factors. Emotion recognition algorithms are not perfect and the accuracy is influenced by multiple factors. Leveraging these factors properly can provide useful cues for inferring underlying patterns. For example, the accuracy of emotion recognition probably decreases, when the algorithm processes a child face image with a small face size in the video or occluded by others. It would also be better to allow users to investigate model accuracy and correct corresponding errors if needed.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "We can easily observe detailed emotion information and influencing factors for the person of interest with such a tailored donut chart design. ", "solution_category": "data_manipulation", "solution_axial": "Sampling,Modeling", "solution_compoent": "Videosampling,categoricalmodels", "axial_code": ["Modeling", "Sampling"], "componenet_code": ["modeling", "sampling"]}]}, {"author": "zsz", "index_original": 27, "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos", "pub_year": 2021, "domain": "Emotion", "requirement": {"requirement_text": "Provide context for video analysis. The visual analytics system is based on complex recognition models and abstract data representation. Users also want to know the original video context, which helps them understand the analytical results and validate assumptions. For example, what kind of scenario leads to a change of emotions? Do their assumptions about these findings make sense?", "requirement_code": {"describe_observation_item": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "We provide the original video for users to explore in the video view. Users can play the video at slow, normal, or fast speeds. When users pause the video, the corresponding faces in each frame are highlighted. Users can also pick out the parts of interest for further exploration based on their observation from the emotion flow. This view is mainly used for providing evidence for users. When users explore other views and find something interesting, they can link to corresponding frames in the video view. Accurately extracting information from a video is a challenge. Therefore, we provide an interactive way for users to correct this inaccuracy. When users identify a wrongly labeled person, they can click on the person and select the correct label. The face label will be automatically updated in the database. Similarly, users can correct the emotion information. With this method, users are allowed to interactively correct any inaccurate information caused by models.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "Video", "axial_code": [], "componenet_code": ["video"]}]}, {"author": "zsz", "index_original": 28, "paper_title": "Interactive Correction of Mislabeled Training Data", "pub_year": 2019, "domain": "label", "requirement": {"requirement_text": "Handling large-scale data. The machine learning problems that our experts face typically involve very complex feature space and statistical characteristics, which require a large volume of training data for model learning.", "requirement_code": {"flexibility_and_scalability": 1}}, "data": {"data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "textual": 1}}, "solution": [{"solution_text": "An item view that utilizes a tSNE-based hierarchical visualization to display the distribution of training items, dis-closes patterns of clusters and outliers, and supports interactive exploration of the details.", "solution_category": "data_manipulation", "solution_axial": "Rectification,Sampling,DimensionalityReduction", "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE", "axial_code": ["Rectification", "Sampling", "DimensionalityReduction"], "componenet_code": ["rectification", "sampling", "dimensionality_reduction"]}]}, {"author": "zsz", "index_original": 29, "paper_title": "Interactive Correction of Mislabeled Training Data", "pub_year": 2019, "domain": "label", "requirement": {"requirement_text": "Handling large-scale data. The machine learning problems that our experts face typically involve very complex feature space and statistical characteristics, which require a large volume of training data for model learning.", "requirement_code": {"flexibility_and_scalability": 1}}, "data": {"data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "textual": 1}}, "solution": [{"solution_text": "The itemview supports a hierarchical exploration of item distribution. Users are first presented with the overview, i.e., the top level of the hierarchical structure. The classes of items are encoded by colors. The selected items are emphasized by thick edges. Ideally, items are visually clustered by labels.", "solution_category": "data_manipulation", "solution_axial": "Rectification,Sampling,DimensionalityReduction", "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE", "axial_code": ["Rectification", "Sampling", "DimensionalityReduction"], "componenet_code": ["rectification", "sampling", "dimensionality_reduction"]}]}, {"author": "zsz", "index_original": 30, "paper_title": "Interactive Correction of Mislabeled Training Data", "pub_year": 2019, "domain": "label", "requirement": {"requirement_text": "Handling large-scale data. The machine learning problems that our experts face typically involve very complex feature space and statistical characteristics, which require a large volume of training data for model learning.", "requirement_code": {"flexibility_and_scalability": 1}}, "data": {"data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "textual": 1}}, "solution": [{"solution_text": "In the item view, there could be overlaps between data items due to the large scale. Our hierarchical visualization is designed to address this issue (R1). During hierarchical navigation, the user can select a small number of items in a drill down operation to reduce overlaps in the new layout. In addition, we provide a density map to display the distribution of items", "solution_category": "data_manipulation", "solution_axial": "Rectification,Sampling,DimensionalityReduction", "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE", "axial_code": ["Rectification", "Sampling", "DimensionalityReduction"], "componenet_code": ["rectification", "sampling", "dimensionality_reduction"]}]}, {"author": "zsz", "index_original": 31, "paper_title": "Interactive Correction of Mislabeled Training Data", "pub_year": 2019, "domain": "label", "requirement": {"requirement_text": "Examining unusual distribution for identifying labelingerrors. A cleaning pipeline oftenstarts by identifying local regions with unusual patterns of data or la-bel distribution, upon which mislabeled training items can be largelyidentified and inspected. Thus, the experts wanted to quickly locatesuch suspicious regions. Moreover, in each region, the mislabeledtraining items should always be displayed with priority no matterwhich data filter or sampling strategy is applied.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "textual": 1}}, "solution": [{"solution_text": "An item view that utilizes a tSNE-based hierarchical visualization to display the distribution of training items, discloses patterns of clusters and outliers, and supports interactive exploration of the details.", "solution_category": "data_manipulation", "solution_axial": "Rectification,Sampling,DimensionalityReduction", "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE", "axial_code": ["Rectification", "Sampling", "DimensionalityReduction"], "componenet_code": ["rectification", "sampling", "dimensionality_reduction"]}]}, {"author": "zsz", "index_original": 32, "paper_title": "Interactive Correction of Mislabeled Training Data", "pub_year": 2019, "domain": "label", "requirement": {"requirement_text": "Examining unusual distribution for identifying labelingerrors. A cleaning pipeline oftenstarts by identifying local regions with unusual patterns of data or la-bel distribution, upon which mislabeled training items can be largelyidentified and inspected. Thus, the experts wanted to quickly locatesuch suspicious regions. Moreover, in each region, the mislabeledtraining items should always be displayed with priority no matterwhich data filter or sampling strategy is applied.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "textual": 1}}, "solution": [{"solution_text": "The labeling noises in the training data often result in the mixed color distribution in some regions. These suspicious regions indicate potential mislabeled items and deserve further examination, e.g., zooming into the regions to explore more items.", "solution_category": "data_manipulation", "solution_axial": "Rectification,Sampling,DimensionalityReduction", "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE", "axial_code": ["Rectification", "Sampling", "DimensionalityReduction"], "componenet_code": ["rectification", "sampling", "dimensionality_reduction"]}]}, {"author": "zsz", "index_original": 33, "paper_title": "Interactive Correction of Mislabeled Training Data", "pub_year": 2019, "domain": "label", "requirement": {"requirement_text": "Recommending and verifying trusted items. While manual inspection and correction is a routine for debugging mislabeled items, the experts need automated approaches to improve efficiency. Methods such as propagation of trusted items are the most discussed and recognized. However, without efficient algorithms and interactive tools, selecting and validating a set of trusted items from the cluttered visualization can still become laborious. Two requirements were identified by our collaborators based on their experience. First, automatic recommendation algorithms are desired to locate trusted items quickly. Second, flexible ways to examine and compare trusted items are required for further verification", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "media": 1}}, "solution": [{"solution_text": "Identifying and selecting trusted items (R3). The item view and other three views cooperate to support the identi\ufb01cation, selection, and correction of trusted items. During the exploration, the user can select a set of items and add them into the selected item view. If s/he wants to reduce the number of selected items, s/he can use the \u201cRecommend trusted items\u201d operation to select a representative subset. Images of the selected items are shown in the selected items view Fig. 1 (c), where the user can look at the images, relate to the distribution in the item view, re\ufb01ne the selection accordingly, and correct the labels if labeling errors are noticed. After the re\ufb01nement, the selected items can be added into trusted items. The trusted item view (Fig. 1 (d)) displays the images and labels of all the trusted items. Further editing can be performed in the trusted item view. Usually, the identi\ufb01cation and selection of trusted items in different regions require different strategies. In a region without labeling outliers, the user usually selects trusted items using system \u201crecommendation\u201d, and verifies the labels by only a glance at the images in the selected item view. In a region with labeling outliers, more careful examinations and operations are required from the user, including investigating the distribution at different levels, selecting class-balanced trusted items, and correcting the labels step-by-step for each class.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "image+bar", "axial_code": [], "componenet_code": ["bar", "image"]}]}, {"author": "zsz", "index_original": 34, "paper_title": "Interactive Correction of Mislabeled Training Data", "pub_year": 2019, "domain": "label", "requirement": {"requirement_text": "Recommending and verifying trusted items. While manual inspection and correction is a routine for debugging mislabeled items, the experts need automated approaches to improve efficiency. Methods such as propagation of trusted items are the most discussed and recognized. However, without efficient algorithms and interactive tools, selecting and validating a set of trusted items from the cluttered visualization can still become laborious. Two requirements were identified by our collaborators based on their experience. First, automatic recommendation algorithms are desired to locate trusted items quickly. Second, flexible ways to examine and compare trusted items are required for further verification", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "media": 1}}, "solution": [{"solution_text": "The trusted item view displays the images and labels of all the trusted items. Further editing can be performed in the trusted item view. Usually, the identification and selection of trusted items in different regions require different strategies. In a region without labeling outliers, the user usually selects trusted items using system \u201crecommendation\u201d, and verifies the labels by only a glance at the images in the selected item view. In a region with labeling outliers, more careful examinations and operations are required from the user, including investigating the distribution at different levels, selecting class-balanced trusted items, and correcting the labels step-by-step for each class.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "image+bar", "axial_code": [], "componenet_code": ["bar", "image"]}]}, {"author": "zsz", "index_original": 35, "paper_title": "Interactive Correction of Mislabeled Training Data", "pub_year": 2019, "domain": "label", "requirement": {"requirement_text": "Recommending and verifying trusted items. While manual inspection and correction is a routine for debugging mislabeled items, the experts need automated approaches to improve efficiency. Methods such as propagation of trusted items are the most discussed and recognized. However, without efficient algorithms and interactive tools, selecting and validating a set of trusted items from the cluttered visualization can still become laborious. Two requirements were identified by our collaborators based on their experience. First, automatic recommendation algorithms are desired to locate trusted items quickly. Second, flexible ways to examine and compare trusted items are required for further verification", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "media": 1}}, "solution": [{"solution_text": "Propagating trusted items to improve the quality of the training set (R3). The propagation of trusted items is performed iteratively. In each iteration, newly selected trusted items are added into the trusted item set of the last iteration. They are fed into the correction module, which propagates the trusted items to the entire dataset. After the propagation, users can verify the quality improvements from the updated item distribution in the item view and the action trail. The action trail (Fig. 1 (e)) represents the historical record of correction iterations as a tree. Each iteration is represented by a node containing two bar charts: the chart on the top displays the number of newly added trusted items while the other counts the corrected items by the data correction module. In the case of undesired correction resulting, rolling back to previous iterations allows the user to reselect trusted items to re\ufb01ne the propagation results. If there is a lack of obvious visible quality improvements in sequences of iterations, the user can stop the iteration to \ufb01nish the correction process.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "image+bar", "axial_code": [], "componenet_code": ["bar", "image"]}]}, {"author": "zsz", "index_original": 36, "paper_title": "Interactive Correction of Mislabeled Training Data", "pub_year": 2019, "domain": "label", "requirement": {"requirement_text": "Exploring the details. Exploring the details of training items was considered an essential step to verify the labeling correctness. Specifically, the experts would like to be able to explore the details in the context of a distribution visualization so that they can quickly compare groups of items in local regions.", "requirement_code": {"collect_evidence": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "textual": 1}}, "solution": [{"solution_text": "An item view that utilizes a tSNE-based hierarchical visualization to display the distribution of training items, discloses patterns of clusters and outliers, and supports interactive exploration of the details.", "solution_category": "data_manipulation", "solution_axial": "Rectification,Sampling,DimensionalityReduction", "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE", "axial_code": ["Rectification", "Sampling", "DimensionalityReduction"], "componenet_code": ["rectification", "sampling", "dimensionality_reduction"]}]}, {"author": "zsz", "index_original": 37, "paper_title": "Interactive Correction of Mislabeled Training Data", "pub_year": 2019, "domain": "label", "requirement": {"requirement_text": "Exploring the details. Exploring the details of training items was considered an essential step to verify the labeling correctness. Specifically, the experts would like to be able to explore the details in the context of a distribution visualization so that they can quickly compare groups of items in local regions.", "requirement_code": {"collect_evidence": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "textual": 1}}, "solution": [{"solution_text": "The labeling noises in the training data often result in the mixed color distribution in some regions. These suspicious regions indicate potential mislabeled items and deserve further examination, e.g., zooming into the regions to explore more items.", "solution_category": "data_manipulation", "solution_axial": "Rectification,Sampling,DimensionalityReduction", "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE", "axial_code": ["Rectification", "Sampling", "DimensionalityReduction"], "componenet_code": ["rectification", "sampling", "dimensionality_reduction"]}]}, {"author": "zsz", "index_original": 38, "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections", "pub_year": 2020, "domain": "XAI", "requirement": {"requirement_text": "Facilitate the choice of hyper-parameters through visual exploration and the use of quality metrics.", "requirement_code": {"parameter_setting": 1}}, "data": {"data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "A Grid Search mode initiates a systematic parameter search that computes 500 projections by varying the parameters perplexity, learning rate, and max iterations.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 39, "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections", "pub_year": 2020, "domain": "XAI", "requirement": {"requirement_text": "Facilitate the choice of hyper-parameters through visual exploration and the use of quality metrics.", "requirement_code": {"parameter_setting": 1}}, "data": {"data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "From this pool of 500 projections, 25 representative examples are singled out and shown to the user\u2014in a matrix of thumbnails depicted in Fig. 2\u2014as suggestions of possible projections of the data. In order to choose the representatives, we partition the pool of 500 projections into 25 clusters (with K-Medoids [55]), using Procrustes distance [56] as the dissimilarity measure. The medoids of the 25 resulting clusters are used as representatives. This whole process is transparent to the user and happens in the backend.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 40, "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections", "pub_year": 2020, "domain": "XAI", "requirement": {"requirement_text": "Facilitate the choice of hyper-parameters through visual exploration and the use of quality metrics.", "requirement_code": {"parameter_setting": 1}}, "data": {"data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "Only the representatives are shown. We give extra support to the user by providing the results of 5 quality measures for each representative projection: neighborhood hit (NH), trustworthiness (T), continuity (C), normalized stress (S), and Shepard diagram correlation (SDC), accompanied by the quality metrics average (QMA). They are shown as a grayscale heatmap under each cell of the thumbnail matrix.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 41, "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections", "pub_year": 2020, "domain": "XAI", "requirement": {"requirement_text": "Facilitate the choice of hyper-parameters through visual exploration and the use of quality metrics.", "requirement_code": {"parameter_setting": 1}}, "data": {"data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "It is then the task of users\u2014through visual exploration and by matching their own personal preferences\u2014to choose the one that looks more promising. After choosing a projection, users will proceed with the visual analysis using all the functionalities described in the next sections. However, the hyper-parameter exploration does not necessarily stop here. The top 6 representatives (according to a user-selected quality measure) are still shown at the top of the main view, and the projection can be switched at any time if the user is not satisfied with the initial choice.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 42, "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections", "pub_year": 2020, "domain": "XAI", "requirement": {"requirement_text": "Provide a quick overview of the accuracy of the projection, to support the decision of either moving forward with the analysis or repeating the process of hyper-parameter exploration. ", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "The main view of the tool presents the t-SNE results as an interactive scatterplot, with specific mappings on the points\u2019 colors and sizes.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 43, "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections", "pub_year": 2020, "domain": "XAI", "requirement": {"requirement_text": "Provide a quick overview of the accuracy of the projection, to support the decision of either moving forward with the analysis or repeating the process of hyper-parameter exploration. ", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "There are four Interaction Modes for this view, as described next. The first (and default) mode\u2014t-SNE Points Exploration\u2014activates panning, zooming, and hovering, supporting the user to focus on individual patterns of the projection, and to investigate specific points\u2019 dimensions. The second mode\u2014Group Selection\u2014provides a lasso selection tool that triggers updates in other views, such as the Neighborhood Preservation view and the Adaptive PCP. The third option\u2014Dimension Correlation\u2014provides a tool for the user to check the hypothesis that a visual pattern, as observed, is strongly correlated to a pattern in the high-dimensional space. The final mode\u2014Reset Filters\u2014removes every filter applied with the previously-described interaction modes. ", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 44, "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections", "pub_year": 2020, "domain": "XAI", "requirement": {"requirement_text": "Provide a quick overview of the accuracy of the projection, to support the decision of either moving forward with the analysis or repeating the process of hyper-parameter exploration. ", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "To complement the main view, the Overview shows the static t-SNE projection and serves as a contextual anchor that is independent of the interactions and/or filters applied to the main view. Data-specific labels (when those exist) are shown using a categorical colormap, along with simple statistics about the data set.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 45, "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections", "pub_year": 2020, "domain": "XAI", "requirement": {"requirement_text": "Provide the means to investigate quality further, differentiating between the trustworthiness of different regions of the projection.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "To avoid this clutter problem and increase the readability of the Shepard Diagram for large data sets, we propose the Shepard Heatmap, which is an aggregated version of the Shepard Diagram, with the information of the number of points in each cell mapped to a single-hue colormap. The main goal of the Shepard Heatmap is to offer a broad, simplified overview of the accuracy of the projection in terms of distance preservation: cells close to the main diagonal of the heatmap indicate that the respective pairs of instances have been represented in the 2-D space with distances that are comparable to their original N-D distances. ", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 46, "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections", "pub_year": 2020, "domain": "XAI", "requirement": {"requirement_text": "Provide the means to investigate quality further, differentiating between the trustworthiness of different regions of the projection.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "Visual Mapping. The Visual Mapping panel includes controls for mapping Density and Remaining Cost of each point to either color or size in the main view. These correspond to information extracted from the t-SNE algorithm itself, which would otherwise be hidden from the analyst. Their inspection, however, may prove fruitful.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 47, "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections", "pub_year": 2020, "domain": "XAI", "requirement": {"requirement_text": "Provide the means to investigate quality further, differentiating between the trustworthiness of different regions of the projection.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "The ability to investigatethe extent to which such neighborhoods are preserved is one important piece of the puzzle that forms a full assessment of the accuracy of a t-SNE projection.We present a Neighborhood Preservation plot that shows an overview of the preservation of neighborhoods of different sizes (k) in both the entire projection and the current selection, based on the Jaccard distance between sets. For each value of k, NPk yields the average preservation of neighborhoods of up to k points, centered at the n selected points (or for the entire projection, if nothing is selected). The default visualization for the Neighborhood Preservation is a bar chart, but users have two more options to visualize the same information using line plots. The black bars are always fixed, showing the average preservation for all points of the projection. The ability to investigatethe extent to which such neighborhoods are preserved is one important piece of the puzzle that forms a full assessment of the accuracy of a t-SNE projection.We present a Neighborhood Preservation plot that shows an overview of the preservation of neighborhoods of different sizes (k) in both the entire projection and the current selection, based on the Jaccard distance between sets. For each value of k, NPk yields the average preservation of neighborhoods of up to k points, centered at the n selected points (or for the entire projection, if nothing is selected). The default visualization for the Neighborhood Preservation is a bar chart, but users have two more options to visualize the same information using line plots. The black bars are always fixed, showing the average preservation for all points of the projection.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 48, "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections", "pub_year": 2020, "domain": "XAI", "requirement": {"requirement_text": "Allow the interpretation of different visible patterns of the projection in terms of the original data set\u2019s dimensions.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "Adaptive Parallel Coordinates Plot. Our first proposal to support the task of interpreting patterns in a t-SNE projection is an Adaptive PCP. It highlights the dimensions of the points selected with the lasso tool, using a maximum of 8 axes at any time, to avoid clutter. The shown axes (and their order) are, however, not fixed, as is the usual case. Instead, they are adapted to the selection in the following way. First, a Principal Component Analysis is performed using only the selected points, but with all dimensions. That yields two results: (1) a set of eigenvectors that represent a new base that best explains the variance of the selected points, and (2) a set of eigenvalues that represent how much variance is explained by each eigenvector. Simulating a reduction of the dimensions of the selected points to 1-Dimensional space using PCA, we pick the eigenvector with the largest eigenvalue, i.e., the most representative one. This N-D vector can be seen as sequence w of N weights, one per original dimen-sion, where the value of wj indicates the importance of dimension j in explaining the variance of the user-selected subset of the data. Finally, we sort w in descending order, then pick the dimensions that correspond to the first (up to) 8 values of the sorted w. These are the (up to) 8 dimensions shown in the PCP axes, in the same descending order (from left to right).", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 49, "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections", "pub_year": 2020, "domain": "XAI", "requirement": {"requirement_text": "Allow the interpretation of different visible patterns of the projection in terms of the original data set\u2019s dimensions.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "The results (i.e., relevances of each dimension) are finally shown in an interactive horizontal bar chart, where the dimensions are sorted from top to bottom according to relevance (with the most relevant on the top). While the relevance is computed using the absolute value of the correlation, we decided to show the original value in the bars (including negative correlations to the left of the central axis) to avoid possibly misleading the analyst. The final component of the Dimension Correlation tool is the ability to explore the different dimensions by clicking on the bars, which will change the colormap of the main view to reflect the values of the points for that specific dimension.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 50, "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists", "pub_year": 2021, "domain": "Biology", "requirement": {"requirement_text": "T1: Hand-Craft, Merge, and Split Clusters. Biologists apply their domain knowledge to create customized clusters to better understand which factor(s) is causing the ascertainment bias on the dataset that are being used popularly. For example, one of the biologist stated: \u201cGiven the identified SNPs [single-nucleotide polymorphisms] that are associated with common disease and traits, it\u2019s interesting to create a cluster of SNPs.\u201d In addition, biologists apply their domain expertise to merge or split two or more clusters depending on how related they think the clusters are based on given feature(s). For example, one of the biologists mentioned: \u201cDepending on the evolutionary history of the genes, two or more clusters can be really related to each other. If ascertained they are related, we will merge them as one cluster.\u201d Another biologist reported that \u201cIn my new project, we are comparing Africans to non-Africans. In this case I merge Americans, East Asians, and Europeans as one cluster, and compare that to Africans data.\u201d", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "Merging and Splitting Clusters (T1). To merge two or more  clusters, users first click on a cluster. They then demonstrate their interest in merging two clusters by drag-anddropping the cluster on top of another cluster. Users can drag point(s) out of the cluster and drop into either i) another cluster or ii) a blank space (on the Cluster View). Drag-and-drop items into blank space is translated as forming a new cluster of the selected items outside the current cluster (see Fig. 2). Demonstration-based cluster customization enables users to interact with the data directly and removes any mid-level instruments such as control panels or menus", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 51, "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists", "pub_year": 2021, "domain": "Biology", "requirement": {"requirement_text": "T2: Divide Each Cluster to Sub-Clusters. Biologists often investigate sub-clusters within a specific cluster to: 1) understand which other factors can affect the cluster, 2) compare two clusters based on the member data items in each, and 3) see trends and patterns in the sub-clusters, with respect to chosen features, We noticed that the biologists found existing solutions challenging because they had to write lines of scripts to compute and visualize sub-clusters in a given cluster. Furthermore, the existing methods prohibit rapid iteration and visualization of results, which inevitably prolongs the exploratory clustering process to understand their data better.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "Sub Clustering (T2). Hovering over a cluster reveals a plus button. Users can click on it to open a subcluster panel on the Cluster View, which shows subgroups of the data items within the selected cluster. In addition, a bar chart shows the distribution of a chosen attribute. Alongside, text description highlights the attributes that were used to compute the subclusters. Given that the users are not experts in data science, we do not present the quality metrics (e.g., silhouette scores, homogeneity score, etc.) Instead, we describe cluster models by showing thumbnail previews of clustering results with text descriptions as Fig. 4 shows.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction,Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping", "DimensionalityReduction"], "componenet_code": ["clustering_and_grouping", "dimensionality_reduction"]}]}, {"author": "zsz", "index_original": 52, "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists", "pub_year": 2021, "domain": "Biology", "requirement": {"requirement_text": "T2: Divide Each Cluster to Sub-Clusters. Biologists often investigate sub-clusters within a specific cluster to: 1) understand which other factors can affect the cluster, 2) compare two clusters based on the member data items in each, and 3) see trends and patterns in the sub-clusters, with respect to chosen features, We noticed that the biologists found existing solutions challenging because they had to write lines of scripts to compute and visualize sub-clusters in a given cluster. Furthermore, the existing methods prohibit rapid iteration and visualization of results, which inevitably prolongs the exploratory clustering process to understand their data better.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "Sub Clustering. When triggered by users, the system builds a sub-cluster model Msi, for data instances E, member of a selected cluster Ci. Unlike the set of main cluster models M, only a single sub-cluster model is generated per cluster (T2). For sub-clustering we relied on the parameterization of the best-recommended cluster model for the entire data i.e., best-found parameteriza tion of the K-Means cluster model. To avoid further compute times that may impact real-time interactions, we did not construct and test multiple cluster models for sub-clustering. However, clicking on the \u201cadd subcluster\u201d button again for the same selected cluster Ci, the system recomputes the sub-cluster model Msi, by  randomly choosing a new set of a learning algorithm v and hyperparameters f; e.g., it picks a new \u201ck\u201d on the \u201cK-Means\u201d cluster model. This technique allows users to rapidly browse a large set of sub-cluster models.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 53, "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists", "pub_year": 2021, "domain": "Biology", "requirement": {"requirement_text": "T3: Adjust Feature Contributions. Biologists need to easily see by how much different attributes/features contribute to computing a cluster. Moreover, they often need to adjust the importance of different features used for computing a cluster. Biologists currently have to programmatically adjust the importance of features, execute the code, and visualize the outcome. They often repeat this process multiple times until they achieve a satisfactory result. They need interactive methods to view and refine feature contributions.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "Delete Data Items or Clusters (T3). Our discussion with biologists revealed that they sometimes need to \u2018exclude\u2019 data items or clusters from their analysis while testing a hypothesis. Thus, we initially implemented the \u2018delete\u2019 feature by enabling users to select a subset of items or clusters from the main view and click on the delete icon. However, when we showed it to the biologists, they had trouble due to inconsistencies between the button-based interaction and other demonstration-based interaction. Currently in Geono-Cluster users cam drag-drop a selected cluster on the delete icon shown on the top-left of the interface to show their interest in moving the selected cluster out of the layout. Similarly, they can drag-drop individual data items to demonstrate their interests in removing them from the cluster assignment.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction,Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping", "DimensionalityReduction"], "componenet_code": ["clustering_and_grouping", "dimensionality_reduction"]}]}, {"author": "zsz", "index_original": 54, "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists", "pub_year": 2021, "domain": "Biology", "requirement": {"requirement_text": "T3: Adjust Feature Contributions. Biologists need to easily see by how much different attributes/features contribute to computing a cluster. Moreover, they often need to adjust the importance of different features used for computing a cluster. Biologists currently have to programmatically adjust the importance of features, execute the code, and visualize the outcome. They often repeat this process multiple times until they achieve a satisfactory result. They need interactive methods to view and refine feature contributions.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "User Driven Feature Selection. A cluster model Mi is driven by a set of features F = fi1, fi2; f i3; f i4 . . . . . . :fik as input to compute the distance function which assigns a set of data items D to individual clusters C. In Geono-Cluster, the set of features F is either computed using feature selection methods e.g., \u201cselect K Best\u201d [43], \u201cPCA\u201d [44] or can be retrieved from users if they specify a set of features and their relative weights (from the Attribute Panel supporting the task T3). When users specify a set of k features Fu = fi1, fi2; f i3; f i4 . . . fik with respective weights for each feature (Wu = wi1, wi2; w i3; w i4 . . . wik, the system updates the distance function in the clustering algorithm.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 55, "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists", "pub_year": 2021, "domain": "Biology", "requirement": {"requirement_text": "T3: Adjust Feature Contributions. Biologists need to easily see by how much different attributes/features contribute to computing a cluster. Moreover, they often need to adjust the importance of different features used for computing a cluster. Biologists currently have to programmatically adjust the importance of features, execute the code, and visualize the outcome. They often repeat this process multiple times until they achieve a satisfactory result. They need interactive methods to view and refine feature contributions.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "Similar Item Selection. Users click on a cell (qj) of a quantitative attribute on the Table View to select a value vj of the data item di. Geono-Cluster finds a set of r data instances, U = da; db; dc . . .dr, each of whose value vj falls within a threshold range, say [+eps, -_x0003_eps]. The parameter eps is set for each quantitative attribute Q by heuristics and can be adjusted. This technique allows users to pick data instances which are similar, based on the selected quantitative attribute qj. Further, users can select another quantitative attribute cell qk. Next, from the set of selected data instances U, the system finds all instances V which fall within a threshold range of the value selected for attribute qk. Here the size of V is less than that of U. This technique allow users to filter and select a subset of data instances V from the Table View. For categorical features X, Geono-Cluster performs exact feature value matching instead of matching data items based on a predefined range. Users can drag-drop these V data items to the Cluster View as a single cluster (C = C1). They can continue selecting another set of data items, then add them to the cluster view as a new cluster (C = C1, C 2). Users complete the data exploration or they can request the system to find a model Mi iteratively (T3).", "solution_category": "interaction", "solution_axial": "Selecting.Filtering", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "zsz", "index_original": 56, "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists", "pub_year": 2021, "domain": "Biology", "requirement": {"requirement_text": "Shifting the Burden of Specification From the Biologists to the Systems. The existing tools and technologies put the burden of specification on biologists. Instead of requiring biologists to specify the clustering models by programming or going through layers of menus, the tool should provide an environment that enables them to demonstrate how the expected clustering outcomes should look like. By translating the given demonstrations, the system could estimate the biologist\u2019s intention and generate appropriate results. This way we could balance the responsibility between the biologist and the system\u2014biologists provide visual demonstrations, based on this, the system infers potential clustering results and recommends them.", "requirement_code": {"knowledge_injection": 1}}, "data": {"data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "Cluster View visualizes the clustered data. For testing their hypotheses, biologists often perform actions at the level of data items (e.g., move data items from one cluster to another). We visually present each cluster and its members on the Cluster View. The colored circles in each group represent members of a cluster; the surrounding hull represents the cluster. Users can hover over a circle, which prompts relevant attribute details of the data. Users can specify the number of clusters using the slider shown on the top-left. Cluster View is an environment similar to a spatial workspace in which users can move data items to structure their information and provide visual demonstrations.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction,Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping", "DimensionalityReduction"], "componenet_code": ["clustering_and_grouping", "dimensionality_reduction"]}]}, {"author": "zsz", "index_original": 57, "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists", "pub_year": 2021, "domain": "Biology", "requirement": {"requirement_text": "Enable User Interaction to Drive Recommendations. As analysts explore their data, their interests will evolve. Our initial observations and interviews also showed that biologists need to explore various clustering models rapidly during their data analysis process. One potential approach to support such a rapid data analysis is to recommend potential cluster models that biologists should consider during their data analysis process. Furthermore, the clustering recommendations should be adapted for biologists\u2019 analytic goals. The recommendation engine should steer multiple clustering models based on biologist-specified expected visual outcomes. In addition, biologists can also directly adjust feature contributions to update the clustering results. In aggregate, these interactions create demonstrations which serve as the primary units by which biologists communicate their expected changes to the system.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "Cluster View is an environment similar to a spatial workspace in which users can move data items to structure their information and provide visual demonstrations. For example, a biologist might notice a set of data items should not be in a specific cluster. Thus, she can demonstrate that those points belong to a different cluster by dragging them from one cluster to another. The system uses the visual demonstrations provided by the users to steer the underlying recommendation engine.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction,Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping", "DimensionalityReduction"], "componenet_code": ["clustering_and_grouping", "dimensionality_reduction"]}]}, {"author": "zsz", "index_original": 58, "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists", "pub_year": 2021, "domain": "Biology", "requirement": {"requirement_text": "Enable User Interaction to Drive Recommendations. As analysts explore their data, their interests will evolve. Our initial observations and interviews also showed that biologists need to explore various clustering models rapidly during their data analysis process. One potential approach to support such a rapid data analysis is to recommend potential cluster models that biologists should consider during their data analysis process. Furthermore, the clustering recommendations should be adapted for biologists\u2019 analytic goals. The recommendation engine should steer multiple clustering models based on biologist-specified expected visual outcomes. In addition, biologists can also directly adjust feature contributions to update the clustering results. In aggregate, these interactions create demonstrations which serve as the primary units by which biologists communicate their expected changes to the system.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "Attribute Panel lists the attributes of the loaded data set. Users can turn on and off a set of attributes which directly affects the clustering algorithm. Furthermore, users can also adjust attribute contributions, specify-ing relative importance of the selected attributes to define cluster memberships.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 59, "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists", "pub_year": 2021, "domain": "Biology", "requirement": {"requirement_text": "Enable User Interaction to Drive Recommendations. As analysts explore their data, their interests will evolve. Our initial observations and interviews also showed that biologists need to explore various clustering models rapidly during their data analysis process. One potential approach to support such a rapid data analysis is to recommend potential cluster models that biologists should consider during their data analysis process. Furthermore, the clustering recommendations should be adapted for biologists\u2019 analytic goals. The recommendation engine should steer multiple clustering models based on biologist-specified expected visual outcomes. In addition, biologists can also directly adjust feature contributions to update the clustering results. In aggregate, these interactions create demonstrations which serve as the primary units by which biologists communicate their expected changes to the system.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "Recommendation Technique. Geono-Cluster ranks the models in M by their scores S explained below, and visualizes the best clustering layout in the Cluster View. Further, the system allows the user to inspect top f best cluster models from the ranked models M, through the Recommendation Panel. If a user makes any customization to the shown cluster model Mc (e.g., merge or split clusters), the system automatically updates the recommendations by computing a new set of M cluster models, except the model Mc, which is currently shown in the Cluster View. Per iteration, the system updates S and the ranking of the models M based on user interactions with the data. Next it visualizes the best model in M in the Cluster View and shows thumbnail previews of the top f models in the Recommendation Panel.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction,Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping", "DimensionalityReduction"], "componenet_code": ["clustering_and_grouping", "dimensionality_reduction"]}]}, {"author": "zsz", "index_original": 60, "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists", "pub_year": 2021, "domain": "Biology", "requirement": {"requirement_text": "Enhance Interpretability of Recommendations. Biologists reported their interest in seeing more details about different clustering results while skimming through different recommendations. However, not all biologists might be familiar with technical terms used to describe a cluster such as silhouette value. Therefore, recommended clustering results should be presented in a transparent manner so that biologists can extract the most important and understandable information (e.g., contributing features) used for clustering results. One powerful approach to enhance transparency of the recommended clustering options is to use natural language to explain them. This way biologists can learn about the recommended clustering outcomes without having to know about more technical terms describing each clustering outcome.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "Recommendation Technique. Geono-Cluster ranks the models in M by their scores S explained below, and visualizes the best clustering layout in the Cluster View. Further, the system allows the user to inspect top f best cluster models from the ranked models M, through the Recommendation Panel. If a user makes any customization to the shown cluster model Mc (e.g., merge or split clusters), the system automatically updates the recommendations by computing a new set of M cluster models, except the model Mc, which is currently shown in the Cluster View. Per iteration, the system updates S and the ranking of the models M based on user interactions with the data. Next it visualizes the best model in M in the Cluster View and shows thumbnail previews of the top f models in the Recommendation Panel.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction,Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping", "DimensionalityReduction"], "componenet_code": ["clustering_and_grouping", "dimensionality_reduction"]}]}, {"author": "zsz", "index_original": 62, "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration", "pub_year": 2022, "domain": "Weather forecast", "requirement": {"requirement_text": "R.1 Extract Bias Patterns. As indicated in the previous section, bias patterns are a kind of statistical abstraction of the historical data in a certain spatiotemporal interval. However, methods that aim to extract the potential bias patterns from the reanalysis data are limited.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ", "data_code": {"geometry": 1, "sequential": 1, "fields": 1, "temporal": 1}}, "solution": [{"solution_text": "In the bias pattern extraction module, given a forecast result in a spatiotemporal interval and the corresponding observation data, the forecast-observation probability density function at each grid is initially generated via a kernel density estimation (KDE) (R.1). The distribution \ufb01eld can represent the bias pattern at the grid in a given time range because the probability density distribution of the observation-forecast values can re\ufb02ect the distribution characteristics of the forecast and the observation data in the current time window. Furthermore, the probability density distribution can derive the distribution characteristics of the bias, which is equal to the difference between the observation and the forecast data. We used a bottom-up hierarchical clustering algorithm to aggregate a single grid point into a connected area on the basis of the similarity of the PDF among the grids. The result of the hierarchical clustering can be considered a clustering tree of the connected areas with similar bias patterns, which are subsequently fed into the following visual analysis module for further exploration.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "kerneldensityestimation", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 63, "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration", "pub_year": 2022, "domain": "Weather forecast", "requirement": {"requirement_text": "R.2 Visualize the Spatiotemporal Distribution of Similar Bias Patterns. E.2 hoped to have a visualization that can well demonstrate the bias patterns in the spatial and temporal dimensions for an overview, which can help him immediately identify the basic properties and distribution of the bias patterns present in the data.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ", "data_code": {"geometry": 1, "sequential": 1, "fields": 1, "temporal": 1}}, "solution": [{"solution_text": "In the visual analysis module, several coordinated views and intuitive interactions are provided to analyze the bias patterns. Specifically, we proposed a relative position and similarity-encoded scatter plot to visualize the spatiotemporal distribution of the bias patterns.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "kerneldensityestimation", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 64, "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration", "pub_year": 2022, "domain": "Weather forecast", "requirement": {"requirement_text": "R.2 Visualize the Spatiotemporal Distribution of Similar Bias Patterns. E.2 hoped to have a visualization that can well demonstrate the bias patterns in the spatial and temporal dimensions for an overview, which can help him immediately identify the basic properties and distribution of the bias patterns present in the data.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ", "data_code": {"geometry": 1, "sequential": 1, "fields": 1, "temporal": 1}}, "solution": [{"solution_text": "The spatiotemporal bias pattern view (Fig. 4a) provides an overview of the areas with similar bias patterns extracted from the reanalysis data in each time window (R.2). ", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "kerneldensityestimation", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 65, "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration", "pub_year": 2022, "domain": "Weather forecast", "requirement": {"requirement_text": "R.2 Visualize the Spatiotemporal Distribution of Similar Bias Patterns. E.2 hoped to have a visualization that can well demonstrate the bias patterns in the spatial and temporal dimensions for an overview, which can help him immediately identify the basic properties and distribution of the bias patterns present in the data.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ", "data_code": {"geometry": 1, "fields": 1}}, "solution": [{"solution_text": "The map view was designed to visualize the spatial distribution of the similar bias pattern areas and grid points. It also corresponds to the interaction conducted in the spatiotemporal bias pattern view.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "kerneldensityestimation", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 66, "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration", "pub_year": 2022, "domain": "Weather forecast", "requirement": {"requirement_text": "R.2 Visualize the Spatiotemporal Distribution of Similar Bias Patterns. E.2 hoped to have a visualization that can well demonstrate the bias patterns in the spatial and temporal dimensions for an overview, which can help him immediately identify the basic properties and distribution of the bias patterns present in the data.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ", "data_code": {"geometry": 1, "fields": 1}}, "solution": [{"solution_text": "The map view was designed to visualize the spatial distribution of the similar bias pattern areas and grid points. It also corresponds to the interaction conducted in the spatiotemporal bias pattern view.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 67, "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration", "pub_year": 2022, "domain": "Weather forecast", "requirement": {"requirement_text": "R.3 Reveal the Differences and Correlations Among Different Spatiotemporal Bias Patterns. E.2 commented that the current bias analysis methods only focus on a single location, i.e., calibrating precipitation at a single station instead of analyzing its correlation among its neighbors. Understanding the stability and uniqueness of the bias patterns can help explore and make informed decisions. Thus, E.2 wanted to observe the difference and correlations among different spatiotemporal bias patterns.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ", "data_code": {"geometry": 1, "sequential": 1, "fields": 1, "temporal": 1}}, "solution": [{"solution_text": "The spatiotemporal bias pattern view also visualizes the transition of grid points between areas with similar bias patterns in the adjacent time windows.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "kerneldensityestimation", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 68, "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration", "pub_year": 2022, "domain": "Weather forecast", "requirement": {"requirement_text": "R.3 Reveal the Differences and Correlations Among Different Spatiotemporal Bias Patterns. E.2 commented that the current bias analysis methods only focus on a single location, i.e., calibrating precipitation at a single station instead of analyzing its correlation among its neighbors. Understanding the stability and uniqueness of the bias patterns can help explore and make informed decisions. Thus, E.2 wanted to observe the difference and correlations among different spatiotemporal bias patterns.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ", "data_code": {"geometry": 1, "fields": 1}}, "solution": [{"solution_text": "A contour map is used in the map view to convey the stability of each grid point in the space.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "kerneldensityestimation", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 69, "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration", "pub_year": 2022, "domain": "Weather forecast", "requirement": {"requirement_text": "R.4 Select Bias Patterns of Interest. E.1 needed to \ufb01lter out the bias patterns of interest from the data overview rapidly for a detailed analysis. Thus, a visual interactive mechanism and visual cues should be provided to assist him in selecting the bias patterns of interest.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ", "data_code": {"geometry": 1, "sequential": 1, "fields": 1, "temporal": 1}}, "solution": [{"solution_text": "The users can further filter similar bias pattern areas to explore the bias patterns through interaction further, thereby observing the spatiotemporal characteristics of the bias patterns in conjunction with their domain expertise and interests. For instance, we can click on a similar bias pattern area, and the corresponding areas in each time window would be highlighted automatically. We also support similarity-based filtering interaction. The users can directly input a similar threshold, and the areas with a similarity higher than the threshold would be filtered.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "kerneldensityestimation", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 70, "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration", "pub_year": 2022, "domain": "Weather forecast", "requirement": {"requirement_text": "R.5 Facilitate Parameter Comparison. To understand the similarities and differences among bias patterns, comparing the atmospheric parameters when generating the corresponding bias patterns is essential to understand the underlying causes of the bias patterns and to further support making informed decisions.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ", "data_code": {"geometry": 1, "sequential": 1, "fields": 1, "temporal": 1}}, "solution": [{"solution_text": "The forecast-observation PDF view provides detailed information on the bias pattern and assists analysts in drafting the calibration curve on the basis of the bias patterns.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "kerneldensityestimation", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 71, "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration", "pub_year": 2022, "domain": "Weather forecast", "requirement": {"requirement_text": "R.5 Facilitate Parameter Comparison. To understand the similarities and differences among bias patterns, comparing the atmospheric parameters when generating the corresponding bias patterns is essential to understand the underlying causes of the bias patterns and to further support making informed decisions.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ", "data_code": {"geometry": 1, "sequential": 1, "fields": 1, "temporal": 1}}, "solution": [{"solution_text": "The design of the parameter comparison view focused on the correlation between the distribution of the data dimensions and the bias, thus enabling the analysis and comparison of atmospheric parameters generated by the forecast ensemble. That is, it visualizes the data distribution in the five dimensions in a bias pattern and the correlation between the data distribution and the bias of the accumulated precipitation instead of visualizing the correlation among these dimensions.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "kerneldensityestimation", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 72, "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration", "pub_year": 2022, "domain": "Weather forecast", "requirement": {"requirement_text": "R.6 Support Detailed Calibrations. According to E.1, the accuracy of weather forecasts largely depends on the subjective judgment of the forecaster in terms of his/her quali\ufb01cation, status, and working conditions, thereby increasing the unreliability of the calibrated results. Therefore, an interactive calibration mechanism is desired to support the combination of domain experts\u2019 experience and bias patterns extracted from historical data; in this manner, the accuracy of forecast calibration is improved.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ", "data_code": {"geometry": 1, "sequential": 1, "fields": 1, "temporal": 1}}, "solution": [{"solution_text": "An interactive semi-automatic calibration curve based on the detailed visualization of a specific bias pattern was designed to assist the forecast calibration.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "kerneldensityestimation", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 73, "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "pub_year": 2021, "domain": "Graph mining", "requirement": {"requirement_text": "D1 Visualize the Instance-level Sensitivity. The system should visualize ranking and auditing results for all instances (T1). The view for summarizing the instance-level sensitivity should include the sensitivity index (T1.1) for all nodes with respect to the node attributes (T1.3).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "The system visualizes the output in the sortable sensitivity index list, which shows each node\u2019s current ranking and sensitivity indices with respect to the node\u2019s class label(s).", "solution_category": "data_manipulation", "solution_axial": "Excluding", "solution_compoent": "", "axial_code": ["Excluding"], "componenet_code": ["excluding"]}]}, {"author": "zsz", "index_original": 74, "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "pub_year": 2021, "domain": "Graph mining", "requirement": {"requirement_text": "D2 Visualize the Effect of Perturbation. The system should be able to guide analysts to explore the perturbation effect of certain node\u2019s removal and support interactions such as sorting, searching and \ufb01ltering to inspect the auditing results and corresponding perturbation effects (T2, T3). This view should include:D2.1 In\ufb02uence Overview, which summarizes the perturbation\u2019s in\ufb02uence, the degree of ranking changes, and the proportion of nodes whose rankings are increased/decreased, etc. (T1.2, T2.1, T3).", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "The influence overview provides basic information on changes caused by removing a specific node (D2.1). These changes include 1) the number of influenced nodes which have ranking changes after the perturbation; 2, 3) the number of influenced nodes whose ranking increased/decreased after the perturbation; 4, 5) the max/min of increased/decreased ranking changes; 6, 7) the median of increased/decreased ranking changes; and 8) the degrees of the node. These 8 metrics provide the analyst with a statistical overview of the ranking influence of nodes due to perturbations. The radar is used to provide an overview of the sensitivity metrics with respect to the effects of a perturbation. The radar allows for the further addition of new metrics and can also preserve the overall information of the perturbation when the analyst switches between multiple perturbation diagnoses through the tabs on the top of the view.", "solution_category": "data_manipulation", "solution_axial": "Excluding,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding"], "componenet_code": ["algorithmic_calculation", "excluding"]}]}, {"author": "zsz", "index_original": 75, "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "pub_year": 2021, "domain": "Graph mining", "requirement": {"requirement_text": "D2 Visualize the Effect of Perturbation. The system should be able to guide analysts to explore the perturbation effect of certain node\u2019s removal and support interactions such as sorting, searching and \ufb01ltering to inspect the auditing results and corresponding perturbation effects (T2, T3). This view should include: D2.2 Distribution View, which shows how the ranking position changes caused by the perturbation are distributed for each instance and the ranking distribution for each group of nodes. (T2.2, T1.3)", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "Ranking Change Distribution View: A bar chart is used to show the ranking change distribution. Each bar is a node. The position of the bar on the x axis denotes the original ranking position for the node. The height of the bar on the y axis denotes the ranking change for the node. Colors represent the node labels. We scale the axes of the bar chart such that a 90-degree clockwise rotation of the bar also allows the analyst to infer the future rank of the node.", "solution_category": "data_manipulation", "solution_axial": "Excluding", "solution_compoent": "", "axial_code": ["Excluding"], "componenet_code": ["excluding"]}]}, {"author": "zsz", "index_original": 76, "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "pub_year": 2021, "domain": "Graph mining", "requirement": {"requirement_text": "D2 Visualize the Effect of Perturbation. The system should be able to guide analysts to explore the perturbation effect of certain node\u2019s removal and support interactions such as sorting, searching and \ufb01ltering to inspect the auditing results and corresponding perturbation effects (T2, T3). This view should include: D2.2 Distribution View, which shows how the ranking position changes caused by the perturbation are distributed for each instance and the ranking distribution for each group of nodes. (T2.2, T1.3)", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "Top-k Proportional Distribution View: When applying graph-based ranking algorithms for search engines, there could be an argument that ranking changes of webpages that are not part of the top-k ranking are less important than those in the top-k. This argument can be extended to any general graph ranking problem, where the analyst can choose a k for which elements below this ranking will not be considered. For example, a hiring manager may not be interested in resumes ranked outside of the top-25, but it important to understand whether certain node attributes are underrepresented in the top-k. In the Top-k Proportional Distribution View, we use two donut charts to represent the proportions of nodes of different categories belonging to ranking 1 to ranking k before and after the perturbation, and k is interactively specified.", "solution_category": "data_manipulation", "solution_axial": "Excluding", "solution_compoent": "", "axial_code": ["Excluding"], "componenet_code": ["excluding"]}]}, {"author": "zsz", "index_original": 77, "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "pub_year": 2021, "domain": "Graph mining", "requirement": {"requirement_text": "D2 Visualize the Effect of Perturbation. The system should be able to guide analysts to explore the perturbation effect of certain node\u2019s removal and support interactions such as sorting, searching and \ufb01ltering to inspect the auditing results and corresponding perturbation effects (T2, T3). This view should include: D2.3 Ranking Change Detail View, which lists the in\ufb02uenced nodes for this perturbation. The view should support basic query operations, e.g., sorting, \ufb01ltering and searching, etc.(T2.1, T3)", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "The view should support basic query operations, e.g., sorting, filtering and searching, etc.", "solution_category": "data_manipulation", "solution_axial": "Excluding", "solution_compoent": "", "axial_code": ["Excluding"], "componenet_code": ["excluding"]}]}, {"author": "zsz", "index_original": 78, "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "pub_year": 2021, "domain": "Graph mining", "requirement": {"requirement_text": "D2 Visualize the Effect of Perturbation. The system should be able to guide analysts to explore the perturbation effect of certain node\u2019s removal and support interactions such as sorting, searching and \ufb01ltering to inspect the auditing results and corresponding perturbation effects (T2, T3). This view should include: D2.4 Local In\ufb02uence Graph View, which illustrates the relationship between the ranking changes of nodes and the topological changes caused by the perturbation. (T2.3, T3)", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "We visualize the influence caused by removing/perturbing a node as a customized radial graph layout. In this customized layout, the removed node is set as the center of the force, and the strength of the charge force for each type of the node (hop-1 node, hop-2 node, etc.) is increased gradually based on the number n of hop-n.", "solution_category": "data_manipulation", "solution_axial": "Excluding", "solution_compoent": "", "axial_code": ["Excluding"], "componenet_code": ["excluding"]}]}, {"author": "zsz", "index_original": 81, "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "pub_year": 2021, "domain": "Tax evasion", "requirement": {"requirement_text": "R1 Enable interactive con\ufb01gurations for suspicious RPTTE groups detection. The automated algorithms signi\ufb01cantly improve the ef\ufb01ciency of the tax inspection procedure when they can successfully identify the most relevant suspicious RPTTE groups. As mentioned by E2, different users may want to explore RPTTE groups that satisfy speci\ufb01c conditions and would like to con\ufb01gure different parameters to extend or narrow down the scope of suspicious groups by \ufb01ltering taxpayers not tightly connected to the related party transactions. For example, different users may have an interest in detecting tax evasion groups from different periods, and exploring tax evasion groups with a speci\ufb01c relevance or complexity", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.", "data_code": {"tables": 1, "categorical": 1, "textual": 1, "network_and_trees": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Network Construction. Similar to prior research [35], we also fuse multiple data sources and construct a taxpayer network to facilitate further analysis. We \ufb01rst formulate the taxpayer network with taxpayers and investors, where the nodes can be both taxpayers and investors, and edges represent investment relationships. Then we provide more information about the nodes by fusing their pro\ufb01le information and audit records to the network. A connected component in the resulting directed graph re\ufb02ects that of a related party. We further adjust the taxpayer network according to our design requirements (R1) and domain knowledge.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "zsz", "index_original": 82, "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "pub_year": 2021, "domain": "Tax evasion", "requirement": {"requirement_text": "R1 Enable interactive con\ufb01gurations for suspicious RPTTE groups detection. The automated algorithms signi\ufb01cantly improve the ef\ufb01ciency of the tax inspection procedure when they can successfully identify the most relevant suspicious RPTTE groups. As mentioned by E2, different users may want to explore RPTTE groups that satisfy speci\ufb01c conditions and would like to con\ufb01gure different parameters to extend or narrow down the scope of suspicious groups by \ufb01ltering taxpayers not tightly connected to the related party transactions. For example, different users may have an interest in detecting tax evasion groups from different periods, and exploring tax evasion groups with a speci\ufb01c relevance or complexity", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.", "data_code": {"tables": 1, "categorical": 1, "textual": 1, "network_and_trees": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The Control Panel consists of a bar chart and a parameter selector to support network fusion through interactively setting the preferred period or relevant thresholds. The bar chart on the right-hand side of the Control Panel offers a temporal summary of the daily related party transaction amount, which reveals a cyclic pattern where most of the peaks are near the end of the month. Since the profit manipulation should happen after tax evaders know how much taxable income they have, we speculate that this information can work as a visual cue to guide the tax administration officers in conducting their exploration of the suspicious RPTTE groups. Users can select their period of concern by brushing or clicking a bar to automatically select the quarter, which is a typical tax period.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "zsz", "index_original": 83, "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "pub_year": 2021, "domain": "Tax evasion", "requirement": {"requirement_text": "R1 Enable interactive con\ufb01gurations for suspicious RPTTE groups detection. The automated algorithms signi\ufb01cantly improve the ef\ufb01ciency of the tax inspection procedure when they can successfully identify the most relevant suspicious RPTTE groups. As mentioned by E2, different users may want to explore RPTTE groups that satisfy speci\ufb01c conditions and would like to con\ufb01gure different parameters to extend or narrow down the scope of suspicious groups by \ufb01ltering taxpayers not tightly connected to the related party transactions. For example, different users may have an interest in detecting tax evasion groups from different periods, and exploring tax evasion groups with a speci\ufb01c relevance or complexity", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.", "data_code": {"tables": 1, "categorical": 1, "textual": 1, "network_and_trees": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The parameter selector allows users to configure the maximum transaction chain length and maximum control chain length for the algorithm to determine the extent of the suspicious RPTTE groups.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 84, "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "pub_year": 2021, "domain": "Tax evasion", "requirement": {"requirement_text": "R1 Enable interactive con\ufb01gurations for suspicious RPTTE groups detection. The automated algorithms signi\ufb01cantly improve the ef\ufb01ciency of the tax inspection procedure when they can successfully identify the most relevant suspicious RPTTE groups. As mentioned by E2, different users may want to explore RPTTE groups that satisfy speci\ufb01c conditions and would like to con\ufb01gure different parameters to extend or narrow down the scope of suspicious groups by \ufb01ltering taxpayers not tightly connected to the related party transactions. For example, different users may have an interest in detecting tax evasion groups from different periods, and exploring tax evasion groups with a speci\ufb01c relevance or complexity", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.", "data_code": {"tables": 1, "categorical": 1, "textual": 1, "network_and_trees": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The nodes are labeled with their IDs for users to track the entity down in the Control Panel. Through the Graph View, users can gain an impression about whether the relevant thresholds have over-pruned or under-pruned the group and require a rerun of the network fusion algorithm with a different parameter setting. By hovering over the related party transactions, the common beneficial owners and the entire ownership chain is highlighted to reveal deceptive cases where tax evaders use a complex ownership structure to hide their identities. Clicking on the related party transaction link propagates the details to the detail view for further analysis.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "zsz", "index_original": 85, "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "pub_year": 2021, "domain": "Tax evasion", "requirement": {"requirement_text": "R2 Rank suspicious tax evasion groups with multiple criteria. Given the vast number of suspicious groups, it is critical to help users quickly locate groups with the highest risk level. The system should support the sorting of the groups based on multiple criteria which re\ufb02ects the suspicion in different dimensions. For example, according to E1\u2019s audit experience, one of the major suspicion criteria is the existence of historical tax evasion records because those groups are likely to commit tax evasion again.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.", "data_code": {"tables": 1, "categorical": 1, "textual": 1, "network_and_trees": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The Group Overview shows a list of suspicious RPTTE groups, in which each row represents a suspicious RPTTE group, and consists of an arc diagram based glyph and a bar chart to help users focus on the most suspicious groups. We extend the arc diagram as a glyph to visualize the topology of the related party transactions, which allows users to estimate the complexity of the suspicious RPTTE group quickly.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "zsz", "index_original": 86, "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "pub_year": 2021, "domain": "Tax evasion", "requirement": {"requirement_text": "R2 Rank suspicious tax evasion groups with multiple criteria. Given the vast number of suspicious groups, it is critical to help users quickly locate groups with the highest risk level. The system should support the sorting of the groups based on multiple criteria which re\ufb02ects the suspicion in different dimensions. For example, according to E1\u2019s audit experience, one of the major suspicion criteria is the existence of historical tax evasion records because those groups are likely to commit tax evasion again.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.", "data_code": {"tables": 1, "categorical": 1, "textual": 1, "network_and_trees": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The bar chart shows three group features, including the number of taxpayers with tax evasion records, the related party transaction amount, and the number of effective related party transactions. By default, the number of effective related party transactions is selected to rank the groups because the feature is engineered to represent one of the standard settings for transfer pricing. Users can click the sort icons located at the top of the list to sort the groups. Hovering over the nodes in the glyph highlights the corresponding node in the graph view. In addition, clicking the row propagates the details of any suspicious RPTTE group to the graph view for further analysis.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "zsz", "index_original": 87, "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "pub_year": 2021, "domain": "Tax evasion", "requirement": {"requirement_text": "R3 Support the interactive exploration of the common bene\ufb01cial owners of taxpayers who conduct the related party transactions and their attributes. As at least one common bene\ufb01cial owner will bene\ufb01t from RPTTE behaviors, exploring the investment and trading relationships helps users to understand how the tax evasion scheme works among taxpayers. In addition, the attributes of taxpayers such as historical tax evasion records provide the tax administration of\ufb01cers a context for suspicion and risk evaluation. Therefore, the experts require that all taxpayer with a common bene\ufb01cial owner should also be clearly visualized, facilitating the deep exploration and inspection of any highly suspicious related party transactions.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.", "data_code": {"tables": 1, "categorical": 1, "textual": 1, "network_and_trees": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The Graph View shows the hierarchical investment relationship and related party transactions within the selected suspicious group. We employ the method proposed by Jnger and Mutzel to conduct the graph layout, as it is more efficient in a layered graph drawing and can effectively reduce the crossed links. Also, we propose several encoding schemes to offer context for group assessment. The color of the borders encodes the entity type of node (investor or taxpayer), and the corresponding period-end profit status for the node is encoded by the fill colors where the diverging color scheme is the same as the node color of the detailed view. For the links, the type of relations uses color encoding. To stay consistent with the color encoding scheme of the node type, blue represents the related party transaction, while orange represents an investment. We emphasize the taxpayers with tax evasion records by drawing an exclamation mark in the circle.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "zsz", "index_original": 88, "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "pub_year": 2021, "domain": "Tax evasion", "requirement": {"requirement_text": "R4 Provide convenient pro\ufb01t analysis of taxpayers that conducted related party transactions. To facilitate the tax inspection process, the users need to know how the taxpayers redistribute their pro\ufb01ts through RPTTE behaviors. It is also important to present the related party transaction as evidence for users to quickly make audit decisions. Both E1 and E2 agree that the evaluation of suspicious related party transactions is challenging because tax evaders try their best to disguise their transactions as legal ones. However, the intent of such transactions must be re\ufb02ected in reported pro\ufb01ts, which leads to a lower tax burden. Therefore, pro\ufb01t analysis can act as the critical context to help users in decision-making. The visualization should display the pro\ufb01t status of the taxpayers, as the analysis of pro\ufb01t variations can reveal whether the related party transaction behaviors affect the overall tax burden.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.", "data_code": {"tables": 1, "categorical": 1, "textual": 1, "network_and_trees": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We proposed using two calendar heatmaps to visualize the profit status of two traders who conducted the selected related party transaction. As shown in figure, we encode the cumulative daily profit with the background color of each visual mark in the calendar heatmap. We proposed two diverging color schemes with the help of ColorBrewer, namely, the brown-blue-green and the red-yellow-green. The former is colorblind-friendly, and the latter is consistent with conventional color usages in the Chinese stock market (i.e., red indicates profit while green represents loss).", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "zsz", "index_original": 89, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "The top straight line in the DAG of a Git repository generally represents the master branch. However, an overwhelming number of branches and the connected links between them could hinder tracking down the origin of changes even for commits in the master branch. To alleviate this problem, Githru removes the connected links between the branches in a DAG to form a group of stems. A stem is a list of ancestor nodes for a specific commit that includes only one of the parents when there are multiple preceding nodes. It is similar to the first-parent option of the git log command, which removes other parent nodes from a branch.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "network+vector", "axial_code": [], "componenet_code": ["vector", "network"]}]}, {"author": "zsz", "index_original": 90, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "The CSM can drastically reduce the number of stems and commits on the screen by removing implicit stems (Fig. 4d), stems corresponding to merged branches, or merged PRs (R1). LinVis [72] proposed a similar approach that grouped parents into a hierarchical structure and presented the structure in Merge-Tree. However, this prior work focused mainly on analyzing the details of CSM-sources (i.e., parent commits) in the master branch and presenting a visual representation of the hierarchy. In contrast, Githru provides an overview of the entire repository by applying the CSM to every stem. Furthermore, users can decide whether to apply a CSM or not, depending on their task. Users can also, if necessary, explicitly visualize the edges between the CSM-base and CSM-sources.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 91, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "In another effort to improve scalability, we adopt a clustering technique to group neighboring commits in each stem. The scope of the grouping is confined to similar commits in each stem to preserve the temporal sequence and topology. We exploit the Simple AdditiveWeighting (SAW) model in calculating similarity since this model is intuitive for users to understand and is known to serve exploration well.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "Clusterneighboringcommitsinstemstopreservetemporalsequence,topology.SAWmodelforsimilarity.", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 92, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "If there is still an overwhelming number of nodes even after the above techniques are applied, users can additionally apply Non-Con\ufb02ict Commits Clustering, which can group non-neighbor commits (R1). For instance, suppose that a cluster A is not adjacent to a cluster C, but their similarity is above the threshold (being suf\ufb01ciently similar). If the cluster A has no commonly modi\ufb01ed \ufb01les with cluster B (an inbetween cluster of A and C), changing the order of B and C could further simplify the underlying structure by grouping the clusters A and C as illustrated in Fig. 5. However, we make this process optional because it considers only the con\ufb02ict coming from modi\ufb01ed \ufb01les and not the contextual con\ufb02ict", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "Clusternon-neighborcommits,simplifystructureoptionally.", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 93, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "We emphasize each block using a black border line when it has a CSM-base and the CSM is enabled; the line turns dashed gray if the CSM is disabled. Regarding visual clutter of borders, adjacent clusters with identical pale colors were hardly distinguishable without any additional visual cues. This issue was raised during the interviews with domain experts, and we eventually included borders. The visibility of the edges between the CSM-base and CSM-sources also changes accordingly. This allows us to reduce the number of visualelements in the horizontal dimension without losing the temporal order of commits across stems.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 94, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "Grouped Summary View shows a brief overview of the selected clusters as shown in Fig. 1d (R1). The columns in the view are mapped to individual clusters and the width of each column is proportional to the number of commits. This view enables a visual comparison of the relative size among selected clusters, which was frequently cited as a needed task in the requirement analysis (R4). Each column has a group of horizontal bars that brie\ufb02y show the top two or three values from the clustering criteria (i.e., author, commit types, modi\ufb01ed \ufb01les, and keywords). In addition, there are bars for the list of modi\ufb01ed directories and \ufb01les to offer more context. Furthermore, the length of each bar is proportional to the number of relevant commits that users could visually compare. For instance, users could \ufb01nd the author who has contributed the most to the cluster by \ufb01nding the longest bar. Enabling the Summary by CLOC option changes the width of each column and the length of the \ufb01le criteria bar proportionally to the number of CLOCs (changed LOCs, added LOCs + deleted LOCs).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 95, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "When users select a cluster in Grouped Summary View, Cluster Detail View appears at the bottom. This view provides commit-level details along with a visual summary of the affected \ufb01les and directories on the left (R1). A list of raw commit metadata is presented in a tabular form by date in ascending order (Fig. 1f). In the case of a CSM commit, it shows only the CSM-base at \ufb01rst, but users can expand the row to also see the relevant CSM-source commits. On the left of the table, we prepared a \ufb01le icicle tree [39] (Fig. 1e). Since \ufb01les and directories are organized in a hierarchy, we consider a number of space-\ufb01lling approaches to maximize space utilization [42]. Among Tree-Map [42], SunBurst [64], and the icicle tree, we \ufb01nally choose the last to comply with the task requirements. Tree-Map shows limitations in the structural interpretation task [18] and SunBurst is inadequate to embed long \ufb01le names because of the radial coordinate. On the other hand, the icicle tree explicitly shows a structural hierarchy in a Cartesian coordinate system well suited for displaying a string (e.g., \ufb01le-name) horizontally [39]. Also, as the depth of the modi\ufb01ed \ufb01le structure can vary, we enable users to zoom in or out with a mouse click on the icicle tree (R3).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 96, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "Selection Cards represent corresponding selections. The stem information, such as a branch name or PR number, is prominently presented on each card, as they directly represent the characteristics of the cluster. For the same reason, the color of the card is also derived from the color of the stem where the selected cluster(s) is located.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 97, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R2: Visualize a graph while preserving topology. The graph representing the abstracted data should be visualized in an interpretable form. The graph should contain abstracted topological data that include (a) the temporal sequence of each node (i.e., commit) and (b) branch information and merge relation; and (c) the graph should be navigable with minimal interactions (T2).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "The DAG representation of a Git repository suffers not only from a large number of nodes (i.e., commits) but also from diverging and converging links at implicit and explicit branches. As the number of commits and branches inevitably increases over time in an ongoing project, scalability is crucial for DAG-based visual analysis. As a remedy, we introduce graph reorganizing techniques tailored to the Git metadata, which could interactively reduce the number of nodes and links during analysis.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 98, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R2: Visualize a graph while preserving topology. The graph representing the abstracted data should be visualized in an interpretable form. The graph should contain abstracted topological data that include (a) the temporal sequence of each node (i.e., commit) and (b) branch information and merge relation; and (c) the graph should be navigable with minimal interactions (T2).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "As a resolution, we propose a Context-preserving Squash Merge (CSM). CSM fuses relevant commits (i.e., second parent commits from the merged branch [22]) into a single node for simplicity (Fig. 4c) and fetches messages from the stems to preserve the merge context (R2). For each merge commit on the main stem (i.e., CSM-base), CSM traverses every parent commit (i.e., the CSM-source) on the other stems. When a commit is a parent of multiple CSM-bases, we select the leftmost commit as a base to avoid redundant merges. CSM gathers contextual information from every CSM-source (e.g., author, commit type, and log message) and appends it to the end of the corresponding \ufb01eld in the CSM-base. For instance, the authors of CSM-sources become coauthors of the corresponding CSM-base. However, the list of changed \ufb01les remains the same since CSM-base encompasses the changes from CSM-sources.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 99, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R2: Visualize a graph while preserving topology. The graph representing the abstracted data should be visualized in an interpretable form. The graph should contain abstracted topological data that include (a) the temporal sequence of each node (i.e., commit) and (b) branch information and merge relation; and (c) the graph should be navigable with minimal interactions (T2).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "We emphasize each block using a black border line when it has a CSM-base and the CSM is enabled; the line turns dashed gray if the CSM is disabled. Regarding visual clutter of borders, adjacent clusters with identical pale colors were hardly distinguishable without any additional visual cues. This issue was raised during the interviews with domain experts, and we eventually included borders. The visibility of the edges between the CSM-base and CSM-sources also changes accordingly. This allows us to reduce the number of visualelements in the horizontal dimension without losing the temporal order of commits across stems.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 100, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R2: Visualize a graph while preserving topology. The graph representing the abstracted data should be visualized in an interpretable form. The graph should contain abstracted topological data that include (a) the temporal sequence of each node (i.e., commit) and (b) branch information and merge relation; and (c) the graph should be navigable with minimal interactions (T2).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "One can control the granularity of clustering by setting a ClusteringStep encoded as a vertical slider. The desired level of abstraction can be set by adjusting the maximum difference value (threshold)to be clustered. For instance, if one moves up the slider, the clustering becomes granular. Thus, one can analyze fine-grained clusters by moving up the slider. For the same reason, we also provided a way to set Preference Weights for each similarity criterion. For instance, if one wants to cluster only commits with similar commit types, one can simply set the weight of the commit type to 1 and the rest to 0. Such a capability reveals the underlying policy of clustering, helping users to understand the context.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 101, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "On the left of the table, we prepared a file icicle tree. Since files and directories are organized in a hierarchy, we finally choose the icicle tree to comply with the task requirements. Because the icicle tree explicitly shows a structural hierarchy in a Cartesian coordinate system well suited for displaying a string (e.g., file-name) horizontally]. Also, as the depth of the modified file structure can vary, we enable users to zoom in or out with a mouse click on the icicle tree.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 102, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "One can control the granularity of clustering by setting a ClusteringStep encoded as a vertical slider. The desired level of abstraction can be set by adjusting the maximum difference value (threshold)to be clustered. For instance, if one moves up the slider, the clustering becomes granular. Thus, one can analyze fine-grained clusters by moving up the slider. For the same reason, we also provided a way to set Preference Weights for each similarity criterion. For instance, if one wants to cluster only commits with similar commit types, one can simply set the weight of the commit type to 1 and the rest to 0. Such a capability reveals the underlying policy of clustering, helping users to find the information they want by allowing them to set appropriate clustering schemes for their task.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 103, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "In many tasks, understanding what happened over a period of time is important. Therefore, we provided a Global Temporal Filter with ways to filter for a certain time period: Brushing and a Select Box. Githru provides two horizontal bars that can be brushed. The bar at the top includes two areas aligned vertically, which represent the number of commits and LOCs by date respectively. The bar at the bottom is a horizontal list of boxes that encodes each commit ordered by date. Both brushes allow for filtering in a specific range and they are synchronized. This method allows users to effectively select a specific period of dates or commits. Users can also select a specific date or release tag using the Select Box. This solves the problem that occurs in brushing when the user has to choose an exact position, which is difficult to select.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 104, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "Stem Type Filter Each stem type in Githru has various characteristics, such as the existence of a name, its relation to PRs, and its PR status. Users may want to focus on a particular stem type depending on their task. For instance, there is no need to see merged or closed branches when looking for recently opened PRs. Thus, we offered options to show or hide each stem type (R3).", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 105, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "Search and Highlight If one searches for a certain keyword, Githru scans branch names, tags, commit messages, authors, commit IDs, and modi\ufb01ed \ufb01les. Then, it highlights every block that matches. Multiple keyword highlighting is also allowed (R3).", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 106, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"textual": 1, "network_and_trees": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Diff View shows a two-way comparison between selections for authors, commit types, files, and keywords. Since comparison becomes difficult as the number of objects increases and Grouped Summary View already provides a rough overview of a multi-way comparison, a two-way comparison fits the details-on-demand strategy.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 107, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R4: Support comparison. The system should facilitate comparisons (a) based on the number of commits and LOC. The magnitude can be compared according to (b) overall trends, or (c) within/between user-selections. (d) In particular, the information in the changed \ufb01les should be compared while being organized according to the directory that contains the structure of the source code (T3).", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"temporal": 1, "network_and_trees": 1, "sequential": 1}}, "solution": [{"solution_text": "This view enables a visual comparison of the relative size among selected clusters, which was frequently cited as a needed task in the requirement analysis.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 108, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R4: Support comparison. The system should facilitate comparisons (a) based on the number of commits and LOC. The magnitude can be compared according to (b) overall trends, or (c) within/between user-selections. (d) In particular, the information in the changed \ufb01les should be compared while being organized according to the directory that contains the structure of the source code (T3).", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"textual": 1, "network_and_trees": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Comparison View provides a detailed comparison of clusters or cluster sets (R4) based on similarity criteria and stem topology. It is designed following the details-on-detail strategy: a rough comparison in Grouped Summary View, and a detailed comparison in Comparison View. We used keywords instead of raw messages for comparison since it was more dif\ufb01cult to use unstructured strings than keywords when visualizing the differences and commonalities between clusters. We used Selection Cards to compare stem information and date and used a Diff View for the other criteria.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 109, "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "pub_year": 2021, "domain": "git", "requirement": {"requirement_text": "R4: Support comparison. The system should facilitate comparisons (a) based on the number of commits and LOC. The magnitude can be compared according to (b) overall trends, or (c) within/between user-selections. (d) In particular, the information in the changed \ufb01les should be compared while being organized according to the directory that contains the structure of the source code (T3).", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "an in-house repository dataset, the public GitHub repository realm-java", "data_code": {"textual": 1, "network_and_trees": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Diff View shows a two-way comparison between selections for authors, commit types, files, and keywords. Since comparison becomes difficult as the number of objects increases and Grouped Summary View already provides a rough overview of a multi-way comparison, a two-way comparison fits the details-on-demand strategy.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 110, "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles", "pub_year": 2021, "domain": "Functional Safety", "requirement": {"requirement_text": "DG1: FACILITATE E XPLORATION OF A P ROJECT User tasks such as \u201cFind missing links\u201d, \u201cLookup and analyze a node\u2019s end-to-end traceability\u201d are network exploration tasks. Hence, we modeled Functional Safety data as a network and derived this design goal to support tasks like fetching node details on hover, \ufb01nding adjacent nodes, and \ufb01nding paths from one node to another based on the taxonomies by Lee et al. [34] and Pretorius et al. [40].", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Since the visualization canvas and the tab layout are vertically stacked within a project panel, we positioned each panel side by side. This way, SafetyLens would facilitate exploratory analyses within and comparative analyses across projects ", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "scatter+table", "axial_code": [], "componenet_code": ["scatter", "table"]}]}, {"author": "zsz", "index_original": 111, "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles", "pub_year": 2021, "domain": "Functional Safety", "requirement": {"requirement_text": "DG2: FACILITATE C OMPARISON AMONG P ROJECTS Teams within functional safety can be system-speci\ufb01c and may not always be aware of the day to day progress made by other teams. This may result in duplicate work (e.g., a team may re-implement an artifact from scratch instead of re-using the one already implemented by another team, or for another vehicle). A core goal for SafetyLens, thus, was to provide a unified interface where users can explore and compare multiple projects to find shared (common) nodes, links, or even subgraphs (combination of nodes and links). This unified interface can foster better collaboration among teams while also saving time and resources for the organization.", "requirement_code": {"discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "D ASHBOARD V IEW, shown in Figure 4, is the landing page and provides an overview of the Functional Safety ecosystem. It consists of a clustered node visualization with each node representing a project. We complemented the visualization with a table to provide a familiar interface to users who are more comfortable with data in spreadsheets. The table consists of project-level information such as {Name, Department, Project In-Charge, and Location}. SafetyLens supports multiple starting points for users to analyze functional safety data. By providing access to all projects within the organization, SafetyLens facilitates collaboration (DG2) among domain experts with different roles (DG5).", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "scatter+table", "axial_code": [], "componenet_code": ["scatter", "table"]}]}, {"author": "zsz", "index_original": 112, "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles", "pub_year": 2021, "domain": "Functional Safety", "requirement": {"requirement_text": "DG2: FACILITATE C OMPARISON AMONG P ROJECTS Teams within functional safety can be system-speci\ufb01c and may not always be aware of the day to day progress made by other teams. This may result in duplicate work (e.g., a team may re-implement an artifact from scratch instead of re-using the one already implemented by another team, or for another vehicle). A core goal for SafetyLens, thus, was to provide a unified interface where users can explore and compare multiple projects to find shared (common) nodes, links, or even subgraphs (combination of nodes and links). This unified interface can foster better collaboration among teams while also saving time and resources for the organization.", "requirement_code": {"discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Since the visualization canvas and the tab layout are vertically stacked within a project panel, we positioned each panel side by side. This way, SafetyLens would facilitate exploratory analyses within and comparative analyses across projects ", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "scatter+table", "axial_code": [], "componenet_code": ["scatter", "table"]}]}, {"author": "zsz", "index_original": 113, "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles", "pub_year": 2021, "domain": "Functional Safety", "requirement": {"requirement_text": "DG2: FACILITATE C OMPARISON AMONG P ROJECTS Teams within functional safety can be system-speci\ufb01c and may not always be aware of the day to day progress made by other teams. This may result in duplicate work (e.g., a team may re-implement an artifact from scratch instead of re-using the one already implemented by another team, or for another vehicle). A core goal for SafetyLens, thus, was to provide a unified interface where users can explore and compare multiple projects to find shared (common) nodes, links, or even subgraphs (combination of nodes and links). This unified interface can foster better collaboration among teams while also saving time and resources for the organization.", "requirement_code": {"discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "S HARED V IEW shows the nodes and links that are common to / shared by all loaded projects (DG2). These are computed by performing a set intersection operation across project graphs based on unique node identi\ufb01ers. We show it in a separate view instead of overlaying or highlighting in the same view to make it easier for domain experts to begin their analysis.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "scatter+table", "axial_code": [], "componenet_code": ["scatter", "table"]}]}, {"author": "zsz", "index_original": 114, "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles", "pub_year": 2021, "domain": "Functional Safety", "requirement": {"requirement_text": "DG3: D ISCOVER PATTERNS AND A NOMALIES The domain experts we spoke to make use of existing commercial and open source tools. These tools support basic exploration and comparison of Functional Safety projects but may fall short when the scale and complexity of data increases. Thus, SafetyLens should support discovering interesting patterns within and across projects.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "S UMMARY V IEW, shown in Figure 5, provides a summary of the projects selected in the Dashboard View (DG6). There are juxtaposed heatmap visualizations for Node Type, Link Relation, and ASIL respectively positioned next to each other. For each attribute table, projects are along the column axis and the corresponding attribute values are along the row axis. An additional column titled \u201cS\u201d is added to show the number of entities (nodes and links) that are shared among these projects. The cells show the per-project entity counts for the corresponding attribute, colored using a continuous color scale (white-to-gray) to help the user discover patterns within as well as across projects (DG3)", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "heatmap", "axial_code": [], "componenet_code": ["heatmap"]}]}, {"author": "zsz", "index_original": 115, "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles", "pub_year": 2021, "domain": "Functional Safety", "requirement": {"requirement_text": "DG3: D ISCOVER PATTERNS AND A NOMALIES The domain experts we spoke to make use of existing commercial and open source tools. These tools support basic exploration and comparison of Functional Safety projects but may fall short when the scale and complexity of data increases. Thus, SafetyLens should support discovering interesting patterns within and across projects.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "VISUALIZATION CANVAS shows a node-link group visualiza- tion. Each node (small circle) is an Element of a functional safety project. These nodes are clustered together based on attributes (e.g., Type). The largest circles represent the group nodes and are labeled with the Type and the number of nodes that are part of it. The boundary marking the extent of the groups (convex hull) is highlighted. The nodes are positioned in each others\u2019 vicinity using an implementation of the circle packing algorithm. The node size and color can be mapped to attributes such as {ASIL, Type, Degree (number of edges to a node)}which in the presence of multiple nodes across multiple projects will create visual clusters leading to discoveries of patterns and anomalies. ", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}]}, {"author": "zsz", "index_original": 116, "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles", "pub_year": 2021, "domain": "Functional Safety", "requirement": {"requirement_text": "DG4: FACILITATE T RACEABILITY AND D ECOMPOSITION OF ASIL S An important task for domain experts is to trace the ASIL from one node to another (e.g., {MB \u2192 HzE \u2192 SG \u2192 FSR \u2192 TSR}). Since ASILs determine the extent of safety mechanisms for elements, any discrepancy such as an element assigned an ASIL=A instead of ASIL=D is an important concern. To diagnose the problem, users should be able to decompose the ASIL into its Severity (S), Exposure (E), and Controllability (C). Thus, it is an important design goal for SafetyLens to unify tasks allowing users to ef\ufb01ciently detect, diagnose, and \ufb01x ASIL assignment issues", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "SafetyLens first checks if a path exists between the two nodes and overlays it onto the node-link group visualization. It also returns a linearized node-link diagram showing the entire route from source to destination. Below the node link diagram is a heatmap showing the S-E-C (Severity Exposure-Controllability) break up of the ASILs.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "scatter+table+heatmap", "axial_code": [], "componenet_code": ["heatmap", "scatter", "table"]}, {"solution_text": "TRACE TAB allows the user to find and visualize if a path exists between two nodes as well as trace their ASILs (e.g., tracing the ASIL from a System Behavior to a Technical Software Requirement). The user can set a node as Source and another as Destination.", "solution_category": "interaction", "solution_axial": "Selecting,", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 117, "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles", "pub_year": 2021, "domain": "Functional Safety", "requirement": {"requirement_text": "DG6: P ROVIDE A S UMMARY OF K EY M ETRICS The user tasks suggested that key metrics should be readily available (e.g., total number of nodes, total number of nodes with ASIL=D, etc).", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "S UMMARY V IEW, shown in Figure 5, provides a summary of the projects selected in the Dashboard View (DG6). There are juxtaposed heatmap visualizations for Node Type, Link Relation, and ASIL respectively positioned next to each other. For each attribute table, projects are along the column axis and the corresponding attribute values are along the row axis. An additional column titled \u201cS\u201d is added to show the number of entities (nodes and links) that are shared among these projects. The cells show the per-project entity counts for the corresponding attribute, colored using a continuous color scale (white-to-gray) to help the user discover patterns within as well as across projects (DG3)", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "heatmap", "axial_code": [], "componenet_code": ["heatmap"]}]}, {"author": "zsz", "index_original": 119, "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach", "pub_year": 2021, "domain": "Bus route planning", "requirement": {"requirement_text": "P1: Obtain the spatial overview of the bus network and its performance. The experts requested to see a map-based overview of the bus network similar to other GIS software [20, 53]. Such an overview should help users to rapidly orient themselves in the spatial context and grasp the spatial distribution of routes. The overview should also include the visualization of the network performance to guide users in performing a drill-down analysis on the potentially ineffective routes.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1}}, "solution": [{"solution_text": "For the network-level analysis, a spatial aggregation view is designed to provide a spatial overview of the entire network and help the users filter routes with spatial constraints", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "map+radar+area", "axial_code": [], "componenet_code": ["map", "radar", "area"]}]}, {"author": "zsz", "index_original": 120, "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach", "pub_year": 2021, "domain": "Bus route planning", "requirement": {"requirement_text": "P1: Obtain the spatial overview of the bus network and its performance. The experts requested to see a map-based overview of the bus network similar to other GIS software [20, 53]. Such an overview should help users to rapidly orient themselves in the spatial context and grasp the spatial distribution of routes. The overview should also include the visualization of the network performance to guide users in performing a drill-down analysis on the potentially ineffective routes.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1}}, "solution": [{"solution_text": "For the route-level analysis, a route ranking view is implemented to depict the performance of the routes, allowing the users to find inefficient routes based on performance criteria.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "table+area", "axial_code": [], "componenet_code": ["area", "table"]}]}, {"author": "zsz", "index_original": 121, "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach", "pub_year": 2021, "domain": "Bus route planning", "requirement": {"requirement_text": "P1: Obtain the spatial overview of the bus network and its performance. The experts requested to see a map-based overview of the bus network similar to other GIS software [20, 53]. Such an overview should help users to rapidly orient themselves in the spatial context and grasp the spatial distribution of routes. The overview should also include the visualization of the network performance to guide users in performing a drill-down analysis on the potentially ineffective routes.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1}}, "solution": [{"solution_text": "6.1.1 Network-Level Analysis An overview of the bus network is essential in locating the areas where the inef\ufb01cient routes most likely exist. The spatial aggregation view (Fig. 1B), designed for the network-level analysis, comprises three linked layers, namely, the map, route, and aggregation layers, to help users analyze the network on a broad scale. The map layer comprises a base map. The route layer draws all routes on the map in blue with opacity. However, the route layer cannot depict the network topology because of the overlapping routes. Therefore, we designed the aggregation layer to visualize the topology with an aggregation graph. Each node in the aggregation graph corresponds to a group of bus stops aggregated spatially with hierarchical clustering [33] by balancing the number of stops in each group. These groups divide the city into transportation zones. The boundaries of these zones are computed with a Voronoi diagram [23] by unifying the polygons that enclose the bus stops inside the zones. In addition, the numbers of the routes between the zones are encoded with the link widths. A zone glyph (Fig. 1D) is placed at the centroid of each transportation zone to summarize the key statistics of this zone. The radar at the glyph center encodes six averaged criteria, namely, route length (RL), number of stops (NS), passenger volume (PV), average load (AL), route directness (DR), and service cost (SC).", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "map+radar+area", "axial_code": [], "componenet_code": ["map", "radar", "area"]}, {"solution_text": "Users can con\ufb01gure the visibility and order of the dimensions \ufb02exibly in the context menu. Two diverging circular distributions around the glyph visualize the amount of passenger \ufb02ows by the geographical directions in which the passengers in this zone leave (green) or enter (orange). Double clicks magnify the glyphs, allowing users to obtain a clearer view of the radars inside. The design of this glyph is kept simple yet informative, such that the users can naturally obtain and compare the performance of different zones with a number of glyphs.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 122, "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach", "pub_year": 2021, "domain": "Bus route planning", "requirement": {"requirement_text": "P1: Obtain the spatial overview of the bus network and its performance. The experts requested to see a map-based overview of the bus network similar to other GIS software [20, 53]. Such an overview should help users to rapidly orient themselves in the spatial context and grasp the spatial distribution of routes. The overview should also include the visualization of the network performance to guide users in performing a drill-down analysis on the potentially ineffective routes.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1}}, "solution": [{"solution_text": "6.1.2 Route-Level Analysis In the route-level analysis, we focus on assisting the users in \ufb01nding inef\ufb01cient routes based on performance criteria in complementary to the spatial information. Inspired by LineUp [27], a table-based ranking visualization is included in the route ranking view to facilitate the multicriteria analysis of the routes.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "table+area", "axial_code": [], "componenet_code": ["area", "table"]}, {"solution_text": "As illustrated in Fig. 1F, the columns represent six criteria similar to the zone glyphs, and each row is a route that can be ranked by selecting any column. The criteria can also be grouped and sorted with different weights. In addition, the criterion distributions are shown in the column headers, providing an overview and range \ufb01lters for the routes in the table.", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "zsz", "index_original": 123, "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach", "pub_year": 2021, "domain": "Bus route planning", "requirement": {"requirement_text": "P2: Analyze the passenger \ufb02ows of bus routes to \ufb01nd weaknesses. The movement of passengers through the bus network provides key insights for the experts to evaluate the performance of the routes. For example, some parts of a route might be non-functional if few passengers were getting on or off in these parts. Therefore, intuitive visualization of passenger \ufb02ows is highly demanded to facilitate the identi\ufb01cation of de\ufb01cient routes. Moreover, such a visualization should also enable the experts to analyze the transfers among multiple bus routes, which may reveal patterns to improve route design.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1}}, "solution": [{"solution_text": "For the network-level analysis, a spatial aggregation view is designed to provide a spatial overview of the entire network and help the users filter routes with spatial constraints.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "map+radar+area", "axial_code": [], "componenet_code": ["map", "radar", "area"]}]}, {"author": "zsz", "index_original": 124, "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach", "pub_year": 2021, "domain": "Bus route planning", "requirement": {"requirement_text": "P2: Analyze the passenger \ufb02ows of bus routes to \ufb01nd weaknesses. The movement of passengers through the bus network provides key insights for the experts to evaluate the performance of the routes. For example, some parts of a route might be non-functional if few passengers were getting on or off in these parts. Therefore, intuitive visualization of passenger \ufb02ows is highly demanded to facilitate the identi\ufb01cation of de\ufb01cient routes. Moreover, such a visualization should also enable the experts to analyze the transfers among multiple bus routes, which may reveal patterns to improve route design.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1}}, "solution": [{"solution_text": "For the stop-level analysis, a route matrix view is proposed to visualize the passenger flows and transfers among the stops in a selected route with matrices, establishing fine-grained inspection of route performance.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "matrix+bar", "axial_code": [], "componenet_code": ["bar", "matrix"]}]}, {"author": "zsz", "index_original": 125, "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach", "pub_year": 2021, "domain": "Bus route planning", "requirement": {"requirement_text": "P2: Analyze the passenger \ufb02ows of bus routes to \ufb01nd weaknesses. The movement of passengers through the bus network provides key insights for the experts to evaluate the performance of the routes. For example, some parts of a route might be non-functional if few passengers were getting on or off in these parts. Therefore, intuitive visualization of passenger \ufb02ows is highly demanded to facilitate the identi\ufb01cation of de\ufb01cient routes. Moreover, such a visualization should also enable the experts to analyze the transfers among multiple bus routes, which may reveal patterns to improve route design.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1}}, "solution": [{"solution_text": "6.1.1 Network-Level Analysis An overview of the bus network is essential in locating the areas where the inef\ufb01cient routes most likely exist. The spatial aggregation view (Fig. 1B), designed for the network-level analysis, comprises three linked layers, namely, the map, route, and aggregation layers, to help users analyze the network on a broad scale. The map layer comprises a base map. The route layer draws all routes on the map in blue with opacity. However, the route layer cannot depict the network topology because of the overlapping routes. Therefore, we designed the aggregation layer to visualize the topology with an aggregation graph. Each node in the aggregation graph corresponds to a group of bus stops aggregated spatially with hierarchical clustering [33] by balancing the number of stops in each group. These groups divide the city into transportation zones. The boundaries of these zones are computed with a Voronoi diagram [23] by unifying the polygons that enclose the bus stops inside the zones. In addition, the numbers of the routes between the zones are encoded with the link widths. A zone glyph (Fig. 1D) is placed at the centroid of each transportation zone to summarize the key statistics of this zone. The radar at the glyph center encodes six averaged criteria, namely, route length (RL), number of stops (NS), passenger volume (PV), average load (AL), route directness (DR), and service cost (SC).", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "map+radar+area", "axial_code": [], "componenet_code": ["map", "radar", "area"]}, {"solution_text": "Users can con\ufb01gure the visibility and order of the dimensions \ufb02exibly in the context menu. Two diverging circular distributions around the glyph visualize the amount of passenger \ufb02ows by the geographical directions in which the passengers in this zone leave (green) or enter (orange). Double clicks magnify the glyphs, allowing users to obtain a clearer view of the radars inside. The design of this glyph is kept simple yet informative, such that the users can naturally obtain and compare the performance of different zones with a number of glyphs.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 126, "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach", "pub_year": 2021, "domain": "Bus route planning", "requirement": {"requirement_text": "P2: Analyze the passenger \ufb02ows of bus routes to \ufb01nd weaknesses. The movement of passengers through the bus network provides key insights for the experts to evaluate the performance of the routes. For example, some parts of a route might be non-functional if few passengers were getting on or off in these parts. Therefore, intuitive visualization of passenger \ufb02ows is highly demanded to facilitate the identi\ufb01cation of de\ufb01cient routes. Moreover, such a visualization should also enable the experts to analyze the transfers among multiple bus routes, which may reveal patterns to improve route design.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "6.1.3 Stop-Level Analysis The stop-level analysis enables the users to explore and evaluate the passenger \ufb02ows and transfers among the stops in a selected route. A \ufb02ow matrix (Fig. 4A) is designed to visualize the passenger \ufb02ows and transfers. The columns and rows of the matrix correspond to the bus stops, and the color intensity of each cell in the matrix encodes the number of passengers traveling between the stops. The color intensity is computed by normalizing the number of passengers into [0, 1] against a global passenger \ufb02ow threshold, which can be changed by users. The horizon charts (Fig. 4B) visualize the number of passengers, aggregated by time, checking in or out at the corresponding stops. The visibility of the horizon charts can be toggled in the context menu to simplify the view. Three-band horizon charts are chosen over bar charts because the estimation accuracy of horizon charts is better than that of bar charts when chart height is limited [30]. In addition, the bars (Fig. 4C) encoding the number of passengers per stop are positioned to the bottom and right of the matrix. In case of long bus routes, users can right click on the matrix to select stops they wish to keep in the view. Massive transfers may indicate that the routes are not well planned and discourage the use of bus transportation [75]. To visualize transfer information, we encode the number of passengers transferred to or from other routes with the opacity of the circles (Fig. 4D) next to the station names. The numbers on the circles indicate how many routes passengers have transferred to or from. The minimum opacity of the circles has been tuned to maintain the visibility of the numbers inside them. Clicking on a circle expands a list of the associated routes (Fig. 4E), each preceded by a small pie chart indicating the percentage of passenger \ufb02ows transferred to or from the route. Selecting a route in the list reveals another \ufb02ow matrix visualizing this route, which is aligned and linked to the original matrix (Fig. 4F). Inspired by MatrixWave [79], we rotate the matrices by 45 \u00b0 clockwise to accommodate them linearly for enhanced scalability. An overview of the matrices (Fig. 4G) is provided at the bottom-left of the view, where each matrix is represented with a square. The currently focused matrix is outlined in the overview, and the numbers of transferred routes are enclosed in the dashed squares.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "matrix+bar", "axial_code": [], "componenet_code": ["bar", "matrix"]}, {"solution_text": "6.1.3 Stop-Level Analysis The stop-level analysis enables the users to explore and evaluate the passenger \ufb02ows and transfers among the stops in a selected route. A \ufb02ow matrix (Fig. 4A) is designed to visualize the passenger \ufb02ows and transfers. The columns and rows of the matrix correspond to the bus stops, and the color intensity of each cell in the matrix encodes the number of passengers traveling between the stops. The color intensity is computed by normalizing the number of passengers into [0, 1] against a global passenger \ufb02ow threshold, which can be changed by users. The horizon charts (Fig. 4B) visualize the number of passengers, aggregated by time, checking in or out at the corresponding stops. The visibility of the horizon charts can be toggled in the context menu to simplify the view. Three-band horizon charts are chosen over bar charts because the estimation accuracy of horizon charts is better than that of bar charts when chart height is limited [30]. In addition, the bars (Fig. 4C) encoding the number of passengers per stop are positioned to the bottom and right of the matrix. In case of long bus routes, users can right click on the matrix to select stops they wish to keep in the view. Massive transfers may indicate that the routes are not well planned and discourage the use of bus transportation [75]. To visualize transfer information, we encode the number of passengers transferred to or from other routes with the opacity of the circles (Fig. 4D) next to the station names. The numbers on the circles indicate how many routes passengers have transferred to or from. The minimum opacity of the circles has been tuned to maintain the visibility of the numbers inside them. Clicking on a circle expands a list of the associated routes (Fig. 4E), each preceded by a small pie chart indicating the percentage of passenger \ufb02ows transferred to or from the route. Selecting a route in the list reveals another \ufb02ow matrix visualizing this route, which is aligned and linked to the original matrix (Fig. 4F). Inspired by MatrixWave [79], we rotate the matrices by 45 \u00b0 clockwise to accommodate them linearly for enhanced scalability. An overview of the matrices (Fig. 4G) is provided at the bottom-left of the view, where each matrix is represented with a square. The currently focused matrix is outlined in the overview, and the numbers of transferred routes are enclosed in the dashed squares.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 127, "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach", "pub_year": 2021, "domain": "Bus route planning", "requirement": {"requirement_text": "M1: Generate a set of alternative routes based on the constraints. The experts prefer to interact with the model and generate the Paretooptimal routes as the potential replacements of the selected ineffective route. To minimize the disruption caused by route changes, the experts may preserve several primary stops from the original route. Moreover, certain constraints, such as the construction cost and maximum route length, may be imposed on route generation. An easy-to-use interface is required for translating these constraints into the complex parameters required by the model.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "This subsection summarizes the core idea of Weng et al.\u2019s method [63] that searches Pareto-optimal transit routes based on the Monte-Carlo search tree. The Monte-Carlo search tree [14] is studied to search the best next move in a game. Starting from a given game state, the search repeats four stages, namely, selection, expansion, simulation, and backpropagation. First, the most promising state is selected. Then, a new state is created based on the estimated best next move of the selected state. Next, the game result is obtained via simulation. Finally, the estimated value of the states in the tree is updated accordingly.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The model results will be streamed and visualized in real-time to help the users determine the quality of the generated routes.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "map+area", "axial_code": [], "componenet_code": ["map", "area"]}, {"solution_text": "After a deficient route has been determined with the exploration inter- face, the users can obtain replacement routes from the manipulation interface. This interface allows the users to control the model by speci- fying model parameters, criterion filters, and anchored stops.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 128, "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach", "pub_year": 2021, "domain": "Bus route planning", "requirement": {"requirement_text": "M2: Inspect the quality of the generated routes in real time. Considering that the model is progressive and does not stop automatically, the experts need to know when the results are suf\ufb01ciently good to stop the route generation process. Hence, tailored visualizations are required to depict the current status of the generation process in real-time and provide the early quality preview of the alternative routes as the process continues. Moreover, such visualizations should allow some undesired routes to be removed from the search space to interactively guide and accelerate the search process.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The model results will be streamed and visualized in real-time to help the users determine the quality of the generated routes.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "map+area", "axial_code": [], "componenet_code": ["map", "area"]}]}, {"author": "zsz", "index_original": 129, "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach", "pub_year": 2021, "domain": "Bus route planning", "requirement": {"requirement_text": "L1: Compare the generated routes based on topologies. To help the experts identify the most promising ones from the generated routes, the proposed system must reveal the topological similarities and differences among these routes. The experts may want to know: What stops do these two routes share? Which pair of consecutive stops is the most frequently selected? How much does a route deviate from another one by taking a detour? Integrating topological information can help experts estimate the performance of these routes and eliminate the undesired ones that share similar characteristics", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1}}, "solution": [{"solution_text": "To aid the users in such progressive decision-making processes, the routes and their topologies are depicted with conflict markers on the map.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "matrix+bar", "axial_code": [], "componenet_code": ["bar", "matrix"]}]}, {"author": "zsz", "index_original": 130, "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach", "pub_year": 2021, "domain": "Bus route planning", "requirement": {"requirement_text": "L2: Compare the generated routes based on multiple criteria. The most promising alternative route should also be determined in terms of the performance criteria. However, experts may treat each criterion differently under different circumstances. For example, the distances between the stops in a suburban bus route will be considerably longer than those between the stops in a city bus route. To facilitate a judicious decision-making process, the system should enable the experts to inspect the criteria of the generated routes and identify the most optimal one ef\ufb01ciently with tailored ranking models.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1}}, "solution": [{"solution_text": "The criteria of the available choices are visualized in the ranking view to facilitate the analysis of route performance", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "table+area", "axial_code": [], "componenet_code": ["area", "table"]}]}, {"author": "zsz", "index_original": 131, "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening", "pub_year": 2021, "domain": "cheminformatics", "requirement": {"requirement_text": "R1: Overview and detailed analysis of a molecular ensemble in the low-dimensional space. For large datasets, scatter plots, which are commonly used to represent the DR output, may suffer from occlusion problems for large datasets. Therefore, the tool should provide visual support for the analysis of data on different levels of abstraction, from the overall distribution of the compounds within the 2D space to the detailed view of individual compounds for a selected region of interest.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "User can get an overview of the distribution of compounds in a selected DR projection using a given molecular representation. ", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The core of ChemVA consists of 2D plots, which give the user an overview of the distribution of compounds in a selected DR projection using a given molecular representation. This overview is supported by the Hexagonal view, a well-adopted and commonly used approach to visualize the outcome of DR techniques [57]. The Hexagonal view aims to overcome the overplotting problem, in which the projected data items overlap and cause visual clutter , thus limiting the interpretability, especially for datasets evincing high similarity between data items. The Hexagonal view of ChemVA seeks to solve this problem by aggregating individual data items into individual hexagons. The user can interactively select a subset of hexagons of interest and explore the distribution of individual data items within the Detail view. The combination of the Hexagonal view and the Detail view aims to ful\ufb01ll requirement R1. Finally, since ChemVA is tailored to support the visual comparison of different projections, it also offers the Difference view. This view was speci\ufb01cally designed to address that task, which is stated in requirements R2 and R3.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "hexagonarea+scatter", "axial_code": [], "componenet_code": ["hexagonarea", "scatter"]}]}, {"author": "zsz", "index_original": 132, "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening", "pub_year": 2021, "domain": "cheminformatics", "requirement": {"requirement_text": "R2: Visual inspection of multiple projections. A set of compounds can be expressed by different vector-based molecular representations, each yielding a different DR projection. The tool should enable the user to intuitively combine information encoded in the individual projections to allow studying at once the similarity between compounds based on different molecular representations. More speci\ufb01cally, this includes exploring similarities and differences between chemical compounds, expressed by the different projections. Additionally, the visual representations and interactions should help the domain experts evaluate the suitability of the selected DR model.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "User can get an overview of the distribution of compounds in a selected DR projection using a given molecular representation. ", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "In order to support the task of comparing the outputs of different 2D projections, as stated in our requirement R2, we propose a novel view called Difference view, that combines and contrasts two selected 2D Hexagonal views, A and B. Initially it displays a hexagonal layout similar to that presented in the Hexagonal view, where the opacity of each hexagon encodes the computed correlation score of the trustworthiness of the projections A and B under study (requirement R3)", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "hexagonarea", "axial_code": [], "componenet_code": ["hexagonarea"]}]}, {"author": "zsz", "index_original": 133, "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening", "pub_year": 2021, "domain": "cheminformatics", "requirement": {"requirement_text": "R3: Evaluation of the trustworthiness of projections. Users need proper visual support for assessing the trustworthiness of a lowdimensional projection based on the distortion with regard to pairwise distances between compounds in the original space. When such trustworthiness can be compared on different DR projections, users can focus the exploration on a subset of molecular representations.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "User can get an overview of the distribution of compounds in a selected DR projection using a given molecular representation. ", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The color of a hexagon encodes the prevailing trend among its compounds for a selected feature, which is by default their bioactivity but can be switched to other molecular properties, including the trustworthiness of the projection.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "hexagonarea", "axial_code": [], "componenet_code": ["hexagonarea"]}, {"solution_text": "The color of a hexagon encodes the prevailing trend among its compounds for a selected feature, which is by default their bioactivity but can be switched to other molecular properties, including the trustworthiness of the projection.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 134, "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening", "pub_year": 2021, "domain": "cheminformatics", "requirement": {"requirement_text": "R3: Evaluation of the trustworthiness of projections. Users need proper visual support for assessing the trustworthiness of a lowdimensional projection based on the distortion with regard to pairwise distances between compounds in the original space. When such trustworthiness can be compared on different DR projections, users can focus the exploration on a subset of molecular representations.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "In addition, all properties and features described in Section 3 can be color-encoded on points representing each compound. These features are selected from a drop-down menu, and their color encodings are chosen according to their type, i.e., quantitative, such as molecular weight, or categorical, such as bioactivity towards a target protein. Another quantitative property that can be used for color encoding corresponds to the correlation scores, which encode the trustworthiness of the DR projection of the compound (requirement R3). These scores were computed using Pearson and Kendall correlation, whose calculation is explained in the Supplementary Material.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Upon selecting a subset of compounds in the Hexagonal view, the user can explore the selected data in the Detail view, depicted in Fig_x0002_ure 3. In this view, the compounds are represented using a standard scatter plot, enhanced by a subtle overlay of the same hexagonal grid as in the Hexagonal view, which helps users keep the correspondence between the zoom level in these views. ", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "hexagonarea+scatter", "axial_code": [], "componenet_code": ["hexagonarea", "scatter"]}, {"solution_text": "The Detail view displays only the selected hexagons zoomed in after the selection. To further enhance the link between the Hexagonal and Detail views, the corresponding hexagons are highlighted when the user hovers over them in any of these views. In order to perform a selection of individual compounds in this view, a lasso-shaped selector is supported. The selected compounds are then displayed in the 3D view and also highlighted in the Table view (Section 5.1.2).", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 135, "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening", "pub_year": 2021, "domain": "cheminformatics", "requirement": {"requirement_text": "R3: Evaluation of the trustworthiness of projections. Users need proper visual support for assessing the trustworthiness of a lowdimensional projection based on the distortion with regard to pairwise distances between compounds in the original space. When such trustworthiness can be compared on different DR projections, users can focus the exploration on a subset of molecular representations.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "User can get an overview of the distribution of compounds in a selected DR projection using a given molecular representation. ", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "In order to support the task of comparing the outputs of different 2D projections, as stated in our requirement R2, we propose a novel view called Difference view, that combines and contrasts two selected 2D Hexagonal views, A and B. Initially it displays a hexagonal layout similar to that presented in the Hexagonal view, where the opacity of each hexagon encodes the computed correlation score of the trustworthiness of the projections A and B under study (requirement R3)", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "hexagonarea", "axial_code": [], "componenet_code": ["hexagonarea"]}]}, {"author": "zsz", "index_original": 136, "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening", "pub_year": 2021, "domain": "cheminformatics", "requirement": {"requirement_text": "R4: Comparison of compounds according to features related to drug-likeness. Chemical compounds have many features and descriptors related to drug-likeness that can complement other molecular representations in the task of virtual screening. Therefore, it is desirable to provide users with an option to visualize these additional features along with the projections.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "Besides from the vector-based molecular representations used in the DR projections and displayed in our 2D plot views, there are several other molecular features related to drug-likeness that should be taken into consideration when analyzing the compounds, as stated in requirement R4. ChemVA enables the users to explore such features, listed in Section 3.2, by means of a Table view which offers advanced interaction options. We adopted a well-established tool, published by Gratzl et al. [13], and its extension [11]. As these tools perfectly \ufb01t to our needs, we incorporate them to ChemVA. Further details about the broad range of interaction possibilities can be found in the original papers. In addition to the list of compounds, the Table view provides the users with graphical elements in the form of juxtaposed bar charts and box plots in the right side panel. By default, the compounds in the table are logically grouped by their membership to hexagons in the Hexagonal view. These groups can be either expanded or compressed. When compressed, the table displays the box plots of the distribution of the values for each feature in the hexagon.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "bar+box", "axial_code": [], "componenet_code": ["bar", "boxplot"]}, {"solution_text": "As shown in Figure 5. This view is interactively linked with the other visual components of ChemVA. When performing a selection in the 2D plot views, the corresponding compounds are automatically highlighted in the Table view. Conversely, when the user selects compounds in the Table view, the corresponding compounds are highlighted in the Detail view, displayed in the 3D view, and the corresponding hexagons are highlighted in the Hexagonal view", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 137, "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening", "pub_year": 2021, "domain": "cheminformatics", "requirement": {"requirement_text": "R5: Comprehensible viewing of 3D structural similarity. The tool should support the inspection of individual compounds in terms of their 3D geometry, as well as the visual comparison of common 3D substructures in a selected set of compounds. For multiple compounds, such a view should convey the information about similarities and differences in their 3D structure.", "requirement_code": {"describe_observation_item": 1, "compare_entities": 1}}, "data": {"data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "Compound similarity can be better perceived when the compounds are structurally aligned in the view. To serve this purpose, we use a structural alignment functionality provided by the OpenBabel tool [43]. Once molecules are aligned, the user should be able to easily identify their common parts, i.e., the subsets of atoms and bonds that are present in most of the selected compounds.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "3Dstructure", "axial_code": [], "componenet_code": ["3Dstructure"]}]}, {"author": "zsz", "index_original": 138, "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening", "pub_year": 2021, "domain": "cheminformatics", "requirement": {"requirement_text": "R6: Possibility to add new compounds and comparison with the existing data. The tool should support the process of exploration of different features and bioactivity for newly added compounds. The new compound should be projected using the DR model and integrated into the remaining views, so that the user can compare its features to those of the compounds in the existing dataset.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "ChemVA provides an option to add new compounds to the dataset being studied in order to explore their features and compare them with those of other compounds. By means of this functionality, the expert can assess the potential of this newly added compound prior to extensive wet lab testing.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 155, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T1.1 Match source code with binary cod", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "The source code view displays a single source code file. By default, it displays the one with the most data, but the file can be changed in the interface.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+text", "axial_code": [], "componenet_code": ["text", "table"]}, {"solution_text": "The source code view displays a single source code file. By default, it displays the one with the most data, but the file can be changed in the interface.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 156, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T1.1 Match source code with binary cod", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "Multiple lines can be selected and will be highlighted across other views, supporting the task of matching the source code and disassembly.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 157, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T1.1 Match source code with binary cod", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "The disassembly represents the ground truth of the compiled program. One strategy commonly employed by users was to use linked navigation to get close to an area of interest not otherwise selectable with information from our automated analysis and then search by scrolling from there, so we include it in its entirety.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+text", "axial_code": [], "componenet_code": ["text", "table"]}, {"solution_text": "The disassembly represents the ground truth of the compiled program. One strategy commonly employed by users was to use linked navigation to get close to an area of interest not otherwise selectable with information from our automated analysis and then search by scrolling from there, so we include it in its entirety.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 158, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T1.1 Match source code with binary cod", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "Basic blocks (nodes) in the CFG can be selected individually or by brush and will update all views. The CFG view also supports panning and zooming.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 159, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T1.2 Identify/Relate structures with code", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.", "solution_category": "data_manipulation", "solution_axial": "Excluding", "solution_compoent": "", "axial_code": ["Excluding"], "componenet_code": ["excluding"]}, {"solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+text", "axial_code": [], "componenet_code": ["text", "table"]}, {"solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 160, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T1.2 Identify/Relate structures with code", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "The CFG view shows a subgraph of the full binary CFG, based on the current selection. Prior work on CFGs by the visualization experts led to this project. However, early meetings indicated matching of source and disassembly was the main workflow. Thus, our initial prototypes did not include a CFG. In subsequent meetings, we observed our domain experts had difficulty understanding structures such as loops with only matching or nesting. We thus chose to provide such context with a CFG view.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "network+contour+text", "axial_code": [], "componenet_code": ["text", "contour", "network"]}]}, {"author": "zsz", "index_original": 161, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T1.2 Identify/Relate structures with code", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}, {"solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "text", "axial_code": [], "componenet_code": ["text"]}]}, {"author": "zsz", "index_original": 162, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T1.2 Identify/Relate structures with code", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "Identifying or navigating to a particular loop is a common operation, so we chose to directly support it by creating a loop-centric view.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "tree+text", "axial_code": [], "componenet_code": ["text", "tree"]}]}, {"author": "zsz", "index_original": 163, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T1.2 Identify/Relate structures with code", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "Consistent with the function inlining view, selections in other views will filter this one, providing loop context to those other views.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 164, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T1.2 Identify/Relate structures with code", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "The call graph view shows a subgraph of the full call graph, with all functions reported by our analysis regardless of whether they were inlined. This view provides a way to relate selected disassembly to the functions and call stack.  Inlined calls are shown with a dashed red line to help identify them.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "network+text", "axial_code": [], "componenet_code": ["text", "network"]}]}, {"author": "zsz", "index_original": 165, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T1.3 Annotate relations", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.", "solution_category": "data_manipulation", "solution_axial": "Excluding", "solution_compoent": "", "axial_code": ["Excluding"], "componenet_code": ["excluding"]}, {"solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+text", "axial_code": [], "componenet_code": ["text", "table"]}, {"solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 166, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T1.3 Annotate relations", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "Annotating the disassembly with source code variable names is a common task. While our automated analysis provides a best-effort annotation, it is incomplete. We allow the user to manually add annotations with this view. The view further summarizes all active renamings.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 167, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T1.4 Trace variable", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.", "solution_category": "data_manipulation", "solution_axial": "Excluding", "solution_compoent": "", "axial_code": ["Excluding"], "componenet_code": ["excluding"]}, {"solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+text", "axial_code": [], "componenet_code": ["text", "table"]}, {"solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 168, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T1.4 Trace variable", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "The call graph view shows a subgraph of the full call graph, with all functions reported by our analysis regardless of whether they were inlined. This view provides a way to relate selected disassembly to the functions and call stack.  Inlined calls are shown with a dashed red line to help identify them.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "network+text", "axial_code": [], "componenet_code": ["text", "network"]}]}, {"author": "zsz", "index_original": 169, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T2.1 Find areas of interest", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "Another change from CFGExplorer is filtering the graph to a k-hop region of interest around selected basic blocks. Our data creates CFGs that are too large for Sugiyama-style layouts. To support the winnowing of data to find areas of interest, k is configurable via the interface, with a default of k=3 determined through our users\u2019 experience.", "solution_category": "interaction", "solution_axial": "Filtering,Participation/Collaboration", "solution_compoent": "", "axial_code": ["Filtering", "Participation/Collaboration"], "componenet_code": ["filtering", "participation_collaboration"]}]}, {"author": "zsz", "index_original": 170, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T2.1 Find areas of interest", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "Function inlining is one of the most common optimizations performed by compiler and is of great interest to our collaborators. Thus, we design a separate panel for inlining information to help identify and navigate to them.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "tree+text", "axial_code": [], "componenet_code": ["text", "tree"]}]}, {"author": "zsz", "index_original": 171, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T2.1 Find areas of interest", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "Identifying or navigating to a particular loop is a common operation, so we chose to directly support it by creating a loop-centric view.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "tree+text", "axial_code": [], "componenet_code": ["text", "tree"]}]}, {"author": "zsz", "index_original": 172, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T2.1 Find areas of interest", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "Consistent with the function inlining view, selections in other views will filter this one, providing loop context to those other views.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 173, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T2.2 Identify optimizations", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}, {"solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "text", "axial_code": [], "componenet_code": ["text"]}]}, {"author": "zsz", "index_original": 174, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T2.2 Identify optimizations", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "Function inlining is one of the most common optimizations performed by compiler and is of great interest to our collaborators. Thus, we design a separate panel for inlining information to help identify and navigate to them.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "tree+text", "axial_code": [], "componenet_code": ["text", "tree"]}]}, {"author": "zsz", "index_original": 175, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T2.2 Identify optimizations", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "The call graph view shows a subgraph of the full call graph, with all functions reported by our analysis regardless of whether they were inlined. This view provides a way to relate selected disassembly to the functions and call stack.  Inlined calls are shown with a dashed red line to help identify them.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "network+text", "axial_code": [], "componenet_code": ["text", "network"]}]}, {"author": "zsz", "index_original": 176, "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code", "pub_year": 2021, "domain": "compilation", "requirement": {"requirement_text": "T2.3 Assess optimizations", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}, {"solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "text", "axial_code": [], "componenet_code": ["text"]}]}, {"author": "zsz", "index_original": 178, "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting", "pub_year": 2021, "domain": "bias ", "requirement": {"requirement_text": "R1. Identify relevant dimensions that exhibit high levels of bias.Users should be able to see which dimensions exhibit high lev-els of selection bias and understand which of those high-biasdimensions, and groups of dimensions, are most relevant for theiranalytical question.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.", "data_code": {"network_and_trees": 1, "quantitative": 1}}, "solution": [{"solution_text": "The split icicle plot was developed to visualize shifts in distribution for dimensions in large hierarchies, indicating potential selection bias [6]. Although effective for communicating this information, it exhibits some limitations when used for DR. We therefore developed the icicle table to address these limitations and add functionality useful for DR.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "icicle", "axial_code": [], "componenet_code": ["icicle"]}]}, {"author": "zsz", "index_original": 179, "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting", "pub_year": 2021, "domain": "bias ", "requirement": {"requirement_text": "R1. Identify relevant dimensions that exhibit high levels of bias.Users should be able to see which dimensions exhibit high lev-els of selection bias and understand which of those high-biasdimensions, and groups of dimensions, are most relevant for theiranalytical question.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.", "data_code": {"network_and_trees": 1, "quantitative": 1}}, "solution": [{"solution_text": "The split icicle plot modifies the strict hierarchical icicle plot layout by splitting certain nodes, enabling more effective sorting of the plot by the maximum distance along each path from a leaf node to the root. Thus areas with large shifts can be sorted toward the top of the plot to help the user prioritize dimensions to investigate for selection bias.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "icicle", "axial_code": [], "componenet_code": ["icicle"]}]}, {"author": "zsz", "index_original": 180, "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting", "pub_year": 2021, "domain": "bias ", "requirement": {"requirement_text": "R1. Identify relevant dimensions that exhibit high levels of bias.Users should be able to see which dimensions exhibit high lev-els of selection bias and understand which of those high-biasdimensions, and groups of dimensions, are most relevant for theiranalytical question.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.", "data_code": {"network_and_trees": 1, "tables": 1, "quantitative": 1}}, "solution": [{"solution_text": "Integrating a table with the split icicle plot enables the inclusion of multi-attribute information to help the user select appropriate dimen- sions for reweighting.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "icicle+table", "axial_code": [], "componenet_code": ["icicle", "table"]}]}, {"author": "zsz", "index_original": 181, "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting", "pub_year": 2021, "domain": "bias ", "requirement": {"requirement_text": "R1. Identify relevant dimensions that exhibit high levels of bias.Users should be able to see which dimensions exhibit high lev-els of selection bias and understand which of those high-biasdimensions, and groups of dimensions, are most relevant for theiranalytical question.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.", "data_code": {"network_and_trees": 1, "tables": 1, "quantitative": 1}}, "solution": [{"solution_text": "Three additional visualizations show the effect of reweighting on per- dimension distances and outcome correlations for the baseline and focus cohorts and enable the selection of reweighting dimensions: a scatter plot, contour plot, and vector plot. Each view shows correlation with outcome along the x-axis and focus-to-baseline distance along the y-axis. The user can switch views on demand, with linked selection between the three views and the icicle table.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "scatter+contour+vector", "axial_code": [], "componenet_code": ["contour", "scatter", "vector"]}]}, {"author": "zsz", "index_original": 182, "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting", "pub_year": 2021, "domain": "bias ", "requirement": {"requirement_text": "R2. Apply bias correction based on user-selected dimensions.Users should be able to specify one or more dimensions for biascorrection and have the system automatically determine the re-quired sample weighting to perform the correction.", "requirement_code": {"collect_evidence": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.", "data_code": {"network_and_trees": 1, "quantitative": 1}}, "solution": [{"solution_text": "The split icicle plot was developed to visualize shifts in distribution for dimensions in large hierarchies, indicating potential selection bias [6]. Although effective for communicating this information, it exhibits some limitations when used for DR. We therefore developed the icicle table to address these limitations and add functionality useful for DR.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "icicle", "axial_code": [], "componenet_code": ["icicle"]}]}, {"author": "zsz", "index_original": 183, "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting", "pub_year": 2021, "domain": "bias ", "requirement": {"requirement_text": "R2. Apply bias correction based on user-selected dimensions.Users should be able to specify one or more dimensions for biascorrection and have the system automatically determine the re-quired sample weighting to perform the correction.", "requirement_code": {"collect_evidence": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.", "data_code": {"network_and_trees": 1, "tables": 1, "quantitative": 1}}, "solution": [{"solution_text": "To further support the DR workflow, we have added a replace reweight mode. When fine-tuning the reweighting configuration, the user may wish to adjust a reweight dimension by moving up in the hierarchy to a more general type or down to a more specific type. To do so, the user can enter replace reweight mode, causing the icicle plot to show only the reweight dimension, its ancestors, and its descendants. All ancestors, the reweight dimension, and two levels of children are marked as salient, providing a detailed view of the local neighborhood around the reweight dimension. The user can then select a dimension to take its place in the list of reweight dimensions.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "icicle+table", "axial_code": [], "componenet_code": ["icicle", "table"]}, {"solution_text": "To further support the DR workflow, we have added a replace reweight mode. When fine-tuning the reweighting configuration, the user may wish to adjust a reweight dimension by moving up in the hierarchy to a more general type or down to a more specific type. To do so, the user can enter replace reweight mode, causing the icicle plot to show only the reweight dimension, its ancestors, and its descendants. All ancestors, the reweight dimension, and two levels of children are marked as salient, providing a detailed view of the local neighborhood around the reweight dimension. The user can then select a dimension to take its place in the list of reweight dimensions.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 184, "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting", "pub_year": 2021, "domain": "bias ", "requirement": {"requirement_text": "R2. Apply bias correction based on user-selected dimensions.Users should be able to specify one or more dimensions for biascorrection and have the system automatically determine the re-quired sample weighting to perform the correction.", "requirement_code": {"collect_evidence": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.", "data_code": {"network_and_trees": 1, "tables": 1, "quantitative": 1}}, "solution": [{"solution_text": "The balance panel includes visualizations and controls for the DR process. A reweight list shows all dimensions selected for reweighting. The user can remove dimensions from the list, initiate the reweighting process, and use a slider to control the amount of reweighting. A detailed view of the per-cohort subgroups used for reweighting is shown in the reweight set visualization.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 185, "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting", "pub_year": 2021, "domain": "bias ", "requirement": {"requirement_text": "R3. Understand the effect of bias correction. This includes two keyaspects. First (R3.1), as correcting for a small set of speci\ufb01eddimensions will affect a larger set of dimensions, the user shouldbe made aware of how widespread the effects of reweighting are,where bias was reduced, and where and how much bias remains.Second (R3.2), users must understand the effect of bias correctionon the visualizations driving their primary analysis.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.", "data_code": {"network_and_trees": 1, "tables": 1, "quantitative": 1}}, "solution": [{"solution_text": "An improved aggregate distance measure is included to better sum- marize the effect of reweighting upon a cohort (R3.1). In a typical cohort exhibiting selection bias, the vast majority of dimensions un- dergo small to moderate shifts in distribution, whereas a smaller number of dimensions that are highly correlated with the dimensions used for selection will undergo larger shifts. It is typically these high-bias di- mensions that the user wishes to correct.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "As the event sequence analysis capabilities of Cadence are used to filter existing cohorts to create new cohorts, representations of each cohort and their provenance are shown in the cohort provenance tree (Figure 1-a). Each cohort is represented by a glyph encoding cohort size and aggregate distance across all data dimensions, along with icons indicating the baseline and focus cohorts (Figure 3-a and b).", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "tree+glyph", "axial_code": [], "componenet_code": ["tree", "glyph"]}]}, {"author": "zsz", "index_original": 186, "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting", "pub_year": 2021, "domain": "bias ", "requirement": {"requirement_text": "R3. Understand the effect of bias correction. This includes two keyaspects. First (R3.1), as correcting for a small set of speci\ufb01eddimensions will affect a larger set of dimensions, the user shouldbe made aware of how widespread the effects of reweighting are,where bias was reduced, and where and how much bias remains.Second (R3.2), users must understand the effect of bias correctionon the visualizations driving their primary analysis.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.", "data_code": {"network_and_trees": 1, "quantitative": 1}}, "solution": [{"solution_text": "The split icicle plot was developed to visualize shifts in distribution for dimensions in large hierarchies, indicating potential selection bias [6]. Although effective for communicating this information, it exhibits some limitations when used for DR. We therefore developed the icicle table to address these limitations and add functionality useful for DR.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "icicle", "axial_code": [], "componenet_code": ["icicle"]}]}, {"author": "zsz", "index_original": 187, "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting", "pub_year": 2021, "domain": "bias ", "requirement": {"requirement_text": "R3. Understand the effect of bias correction. This includes two keyaspects. First (R3.1), as correcting for a small set of speci\ufb01eddimensions will affect a larger set of dimensions, the user shouldbe made aware of how widespread the effects of reweighting are,where bias was reduced, and where and how much bias remains.Second (R3.2), users must understand the effect of bias correctionon the visualizations driving their primary analysis.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.", "data_code": {"network_and_trees": 1, "tables": 1, "quantitative": 1}}, "solution": [{"solution_text": "Three additional visualizations show the effect of reweighting on per- dimension distances and outcome correlations for the baseline and focus cohorts and enable the selection of reweighting dimensions: a scatter plot, contour plot, and vector plot. Each view shows correlation with outcome along the x-axis and focus-to-baseline distance along the y-axis. The user can switch views on demand, with linked selection between the three views and the icicle table.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "scatter+contour+vector", "axial_code": [], "componenet_code": ["contour", "scatter", "vector"]}]}, {"author": "zsz", "index_original": 189, "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting", "pub_year": 2021, "domain": "bias ", "requirement": {"requirement_text": "R4. Prevent over\ufb01tting for poorly represented subgroups. Insome cases the weighted samples used for bias correction canexcessively amplify poorly sampled subgroups, similar to theproblem of model over\ufb01tting. Users must be able to understandwhen a proposed bias correction poses a risk of over\ufb01tting dueto limitations in the underlying data, and be able to revise thereweighting con\ufb01guration appropriately.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.", "data_code": {"network_and_trees": 1, "tables": 1, "quantitative": 1}}, "solution": [{"solution_text": "The danger score alerts the user to potential problems with the current reweighting configuration. A score is computed for each cohort to identify those with poorly sampled subgroups. An indicator is shown next to any cohort with a score approaching a system-defined threshold, indicating that adjustments to the current reweighting configuration may be warranted.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "zsz", "index_original": 190, "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting", "pub_year": 2021, "domain": "bias ", "requirement": {"requirement_text": "R4. Prevent over\ufb01tting for poorly represented subgroups. Insome cases the weighted samples used for bias correction canexcessively amplify poorly sampled subgroups, similar to theproblem of model over\ufb01tting. Users must be able to understandwhen a proposed bias correction poses a risk of over\ufb01tting dueto limitations in the underlying data, and be able to revise thereweighting con\ufb01guration appropriately.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.", "data_code": {"network_and_trees": 1, "tables": 1, "quantitative": 1}}, "solution": [{"solution_text": "To further support the DR workflow, we have added a replace reweight mode. When fine-tuning the reweighting configuration, the user may wish to adjust a reweight dimension by moving up in the hierarchy to a more general type or down to a more specific type. To do so, the user can enter replace reweight mode, causing the icicle plot to show only the reweight dimension, its ancestors, and its descendants. All ancestors, the reweight dimension, and two levels of children are marked as salient, providing a detailed view of the local neighborhood around the reweight dimension. The user can then select a dimension to take its place in the list of reweight dimensions.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "icicle+table", "axial_code": [], "componenet_code": ["icicle", "table"]}, {"solution_text": "To further support the DR workflow, we have added a replace reweight mode. When fine-tuning the reweighting configuration, the user may wish to adjust a reweight dimension by moving up in the hierarchy to a more general type or down to a more specific type. To do so, the user can enter replace reweight mode, causing the icicle plot to show only the reweight dimension, its ancestors, and its descendants. All ancestors, the reweight dimension, and two levels of children are marked as salient, providing a detailed view of the local neighborhood around the reweight dimension. The user can then select a dimension to take its place in the list of reweight dimensions.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 191, "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting", "pub_year": 2021, "domain": "bias ", "requirement": {"requirement_text": "R4. Prevent over\ufb01tting for poorly represented subgroups. Insome cases the weighted samples used for bias correction canexcessively amplify poorly sampled subgroups, similar to theproblem of model over\ufb01tting. Users must be able to understandwhen a proposed bias correction poses a risk of over\ufb01tting dueto limitations in the underlying data, and be able to revise thereweighting con\ufb01guration appropriately.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.", "data_code": {"network_and_trees": 1, "tables": 1, "quantitative": 1}}, "solution": [{"solution_text": "The balance panel includes visualizations and controls for the DR process. A reweight list shows all dimensions selected for reweighting. The user can remove dimensions from the list, initiate the reweighting process, and use a slider to control the amount of reweighting. A detailed view of the per-cohort subgroups used for reweighting is shown in the reweight set visualization.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 192, "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting", "pub_year": 2021, "domain": "bias ", "requirement": {"requirement_text": "R4. Prevent over\ufb01tting for poorly represented subgroups. Insome cases the weighted samples used for bias correction canexcessively amplify poorly sampled subgroups, similar to theproblem of model over\ufb01tting. Users must be able to understandwhen a proposed bias correction poses a risk of over\ufb01tting dueto limitations in the underlying data, and be able to revise thereweighting con\ufb01guration appropriately.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.", "data_code": {"network_and_trees": 1, "tables": 1, "quantitative": 1}}, "solution": [{"solution_text": "The reweight set visualization is based on Upset [26]. It shows how the reweight dimensions combine to form subgroups of each cohort to be reweighted (Section 6), and enables identification and cor- rection of potential reweighting issues that may result in unreasonably high weights for small subgroups of individuals.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "matrix+bar", "axial_code": [], "componenet_code": ["bar", "matrix"]}]}, {"author": "zsz", "index_original": 193, "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection", "pub_year": 2021, "domain": "autonomous driving", "requirement": {"requirement_text": "RD1: Human-friendly data representation and summarization. This issue arises from the nature of high dimensionality and sheer volume of images. We need a representation to capture the intrinsic attributes of images in a lower dimension space and then summarize them in a human-friendly way [27,28].", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.", "data_code": {"clusters_and_sets_and_lists": 1, "media": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "In the second module, two core learning components, disentangled representation learning and semantic adversarial learning, are introduced to augment our analysis and meet the requirements of RD1 and RD2. Disentangled representation learning extracts intrinsic and interpretable attributes of traffic lights, such as colors, brightness, and background (Fig. 4b1 ). This component first provides a human-friendly data presentation, and also offers a data space where an adversarial generation can efficiently search. Semantic adversarial learning learns prediction behaviors of a detector, and generates meaningful adversarial examples (Fig. 4b2 ) on top of disentangled representation learning. After this, both acquired data (Fig. 4b3 ) and unseen data (Fig. 4b4 ) are passed to the detector to obtain detection results.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "Disentangledrepresentationandsemanticadversariallearning.", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "zsz", "index_original": 194, "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection", "pub_year": 2021, "domain": "autonomous driving", "requirement": {"requirement_text": "RD1: Human-friendly data representation and summarization. This issue arises from the nature of high dimensionality and sheer volume of images. We need a representation to capture the intrinsic attributes of images in a lower dimension space and then summarize them in a human-friendly way [27,28].", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.", "data_code": {"clusters_and_sets_and_lists": 1, "media": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "Disentangled Representation Learning (DRL) is introduced to extract semantic latent representation of traffic lights (e.g. colors, background, rotation, etc.), shown in Fig.5-a and generate more data with controllable semantics for data augmentation. The semantic latent representation also serves as a cornerstone for both human-friendly data summarization (RD1) and semantic adversarial learning (RD2).", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "Semanticlatentrepresentationextractionfortrafficlights.", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "zsz", "index_original": 195, "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection", "pub_year": 2021, "domain": "autonomous driving", "requirement": {"requirement_text": "RD2: Efficient generation of unseen test cases. We seek for a method generating edge cases to probe model robustness. These test cases should be different from the imperceivable noises that learned from traditional adversarial approaches, and have semantic meanings to guide human to improve the robustness [10,23].", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.", "data_code": {"clusters_and_sets_and_lists": 1, "media": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "In the second module, two core learning components, disentangled representation learning and semantic adversarial learning, are introduced to augment our analysis and meet the requirements of RD1 and RD2. Disentangled representation learning extracts intrinsic and interpretable attributes of traffic lights, such as colors, brightness, and background (Fig. 4b1 ). This component first provides a human-friendly data presentation, and also offers a data space where an adversarial generation can efficiently search. Semantic adversarial learning learns prediction behaviors of a detector, and generates meaningful adversarial examples (Fig. 4b2 ) on top of disentangled representation learning. After this, both acquired data (Fig. 4b3 ) and unseen data (Fig. 4b4 ) are passed to the detector to obtain detection results.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "Disentangledrepresentationandsemanticadversariallearning.", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "zsz", "index_original": 196, "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection", "pub_year": 2021, "domain": "autonomous driving", "requirement": {"requirement_text": "RD2: Efficient generation of unseen test cases. We seek for a method generating edge cases to probe model robustness. These test cases should be different from the imperceivable noises that learned from traditional adversarial approaches, and have semantic meanings to guide human to improve the robustness [10,23].", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.", "data_code": {"clusters_and_sets_and_lists": 1, "media": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "Disentangled Representation Learning (DRL) is introduced to extract semantic latent representation of traffic lights (e.g. colors, background, rotation, etc.), shown in Fig.5-a and generate more data with controllable semantics for data augmentation. The semantic latent representation also serves as a cornerstone for both human-friendly data summarization (RD1) and semantic adversarial learning (RD2).", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "Semanticlatentrepresentationextractionfortrafficlights.", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "zsz", "index_original": 197, "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection", "pub_year": 2021, "domain": "autonomous driving", "requirement": {"requirement_text": "RP1: A contextualized understanding for model performance. In_x0002_stead of using an aggregated metric to evaluate models [18], we would like to put a single score into the contexts of various sizes, IoU thresholds and confident score ranges.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.", "data_code": {"clusters_and_sets_and_lists": 1, "media": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "In the second module, two core learning components, disentangled representation learning and semantic adversarial learning, are introduced to augment our analysis and meet the requirements of RD1 and RD2. Disentangled representation learning extracts intrinsic and interpretable attributes of traffic lights, such as colors, brightness, and background (Fig. 4b1 ). This component first provides a human-friendly data presentation, and also offers a data space where an adversarial generation can efficiently search. Semantic adversarial learning learns prediction behaviors of a detector, and generates meaningful adversarial examples (Fig. 4b2 ) on top of disentangled representation learning. After this, both acquired data (Fig. 4b3 ) and unseen data (Fig. 4b4 ) are passed to the detector to obtain detection results.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "Disentangledrepresentationandsemanticadversariallearning.", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The metric charts are coordinated with each other to filter data and support multi-faceted performance analysis for accuracy and robustness in other views. This is designed to support contextualized understanding of performance.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+bar", "axial_code": [], "componenet_code": ["bar", "table"]}, {"solution_text": "The metric charts are coordinated with each other to filter data and support multi-faceted performance analysis for accuracy and robustness in other views. This is designed to support contextualized understanding of performance.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 199, "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection", "pub_year": 2021, "domain": "autonomous driving", "requirement": {"requirement_text": "RP3: Injecting human intelligence for performance improvement with minimal human interaction. The ultimate goal of model evaluation and interpretation is to improve its performance with human knowledge in the loop [12]. Meanwhile, as we are working with domain experts, we found they have limited bandwidth to conduct in-depth exploration, but focus on key insights with few interactions [8]. This calls for maximazing insight generation and injection with minimal interaction.", "requirement_code": {"knowledge_injection": 1}}, "data": {"data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.", "data_code": {"clusters_and_sets_and_lists": 1, "media": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "In the second module, two core learning components, disentangled representation learning and semantic adversarial learning, are introduced to augment our analysis and meet the requirements of RD1 and RD2. Disentangled representation learning extracts intrinsic and interpretable attributes of traffic lights, such as colors, brightness, and background (Fig. 4b1 ). This component first provides a human-friendly data presentation, and also offers a data space where an adversarial generation can efficiently search. Semantic adversarial learning learns prediction behaviors of a detector, and generates meaningful adversarial examples (Fig. 4b2 ) on top of disentangled representation learning. After this, both acquired data (Fig. 4b3 ) and unseen data (Fig. 4b4 ) are passed to the detector to obtain detection results.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "Disentangledrepresentationandsemanticadversariallearning.", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Semantic interpretation by coordinating with TileScape. Two mechanisms are introduced to help users understand dimension seman- tics and also interpret their impact over performance.", "solution_category": "visualization", "solution_axial": "annotation", "solution_compoent": "matrix+image", "axial_code": [], "componenet_code": ["image", "matrix"]}]}, {"author": "zsz", "index_original": 200, "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection", "pub_year": 2021, "domain": "autonomous driving", "requirement": {"requirement_text": "RP3: Injecting human intelligence for performance improvement with minimal human interaction. The ultimate goal of model evaluation and interpretation is to improve its performance with human knowledge in the loop [12]. Meanwhile, as we are working with domain experts, we found they have limited bandwidth to conduct in-depth exploration, but focus on key insights with few interactions [8]. This calls for maximazing insight generation and injection with minimal interaction.", "requirement_code": {"knowledge_injection": 1}}, "data": {"data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.", "data_code": {"clusters_and_sets_and_lists": 1, "media": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "Finally, with minimal human interaction from visual interface, actionable insights are derived to generate more data that attempt to \u201clift distribution up\u201d via data augmentation (Fig. 4d ). This also enables us inject human intelligence to improve model accuracy and robustness, aiming at the requirement RP3.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}, {"solution_text": "Finally, with minimal human interaction from visual interface, actionable insights are derived to generate more data that attempt to \u201clift distribution up\u201d via data augmentation (Fig. 4d ). This also enables us inject human intelligence to improve model accuracy and robustness, aiming at the requirement RP3.", "solution_category": "data_manipulation", "solution_axial": "UserInput", "solution_compoent": "Generateactionableinsights,augmentdatadistribution.", "axial_code": ["UserInput"], "componenet_code": ["user_input"]}]}, {"author": "zsz", "index_original": 201, "paper_title": "Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images", "pub_year": 2021, "domain": "x-ray image classi\ufb01cation", "requirement": {"requirement_text": "T1. Analysis in Model Spaces: Investigate scienti\ufb01c images withinthe spaces of ACT, PRD, and FEA, for users to understand how theimages are modeled by the ResNet in the feature space and then classi-\ufb01ed by fully connected layers in the prediction space, with respect tothe real labels. Users can study ResNet model performance by com-paring the distributions of images after feature extraction (FEA), afterclassi\ufb01cation (PRD), and with actual labels (ACT). This study needs tobe performed in an exploratory process. Therefore, it is important tovisualize the images in the three spaces at the same time", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We utilize an open x-ray scattering dataset [36], and an updated ResNet model [33] that was designed for multiple attributes classification of the dataset. About 1,000 x-ray scattering images were employed in our visualization system. They include different types of images including semiconductors, nano-particles, polymer, lithographic gratings, and so on. The attributes in these images are either labeled by domain experts or synthetically generated by a simulation algorithm [33]. Each image thus has an actual attribute vector (ACT vector) consisting of 17 Boolean (0 or 1) values to show if the image has a number of the 17 attributes.", "data_code": {"clusters_and_sets_and_lists": 1, "categorical": 1, "media": 1, "tables": 1}}, "solution": [{"solution_text": "The goal is to allow users to interactively select, compare, and study images of interest. Therefore, the 2048-dimensional FEA space and 17-dimensional ACT and PRD spaces are projected to the embedded 2D spaces to fulfill the goal. In our system, we have included two commonly used dimension reduction (DR) algorithms, t-SNE and PCA, for deep learning visualization. Other DR methods may further be added.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "Coordinated Visualization in Embedded Spaces: Images are visualized in the 2D canvases of ACT, PRD and FEA spaces, respectively. The goal is to allow users to easily observe many x-ray images and their relationships in these spaces simultaneously.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}]}, {"author": "zsz", "index_original": 202, "paper_title": "Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images", "pub_year": 2021, "domain": "x-ray image classi\ufb01cation", "requirement": {"requirement_text": "T2. Analysis with Group Behaviors: Select and examine speci\ufb01cgroups of images in the ACT, PRD, and FEA spaces, in order to \ufb01ndimportant clusters and outliers with respect to the learning model.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "We utilize an open x-ray scattering dataset [36], and an updated ResNet model [33] that was designed for multiple attributes classification of the dataset. About 1,000 x-ray scattering images were employed in our visualization system. They include different types of images including semiconductors, nano-particles, polymer, lithographic gratings, and so on. The attributes in these images are either labeled by domain experts or synthetically generated by a simulation algorithm [33]. Each image thus has an actual attribute vector (ACT vector) consisting of 17 Boolean (0 or 1) values to show if the image has a number of the 17 attributes.", "data_code": {"clusters_and_sets_and_lists": 1, "categorical": 1, "tables": 1}}, "solution": [{"solution_text": "Image Group Selection and Visualization: Within the embedded spaces, users are enabled to flexibly select images into groups at each embedded space by lasso tools. Then the selected images in each group are visually highlighted in other spaces. This function is very important for users to freely explore images of interest and conduct comparative analysis among the three spaces. The grouped images are also visualized by a statistic view of image metrics and an image gallery view. They can be further clustered for drill-down study and comparison.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "radialbarchart", "axial_code": [], "componenet_code": ["radialbar"]}, {"solution_text": "Image Group Selection and Visualization: Within the embedded spaces, users are enabled to flexibly select images into groups at each embedded space by lasso tools. Then the selected images in each group are visually highlighted in other spaces. This function is very important for users to freely explore images of interest and conduct comparative analysis among the three spaces. The grouped images are also visualized by a statistic view of image metrics and an image gallery view. They can be further clustered for drill-down study and comparison.", "solution_category": "interaction", "solution_axial": "Selecting,Participation/Collaboration", "solution_compoent": "", "axial_code": ["Selecting", "Participation/Collaboration"], "componenet_code": ["selecting", "participation_collaboration"]}]}, {"author": "zsz", "index_original": 203, "paper_title": "Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images", "pub_year": 2021, "domain": "x-ray image classi\ufb01cation", "requirement": {"requirement_text": "T3. Analysis with Image Attributes: Identify important image in-stances with the performance metrics of individual attributes and co-existent attributes to perform the \ufb01rst two tasks.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "We utilize an open x-ray scattering dataset [36], and an updated ResNet model [33] that was designed for multiple attributes classification of the dataset. About 1,000 x-ray scattering images were employed in our visualization system. They include different types of images including semiconductors, nano-particles, polymer, lithographic gratings, and so on. The attributes in these images are either labeled by domain experts or synthetically generated by a simulation algorithm [33]. Each image thus has an actual attribute vector (ACT vector) consisting of 17 Boolean (0 or 1) values to show if the image has a number of the 17 attributes.", "data_code": {"clusters_and_sets_and_lists": 1, "categorical": 1, "tables": 1}}, "solution": [{"solution_text": "Attribute Co-existence Visualization: The model perfor- mance with the relations of co-existing attributes is visualized in an interactive view. Users can then define image groups based on the visual cues of model performance.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table", "axial_code": [], "componenet_code": ["table"]}]}, {"author": "zsz", "index_original": 204, "paper_title": "Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images", "pub_year": 2021, "domain": "x-ray image classi\ufb01cation", "requirement": {"requirement_text": "T4. Analysis with Comparisons: Compare individual images andimage clusters for the model prediction performance.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "We utilize an open x-ray scattering dataset [36], and an updated ResNet model [33] that was designed for multiple attributes classification of the dataset. About 1,000 x-ray scattering images were employed in our visualization system. They include different types of images including semiconductors, nano-particles, polymer, lithographic gratings, and so on. The attributes in these images are either labeled by domain experts or synthetically generated by a simulation algorithm [33]. Each image thus has an actual attribute vector (ACT vector) consisting of 17 Boolean (0 or 1) values to show if the image has a number of the 17 attributes.", "data_code": {"clusters_and_sets_and_lists": 1, "categorical": 1, "media": 1, "tables": 1}}, "solution": [{"solution_text": "Group Comparative View and Image Comparison: The selected groups can be easily investigated and compared with group panels for detailed views. Through interactive selection over all the above visualizations, users can also open multiple images to compare their details of raw data and model predictions.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "image+matrix", "axial_code": [], "componenet_code": ["image", "matrix"]}, {"solution_text": "Group Comparative View and Image Comparison: The selected groups can be easily investigated and compared with group panels for detailed views. Through interactive selection over all the above visualizations, users can also open multiple images to compare their details of raw data and model predictions.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 221, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T1 Detect Outbreak. Is there an outbreak? When the number ofinfected patients rises above a normal level within a certain period,i.e., the endemic level, an outbreak occurs. Depending on thepathogen, this endemic level can be two or more patients. Aspatients may not be tested or screened generally, an outbreak isdetermined by manual inspection.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Epidemic Curve View shows the number of infected persons per day in order to support Task 1\u2013 outbreak detection. The infection control expert can select the total number of infections or only new ones (i.e., without copystrains). To see how it relates to the endemic level, a moving average of the user-selected time period is shown. This view can show data for the hospital or specific wards. It supports longer time periods via focus-and-context methods inspired by [77].", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "Epidemic Curve View shows the number of infected persons per day in order to support Task 1\u2013 outbreak detection. The infection control expert can select the total number of infections or only new ones (i.e., without copystrains). To see how it relates to the endemic level, a moving average of the user-selected time period is shown. This view can show data for the hospital or specific wards. It supports longer time periods via focus-and-context methods inspired by [77].", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 222, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.1 Determine transmission contacts. Did contacts occur be-tween patients that could have led to pathogen transmis-sion? If yes, where, when and with whom?", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Patient lines pass close to each other for every potential contact.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "area+line", "axial_code": [], "componenet_code": ["line", "area"]}]}, {"author": "zsz", "index_original": 223, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.1 Determine transmission contacts. Did contacts occur be-tween patients that could have led to pathogen transmis-sion? If yes, where, when and with whom?", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The circle color encodes the type of relevant contact. We show all possible transmission events \u2013 i.e., several flashback lines.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "area+line", "axial_code": [], "componenet_code": ["line", "area"]}]}, {"author": "zsz", "index_original": 224, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.1 Determine transmission contacts. Did contacts occur be-tween patients that could have led to pathogen transmis-sion? If yes, where, when and with whom?", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We propose a modified storyline [46] layout algorithm to support our tasks. While typical approaches to storyline drawing optimize the number of edge crossings and minimize bends, we have the constraint of patient locations, including ward (Task 3), and patient contacts (Task 2). Fast layout is required for interactive exploration, as sets of patients in the view can change when filters are applied, and data sets are loaded. Thus, we prioritize runtime over crossing optimization and minimizing bends (see Sect. 8). We build upon existing layouts and combine them and adapt them for our purposes.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "area+line", "axial_code": [], "componenet_code": ["line", "area"]}]}, {"author": "zsz", "index_original": 225, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.1 Determine transmission contacts. Did contacts occur be-tween patients that could have led to pathogen transmis-sion? If yes, where, when and with whom?", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The Transmission Pathway View shows patient infection status and contacts over time and across locations (Fig. 1 (3)). Outbreak duration, potential transmission contacts between patients, and patient locations are visible (Task 2\u20134). Each line represents a patient and the x-axis encodes time (Task 4). Each patient line starts with the earliest recorded admission to hospital and ends with the last recorded stay. Temporary home stays are also shown. This helps in detecting hospital-associated transmissions during previous stays (Task 3.1). Patient lines pass close to each other for every potential contact (Task 2.1). The y-axis encodes patient location. Fixed vertical positions for individual wards (as in Baling et al. [8]) is not scalable due to the larger number of wards and patients (hundreds), but our layout still aims at preserving the vertical position of wards [4]. Line color conveys infection status, and background color is used to encode location information.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "area+line", "axial_code": [], "componenet_code": ["line", "area"]}]}, {"author": "zsz", "index_original": 226, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.1 Determine transmission contacts. Did contacts occur be-tween patients that could have led to pathogen transmis-sion? If yes, where, when and with whom?", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Finalization Lines are ordered from top to bottom according to the order patients entered the ward (see Fig. 4e). Even though this may cause more edge crossings, this order helps support Task 2: tracing transmissions.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "zsz", "index_original": 227, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.1 Determine transmission contacts. Did contacts occur be-tween patients that could have led to pathogen transmis-sion? If yes, where, when and with whom?", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Infection status is encoded using color (see Fig. 5 left) in order to help in tracing transmissions (Task 2). Differentiating between \u2018unknown\u2019 and \u2018unknown-will be infected\u2019 helps infection control experts track patients over long time periods, reducing the requirement to pan and zoom. Details on microbiological data are shown on demand through a tooltip. The contact location (Task 3), specifically the ward, is shown on demand by a colored background hull.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "area+line", "axial_code": [], "componenet_code": ["line", "area"]}, {"solution_text": "Details on microbiological data are shown on demand through a tooltip.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 228, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.1 Determine transmission contacts. Did contacts occur be-tween patients that could have led to pathogen transmis-sion? If yes, where, when and with whom?", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Using a process inspired by [31,68], patient lines are drawn smoothly. As short periods of contact can lead to transmission, wiggles in the line indicate new contacts. We emphasize contacts between patients on the same ward by minimizing the line width for vertical lines.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "zsz", "index_original": 229, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.1 Determine transmission contacts. Did contacts occur be-tween patients that could have led to pathogen transmis-sion? If yes, where, when and with whom?", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Our interactive approach supports 1) backward tracing \u2013 finding transmission events and patients in the past that could have infected a selected patient. The interaction enables the search for an index patient, i.e., patient zero (Task 2).", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 230, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.2 Determine index patient (patient zero). Who is the source ofthe outbreak? Identifying the transmission pathway shouldlead back to the index patient, i.e., the patient zero.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We propose a modified storyline [46] layout algorithm to support our tasks. While typical approaches to storyline drawing optimize the number of edge crossings and minimize bends, we have the constraint of patient locations, including ward (Task 3), and patient contacts (Task 2). Fast layout is required for interactive exploration, as sets of patients in the view can change when filters are applied, and data sets are loaded. Thus, we prioritize runtime over crossing optimization and minimizing bends (see Sect. 8). We build upon existing layouts and combine them and adapt them for our purposes.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "area+line", "axial_code": [], "componenet_code": ["line", "area"]}]}, {"author": "zsz", "index_original": 231, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.2 Determine index patient (patient zero). Who is the source ofthe outbreak? Identifying the transmission pathway shouldlead back to the index patient, i.e., the patient zero.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The Transmission Pathway View shows patient infection status and contacts over time and across locations (Fig. 1 (3)). Outbreak duration, potential transmission contacts between patients, and patient locations are visible (Task 2\u20134). Each line represents a patient and the x-axis encodes time (Task 4). Each patient line starts with the earliest recorded admission to hospital and ends with the last recorded stay. Temporary home stays are also shown. This helps in detecting hospital-associated transmissions during previous stays (Task 3.1). Patient lines pass close to each other for every potential contact (Task 2.1). The y-axis encodes patient location. Fixed vertical positions for individual wards (as in Baling et al. [8]) is not scalable due to the larger number of wards and patients (hundreds), but our layout still aims at preserving the vertical position of wards [4]. Line color conveys infection status, and background color is used to encode location information.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "area+line", "axial_code": [], "componenet_code": ["line", "area"]}]}, {"author": "zsz", "index_original": 232, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.2 Determine index patient (patient zero). Who is the source ofthe outbreak? Identifying the transmission pathway shouldlead back to the index patient, i.e., the patient zero.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Finalization Lines are ordered from top to bottom according to the order patients entered the ward (see Fig. 4e). Even though this may cause more edge crossings, this order helps support Task 2: tracing transmissions.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "zsz", "index_original": 233, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.2 Determine index patient (patient zero). Who is the source ofthe outbreak? Identifying the transmission pathway shouldlead back to the index patient, i.e., the patient zero.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Infection status is encoded using color (see Fig. 5 left) in order to help in tracing transmissions (Task 2). Differentiating between \u2018unknown\u2019 and \u2018unknown-will be infected\u2019 helps infection control experts track patients over long time periods, reducing the requirement to pan and zoom. Details on microbiological data are shown on demand through a tooltip. The contact location (Task 3), specifically the ward, is shown on demand by a colored background hull.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "area+line", "axial_code": [], "componenet_code": ["line", "area"]}, {"solution_text": "Details on microbiological data are shown on demand through a tooltip.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 234, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.2 Determine index patient (patient zero). Who is the source ofthe outbreak? Identifying the transmission pathway shouldlead back to the index patient, i.e., the patient zero.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Using a process inspired by [31,68], patient lines are drawn smoothly. As short periods of contact can lead to transmission, wiggles in the line indicate new contacts. We emphasize contacts between patients on the same ward by minimizing the line width for vertical lines.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "zsz", "index_original": 235, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.2 Determine index patient (patient zero). Who is the source ofthe outbreak? Identifying the transmission pathway shouldlead back to the index patient, i.e., the patient zero.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Our interactive approach supports 1) backward tracing \u2013 finding transmission events and patients in the past that could have infected a selected patient. The interaction enables the search for an index patient, i.e., patient zero (Task 2).", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 236, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.3 Distinguish between a single or multiple outbreaks. Is the observed outbreak a single outbreak or multiple, simulta_x0002_neous outbreaks of similar pathogens? Depending on the answer, there could be one or more index patients.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We propose a modified storyline [46] layout algorithm to support our tasks. While typical approaches to storyline drawing optimize the number of edge crossings and minimize bends, we have the constraint of patient locations, including ward (Task 3), and patient contacts (Task 2). Fast layout is required for interactive exploration, as sets of patients in the view can change when filters are applied, and data sets are loaded. Thus, we prioritize runtime over crossing optimization and minimizing bends (see Sect. 8). We build upon existing layouts and combine them and adapt them for our purposes.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "area+line", "axial_code": [], "componenet_code": ["line", "area"]}]}, {"author": "zsz", "index_original": 237, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.3 Distinguish between a single or multiple outbreaks. Is the observed outbreak a single outbreak or multiple, simulta_x0002_neous outbreaks of similar pathogens? Depending on the answer, there could be one or more index patients.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The Transmission Pathway View shows patient infection status and contacts over time and across locations (Fig. 1 (3)). Outbreak duration, potential transmission contacts between patients, and patient locations are visible (Task 2\u20134). Each line represents a patient and the x-axis encodes time (Task 4). Each patient line starts with the earliest recorded admission to hospital and ends with the last recorded stay. Temporary home stays are also shown. This helps in detecting hospital-associated transmissions during previous stays (Task 3.1). Patient lines pass close to each other for every potential contact (Task 2.1). The y-axis encodes patient location. Fixed vertical positions for individual wards (as in Baling et al. [8]) is not scalable due to the larger number of wards and patients (hundreds), but our layout still aims at preserving the vertical position of wards [4]. Line color conveys infection status, and background color is used to encode location information.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "area+line", "axial_code": [], "componenet_code": ["line", "area"]}]}, {"author": "zsz", "index_original": 238, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.3 Distinguish between a single or multiple outbreaks. Is the observed outbreak a single outbreak or multiple, simulta_x0002_neous outbreaks of similar pathogens? Depending on the answer, there could be one or more index patients.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Finalization Lines are ordered from top to bottom according to the order patients entered the ward (see Fig. 4e). Even though this may cause more edge crossings, this order helps support Task 2: tracing transmissions.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "zsz", "index_original": 239, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.3 Distinguish between a single or multiple outbreaks. Is the observed outbreak a single outbreak or multiple, simulta_x0002_neous outbreaks of similar pathogens? Depending on the answer, there could be one or more index patients.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Infection status is encoded using color (see Fig. 5 left) in order to help in tracing transmissions (Task 2). Differentiating between \u2018unknown\u2019 and \u2018unknown-will be infected\u2019 helps infection control experts track patients over long time periods, reducing the requirement to pan and zoom. Details on microbiological data are shown on demand through a tooltip. The contact location (Task 3), specifically the ward, is shown on demand by a colored background hull.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "area+line", "axial_code": [], "componenet_code": ["line", "area"]}, {"solution_text": "Details on microbiological data are shown on demand through a tooltip.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 240, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.3 Distinguish between a single or multiple outbreaks. Is the observed outbreak a single outbreak or multiple, simulta_x0002_neous outbreaks of similar pathogens? Depending on the answer, there could be one or more index patients.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Using a process inspired by [31,68], patient lines are drawn smoothly. As short periods of contact can lead to transmission, wiggles in the line indicate new contacts. We emphasize contacts between patients on the same ward by minimizing the line width for vertical lines.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "zsz", "index_original": 241, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T2.3 Distinguish between a single or multiple outbreaks. Is the observed outbreak a single outbreak or multiple, simulta_x0002_neous outbreaks of similar pathogens? Depending on the answer, there could be one or more index patients.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Our interactive approach supports 1) backward tracing \u2013 finding transmission events and patients in the past that could have infected a selected patient. The interaction enables the search for an index patient, i.e., patient zero (Task 2).", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 242, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T3.1 Determine if the outbreak is hospital-associated.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Each patient line starts with the earliest recorded admission to hospital and ends with the last recorded stay. Temporary home stays are also shown. This helps in detecting hospital-associated transmissions during previous stays.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "zsz", "index_original": 243, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T3.1 Determine if the outbreak is hospital-associated.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "When optimizing the layout with the force-directed algorithm, we use a constraint-based approach [19] to enforce these locations and use additional forces to encourage the desired properties of the layout. The first constraint preserves the temporal order of movements along the x-axis. The second constraint restricts movement outside y-axis areas for the three location types.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "zsz", "index_original": 244, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T3.1 Determine if the outbreak is hospital-associated.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We propose a modified storyline [46] layout algorithm to support our tasks. While typical approaches to storyline drawing optimize the number of edge crossings and minimize bends, we have the constraint of patient locations, including ward (Task 3), and patient contacts (Task 2). Fast layout is required for interactive exploration, as sets of patients in the view can change when filters are applied, and data sets are loaded. Thus, we prioritize runtime over crossing optimization and minimizing bends (see Sect. 8). We build upon existing layouts and combine them and adapt them for our purposes.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "area+line", "axial_code": [], "componenet_code": ["line", "area"]}]}, {"author": "zsz", "index_original": 245, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T3.1 Determine if the outbreak is hospital-associated.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Infection status is encoded using color (see Fig. 5 left) in order to help in tracing transmissions (Task 2). Differentiating between \u2018unknown\u2019 and \u2018unknown-will be infected\u2019 helps infection control experts track patients over long time periods, reducing the requirement to pan and zoom. Details on microbiological data are shown on demand through a tooltip. The contact location (Task 3), specifically the ward, is shown on demand by a colored background hull.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "area+line", "axial_code": [], "componenet_code": ["line", "area"]}, {"solution_text": "Details on microbiological data are shown on demand through a tooltip.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 246, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T3.1 Determine if the outbreak is hospital-associated.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We assume the starting time moment of the contact interval (ts,te) where tt = ts as the transmission event time, Lt is transmission location (Task 3) and Pt is transmission contact. Note, P can have several potential transmis_x0002_sion events \u2013 different persons and locations and different times. This analysis is repeated, especially for contacts with unknown infection status, as they could be potentially infected if they had contact to an infected patient before. These critical contacts are computed using a constrained path search in the DAG used for the layout. There is currently no bound on how far back or forward in time the transmission events are searched, but user-specified bounds could be implemented, depending on the specifics of the investigated pathogen.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "Useconstrainedpathsearchtofindcriticalcontacts.", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "zsz", "index_original": 247, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T3.2 Locate ward(s) with pathogen transmissions.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "Additional forces are used to maximize the stability of the y position of a patient during a stay in a ward to help represent contact location.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "zsz", "index_original": 248, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T3.2 Locate ward(s) with pathogen transmissions.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We propose a modified storyline [46] layout algorithm to support our tasks. While typical approaches to storyline drawing optimize the number of edge crossings and minimize bends, we have the constraint of patient locations, including ward (Task 3), and patient contacts (Task 2). Fast layout is required for interactive exploration, as sets of patients in the view can change when filters are applied, and data sets are loaded. Thus, we prioritize runtime over crossing optimization and minimizing bends (see Sect. 8). We build upon existing layouts and combine them and adapt them for our purposes.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "area+line", "axial_code": [], "componenet_code": ["line", "area"]}]}, {"author": "zsz", "index_original": 249, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T3.2 Locate ward(s) with pathogen transmissions.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Infection status is encoded using color (see Fig. 5 left) in order to help in tracing transmissions (Task 2). Differentiating between \u2018unknown\u2019 and \u2018unknown-will be infected\u2019 helps infection control experts track patients over long time periods, reducing the requirement to pan and zoom. Details on microbiological data are shown on demand through a tooltip. The contact location (Task 3), specifically the ward, is shown on demand by a colored background hull.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "area+line", "axial_code": [], "componenet_code": ["line", "area"]}, {"solution_text": "Details on microbiological data are shown on demand through a tooltip.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 250, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T3.2 Locate ward(s) with pathogen transmissions.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We assume the starting time moment of the contact interval (ts,te) where tt = ts as the transmission event time, Lt is transmission location (Task 3) and Pt is transmission contact. Note, P can have several potential transmis_x0002_sion events \u2013 different persons and locations and different times. This analysis is repeated, especially for contacts with unknown infection status, as they could be potentially infected if they had contact to an infected patient before. These critical contacts are computed using a constrained path search in the DAG used for the layout. There is currently no bound on how far back or forward in time the transmission events are searched, but user-specified bounds could be implemented, depending on the specifics of the investigated pathogen.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "Useconstrainedpathsearchtofindcriticalcontacts.", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "zsz", "index_original": 251, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T4 Quantify outbreak duration.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Each line represents a patient and the x-axis encodes time (Task 4). Each patient line starts with the earliest recorded admission to hospital and ends with the last recorded stay. Temporary home stays are also shown.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "zsz", "index_original": 252, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T4 Quantify outbreak duration.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Patients that were in the ward for longer periods of time are more likely to be involved in transmission events. Finally, individual nodes are placed at the precise time of their events along the x-axis, which is important for determining outbreak duration and time of possible pathogen transmissions.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "zsz", "index_original": 253, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T4 Quantify outbreak duration.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We show all possible transmission events \u2013 i.e., several flashback lines. Connections indicate the length of time the potential infection was not detected by the screening or testing.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "area+line", "axial_code": [], "componenet_code": ["line", "area"]}]}, {"author": "zsz", "index_original": 254, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T5 Identify potentially infected patients.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "Contact Network View shows the contacts of selected patients for determining putative infected patients.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "network", "axial_code": [], "componenet_code": ["network"]}]}, {"author": "zsz", "index_original": 255, "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals", "pub_year": 2021, "domain": "infection control", "requirement": {"requirement_text": "T5 Identify potentially infected patients.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Our interactive approach supports forward tracing \u2013 finding patients that could be infected by a selected infected patient at a later point in time, i.e., putative infected patients (Task 5).", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 256, "paper_title": "A Visual Analytics Framework for Contrastive Network Analysis", "pub_year": 2020, "domain": "Network analysis", "requirement": {"requirement_text": "DC1: Support the discovery of whether a target network isunique compared to a background network, and which part of thenetwork relates to the uniqueness. The uniqueness of a target datasetrelative to the base is embedded in the contrastive representation YTgenerated by CL-based representation learning methods, includingcNRL. Many previous works attempted to display this data to revealthe uniqueness [2, 3, 27]. However, because YT only contains theinformation of the target network G T , reviewing only YT is notsufficient to understand how well the CL method finds uniqueness.Also, it is difficult to identify which data points (i.e., network nodesin our case) highly relate to the found uniqueness. The visual an-alytics framework should support discovering the uniqueness andthe associated nodes by presenting the information in both the targetand background networks", "requirement_code": {"compare_entities": 1, "explain_differences": 1}}, "data": {"data_text": "Real_x0002_world networks; We generated random networks (Random 1, 2) with Gilbert\u2019s random graph [10] and scale-free networks (Price 1, 2) with the Price\u2019s preferential attachment mod_x0002_els [73], as well as used several public datasets.", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "NRL with DeepGL. Using DeepGL [77] as NRL, i-cNRL gener_x0002_ates feature matrices XT and XB with interpretable network features. The features consist of the base feature x and relational function f. CL with cPCA. From the target and background feature matrices XT and XB, cPCA [2] produces contrastive principal components (cPCs), which are analogous to principal components (PCs) in ordi_x0002_nary PCA [54]. cPCs are low-dimensional representative directions in which XT has high variance but XB has low variance. That is, YT , an embedding of XT with cPCs, depicts unique characteristics (with the consideration of variance) of a target network GT relative to a background network GB.", "solution_category": "data_manipulation", "solution_axial": "Modeling,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Modeling", "DimensionalityReduction"], "componenet_code": ["modeling", "dimensionality_reduction"]}, {"solution_text": "With the results generated by i-cNRL, the first step of our analysis workflow, ContraNA\u2019s contrastive representation view visualizes the results to reveal whether or not there is uniqueness in the target network compared to the background network, serving as the following step.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "network", "axial_code": [], "componenet_code": ["network"]}]}, {"author": "zsz", "index_original": 257, "paper_title": "A Visual Analytics Framework for Contrastive Network Analysis", "pub_year": 2020, "domain": "Network analysis", "requirement": {"requirement_text": "DC2: Enhance the interpretability of the features learned byNRL and the cPCs generated by CL. Investigating the relationshipsamong the network features, cPCs, and the representation YT is im-portant to interpret the uniqueness of G T . While i-cNRL is designedto provide interpretable network features and cPCs, understandingthem from i-cNRL\u2019s direct outputs is not straightforward. For exam-ple, DeepGL could generate a sophisticated relational function suchas (\u03a6+sum \u25e6 \u03a6max \u25e6 \u03a6\u2212mean)(x). Moreover, examining cPC loadingsfor each feature would be time-consuming when DeepGL producesmany network features. The framework should provide visualiza-tions to facilitate easy understanding of the above information.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "Real_x0002_world networks; We generated random networks (Random 1, 2) with Gilbert\u2019s random graph [10] and scale-free networks (Price 1, 2) with the Price\u2019s preferential attachment mod_x0002_els [73], as well as used several public datasets.", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "NRL with DeepGL. Using DeepGL [77] as NRL, i-cNRL gener_x0002_ates feature matrices XT and XB with interpretable network features. The features consist of the base feature x and relational function f. CL with cPCA. From the target and background feature matrices XT and XB, cPCA [2] produces contrastive principal components (cPCs), which are analogous to principal components (PCs) in ordi_x0002_nary PCA [54]. cPCs are low-dimensional representative directions in which XT has high variance but XB has low variance. That is, YT , an embedding of XT with cPCs, depicts unique characteristics (with the consideration of variance) of a target network GT relative to a background network GB.", "solution_category": "data_manipulation", "solution_axial": "Modeling,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Modeling", "DimensionalityReduction"], "componenet_code": ["modeling", "dimensionality_reduction"]}, {"solution_text": "With the above observation from the contrastive representation view, we move on to interpret the network features and cPCs with the feature contribution view.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "table+matrix", "axial_code": [], "componenet_code": ["matrix", "table"]}]}, {"author": "zsz", "index_original": 258, "paper_title": "A Visual Analytics Framework for Contrastive Network Analysis", "pub_year": 2020, "domain": "Network analysis", "requirement": {"requirement_text": "DC3: Offer intuitiveness in understanding a target network\u2019suniqueness by relating it to common network visualizations. Thecontrastive representation YT generated could contain complicatedpatterns that are dif\ufb01cult to understand. Thus, it is not intuitiveenough to just view these patterns directly based on the i-cNRLresults in the embedding space. To help analyze such patterns, theframework should provide links between the results of i-cNRL andcommonly used visualizations for network analysis, such as laid-outnetworks and probability distributions of network centralities.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Real_x0002_world networks; We generated random networks (Random 1, 2) with Gilbert\u2019s random graph [10] and scale-free networks (Price 1, 2) with the Price\u2019s preferential attachment mod_x0002_els [73], as well as used several public datasets.", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "NRL with DeepGL. Using DeepGL [77] as NRL, i-cNRL gener_x0002_ates feature matrices XT and XB with interpretable network features. The features consist of the base feature x and relational function f. CL with cPCA. From the target and background feature matrices XT and XB, cPCA [2] produces contrastive principal components (cPCs), which are analogous to principal components (PCs) in ordi_x0002_nary PCA [54]. cPCs are low-dimensional representative directions in which XT has high variance but XB has low variance. That is, YT , an embedding of XT with cPCs, depicts unique characteristics (with the consideration of variance) of a target network GT relative to a background network GB.", "solution_category": "data_manipulation", "solution_axial": "Modeling,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Modeling", "DimensionalityReduction"], "componenet_code": ["modeling", "dimensionality_reduction"]}, {"solution_text": "With above results, we further analyze the uniqueness by relating F8 to common network visualizations. ContraNA pro- vides two perspectives for network analysis (DC3-Intuitiveness): probability distributions and laid-out networks [10]. Probability distributions are often used to compare the distributions of target and background networks\u2019 centralities (e.g., whether the degree distribu- tion follows the power law [10]), and laid-out networks are helpful for viewing the topological differences (e.g., whether multiple com- munities exist).", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "line+network", "axial_code": [], "componenet_code": ["network", "line"]}]}, {"author": "zsz", "index_original": 259, "paper_title": "A Visual Analytics Framework for Contrastive Network Analysis", "pub_year": 2020, "domain": "Network analysis", "requirement": {"requirement_text": "DC4: Provide the \ufb02exibility to interactively adjust the i-cNRLparameters to generate results based on the analysts\u2019 interest. Theresults of i-cNRL heavily depend on the parameters used for eachembedding step. For example, changing a value of the contrastparameter \u03b1 might reveal different unique characteristics in G T . Foranalysts with advanced knowledge on NRL and CL, the frameworkshould provide abilities for interactively tuning the i-cNRL resultsbased on their needs.", "requirement_code": {"parameter_setting": 1}}, "data": {"data_text": "Real_x0002_world networks; We generated random networks (Random 1, 2) with Gilbert\u2019s random graph [10] and scale-free networks (Price 1, 2) with the Price\u2019s preferential attachment mod_x0002_els [73], as well as used several public datasets.", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "NRL with DeepGL. Using DeepGL [77] as NRL, i-cNRL gener_x0002_ates feature matrices XT and XB with interpretable network features. The features consist of the base feature x and relational function f. CL with cPCA. From the target and background feature matrices XT and XB, cPCA [2] produces contrastive principal components (cPCs), which are analogous to principal components (PCs) in ordi_x0002_nary PCA [54]. cPCs are low-dimensional representative directions in which XT has high variance but XB has low variance. That is, YT , an embedding of XT with cPCs, depicts unique characteristics (with the consideration of variance) of a target network GT relative to a background network GB.", "solution_category": "data_manipulation", "solution_axial": "Modeling,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Modeling", "DimensionalityReduction"], "componenet_code": ["modeling", "dimensionality_reduction"]}, {"solution_text": "With the results generated by i-cNRL, the first step of our analysis workflow, ContraNA\u2019s contrastive representation view visualizes the results to reveal whether or not there is uniqueness in the target network compared to the background network, serving as the following step.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "network", "axial_code": [], "componenet_code": ["network"]}, {"solution_text": "The cPCA used in i-cNRL automatically selects the contrastive pa- rameter \u03b1and computes cPCs to generate the optimized contrastive representations, i.e., maximizing the variation in XT while simul- taneously minimizing the variation in XB [36]. However, the analyst may want to loosen or strengthen the reduction of the variation of XB in order to elucidate the found patterns or discover different patterns. Also, the resultant cPCs might not apt to interpret visually found patterns. To handle such cases, ContraNA supports interactive adjustments of \u03b1and cPCs (DC4-Flexibility).", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 260, "paper_title": "ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data", "pub_year": 2020, "domain": "Time-series analysis", "requirement": {"requirement_text": "DR1: Provide an overview of concept drift occurrences overtime. The drift occurrences over time can help analysts identifythe interesting time segment. When the analysts\u2019 preferences areunknown, interactive and hierarchical exploration of occurrencesalong time intervals is needed [34, 46].", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We employ the air pollutant data [55] from four nationally-controlled air-quality monitoring sites in Beijing, which was collected every hour from March 1st, 2013 to February 28th, 2017 (34,536 data records per site). 22 meteorology-related dimen_x0002_sions are applied to predict if the air quality index (AQI) is higher than 100 (i.e., worse than mild pollution) after 24 hours. The employed dataset contains player records from three servers (647,800 player records from Server17, 702,125 player records from Server164, and 585,048 player records from Server230) of a MMORPG from August 16th, 2013 to January 19th, 2014. Three servers were started at different timestamps: Server17, Sever164, and Server230, which are in order of time, that is, players on different servers register for the game at different time periods. For each player, 21 attributes, like equipment (i.e., the combat effectiveness score of the player\u2019s equipment), practice (i.e., the level of practice, improved by learning and improving skills and finishing tasks), are recorded every day. The consumption records for the upcoming week of players form a group of time-series.", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The timeline navigator view presents the en- tire timeline and the indices of concept drifts from multiple data sources. Each row corresponds to a data source. ConceptExplorer assigns a unique color to each data source. Due to the limited horizontal space, the distribution of concept drifts may be dense. ConceptExplorer employs a \u201c\u00d7\u201d to mark a concept drift, which can highlight the specific moment by its intersection. Time segments, in which the drift level exceeds a certain value (initialized as the warning level, namely, 2) are highlighted by \u201c\u2212\u201d. These marks indicate various patterns along the timeline, like dense occurrences, outliers, inconsistency with other data sources, periodicity.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "zsz", "index_original": 261, "paper_title": "ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data", "pub_year": 2020, "domain": "Time-series analysis", "requirement": {"requirement_text": "DR2: Integrate features of concept drifts from the predictionmodels. Concept drifts hinder existing prediction models (the modeltrained from historical data) from accurate predictions in new envi-ronments. The accuracy fluctuation of the prediction model indicatesthe occurrence of concept drifts [7, 30, 42, 52, 54]. In addition, pa-rameters of prediction models can reflect the relationship betweendifferent inputs and the label, that is, the model\u2019s understanding ofthe concept [9]", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "We employ the air pollutant data [55] from four nationally-controlled air-quality monitoring sites in Beijing, which was collected every hour from March 1st, 2013 to February 28th, 2017 (34,536 data records per site). 22 meteorology-related dimen_x0002_sions are applied to predict if the air quality index (AQI) is higher than 100 (i.e., worse than mild pollution) after 24 hours. The employed dataset contains player records from three servers (647,800 player records from Server17, 702,125 player records from Server164, and 585,048 player records from Server230) of a MMORPG from August 16th, 2013 to January 19th, 2014. Three servers were started at different timestamps: Server17, Sever164, and Server230, which are in order of time, that is, players on different servers register for the game at different time periods. For each player, 21 attributes, like equipment (i.e., the combat effectiveness score of the player\u2019s equipment), practice (i.e., the level of practice, improved by learning and improving skills and finishing tasks), are recorded every day. The consumption records for the upcoming week of players form a group of time-series.", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "5.2 The Prediction Model ViewThe prediction model view (Figure 1(c)) supports DR2.5.2.1 The Accuracy Fluctuation ChartThe line charts on the left (Figure 1(c)) show the accuracy fluctuationof the prediction models trained by the data from each data source.The occurrences of concept drifts are labeled by \u201c\u00d7\u201d, which isthe same as that in the timeline navigator view. In addition, themoments with warnings are encoded by hollow dots. To explainconcept drift detection, the accuracy fluctuation chart visualizes themagnitude of the accuracy drop of the time segments whose driftlevels are above the warning level (Figure 4(a)). Different datasources may issue drift warnings at similar time segments. To avoidmisunderstandings caused by overlaps, shifted stripes are employedto highlight warning time segments (Figure 4(b)). It can be seenthat even when the warning segments of different data sources arestaggered, the start and end moments of different time segmentscan be clearly distinguished. Vertical stripes are used because theycan emphasize the height, that is, the magnitude of accuracy drops.The results from the consistency judgment model are also shownin the accuracy fluctuation chart. If a concept drift is detectedduring a time segment that is not included by the result from theconsistency judgment model, we emphasize them by a triangle markto distinguish from circles representing others (Figure 4(c))", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "5.2 The Prediction Model ViewThe prediction model view (Figure 1(c)) supports DR2.5.2.1 The Accuracy Fluctuation ChartThe line charts on the left (Figure 1(c)) show the accuracy fluctuationof the prediction models trained by the data from each data source.The occurrences of concept drifts are labeled by \u201c\u00d7\u201d, which isthe same as that in the timeline navigator view. In addition, themoments with warnings are encoded by hollow dots. To explainconcept drift detection, the accuracy fluctuation chart visualizes themagnitude of the accuracy drop of the time segments whose driftlevels are above the warning level (Figure 4(a)). Different datasources may issue drift warnings at similar time segments. To avoidmisunderstandings caused by overlaps, shifted stripes are employedto highlight warning time segments (Figure 4(b)). It can be seenthat even when the warning segments of different data sources arestaggered, the start and end moments of different time segmentscan be clearly distinguished. Vertical stripes are used because theycan emphasize the height, that is, the magnitude of accuracy drops.The results from the consistency judgment model are also shownin the accuracy fluctuation chart. If a concept drift is detectedduring a time segment that is not included by the result from theconsistency judgment model, we emphasize them by a triangle markto distinguish from circles representing others (Figure 4(c))", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "zsz", "index_original": 262, "paper_title": "ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data", "pub_year": 2020, "domain": "Time-series analysis", "requirement": {"requirement_text": "DR2: Integrate features of concept drifts from the predictionmodels. Concept drifts hinder existing prediction models (the modeltrained from historical data) from accurate predictions in new envi-ronments. The accuracy fluctuation of the prediction model indicatesthe occurrence of concept drifts [7, 30, 42, 52, 54]. In addition, pa-rameters of prediction models can reflect the relationship betweendifferent inputs and the label, that is, the model\u2019s understanding ofthe concept [9]", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "We employ the air pollutant data [55] from four nationally-controlled air-quality monitoring sites in Beijing, which was collected every hour from March 1st, 2013 to February 28th, 2017 (34,536 data records per site). 22 meteorology-related dimen_x0002_sions are applied to predict if the air quality index (AQI) is higher than 100 (i.e., worse than mild pollution) after 24 hours. The employed dataset contains player records from three servers (647,800 player records from Server17, 702,125 player records from Server164, and 585,048 player records from Server230) of a MMORPG from August 16th, 2013 to January 19th, 2014. Three servers were started at different timestamps: Server17, Sever164, and Server230, which are in order of time, that is, players on different servers register for the game at different time periods. For each player, 21 attributes, like equipment (i.e., the combat effectiveness score of the player\u2019s equipment), practice (i.e., the level of practice, improved by learning and improving skills and finishing tasks), are recorded every day. The consumption records for the upcoming week of players form a group of time-series.", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "5.2.2 The Projected Parameter ViewThe model parameters updated after each batch during the entiretraining process are projected into a two-dimensional plane usingprincipal components analysis (PCA) based on singular value decom-position (SVD) [47]. The points projected by parameters of the samedata source are connected in order to form a curve. The distancebetween each pair of projected points illustrates the similarity ofcorresponding model parameters, namely, the concept similarity de-scribed by prediction models. Evolution patterns, like intersectionsand bundles [5, 19] can be identi\ufb01ed from the formed curves [10].The convergence and dispersal of curve segments representing differ-ent data sources indicate the agreement and disagreement of relatedmodels on the understanding of the concept. The curve segmentscorresponding to time segments selected in the timeline navigatorview is colored by transparency, from which analysts can learn aboutthe temporal order. The view is automatically zoomed in or out so asto \ufb01t the curves of the entire training process or the selected time seg-ment within the window, as shown in Figure 5. With the backgroundof the entire trajectories (Figure 5(a)), analysts can better measurerelative distances. After zooming in, it can be seen (Figure 5(b))that curves are not overlapped but with similar directions, that is,data sources have similar drifts. Analysts can drag the handle onthe time axis of the accuracy \ufb02uctuation chart to move the circles,which highlight the projected parameters corresponding to the samemoment.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "5.2.2 The Projected Parameter ViewThe model parameters updated after each batch during the entiretraining process are projected into a two-dimensional plane usingprincipal components analysis (PCA) based on singular value decom-position (SVD) [47]. The points projected by parameters of the samedata source are connected in order to form a curve. The distancebetween each pair of projected points illustrates the similarity ofcorresponding model parameters, namely, the concept similarity de-scribed by prediction models. Evolution patterns, like intersectionsand bundles [5, 19] can be identi\ufb01ed from the formed curves [10].The convergence and dispersal of curve segments representing differ-ent data sources indicate the agreement and disagreement of relatedmodels on the understanding of the concept. The curve segmentscorresponding to time segments selected in the timeline navigatorview is colored by transparency, from which analysts can learn aboutthe temporal order. The view is automatically zoomed in or out so asto \ufb01t the curves of the entire training process or the selected time seg-ment within the window, as shown in Figure 5. With the backgroundof the entire trajectories (Figure 5(a)), analysts can better measurerelative distances. After zooming in, it can be seen (Figure 5(b))that curves are not overlapped but with similar directions, that is,data sources have similar drifts. Analysts can drag the handle onthe time axis of the accuracy \ufb02uctuation chart to move the circles,which highlight the projected parameters corresponding to the samemoment.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "zsz", "index_original": 263, "paper_title": "ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data", "pub_year": 2020, "domain": "Time-series analysis", "requirement": {"requirement_text": "DR3: Identify the context of concepts and allow adjustments.Analysts need to know the context of the analyzed data records.Considering that analysts may miss details or may disagree withthe navigation, interactive adjustments for recommended results areneeded [23, 27]", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "We employ the air pollutant data [55] from four nationally-controlled air-quality monitoring sites in Beijing, which was collected every hour from March 1st, 2013 to February 28th, 2017 (34,536 data records per site). 22 meteorology-related dimen_x0002_sions are applied to predict if the air quality index (AQI) is higher than 100 (i.e., worse than mild pollution) after 24 hours. The employed dataset contains player records from three servers (647,800 player records from Server17, 702,125 player records from Server164, and 585,048 player records from Server230) of a MMORPG from August 16th, 2013 to January 19th, 2014. Three servers were started at different timestamps: Server17, Sever164, and Server230, which are in order of time, that is, players on different servers register for the game at different time periods. For each player, 21 attributes, like equipment (i.e., the combat effectiveness score of the player\u2019s equipment), practice (i.e., the level of practice, improved by learning and improving skills and finishing tasks), are recorded every day. The consumption records for the upcoming week of players form a group of time-series.", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The concept-time view displays the time segments in different data sources that are integrated for concept analysis. To display the sources of the applied data records, as mentioned in requirement, each data source is listed in a row to distinguish different data sources. Then, their data records are divided individually to introduce a specific time. Analysts need to make the trade-off between the number of data records and the clarity of concepts for appropriate adjustments. To facilitate decision-making, the drift level and the size of each batch are encoded with the color and height of the bar, respectively. The batches that compose the data records to be analyzed are highlighted. The number of these batches and the total number of the related data records are counted.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "zsz", "index_original": 264, "paper_title": "ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data", "pub_year": 2020, "domain": "Time-series analysis", "requirement": {"requirement_text": "DR4: Study the relationship between attributes and labels.While concepts have not an explicit de\ufb01nition, it is essential to pro-vide a visual explanation. The relationship between labels andattributes are considered to be an important description of con-cepts [14, 18]", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "We employ the air pollutant data [55] from four nationally-controlled air-quality monitoring sites in Beijing, which was collected every hour from March 1st, 2013 to February 28th, 2017 (34,536 data records per site). 22 meteorology-related dimen_x0002_sions are applied to predict if the air quality index (AQI) is higher than 100 (i.e., worse than mild pollution) after 24 hours. The employed dataset contains player records from three servers (647,800 player records from Server17, 702,125 player records from Server164, and 585,048 player records from Server230) of a MMORPG from August 16th, 2013 to January 19th, 2014. Three servers were started at different timestamps: Server17, Sever164, and Server230, which are in order of time, that is, players on different servers register for the game at different time periods. For each player, 21 attributes, like equipment (i.e., the combat effectiveness score of the player\u2019s equipment), practice (i.e., the level of practice, improved by learning and improving skills and finishing tasks), are recorded every day. The consumption records for the upcoming week of players form a group of time-series.", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The data source isconsidered as an attribute to label the context of the data records.For other attributes, the correlation for each batch of data recordsis quantified by the cosine similarity. The attributes are sorted bythe average correlation of the selected batches.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "A correlation matrix (Figure 1(e)) is employed to support DR4because of its representation ability [44, 48]. The data source isconsidered as an attribute to label the context of the data records.For other attributes, the correlation for each batch of data recordsis quantified by the cosine similarity. The attributes are sorted bythe average correlation of the selected batches. ConceptExplorer draws correlation matrices for the data source and analyst-specifiednumber of the attributes with the highest correlations subject to theconcept.For each cell, the horizontal and vertical axes of each matrix aredefined by two attributes. A square in non-diagonal cells representsa set of data records whose two attributes fall into the value rangeswhich are encoded with the position of the square. The differencesbetween the number of records with positive labels and those withnegative labels are counted for each square. The ratio of the dif-ference of two counts over their sum (i.e., #Positives\u2212#Negatives#Positives+#Negatives ) isencoded in color (ranging from red to blue). When the label dis-tribution in the dataset is nonuniform, analysts can reset the colormapping and encode the percentage difference in all data recordsin white. In some specific contexts, certain cells may be empty.To distinguish squares without a record, strokes are added in thesquares with more than one record. Darker strokes imply that therecord number of the square is larger than 5% of the amount ofchosen data records. The matrix view exhibits a symmetrical layout.Taking advantage of this feature, the current correlation pattern canbe compared with the other one. Each cell on the diagonal presentsa pair of histograms (i.e., a grounded histogram for lower-left cornerand an inverted histogram for upper-right corner).", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "matrix+bar", "axial_code": [], "componenet_code": ["bar", "matrix"]}]}, {"author": "zsz", "index_original": 265, "paper_title": "ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data", "pub_year": 2020, "domain": "Time-series analysis", "requirement": {"requirement_text": "DR5: Compare concepts in different contexts. Comparing dif-ferent concepts facilitates the understanding of the evolving conceptsand their contexts, e.g., the trends and outliers. There is a need tocompare a newly identi\ufb01ed concept with previously studied onesand record the identi\ufb01ed concepts [49]. Comparing concepts relatedto a concept drift also favors the understanding of the drift", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "We employ the air pollutant data [55] from four nationally-controlled air-quality monitoring sites in Beijing, which was collected every hour from March 1st, 2013 to February 28th, 2017 (34,536 data records per site). 22 meteorology-related dimen_x0002_sions are applied to predict if the air quality index (AQI) is higher than 100 (i.e., worse than mild pollution) after 24 hours. The employed dataset contains player records from three servers (647,800 player records from Server17, 702,125 player records from Server164, and 585,048 player records from Server230) of a MMORPG from August 16th, 2013 to January 19th, 2014. Three servers were started at different timestamps: Server17, Sever164, and Server230, which are in order of time, that is, players on different servers register for the game at different time periods. For each player, 21 attributes, like equipment (i.e., the combat effectiveness score of the player\u2019s equipment), practice (i.e., the level of practice, improved by learning and improving skills and finishing tasks), are recorded every day. The consumption records for the upcoming week of players form a group of time-series.", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Analysts can identify and record significant concepts that may be involved in subsequent analysis. The identified concepts can be compared with other concepts.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix", "axial_code": [], "componenet_code": ["matrix"]}, {"solution_text": "Analysts can identify and record significant concepts that may be involved in subsequent analysis. The identified concepts can be compared with other concepts.", "solution_category": "interaction", "solution_axial": "Selecting,Connect/Relate", "solution_compoent": "", "axial_code": ["Selecting", "Connect/Relate"], "componenet_code": ["selecting", "connect_relate"]}]}, {"author": "zsz", "index_original": 266, "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports", "pub_year": 2020, "domain": "sports", "requirement": {"requirement_text": "T1 Adjust the weights of different attributes. Domain expertsusually focus on only one attribute or a weighted combinationof multiple attributes for analysis. For example, a domainexpert may only want to know about the position of a player,or may be more interested in knowing the relative positionbetween the player and the ball. Analysts should be allowedto adjust the weight of different attributes to focus on theattributes of interest", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "MDLalgorithmformultivariatepatterns.Weight,merge,lengthcontrol.ImprovedalgorithmMinDL.", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "zsz", "index_original": 267, "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports", "pub_year": 2020, "domain": "sports", "requirement": {"requirement_text": "T1 Adjust the weights of different attributes. Domain expertsusually focus on only one attribute or a weighted combinationof multiple attributes for analysis. For example, a domainexpert may only want to know about the position of a player,or may be more interested in knowing the relative positionbetween the player and the ball. Analysts should be allowedto adjust the weight of different attributes to focus on theattributes of interest", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "First, the weight of each attribute. Analysts can input the weights of different attributesto adjust their focus of analysis (T1). Our measure considers eachattribute with weight to enable the multivariate pattern mining. Theweight of the i \u2212 th attribute a i is denoted as w i.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}, {"solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "MDLalgorithmformultivariatepatterns.Weight,merge,lengthcontrol.ImprovedalgorithmMinDL.", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "zsz", "index_original": 268, "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports", "pub_year": 2020, "domain": "sports", "requirement": {"requirement_text": "T1 Adjust the weights of different attributes. Domain expertsusually focus on only one attribute or a weighted combinationof multiple attributes for analysis. For example, a domainexpert may only want to know about the position of a player,or may be more interested in knowing the relative positionbetween the player and the ball. Analysts should be allowedto adjust the weight of different attributes to focus on theattributes of interest", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "MDLalgorithmformultivariatepatterns.Weight,merge,lengthcontrol.ImprovedalgorithmMinDL.", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "We design Attribute Editor to adjust the weights of different attributes and group similar values. We use a list to show all attributes because the list is clear and concise for domain experts. For each attribute, we display the three-level data structure of \u201cattribute \u2192 groups of similar values \u2192 values\u201d. At the top, the name of the attribute is shown on the left. The checkbox before the name controls whether the attribute is used in pattern mining and visualized in the system. On the right, a slider is used to control the weight of the attribute. To recommend a suitable weight as default, we calculate the diversity of the attribute based on information entropy [36]. As the information entropy of an attribute increases, its amount of information also increases and thus should be assigned a higher weight. The diversity is encoded with the vertical line on the slider. Under the top line, all the groups are shown and can be renamed.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+bara", "axial_code": [], "componenet_code": ["bar", "table"]}, {"solution_text": "We design Attribute Editor to adjust the weights of different attributes and group similar values. We use a list to show all attributes because the list is clear and concise for domain experts. For each attribute, we display the three-level data structure of \u201cattribute \u2192 groups of similar values \u2192 values\u201d. At the top, the name of the attribute is shown on the left. The checkbox before the name controls whether the attribute is used in pattern mining and visualized in the system. On the right, a slider is used to control the weight of the attribute. To recommend a suitable weight as default, we calculate the diversity of the attribute based on information entropy [36]. As the information entropy of an attribute increases, its amount of information also increases and thus should be assigned a higher weight. The diversity is encoded with the vertical line on the slider. Under the top line, all the groups are shown and can be renamed.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 269, "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports", "pub_year": 2020, "domain": "sports", "requirement": {"requirement_text": "T2 Display multiple attributes simultaneously. Domain expertsdesire to obtain insights comprehensively by seeing multi-ple attributes of an event simultaneously. Studies on visualanalytics of sports data prove that an intuitive glyph is pre-ferred [13, 30, 43, 49] because it can use visual metaphors thatcan correspond to the real scene to show multiple attributessimultaneously. Moreover, the system should provide designtemplates of glyphs suitable for many sports", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "MDLalgorithmformultivariatepatterns.Weight,merge,lengthcontrol.ImprovedalgorithmMinDL.", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "We use glyphs to show multiple attributes of an event simultaneously because of their effectiveness [30] and the common usage for encoding multivariate sports data [13, 43, 49]. Our glyph designs use six encoding methods, namely, color, matrix, bar, shape, donut, and circular bar, to cover all the attributes in our datasets, following the design principles proposed by Chung et al.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}]}, {"author": "zsz", "index_original": 270, "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports", "pub_year": 2020, "domain": "sports", "requirement": {"requirement_text": "T3 Merge the patterns. Domain experts prefer to discern differ-ent playing styles when analyzing a player. A playing stylecan be depicted by multiple similar patterns. For example, thesuccessive use of technique Drive and that of technique Volleyare similar, both of which reflect the offensive playing styles intennis. Merging similar patterns based on domain knowledgehelps domain experts evaluate a player\u2019s playing styles quickly.Therefore, the system should support the domain experts tomerge similar patterns based on their knowledge", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "MDLalgorithmformultivariatepatterns.Weight,merge,lengthcontrol.ImprovedalgorithmMinDL.", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "zsz", "index_original": 271, "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports", "pub_year": 2020, "domain": "sports", "requirement": {"requirement_text": "T3 Merge the patterns. Domain experts prefer to discern differ-ent playing styles when analyzing a player. A playing stylecan be depicted by multiple similar patterns. For example, thesuccessive use of technique Drive and that of technique Volleyare similar, both of which reflect the offensive playing styles intennis. Merging similar patterns based on domain knowledgehelps domain experts evaluate a player\u2019s playing styles quickly.Therefore, the system should support the domain experts tomerge similar patterns based on their knowledge", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "Second, the groups of similar values. Experts can group similarvalues to merge similar patterns (T3). In our algorithm, within acertain attribute ai, each value is mapped to one and only one group,where one group is a set of similar values. More speci\ufb01cally, eachgroup Gcategorical in a categorical attribute is a set of discrete valuesthat can be considered similar (e.g., the Offensive group includesall techniques to attack). The numerical attributes are quantized todiscrete ranges, such as Gnumerical = {v | min \u2264 v < max}, wheremin and max are the boundary value of the range.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "Groupsimilarvaluestomergesimilarpatterns.", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 272, "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports", "pub_year": 2020, "domain": "sports", "requirement": {"requirement_text": "T3 Merge the patterns. Domain experts prefer to discern differ-ent playing styles when analyzing a player. A playing stylecan be depicted by multiple similar patterns. For example, thesuccessive use of technique Drive and that of technique Volleyare similar, both of which reflect the offensive playing styles intennis. Merging similar patterns based on domain knowledgehelps domain experts evaluate a player\u2019s playing styles quickly.Therefore, the system should support the domain experts tomerge similar patterns based on their knowledge", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "MDLalgorithmformultivariatepatterns.Weight,merge,lengthcontrol.ImprovedalgorithmMinDL.", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "We design Attribute Editor to adjust the weights of different attributes and group similar values. We use a list to show all attributes because the list is clear and concise for domain experts. For each attribute, we display the three-level data structure of \u201cattribute \u2192 groups of similar values \u2192 values\u201d. At the top, the name of the attribute is shown on the left. The checkbox before the name controls whether the attribute is used in pattern mining and visualized in the system. On the right, a slider is used to control the weight of the attribute. To recommend a suitable weight as default, we calculate the diversity of the attribute based on information entropy [36]. As the information entropy of an attribute increases, its amount of information also increases and thus should be assigned a higher weight. The diversity is encoded with the vertical line on the slider. Under the top line, all the groups are shown and can be renamed.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+bara", "axial_code": [], "componenet_code": ["bar", "table"]}, {"solution_text": "We design Attribute Editor to adjust the weights of different attributes and group similar values. We use a list to show all attributes because the list is clear and concise for domain experts. For each attribute, we display the three-level data structure of \u201cattribute \u2192 groups of similar values \u2192 values\u201d. At the top, the name of the attribute is shown on the left. The checkbox before the name controls whether the attribute is used in pattern mining and visualized in the system. On the right, a slider is used to control the weight of the attribute. To recommend a suitable weight as default, we calculate the diversity of the attribute based on information entropy [36]. As the information entropy of an attribute increases, its amount of information also increases and thus should be assigned a higher weight. The diversity is encoded with the vertical line on the slider. Under the top line, all the groups are shown and can be renamed.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 273, "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports", "pub_year": 2020, "domain": "sports", "requirement": {"requirement_text": "T3 Merge the patterns. Domain experts prefer to discern differ-ent playing styles when analyzing a player. A playing stylecan be depicted by multiple similar patterns. For example, thesuccessive use of technique Drive and that of technique Volleyare similar, both of which reflect the offensive playing styles intennis. Merging similar patterns based on domain knowledgehelps domain experts evaluate a player\u2019s playing styles quickly.Therefore, the system should support the domain experts tomerge similar patterns based on their knowledge", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "MDLalgorithmformultivariatepatterns.Weight,merge,lengthcontrol.ImprovedalgorithmMinDL.", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Users can expand the bottom to view detailed information of values in an attribute and group similar values. The distribution of each attribute is visualized as either a bar chart or an area, depending on whether it is a numerical or categorical attribute.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "bar+area", "axial_code": [], "componenet_code": ["bar", "area"]}, {"solution_text": "Users can expand the bottom to view detailed information of values in an attribute and group similar values. The distribution of each attribute is visualized as either a bar chart or an area, depending on whether it is a numerical or categorical attribute.", "solution_category": "interaction", "solution_axial": "OverviewandExplore,Participation/Collaboration", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Participation/Collaboration"], "componenet_code": ["overview_and_explore", "participation_collaboration"]}]}, {"author": "zsz", "index_original": 274, "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports", "pub_year": 2020, "domain": "sports", "requirement": {"requirement_text": "T4 Provide a comparison with multiple levels of detail.Domain experts need to quickly compare a player with othersto \ufb01nd his/her individual tactical patterns. They also need tocompare the performance of a player in different situations toobtain insights about his/her playing styles comprehensively.Thus, the proposed system should allow users to divide thedataset into two subsets with different conditions, such aswho serves and whether it is a crucial time. Then, interactivevisualization is used to visually and interactively comparethe two subsets and \ufb01nd the differences. However, comparingmultivariate event sequences can lead to visual clutter becauseof the massive information. Therefore, the proposed systemshould support a comparison with multiple levels of detail", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "MDLalgorithmformultivariatepatterns.Weight,merge,lengthcontrol.ImprovedalgorithmMinDL.", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "We design Pattern Comparator to help experts compare patterns one-to-one, where juxtaposition is a commonly used method [21,34]. In our system, each subset occupies one column (Fig. 1(C9)), and each pattern occupies one row (Fig. 1(C4)), where patterns are arranged in descending order according to their frequency in the subset. For each pattern, a series of glyphs represent the events in the pattern and are placed from left to right in the order in which events occur. The color of the glyph encodes the player who hit the ball, where the color of the subset represents the server of the subset, and the dark color represents the opponents. Bar charts in the middle support comparison on frequency (Fig. 1(C7)). The height of a bar encodes the frequency of the corresponding pattern. The scale can be adjusted by the controller (Fig. 1(C3)) above the bar charts, where the two numbers on the side show the total frequency of the corresponding subsets. The luminance of the bar encodes the winning rate naturally because the winning rate is a numerical value ranging from 0 to 1 [37]. When users hover on a bar, the frequency and the winning rate are shown in a tooltip. To compare the frequency of a specific pattern (especially when its glyphs in two subsets are not at the same row, e.g., C4 and C6 in Fig. 1) and further compare the distribution of pattern frequency, we design a line chart with two lines (Fig. 1(C5)). The solid line with the color of the subset connects the tops of all the bars on the hovered side. The dashed line with the color of the other subset shows the frequency of each pattern in the other subset. To avoid visual clutter, users need to hover on the area of bar charts to view the line chart.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph+bar+line", "axial_code": [], "componenet_code": ["bar", "line", "glyph"]}]}, {"author": "zsz", "index_original": 275, "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports", "pub_year": 2020, "domain": "sports", "requirement": {"requirement_text": "T4 Provide a comparison with multiple levels of detail.Domain experts need to quickly compare a player with othersto \ufb01nd his/her individual tactical patterns. They also need tocompare the performance of a player in different situations toobtain insights about his/her playing styles comprehensively.Thus, the proposed system should allow users to divide thedataset into two subsets with different conditions, such aswho serves and whether it is a crucial time. Then, interactivevisualization is used to visually and interactively comparethe two subsets and \ufb01nd the differences. However, comparingmultivariate event sequences can lead to visual clutter becauseof the massive information. Therefore, the proposed systemshould support a comparison with multiple levels of detail", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed; Projection technology can effi_x0002_ciently abstract data.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,DimensionalityReduction", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "DimensionalityReduction"], "componenet_code": ["algorithmic_calculation", "dimensionality_reduction"]}, {"solution_text": "Scatterplot help users overview the patterns of two subsets and quickly focus on interesting ones. Projection technology can effi_x0002_ciently abstract data. Among the commonly used dimensionality reduction algorithm for feature projection, such as PCA, LDA, t_x0002_SNE, etc., we choose the metric MDS algorithm to project patterns for two considerations [3]. 1) Compared to PCA, LDA, and non_x0002_metric MDS, metric MDS can use the distance matrix to project multidimensional data instead of the Euclidean distance. We can hardly define the Euclidean distance between two patterns because patterns have different lengths, and some categorical attributes can hardly define Euclidean distance between values. By contrast, we can use the editing cost between each pair of two patterns to con_x0002_struct the distance matrix. 2) Although t-SNE can also accept a distance matrix, metric MDS is much efficient because t-SNE re_x0002_quires multiple iterations, but metric MDS does not. We project patterns into a plane coordinate system, where each point represents a pattern, and the distance between them shows the similarity between the patterns. For each point, a pie chart encodes statistical data of the pattern, which helps users quickly compare the performance of a pattern in two subsets. The size of the pie encodes the frequency that a pattern appears in two subsets. The entire pie is divided into two sections, each of which represents a subset with the corresponding color. The radian of a section encodes the frequency of the pattern in the corresponding subset. The length of the arc out_x0002_side the pie chart with color encodes the winning percentage of the pattern in the subset, where a semicircle represents all the winnings.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "scatter+pie", "axial_code": [], "componenet_code": ["pie", "scatter"]}]}, {"author": "zsz", "index_original": 276, "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports", "pub_year": 2020, "domain": "sports", "requirement": {"requirement_text": "T5 Show detailed sequences within a pattern. Domain expertsmay examine the raw data for detailed analysis and veri\ufb01cation.Thus, a detailed view should be provided to show the raw data", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "All the sequences with a certain pattern are shown in the Instance View for detailed information, when either the point or the row of glyphs is clicked. In the view, each row displays one sequence. For each row, the rectangle on the left encodes the outcome of the rally, where a solid one represents that the server wins, and a hollow one represents that the opponent wins. A circle in the row represents an event in the sequence. We use a solid circle to encode the event in the pattern and a hollow circle for the event not in the pattern. For a solid circle, the number on it represents the index of the event in the pattern.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}, {"solution_text": "All the sequences with a certain pattern are shown in the Instance View for detailed information, when either the point or the row of glyphs is clicked. In the view, each row displays one sequence. For each row, the rectangle on the left encodes the outcome of the rally, where a solid one represents that the server wins, and a hollow one represents that the opponent wins. A circle in the row represents an event in the sequence. We use a solid circle to encode the event in the pattern and a hollow circle for the event not in the pattern. For a solid circle, the number on it represents the index of the event in the pattern.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+glyph", "axial_code": [], "componenet_code": ["glyph", "table"]}]}, {"author": "zsz", "index_original": 278, "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data", "pub_year": 2022, "domain": "career mobility", "requirement": {"requirement_text": "T1 What are the general characteristics of career mobility?Experts require a quick overview of the data to iden-tify the regions of interest for further analysis. Peoplewith vertical movements (i.e., who changed job lev-els, such as promotion and demotion) or attainedhigh job levels are more likely to attract the experts\u2019attention.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.", "data_code": {"tables": 1, "textual": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The Parameter View is a filtering dashboard for users to choose a population of interest. From top to bottom, users can select a particular career length, time period, and different official backgrounds.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 279, "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data", "pub_year": 2022, "domain": "career mobility", "requirement": {"requirement_text": "T1 What are the general characteristics of career mobility?Experts require a quick overview of the data to iden-tify the regions of interest for further analysis. Peoplewith vertical movements (i.e., who changed job lev-els, such as promotion and demotion) or attainedhigh job levels are more likely to attract the experts\u2019attention.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.", "data_code": {"tables": 1, "textual": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. ", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "In the Distribution View, three bar charts show the ver- tical movement statistics of the chosen population. They include the population distribution of origin and destination job levels, and job level distance of the whole career. Users can quickly locate the data of interest for further analysis.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "In the Distribution View, three bar charts show the ver- tical movement statistics of the chosen population. They include the population distribution of origin and destination job levels, and job level distance of the whole career. Users can quickly locate the data of interest for further analysis.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 280, "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data", "pub_year": 2022, "domain": "career mobility", "requirement": {"requirement_text": "T1 What are the general characteristics of career mobility?Experts require a quick overview of the data to iden-tify the regions of interest for further analysis. Peoplewith vertical movements (i.e., who changed job lev-els, such as promotion and demotion) or attainedhigh job levels are more likely to attract the experts\u2019attention.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.", "data_code": {"tables": 1, "textual": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. ", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The Mobility Rate Timeline shows the vertical mobility rate changing over time, giving an over- view of the data filtered in the Distribution View.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "zsz", "index_original": 281, "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data", "pub_year": 2022, "domain": "career mobility", "requirement": {"requirement_text": "T2 What special features do the groups with vertical move-ments have at different time periods?Further investiga-tion is required to explain the mobility patterns indifferent historical periods. Experts want to \ufb01nd outif there are common features among the promotion/demotion groups.", "requirement_code": {"discover_observation": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.", "data_code": {"tables": 1, "textual": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. ", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The Mobility Rate Timeline shows the vertical mobility rate changing over time, giving an over- view of the data filtered in the Distribution View.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "The Mobility Rate Timeline shows the vertical mobility rate changing over time, giving an over- view of the data filtered in the Distribution View. Hover- ing on a bar unit, a glyph is shown to compare statistics of different groups (i.e., promotion, demotion, and steady groups), thereby revealing the salient features influ- encing vertical mobility at different periods.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}, {"solution_text": "The Mobility Rate Timeline shows the vertical mobility rate changing over time, giving an over- view of the data filtered in the Distribution View. Hover- ing on a bar unit, a glyph is shown to compare statistics of different groups (i.e., promotion, demotion, and steady groups), thereby revealing the salient features influ- encing vertical mobility at different periods.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}]}, {"author": "zsz", "index_original": 282, "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data", "pub_year": 2022, "domain": "career mobility", "requirement": {"requirement_text": "T3 What are the characteristics of different social groups?Identifying latent groups based on similarity in pat-terns of vertical movements is essential to \ufb01ndinghidden rules in career mobility. The system alsoneeds to provide statistics for groups to help \ufb01ndindividual similarities.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.", "data_code": {"tables": 1, "textual": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. Besides latent group identification, we also distilled three social relations and computed several mobility indices. Social Relationships. We have extracted three types of social relations based on experts\u2019 suggestions: colleagues (working in the same department at certain periods), towns_x0002_men (from the same Birthplace), and classmates who passed the exam in the same year. For colleagues, we store the over_x0002_lapped periods and the departments they served together. Career Mobility Indices. There are three indices reflecting career mobility at the individual and group levels [49].", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}, {"solution_text": "Description. The Group View (Fig. 2D) lists latent groups(T3) detected by the algorithm in Section 4 given the offi-cials chosen from the Distribution View. Each group is repre-sented by an intuitive node-link sequence showing thegroup sequential pattern. Numbers in rectangles form intogroup patterns, and two circles at the beginning and endshow the actual minimum and maximum job levels withinthe group. Groups are sorted by the numbers of officials indescending order. Experts require a statistical summary ofeach group to check whether officials have commonalities,so we design a folded information card (Fig. 2D1). We com-pute the average job-level distance and give four horizontalproportional bars to summarize the demographic profile(i.e., Ethnicity, Birthplace, Family Background, and ExamDegree).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph+bar", "axial_code": [], "componenet_code": ["bar", "glyph"]}, {"solution_text": "Experts can quickly identify whether a value domi-nates an attribute, thus finding commonalities. They canalso hover on each horizontal bar to check detailed statisticsof this attribute.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 283, "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data", "pub_year": 2022, "domain": "career mobility", "requirement": {"requirement_text": "T4 What is the mobility pattern for each group?After identi-fying groups, experts want to check the mobility pat-terns at the job-level and department levels within agroup to compare career path similarities (e.g., thepromotion speed and department transfer routines).", "requirement_code": {"discover_observation": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.", "data_code": {"tables": 1, "textual": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. Besides latent group identification, we also distilled three social relations and computed several mobility indices. Social Relationships. We have extracted three types of social relations based on experts\u2019 suggestions: colleagues (working in the same department at certain periods), towns_x0002_men (from the same Birthplace), and classmates who passed the exam in the same year. For colleagues, we store the over_x0002_lapped periods and the departments they served together. Career Mobility Indices. There are three indices reflecting career mobility at the individual and group levels [49].", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}, {"solution_text": "The Population Flow displays a detailed career mobility pattern in two modes (i.e., job-level and depart- ment) with two types of time (i.e., absolute and relative), which can be switched by two buttons. We provide a novel flow design adopting a multi-scale approach to form into the superimposition of three layers, namely, overall mobil- ity flow, group subflow, and individual career thread. The groups of interest or indi- vidual social relationships are embedded into the over- all population. This allows for the study of the mobility of a specific population within its broader historical context.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "sankey", "axial_code": [], "componenet_code": ["sankey"]}, {"solution_text": "The Population Flow displays a detailed career mobility pattern in two modes (i.e., job-level and depart- ment) with two types of time (i.e., absolute and relative), which can be switched by two buttons. We provide a novel flow design adopting a multi-scale approach to form into the superimposition of three layers, namely, overall mobil- ity flow, group subflow, and individual career thread. The groups of interest or indi- vidual social relationships are embedded into the over- all population. This allows for the study of the mobility of a specific population within its broader historical context.", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "zsz", "index_original": 284, "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data", "pub_year": 2022, "domain": "career mobility", "requirement": {"requirement_text": "T4 What is the mobility pattern for each group?After identi-fying groups, experts want to check the mobility pat-terns at the job-level and department levels within agroup to compare career path similarities (e.g., thepromotion speed and department transfer routines).", "requirement_code": {"discover_observation": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.", "data_code": {"tables": 1, "textual": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. Besides latent group identification, we also distilled three social relations and computed several mobility indices. Social Relationships. We have extracted three types of social relations based on experts\u2019 suggestions: colleagues (working in the same department at certain periods), towns_x0002_men (from the same Birthplace), and classmates who passed the exam in the same year. For colleagues, we store the over_x0002_lapped periods and the departments they served together. Career Mobility Indices. There are three indices reflecting career mobility at the individual and group levels [49].", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}, {"solution_text": "The latent groups and individuals\u2019 social relationships can be highlighted in the Population Flow (Fig. 3C) for a detailed investigation (T4, T6). Two modes are supported to portray the career mobility of the selected individuals. The first mode (Fig. 3C2) aggregates the career paths within a group into a subflow, where each flow seg- ment is embedded into the corresponding overall flow seg- ment. It is intuitive and scalable, allowing for inspecting group mobility and proportions relative to the whole population.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "sankey", "axial_code": [], "componenet_code": ["sankey"]}]}, {"author": "zsz", "index_original": 285, "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data", "pub_year": 2022, "domain": "career mobility", "requirement": {"requirement_text": "T5 What are the mobility characteristics for different individ-uals? Besides macro-level analysis, experts also wishto examine speci\ufb01c individuals at a micro-level.Those more in\ufb02uential with long careers and highpositions are interesting targets.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.", "data_code": {"tables": 1, "textual": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. ", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The Person View (Fig. 2E) lists a visual summary of poten-tial in\ufb02uential of\ufb01cials (T5). Since domain experts wish to\ufb01lter in\ufb02uential of\ufb01cials \ufb02exibly using career length and\ufb01nal job level, we thus provide these two constraints. Wealso provide two methods to sort of\ufb01cials (i.e., career start-ing years or career lengths). Each horizontal bar shows anindividual\u2019s job level information. The total length encodesthe career length. It comprises multiple small bars withlength encoding corresponding working periods and colorencoding different job levels. According to our experts, wedivide job levels into four levels represented by four colorcategories: red for high-level jobs (from level 3 to 1), brownfor middle-level (from level 6 to 4), blue for low-level (fromlevel 9 to 7), and grey for the lowest level (level 10). Insideeach level, we use saturation to distinguish job levels. Thedarker the color, the higher the job level. The color schemefor job levels is consistent in the whole system. Users canhave a quick overview of these individuals and \ufb01nd thosewith long careers and great promotion distance", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "Since domain experts wish to\ufb01lter in\ufb02uential of\ufb01cials \ufb02exibly using career length and\ufb01nal job level, we thus provide these two constraints. Wealso provide two methods to sort of\ufb01cials (i.e., career start-ing years or career lengths). Each horizontal bar shows anindividual\u2019s job level information.", "solution_category": "interaction", "solution_axial": "Filtering,Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure", "Filtering"], "componenet_code": ["reconfigure", "filtering"]}]}, {"author": "zsz", "index_original": 286, "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data", "pub_year": 2022, "domain": "career mobility", "requirement": {"requirement_text": "T6 How do the mobility patterns of each individual andhis social relationships change over time? To betterunderstand differences in opportunities, expertswish to inspect individuals\u2019 different social rela-tionships and how their mobility evolves andinteracts with them.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.", "data_code": {"tables": 1, "textual": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. Besides latent group identification, we also distilled three social relations and computed several mobility indices. Social Relationships. We have extracted three types of social relations based on experts\u2019 suggestions: colleagues (working in the same department at certain periods), towns_x0002_men (from the same Birthplace), and classmates who passed the exam in the same year. For colleagues, we store the over_x0002_lapped periods and the departments they served together. Career Mobility Indices. There are three indices reflecting career mobility at the individual and group levels [49].", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}, {"solution_text": "The Population Flow displays a detailed career mobility pattern in two modes (i.e., job-level and depart- ment) with two types of time (i.e., absolute and relative), which can be switched by two buttons. We provide a novel flow design adopting a multi-scale approach to form into the superimposition of three layers, namely, overall mobil- ity flow, group subflow, and individual career thread. The groups of interest or indi- vidual social relationships are embedded into the over- all population. This allows for the study of the mobility of a specific population within its broader historical context.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "sankey", "axial_code": [], "componenet_code": ["sankey"]}, {"solution_text": "The Population Flow displays a detailed career mobility pattern in two modes (i.e., job-level and depart- ment) with two types of time (i.e., absolute and relative), which can be switched by two buttons. We provide a novel flow design adopting a multi-scale approach to form into the superimposition of three layers, namely, overall mobil- ity flow, group subflow, and individual career thread. The groups of interest or indi- vidual social relationships are embedded into the over- all population. This allows for the study of the mobility of a specific population within its broader historical context.", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "zsz", "index_original": 287, "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data", "pub_year": 2022, "domain": "career mobility", "requirement": {"requirement_text": "T6 How do the mobility patterns of each individual andhis social relationships change over time? To betterunderstand differences in opportunities, expertswish to inspect individuals\u2019 different social rela-tionships and how their mobility evolves andinteracts with them.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.", "data_code": {"tables": 1, "textual": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. Besides latent group identification, we also distilled three social relations and computed several mobility indices. Social Relationships. We have extracted three types of social relations based on experts\u2019 suggestions: colleagues (working in the same department at certain periods), towns_x0002_men (from the same Birthplace), and classmates who passed the exam in the same year. For colleagues, we store the over_x0002_lapped periods and the departments they served together. Career Mobility Indices. There are three indices reflecting career mobility at the individual and group levels [49].", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}, {"solution_text": "The latent groups and individuals\u2019 social relationships can be highlighted in the Population Flow (Fig. 3C) for a detailed investigation (T4, T6). Two modes are supported to portray the career mobility of the selected individuals. The first mode (Fig. 3C2) aggregates the career paths within a group into a subflow, where each flow seg- ment is embedded into the corresponding overall flow seg- ment. It is intuitive and scalable, allowing for inspecting group mobility and proportions relative to the whole population.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "sankey", "axial_code": [], "componenet_code": ["sankey"]}]}, {"author": "zsz", "index_original": 288, "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data", "pub_year": 2022, "domain": "career mobility", "requirement": {"requirement_text": "T6 How do the mobility patterns of each individual andhis social relationships change over time? To betterunderstand differences in opportunities, expertswish to inspect individuals\u2019 different social rela-tionships and how their mobility evolves andinteracts with them.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.", "data_code": {"tables": 1, "textual": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Besides latent group identification, we also distilled three social relations and computed several mobility indices. Social Relationships. We have extracted three types of social relations based on experts\u2019 suggestions: colleagues (working in the same department at certain periods), towns_x0002_men (from the same Birthplace), and classmates who passed the exam in the same year. For colleagues, we store the over_x0002_lapped periods and the departments they served together. Career Mobility Indices. There are three indices reflecting career mobility at the individual and group levels [49].", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "To show the social relationships of a selected individual (KP ), users can select specific social relationships in a drop-down menu. The career threads that have social connections with KP will be highlighted over the overall mobility flow. Each thread consists of two line types, namely, solid lines indicating the time period this official worked with KP , and the dashed line meaning they did not work in the same department. Different department glyphs in two colors (with the same encodings in department flow) are overlaid onto the KP \u2019s career thread. The characters of the glyphs are abbreviations of different departments.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "sankey+line", "axial_code": [], "componenet_code": ["sankey", "line"]}, {"solution_text": "To show the social relationships of a selected individual (KP ), users can select specific social relationships in a drop-down menu. The career threads that have social connections with KP will be highlighted over the overall mobility flow.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 289, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R1] Graph query design and matching: This requirementaims to provide the user with a tool to interactively design/construct a query. This query will serve as a starting pointto find all the occurrences on the graph. In the previousexample, the user can easily depict the authors and the con-ferences linking these authors with various venue types asjournals or conferences (e.g. TVCG, ICDM, and KDD).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Fig. 2 shows the query view interface. This view allows theuser to de\ufb01ne the query structure (nodes, edges node/edge-attributes) via interactive functionalities [R1, R3]. This inter-face contains a drawing canvas, two built-in toolbars and astatus bar. The drawing canvas (Fig. 2a) is the area wherethe query is constructed and where interactions happen.The design toolbar (Fig. 2b) provides different actions tobuild the query. From top to bottom, these actions are: adda node of a speci\ufb01c layer, add an edge of a speci\ufb01c layer,delete a node/edge, launch the edge suggestion mechanismand arrange the layout of the query by applying a forcedirected algorithm. The standard toolbar (Fig. 2c) includesfrequently used features such as load, create, and save aquery structure. It also contains the Search button that runsthe graph engine. Finally, the status bar (Fig. 2d) providestextual information about the current state of the view.Actions such as specifying a node type or an attributevalue, specifying an edge type, starting the query matchingprocess and launching the edge suggestion mechanismrequire speci\ufb01c interactions described below", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Network+glyph", "axial_code": [], "componenet_code": ["network", "glyph"]}, {"solution_text": "The design toolbar (Fig. 2b) provides different actions tobuild the query. From top to bottom, these actions are: adda node of a speci\ufb01c layer, add an edge of a speci\ufb01c layer,delete a node/edge, launch the edge suggestion mechanismand arrange the layout of the query by applying a forcedirected algorithm. The standard toolbar (Fig. 2c) includesfrequently used features such as load, create, and save aquery structure. It also contains the Search button that runsthe graph engine. Finally, the status bar (Fig. 2d) providestextual information about the current state of the view.Actions such as specifying a node type or an attributevalue, specifying an edge type, starting the query matchingprocess and launching the edge suggestion mechanismrequire speci\ufb01c interactions described below", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "zsz", "index_original": 290, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R1] Graph query design and matching: This requirementaims to provide the user with a tool to interactively design/construct a query. This query will serve as a starting pointto find all the occurrences on the graph. In the previousexample, the user can easily depict the authors and the con-ferences linking these authors with various venue types asjournals or conferences (e.g. TVCG, ICDM, and KDD).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "The right part of the figure shows the query after adding the above-mentioned suggestions. To accept a suggested edge, the user clicks on the internal (dashed line) or external edge (pie slice). A new internal edge is displayed as a continuous line KDD and SDM links). A new external edge is displayed as a link to a node toward its slice. The query suggestion mechanism provides visual elements to guide the user in the incremental con- struction of the query. Thus, each time the query is extended, the user can retrieve embeddings and further refine his/her searching process. The query suggestion mechanism is activated from the design toolbar.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Network+glyph", "axial_code": [], "componenet_code": ["network", "glyph"]}, {"solution_text": "The right part of the figure shows the query after adding the above-mentioned suggestions. To accept a suggested edge, the user clicks on the internal (dashed line) or external edge (pie slice). A new internal edge is displayed as a continuous line KDD and SDM links). A new external edge is displayed as a link to a node toward its slice. The query suggestion mechanism provides visual elements to guide the user in the incremental con- struction of the query. Thus, each time the query is extended, the user can retrieve embeddings and further refine his/her searching process. The query suggestion mechanism is activated from the design toolbar.", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "zsz", "index_original": 291, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R2] Navigation and exploration of the results: This require-ment aims to provide visualizations and interactions tofacilitate the exploration of the results, or the embeddings5at different granularity levels.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "In order to improve the SRT [R5], VERTIGo tightlyinteracts with the SuMGra system to enable the user to start,pause, and resume the query engine with the possibility tonavigate/explore partially collected embeddings [R2] (thisfunctionality is described in Section 5)", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Network+glyph", "axial_code": [], "componenet_code": ["network", "glyph"]}, {"solution_text": "In order to improve the SRT [R5], VERTIGo tightlyinteracts with the SuMGra system to enable the user to start,pause, and resume the query engine with the possibility tonavigate/explore partially collected embeddings [R2] (thisfunctionality is described in Section 5)", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "zsz", "index_original": 292, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R2] Navigation and exploration of the results: This require-ment aims to provide visualizations and interactions tofacilitate the exploration of the results, or the embeddings5at different granularity levels.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "We adopt a node-link diagram with a force directed lay- out [33] to produce a suitable visualization of the graph. Working with large graphs entails some challenges: aes- thetic criteria, scalability and reasonable interaction time. To deal with these issues, we leverage a multilevel tech- nique named Multipole Multilevel Method (FM3) [34]. FM3 is a well-known approach in the graph drawing field that allows scaling up to large graphs with a good trade off between time and visualization performances. An  example of a graph layout obtained by VERTIGo is pre- sented in Fig. 4a. The input graph is a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types. We note that such a visualization has the advantage to stand out groups of nodes that clearly form communities. In this particular example, we can easily highlight five major groups (clus- ters) of nodes. Edges of the graph layout are not depicted to avoid cluttering issues.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}]}, {"author": "zsz", "index_original": 293, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R2] Navigation and exploration of the results: This require-ment aims to provide visualizations and interactions tofacilitate the exploration of the results, or the embeddings5at different granularity levels.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "In order to display a large number of embeddings, Pientaet al. [11] reduced them to points. This kind of abstractionavoids a cluttered visualization; however, the spatial distri-bution of its elements is not depicted. With the aim to displayin detail the topology of the embeddings (i. e., nodes andedges) [R2], we designed the Embeddings view (Fig. 5). Inthis view, we show in detail the structure of the embeddings(Fig. 5b) containing a given set of nodes (Fig. 5a) (interactionsto select these nodes are described in Section 5).The goal of the Embeddings view is to allow the user tosample the embeddings for any combination of nodesinvolved in the embeddings. For example, considering theembeddings of the query shown in Fig. 2, the Fig. 5b showsthe list of embeddings where Shixia Liu and Yangqiu Songare present. This behaviour enhances the exploration task[R2] since it not only shows the embeddings, but also a sub-set of them.In order to convey the spatial distribution of the embed-dings, we use the Minimum Bounding Rectangle (MBR)value that bounds the nodes of an embedding in the Graphview. This value transmits the extent of the nodes within anembedding. For instance, a low MBR value conveys thecloseness of its nodes, while a high MBR value means thatone or more nodes are far apart from the others, possibly indifferent clusters. The Embeddings view allows users tosort results in an ascending/descending order based ontheir MBR values [R2]. This metric is also used to \ufb01lterresults using a histogram (Fig. 1e) that groups MBR valuesinto n ranges (interactions are described in Section 5.2).With the aim to depict the topological structure of theembeddings, we must consider that some embeddings couldinvolve the same set of nodes, with a different edge topology.Based on this feature, we aggregate embeddings with thesame set of nodes into a single fusion embedding. Fig. 6 showsan example where the fusion embedding (c) is the result ofthe fusion of the embeddings (a) and (b). Thus, this fusionmaintains the topology of nodes, but the topology of edgesevolves according to the links in their aggregated embed-dings. The number of aggregations is represented by thethickness of the edges in Fig. 6c.Each row in the list of embeddings (Fig. 5b) is composedby a fusion embedding and its associated metadata (i. e., theMBR value and the number of aggregated embeddings).This metadata can be particularly useful to explore embed-dings according to their spatial/topological features.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Network", "axial_code": [], "componenet_code": ["network"]}, {"solution_text": "For instance, in a co-authorship network, given a set of authors(Fig. 5a), if the user is interested in knowing other authorswho often collaborate with them, he/she can order up thelist of embeddings (Fig. 5b) by clicking on the last column ofthe view, and recover the embedding(s) with the highestnumber of aggregations (Fig. 5c).", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "zsz", "index_original": 294, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R2] Navigation and exploration of the results: This require-ment aims to provide visualizations and interactions tofacilitate the exploration of the results, or the embeddings5at different granularity levels.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Considering the overview, we employ a heatmap [35] thatdynamically illustrates the portion of the graph in whichembeddings are located. Fig. 4a shows an example of a heat-map of embeddings. The density distribution is representedusing the viridis colour gradient [36] which offers a highrange of perceived values while avoiding red-green colorblindness. Liu and Heer [37] demonstrate that this gradientperforms well to encode quantitative data in terms of timeand error through an experimentation involving 9 competi-tors. The viridis scheme varies in a sequential multi-hue pal-ette from blue (low-dense areas) to yellow (high-denseareas). In the example depicted in Fig. 4b, we can highlightthat the Graph view shows different communities since thelayout algorithm highlights different groups of nodes. Here,the embeddings are not equally distributed among thesegroups. As an interesting point, we can note a high density(high concentration of embeddings depicted in yellow) inthe center of the image while a lower concentration ofembeddings appears in the upper-right corner. Our heatmapcan be employed to visually understand if a query is specificto a certain portion of the graph or not, and thus provides theuser an entry point for the exploration task [R2].We would like to note that VERTIGo allows the user topause and resume the underlying query process (Fig. 7b)This can be particularly useful if the number of embeddingsgrows quickly [R5]. In that scenario, the process can bepaused at any moment, the embeddings are retrieved con-sidering the current state of the query engine and the corre-sponding heatmap (Fig. 7e) is produced or updated.Finally, the user can resume the query process from wherehe/she had stopped.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "scatter+heatmap", "axial_code": [], "componenet_code": ["heatmap", "scatter"]}]}, {"author": "zsz", "index_original": 295, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R2] Navigation and exploration of the results: This require-ment aims to provide visualizations and interactions tofacilitate the exploration of the results, or the embeddings5at different granularity levels.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "The abstraction supplied by the heatmap makes it dif\ufb01cultto know the exact number/distribution of embeddingsfound in a region. To deal with this issue, VERTIGo allowsthe users to change the levels of details zooming on someparticular area [R2] (Fig. 7f).", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}, {"solution_text": "In this case, the nodes of theembeddings are shown in a contrasted color (see the bluenodes in Fig. 8). We use the area of the node to depict thenumber of embeddings it belongs to: bigger nodes corre-spond to nodes occurring in more embeddings. In order toenhance the effectiveness of the exploration task [R2], weshow the labels of embedding nodes. Labeling the nodes isnot a trivial task, since showing labels for all nodes maycause cluttering issues. To overcome this problem, we showoverlapping-free labels based on node weight (i. e., thenumber of times it appears in an embedding), by using agreedy algorithm. It \ufb01rst orders the set of nodes by weight.Next, for each node, it calculates the bounding rectangle ofthe label, and it checks if there is no overlap with the previ-ous labeled nodes. If there is no overlap, the label is shown,otherwise it is not. The complexity of this algorithm isO\u00f0n2\u00de, where n is the set of embedding nodes. The resultdepends on the zoom level. Thus, when the user zooms in/out, the algorithm computes again the displayed labels.Fig. 8 shows an example. Notice that some node labels arebigger (e.g., N. Ramakrishnan) than their neighbors.The labeling algorithm clearly displays the most weigh-ted node labels. However, depending on the zoom level,some nodes with low weight may be underneath others. Toovercome this problem, we implement a text \ufb01eld searchfacility to retrieve nodes by their name. Fig. 8a shows thisoption. Once the node name is selected, the focus of theGraph view is automatically redirected to the node locationand an animation shows its exact location.From the detailed level, the user can select a set ofembedding nodes for further analysis by clicking on them.Their border color becomes red and they are automaticallyadded to the Embeddings view (Fig. 7g). The user can thusvisualize the list of the associated embeddings (Fig. 7h). Inthe following section, we describe the exploration of embed-dings using both: the Embeddings and the Graph views.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "In this case, the nodes of theembeddings are shown in a contrasted color (see the bluenodes in Fig. 8). We use the area of the node to depict thenumber of embeddings it belongs to: bigger nodes corre-spond to nodes occurring in more embeddings. In order toenhance the effectiveness of the exploration task [R2], weshow the labels of embedding nodes. Labeling the nodes isnot a trivial task, since showing labels for all nodes maycause cluttering issues. To overcome this problem, we showoverlapping-free labels based on node weight (i. e., thenumber of times it appears in an embedding), by using agreedy algorithm. It \ufb01rst orders the set of nodes by weight.Next, for each node, it calculates the bounding rectangle ofthe label, and it checks if there is no overlap with the previ-ous labeled nodes. If there is no overlap, the label is shown,otherwise it is not. The complexity of this algorithm isO\u00f0n2\u00de, where n is the set of embedding nodes. The resultdepends on the zoom level. Thus, when the user zooms in/out, the algorithm computes again the displayed labels.Fig. 8 shows an example. Notice that some node labels arebigger (e.g., N. Ramakrishnan) than their neighbors.The labeling algorithm clearly displays the most weigh-ted node labels. However, depending on the zoom level,some nodes with low weight may be underneath others. Toovercome this problem, we implement a text \ufb01eld searchfacility to retrieve nodes by their name. Fig. 8a shows thisoption. Once the node name is selected, the focus of theGraph view is automatically redirected to the node locationand an animation shows its exact location.From the detailed level, the user can select a set ofembedding nodes for further analysis by clicking on them.Their border color becomes red and they are automaticallyadded to the Embeddings view (Fig. 7g). The user can thusvisualize the list of the associated embeddings (Fig. 7h). Inthe following section, we describe the exploration of embed-dings using both: the Embeddings and the Graph views.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "scatter+network", "axial_code": [], "componenet_code": ["scatter", "network"]}]}, {"author": "zsz", "index_original": 296, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R2] Navigation and exploration of the results: This require-ment aims to provide visualizations and interactions tofacilitate the exploration of the results, or the embeddings5at different granularity levels.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Besides the Kelp-based representation, VERTIGo pro-vides a histogram to explore embedding results based ontheir MBR value [R2]. MBR values are grouped into nranges de\ufb01ned by the user, where 1 _x0014_ n _x0014_ 10. Fig. 1e dis-plays a histogram grouped into \ufb01ve ranges. In this example,we can highlight that there is a concentration of embeddingswith a low MBR value, which re\ufb02ects their proximity in thegraph (maybe in the same cluster). On the other hand, thelast two ranges show a low concentration of embeddingswith a high MBR value (nodes of these embeddings mightbe in different clusters). This histogram allows the user to\ufb01lter embeddings by clicking over the desired bar. Thank tothis functionality, users can focus on a speci\ufb01c group or dis-cover anomalous embeddings (please refer to the exampleon Section 6.1 to see this functionality).", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Besides the Kelp-based representation, VERTIGo pro-vides a histogram to explore embedding results based ontheir MBR value [R2]. MBR values are grouped into nranges de\ufb01ned by the user, where 1 _x0014_ n _x0014_ 10. Fig. 1e dis-plays a histogram grouped into \ufb01ve ranges. In this example,we can highlight that there is a concentration of embeddingswith a low MBR value, which re\ufb02ects their proximity in thegraph (maybe in the same cluster). On the other hand, thelast two ranges show a low concentration of embeddingswith a high MBR value (nodes of these embeddings mightbe in different clusters). This histogram allows the user to\ufb01lter embeddings by clicking over the desired bar. Thank tothis functionality, users can focus on a speci\ufb01c group or dis-cover anomalous embeddings (please refer to the exampleon Section 6.1 to see this functionality).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "zsz", "index_original": 297, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R3] Handling multilayer graphs: This requirement aims toallow the user to model and depict a traditional graph or amultilayer graph, i. e., a graph where each layer containsedges of a certain type. Referring to the previous example, alayer can be a specific venue in the co-authorship network.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Fig. 2 shows the query view interface. This view allows theuser to de\ufb01ne the query structure (nodes, edges node/edge-attributes) via interactive functionalities [R1, R3]. This inter-face contains a drawing canvas, two built-in toolbars and astatus bar. The drawing canvas (Fig. 2a) is the area wherethe query is constructed and where interactions happen.The design toolbar (Fig. 2b) provides different actions tobuild the query. From top to bottom, these actions are: adda node of a speci\ufb01c layer, add an edge of a speci\ufb01c layer,delete a node/edge, launch the edge suggestion mechanismand arrange the layout of the query by applying a forcedirected algorithm. The standard toolbar (Fig. 2c) includesfrequently used features such as load, create, and save aquery structure. It also contains the Search button that runsthe graph engine. Finally, the status bar (Fig. 2d) providestextual information about the current state of the view.Actions such as specifying a node type or an attributevalue, specifying an edge type, starting the query matchingprocess and launching the edge suggestion mechanismrequire speci\ufb01c interactions described below", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Network+glyph", "axial_code": [], "componenet_code": ["network", "glyph"]}, {"solution_text": "The design toolbar (Fig. 2b) provides different actions tobuild the query. From top to bottom, these actions are: adda node of a speci\ufb01c layer, add an edge of a speci\ufb01c layer,delete a node/edge, launch the edge suggestion mechanismand arrange the layout of the query by applying a forcedirected algorithm. The standard toolbar (Fig. 2c) includesfrequently used features such as load, create, and save aquery structure. It also contains the Search button that runsthe graph engine. Finally, the status bar (Fig. 2d) providestextual information about the current state of the view.Actions such as specifying a node type or an attributevalue, specifying an edge type, starting the query matchingprocess and launching the edge suggestion mechanismrequire speci\ufb01c interactions described below", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "zsz", "index_original": 298, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R3] Handling multilayer graphs: This requirement aims toallow the user to model and depict a traditional graph or amultilayer graph, i. e., a graph where each layer containsedges of a certain type. Referring to the previous example, alayer can be a specific venue in the co-authorship network.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "VERTIGo allows the users to specify the types of nodes inthe query, which fit the multilayer aspect of the inputgraph [R3]. In this case, the list of matched embeddingsonly contains the nodes holding the selected types, and theSRT of the query process decreases [R5]. Nodes of a giventype are represented by the same icon showing the typesemantic (for instance, a person in Fig. 2e). A specific attribute value can also be added to a node of thequery in order to decrease the number of embeddings and theSRT of the query process [R5]. The query view enables this action by right-clicking on the desired node and selecting avalue from the displayed pop-up list. For instance, in Fig. 2e,one can select an author name in a co-authorship network.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 299, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R3] Handling multilayer graphs: This requirement aims toallow the user to model and depict a traditional graph or amultilayer graph, i. e., a graph where each layer containsedges of a certain type. Referring to the previous example, alayer can be a specific venue in the co-authorship network.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "VERTIGo allows the users to query traditional graphs ormultilayer graphs [R3]. We adopt a well-known way tovisualize the edge types (or layers) by color coding. When apair of nodes is connected by several edges, some scalabilitychallenges arise i) how to organize these links and ii) how todeal with many edge types. To overcome these problems,edges between a pair of nodes are arranged in a parallelway. The relative distance between parallel edges remainsthe same even when the number of links increases, whichallows the users to clearly see all the links, as shown inFig. 2. The upper limit for the number of edges is 10 becauseof the known problem of distinguishing small regions ofcolors [30]. Edges are labeled to enhance the type recogni-tion. In order to add an edge to a query, the user must \ufb01rstchoose an edge type from the drop-down edges button onthe design toolbar (Fig. 2b). Then, using the mouse pointer,the user draws the edge from one node to another. Finally,the edge is automatically arranged. When more than twoedges connect a pair of nodes, the Boolean logical operatorAND is used to perform a logical conjunction between them.Fig. 2 shows the query example described in Section 3,where an author published a TVCG paper with one authorand he/she also published an ICDM and a KDD paper withanother author. More precisely, we explicitly state that theauthor corresponding to the central node has publishedboth an ICDM and a KDD article with the same co-author,and a TVCG paper with another co-author. We can observethat the multilayer graph structure allows us to easily spec-ify multiple constraints between the same pair of nodes", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Network+glyph", "axial_code": [], "componenet_code": ["network", "glyph"]}, {"solution_text": "VERTIGo allows the users to query traditional graphs ormultilayer graphs [R3]. We adopt a well-known way tovisualize the edge types (or layers) by color coding. When apair of nodes is connected by several edges, some scalabilitychallenges arise i) how to organize these links and ii) how todeal with many edge types. To overcome these problems,edges between a pair of nodes are arranged in a parallelway. The relative distance between parallel edges remainsthe same even when the number of links increases, whichallows the users to clearly see all the links, as shown inFig. 2. The upper limit for the number of edges is 10 becauseof the known problem of distinguishing small regions ofcolors [30]. Edges are labeled to enhance the type recogni-tion. In order to add an edge to a query, the user must \ufb01rstchoose an edge type from the drop-down edges button onthe design toolbar (Fig. 2b). Then, using the mouse pointer,the user draws the edge from one node to another. Finally,the edge is automatically arranged. When more than twoedges connect a pair of nodes, the Boolean logical operatorAND is used to perform a logical conjunction between them.Fig. 2 shows the query example described in Section 3,where an author published a TVCG paper with one authorand he/she also published an ICDM and a KDD paper withanother author. More precisely, we explicitly state that theauthor corresponding to the central node has publishedboth an ICDM and a KDD article with the same co-author,and a TVCG paper with another co-author. We can observethat the multilayer graph structure allows us to easily spec-ify multiple constraints between the same pair of nodes", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "zsz", "index_original": 300, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R3] Handling multilayer graphs: This requirement aims toallow the user to model and depict a traditional graph or amultilayer graph, i. e., a graph where each layer containsedges of a certain type. Referring to the previous example, alayer can be a specific venue in the co-authorship network.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Since a multilayer graph involves different nodes and edgetypes, standard query engines such as TurboISO [22] orVF3 [26] are not adapted for such graphs. To deal with mul-tilayer graphs [R3], we integrate into VERTIGo a recent mul-tilayer graph query engine named SuMGra [31]. It exploitsspecialized indexing techniques to speed the backtrackingalgorithm that is commonly employed to deal with the sub-graph isomorphism problem [32]. The Search button in thestandard toolbar (Fig. 2c) starts the query process.", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "zsz", "index_original": 301, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R4] Query suggestion: This requirement aims to suggestan extension of the initial query to the user. Mentioning theprevious example, it could propose another conferencevenue based on the graph structure that links two authorsin order to re\ufb01ne the results.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "The right part of the figure shows the query after adding the above-mentioned suggestions. To accept a suggested edge, the user clicks on the internal (dashed line) or external edge (pie slice). A new internal edge is displayed as a continuous line KDD and SDM links). A new external edge is displayed as a link to a node toward its slice. The query suggestion mechanism provides visual elements to guide the user in the incremental con- struction of the query. Thus, each time the query is extended, the user can retrieve embeddings and further refine his/her searching process. The query suggestion mechanism is activated from the design toolbar.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Network+glyph", "axial_code": [], "componenet_code": ["network", "glyph"]}, {"solution_text": "The right part of the figure shows the query after adding the above-mentioned suggestions. To accept a suggested edge, the user clicks on the internal (dashed line) or external edge (pie slice). A new internal edge is displayed as a continuous line KDD and SDM links). A new external edge is displayed as a link to a node toward its slice. The query suggestion mechanism provides visual elements to guide the user in the incremental con- struction of the query. Thus, each time the query is extended, the user can retrieve embeddings and further refine his/her searching process. The query suggestion mechanism is activated from the design toolbar.", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "zsz", "index_original": 302, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R4] Query suggestion: This requirement aims to suggestan extension of the initial query to the user. Mentioning theprevious example, it could propose another conferencevenue based on the graph structure that links two authorsin order to re\ufb01ne the results.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Based on the initial query structure, VERTIGo automaticallysuggests k edges to the user\u2019s query intention [R4]. To obtaincandidate edges, VIIQ [14] uses a correlation of a subset ofedges of a user log. This approach only ranks a subset ofedges which leads to suggest only a part of the query struc-ture. VERTIGo overcomes this drawback by ranking all theadjacent edges for every node of the embeddings and thensuggests the top-k frequent type of edges found. The graphstructure is used to obtain the adjacent edges for eachembedding node. This mechanism suggests interestingextensions of the query since it leverages informationobtained by the underlying graph", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "Leveragesgraphstructuretorankalladjacentedgesforeachnodeofembeddings.", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}]}, {"author": "zsz", "index_original": 303, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R5] Scalability: This requirement aims to handle queryingon large graphs and recover the embeddings in a reasonableSystem Response Time (SRT). SRT is de\ufb01ned as the timetaken by the matching engine to evaluate the entire query.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "VERTIGo allows the users to specify the types of nodes inthe query, which fit the multilayer aspect of the inputgraph [R3]. In this case, the list of matched embeddingsonly contains the nodes holding the selected types, and theSRT of the query process decreases [R5]. Nodes of a giventype are represented by the same icon showing the typesemantic (for instance, a person in Fig. 2e). A specific attribute value can also be added to a node of thequery in order to decrease the number of embeddings and theSRT of the query process [R5]. The query view enables this action by right-clicking on the desired node and selecting avalue from the displayed pop-up list. For instance, in Fig. 2e,one can select an author name in a co-authorship network.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 304, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R5] Scalability: This requirement aims to handle queryingon large graphs and recover the embeddings in a reasonableSystem Response Time (SRT). SRT is de\ufb01ned as the timetaken by the matching engine to evaluate the entire query.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "A common paradigm in query process is based on a Quer-y!Result approach, i. e., after a query construction, send therequest to the query engine and then visualize the retrievedresults [12]. However, as interactions with the query engineare not available in this paradigm, the process can be affectedby huge SRT when the number of results grows exponen-tially. In order to improve the SRT [R5], VERTIGo tightlyinteracts with the SuMGra system to enable the user to start,pause, and resume the query engine with the possibility tonavigate/explore partially collected embeddings [R2] (thisfunctionality is described in Section 5).", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Network+glyph", "axial_code": [], "componenet_code": ["network", "glyph"]}, {"solution_text": "A common paradigm in query process is based on a Quer-y!Result approach, i. e., after a query construction, send therequest to the query engine and then visualize the retrievedresults [12]. However, as interactions with the query engineare not available in this paradigm, the process can be affectedby huge SRT when the number of results grows exponen-tially. In order to improve the SRT [R5], VERTIGo tightlyinteracts with the SuMGra system to enable the user to start,pause, and resume the query engine with the possibility tonavigate/explore partially collected embeddings [R2] (thisfunctionality is described in Section 5).", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "zsz", "index_original": 305, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R5] Scalability: This requirement aims to handle queryingon large graphs and recover the embeddings in a reasonableSystem Response Time (SRT). SRT is de\ufb01ned as the timetaken by the matching engine to evaluate the entire query.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "We adopt a node-link diagram with a force directed lay- out [33] to produce a suitable visualization of the graph. Working with large graphs entails some challenges: aes- thetic criteria, scalability and reasonable interaction time. To deal with these issues, we leverage a multilevel tech- nique named Multipole Multilevel Method (FM3) [34]. FM3 is a well-known approach in the graph drawing field that allows scaling up to large graphs with a good trade off between time and visualization performances. An  example of a graph layout obtained by VERTIGo is pre- sented in Fig. 4a. The input graph is a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types. We note that such a visualization has the advantage to stand out groups of nodes that clearly form communities. In this particular example, we can easily highlight five major groups (clus- ters) of nodes. Edges of the graph layout are not depicted to avoid cluttering issues.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}]}, {"author": "zsz", "index_original": 306, "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks", "pub_year": 2022, "domain": "Network", "requirement": {"requirement_text": "[R5] Scalability: This requirement aims to handle queryingon large graphs and recover the embeddings in a reasonableSystem Response Time (SRT). SRT is de\ufb01ned as the timetaken by the matching engine to evaluate the entire query.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.", "data_code": {"categorical": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "We would like to note that VERTIGo allows the user to pause and resume the underlying query process. This can be particularly useful if the number of embeddings grows quickly. In that scenario, the process can be paused at any moment, the embeddings are retrieved con- sidering the current state of the query engine and the corre- sponding  is produced or updated. Finally, the user can resume the query process from where he/she had stopped.", "solution_category": "interaction", "solution_axial": "History", "solution_compoent": "", "axial_code": ["History"], "componenet_code": ["history"]}]}, {"author": "zsz", "index_original": 307, "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data", "pub_year": 2022, "domain": "Spatial cascade", "requirement": {"requirement_text": "R1 Provide location overviews. First, the experts need toidentify and select interesting locations like Lianget al. did in their study [28]. They pay attention to thespatial context and temporal distribution of thelocations\u2019 infection events. The system should allowaccess to the spatiotemporal overview for eachlocation.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Maps are widely used in urban analysis scenar- ios [59]. We use a level-of-detail mechanism for the map. Each location is represented by a circle, and the size and opac- ity of the circles encode the number of events. Zooming in on the map reveals the spatiotemporal overviews for the locations. Inspired by the widely used radial layout for temporal data [32], [46], we encode the tem- poral distribution of events with a radial heatmap around the circle for each location. The opacity of each sector encodes the number of events aggregated from the corresponding time- span. Different aggregation strategies can be applied. For example, for traffic congestion events, each sector represents an hour of every day, resulting in 24 sectors; and for air pollu- tion events, 12 sectors correspond to 12 months of a year. The map also depicts the cascading patterns inferred by the model as spatial networks. Each location is assigned with a unique color that is consistent throughout the system.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "map+donut", "axial_code": [], "componenet_code": ["donut", "map"]}]}, {"author": "zsz", "index_original": 308, "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data", "pub_year": 2022, "domain": "Spatial cascade", "requirement": {"requirement_text": "R2 Recommend potential locations. The experts can haveno specific prior knowledge for a target area to ini-tialize the analysis. The system should recommendseveral potential locations, where strong infectiondependencies and valuable patterns may berevealed. In particular, the experts believe that thetemporal co-occurrences of infection events canenhance this recommendation", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The projection (A1) reveals interesting location sub- sets when users have no exact prior knowledge. If 1) a pair of locations are close to each other and 2) the infection events in these two locations frequently co-occur, then these two locations are highly likely infected simulta- neously. The first condition can be easily checked on the map. We design the projection based on a two-level distance measurement to capture the event co-occurrences between two locations, such as, m and n.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}, {"solution_text": "The projection (A1) reveals interesting location sub- sets when users have no exact prior knowledge. If 1) a pair of locations are close to each other and 2) the infection events in these two locations frequently co-occur, then these two locations are highly likely infected simulta- neously. The first condition can be easily checked on the map. We design the projection based on a two-level distance measurement to capture the event co-occurrences between two locations, such as, m and n.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}]}, {"author": "zsz", "index_original": 309, "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data", "pub_year": 2022, "domain": "Spatial cascade", "requirement": {"requirement_text": "R3 Summarize various cascades. A cascading networkcomprises massive cascades of infection events. Eachcascade involves multiple location in\ufb02uences due tothe same contagion. Summarizing these cascadesprior to the in\ufb02uence analysis is strongly required tolearn how contagions propagate; for example, whichlocations are all infected by contagions frequently", "requirement_code": {"identify_main_cause_aggregate": 1}}, "data": {"data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "This section presents the mining framework for inferring cascading patterns in general spatiotemporal domains.", "solution_category": "data_manipulation", "solution_axial": "Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "The influence view adopts a table-based layout to organize and summarize cascades in an occlusion-free manner, as illustrated in figure.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix", "axial_code": [], "componenet_code": ["matrix"]}]}, {"author": "zsz", "index_original": 310, "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data", "pub_year": 2022, "domain": "Spatial cascade", "requirement": {"requirement_text": "R4 Display the uncertainties of in\ufb02uences. An infectionevent has many possible upstream infection events.Although the algorithm has estimated the mostlikely upstream, the system should reveal other pos-sibilities to understand the reliability and uncer-tainty of the inference result, such as how does thepossibility of the inferred upstream differ from that ofothers?", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "This section presents the mining framework for inferring cascading patterns in general spatiotemporal domains.", "solution_category": "data_manipulation", "solution_axial": "Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "Uncertainty Visualization. A cascading network is summa-rized from long-term observations. Thus, for a speci\ufb01c infec-tion event ev, other possibilities ems (m 6 \u00bc u) with a largerlikelihood f\u00f0evje m\u00de than the inferred f\u00f0evje u\u00de may exist.Uncertainty visualization should re\ufb02ect how much theinferred upstream eu exceeds ems.According to the experts\u2019 suggestions, those ems with alarger likelihood and longer duration are denoted morelikely causes. For the in\ufb02uences comprised by the same cell,we derive an uncertainty metric by dividing the number ofthe more likely causes by the number of in\ufb02uences. Themetric is then encoded with the cell luminance. For exam-ple, most cells in Fig. 4 B5, except those at the bottom, arebright, indicating that most of the in\ufb02uences are certain.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix", "axial_code": [], "componenet_code": ["matrix"]}]}, {"author": "zsz", "index_original": 311, "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data", "pub_year": 2022, "domain": "Spatial cascade", "requirement": {"requirement_text": "R4 Display the uncertainties of in\ufb02uences. An infectionevent has many possible upstream infection events.Although the algorithm has estimated the mostlikely upstream, the system should reveal other pos-sibilities to understand the reliability and uncer-tainty of the inference result, such as how does thepossibility of the inferred upstream differ from that ofothers?", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "This section presents the mining framework for inferring cascading patterns in general spatiotemporal domains.", "solution_category": "data_manipulation", "solution_axial": "Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "We design the uncertainty and influence visu- alizations in the influence view to reveal location influences from the two aspects. Users must know uncertainties before the influence analysis. Thus, the uncertainty vis- ualizations are displayed by default. ", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph+matrix", "axial_code": [], "componenet_code": ["matrix", "glyph"]}]}, {"author": "zsz", "index_original": 312, "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data", "pub_year": 2022, "domain": "Spatial cascade", "requirement": {"requirement_text": "R5 Characterize in\ufb02uencing processes. The experts canunderstand how a downstream event depends on itsupstream and what happened between them byexamining location in\ufb02uences. The following ques-tions may be asked: Does the downstream get easilyinfected when its upstream is infected? Are the infectionsituations similar?Therefore, the system should visu-ally depict in\ufb02uencing processes.", "requirement_code": {"collect_evidence": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "However, the visually intermittent appearance preventsusers from analyzing these in\ufb02uences, for example, grasp-ing the distribution of start time differences. Improving thereadability is also an order optimization problem like thatin the in\ufb02uence view. We address this issue based on theHamiltonian path problem again. Here, the weights areeuclidean distances between in\ufb02uence vectors (e.g., hts\u00f0eu\u00de ts\u00f0e v\u00de; te\u00f0eu\u00de  ts\u00f0e v\u00de; te\u00f0ev\u00de  ts\u00f0ev\u00dei in Fig. 6 B1). Such vec-tors properly portray the appearance of the in\ufb02uence glyphin the horizontal direction. We obtain a clear appearance inthe end (Fig. 6 D)", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "In\ufb02uence Visualization. We visualize every single in\ufb02uencewith a timeline-based in\ufb02uence glyph (Fig. 6 B), where the twoinfection events are aligned along the timeline. We placethese two events on the same horizontal position and makethe overlapping part bold for highlight, thereby improvingthe scalability and readability. Colors are assigned accordingto the corresponding locations, and the overlapping partmixes both. Subsequently, we visualize multiple in\ufb02uences bystacking the multiple glyphs and aligning them according tothe downstream events\u2019 start times (Fig. 6 C).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}]}, {"author": "zsz", "index_original": 313, "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data", "pub_year": 2022, "domain": "Spatial cascade", "requirement": {"requirement_text": "R5 Characterize in\ufb02uencing processes. The experts canunderstand how a downstream event depends on itsupstream and what happened between them byexamining location in\ufb02uences. The following ques-tions may be asked: Does the downstream get easilyinfected when its upstream is infected? Are the infectionsituations similar?Therefore, the system should visu-ally depict in\ufb02uencing processes.", "requirement_code": {"collect_evidence": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "This section presents the mining framework for inferring cascading patterns in general spatiotemporal domains.", "solution_category": "data_manipulation", "solution_axial": "Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "We design the uncertainty and influence visu- alizations in the influence view to reveal location influences from the two aspects. Users must know uncertainties before the influence analysis. Thus, the uncertainty vis- ualizations are displayed by default. ", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph+matrix", "axial_code": [], "componenet_code": ["matrix", "glyph"]}]}, {"author": "zsz", "index_original": 314, "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data", "pub_year": 2022, "domain": "Spatial cascade", "requirement": {"requirement_text": "R6 Investigate the temporal characteristics of cascades. Cas-cades can occur many times and lead to differentdegrees of infections. The experts aim to obtain thetemporal distribution of cascades (When do the cas-cades usually happen? Winter? Rush hour?) and the cas-cading effects on the involved locations (Does thecascades last long?). Thus, temporal visualizations forcascades should be implemented in the system", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "This section presents the mining framework for inferring cascading patterns in general spatiotemporal domains.", "solution_category": "data_manipulation", "solution_axial": "Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "We visualize many cascades along a long timeline and expose their temporal characteristics to users. As illustrated in C3, the temporal chart folds the timeline from top to bottom and left to right. Each cascade is represented as a bar. The positions of its upper and lower borders encode the start and end times of the cascade, respectively. The bars\u2019 occurrences are aggregated in both horizontal and vertical directions as the heatmaps above and to the right of the chart, respectively.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "matrix+bar", "axial_code": [], "componenet_code": ["bar", "matrix"]}]}, {"author": "zsz", "index_original": 315, "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data", "pub_year": 2022, "domain": "Spatial cascade", "requirement": {"requirement_text": "R7 Unfold individual cascades. The experts tend to accessdetailed infections and obtain convincing results byunfolding the cascades. They must understand howthe contagion spreads over the locations involved in a cascade to recover the reasons behind urban deterio-ration. Questions may be asked, for instance, when didA get infected? Were B and C infected after A got infected?How long was the delay? How severe was the cascade?", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "This section presents the mining framework for inferring cascading patterns in general spatiotemporal domains.", "solution_category": "data_manipulation", "solution_axial": "Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "Each card presents an individual cascade in detail and mainly constitutes multiple timelines. Each one indicates how a specific contagion infects an involved location and is colored according to the location. The card header displays the start and end times of the cas- cade. The cascading card design is borrowed from the experts\u2019 hand-draft of cascades.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "zsz", "index_original": 317, "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches", "pub_year": 2021, "domain": "sport", "requirement": {"requirement_text": "G1: Steerable projection of all tactics. An overview is re-quired to facilitate the identi\ufb01cation of key tactics (R1). Gen-erally, projection methods are used to provide an overviewof massive high-dimension data. Moreover, to ef\ufb01cientlyand comprehensively identify the key tactics, projections ofall tactics from diverse perspectives are required. Therefore,the projection should be steerable for users to generate anon-demand layout.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "The summary view contains a steerable projection plot and a control panel. In the projection plot, each tactic is embedded and projected as a point. For each point, we use color hue to encode different players since color hue is the most effective visual channel for categorical attributes besides spatial position [56]. The area of a point encodes the frequency, and the opacity encodes the scoring rate of a tactic.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "scatter+glyph", "axial_code": [], "componenet_code": ["scatter", "glyph"]}, {"solution_text": "In the control panel, analysts can switch the projection layout by selecting the attributes they want to focus on figure. In addition, analysts can filter the tactics in the plot according to the hit player, the frequency, and the scoring rate.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 318, "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches", "pub_year": 2021, "domain": "sport", "requirement": {"requirement_text": "G2: Effective glyphs of tactics. An effective glyph formultivariate tactic attributes is necessary for illustrationand investigation of tactics (R2, R3). Experts are oftenoverwhelmed when investigating the multi-dimensional at-tributes of multiple tactics (Fig. 2). A glyph can help solvethis issue [38], [39], [40]. However, the glyph in previouswork [1] cannot be scaled to more attributes and are notef\ufb01cient enough when applied to analysis of massive tacticdata. Therefore, a more scalable and effective glyph is re-quired to facilitate an ef\ufb01cient and comprehensive analysis.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "We placed the shared attribute values onthe plot as hints (Fig. 3(B2)). The widget below the plot dis-plays the details of a particular tactic with glyphs (G2) (to beintroduced in the detail view). Analysts can hover over thepoints to examine this information (Fig. 3(B3)). In the controlpanel, analysts can switch the projection layout by selectingthe attributes they want to focus on (Fig. 3(C1))", "solution_category": "visualization", "solution_axial": "annotation", "solution_compoent": "scatter+text", "axial_code": [], "componenet_code": ["text", "scatter"]}, {"solution_text": "Analysts can hover over thepoints to examine this information (Fig. 3(B3)). In the controlpanel, analysts can switch the projection layout by selectingthe attributes they want to focus on (Fig. 3(C1))", "solution_category": "interaction", "solution_axial": "OverviewandExplore,Filtering", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Filtering"], "componenet_code": ["overview_and_explore", "filtering"]}]}, {"author": "zsz", "index_original": 319, "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches", "pub_year": 2021, "domain": "sport", "requirement": {"requirement_text": "G2: Effective glyphs of tactics. An effective glyph formultivariate tactic attributes is necessary for illustrationand investigation of tactics (R2, R3). Experts are oftenoverwhelmed when investigating the multi-dimensional at-tributes of multiple tactics (Fig. 2). A glyph can help solvethis issue [38], [39], [40]. However, the glyph in previouswork [1] cannot be scaled to more attributes and are notef\ufb01cient enough when applied to analysis of massive tacticdata. Therefore, a more scalable and effective glyph is re-quired to facilitate an ef\ufb01cient and comprehensive analysis.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "We used a novel glyph to encode a stroke and used three glyphs to represent a tactic. Analysts can control the visibility of the attributes on the glyph. The glyph is designed to be circular to cater to the metaphor of the ball.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}]}, {"author": "zsz", "index_original": 320, "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches", "pub_year": 2021, "domain": "sport", "requirement": {"requirement_text": "G3: Comparative analysis of different tactics. Expertscan discover the differences of performances between tacticsby comparing their effects in the matches (e.g., temporalchange of scoring rates and using rates in the match)(R2). Moreover, through comparisons of particular tactic at-tributes, they can determine the reasons for the performanceof speci\ufb01c tactics (R3) through comparisons. They need toknow how a tactic differs from others in terms of technicaland contextual attributes. These differences can be used toidentify the key attributes that affect tactic performance.", "requirement_code": {"compare_entities": 1, "explain_differences": 1}}, "data": {"data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "The tactic list illustrates the most details of the selected tactics with glyphs in structure-driven placement for comparative and correlation analyses.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}]}, {"author": "zsz", "index_original": 321, "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches", "pub_year": 2021, "domain": "sport", "requirement": {"requirement_text": "G3: Comparative analysis of different tactics. Expertscan discover the differences of performances between tacticsby comparing their effects in the matches (e.g., temporalchange of scoring rates and using rates in the match)(R2). Moreover, through comparisons of particular tactic at-tributes, they can determine the reasons for the performanceof speci\ufb01c tactics (R3) through comparisons. They need toknow how a tactic differs from others in terms of technicaland contextual attributes. These differences can be used toidentify the key attributes that affect tactic performance.", "requirement_code": {"compare_entities": 1, "explain_differences": 1}}, "data": {"data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "We also provided the previous andsubsequent strokes of each tactic (Fig. 3(E1)) for correlationanalysis (G3) of the reciprocal relation between tactics ofthe two players", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}]}, {"author": "zsz", "index_original": 322, "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches", "pub_year": 2021, "domain": "sport", "requirement": {"requirement_text": "G3: Comparative analysis of different tactics. Expertscan discover the differences of performances between tacticsby comparing their effects in the matches (e.g., temporalchange of scoring rates and using rates in the match)(R2). Moreover, through comparisons of particular tactic at-tributes, they can determine the reasons for the performanceof speci\ufb01c tactics (R3) through comparisons. They need toknow how a tactic differs from others in terms of technicaland contextual attributes. These differences can be used toidentify the key attributes that affect tactic performance.", "requirement_code": {"compare_entities": 1, "explain_differences": 1}}, "data": {"data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "We provided context information to facilitate comparative and correlation analyses. The point beside each tactic encodes the frequency and scoring rate by its area and transparency, respectively. Additionally, we displayed the distribution of the frequency and scoring rate of a tactic in different conditions defined by the domain experts. The frequency and scoring rate were encoded using the height and luminance of the bar, respectively.", "solution_category": "visualization", "solution_axial": "annotation", "solution_compoent": "bubble+bar", "axial_code": [], "componenet_code": ["bar", "bubble"]}]}, {"author": "zsz", "index_original": 323, "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches", "pub_year": 2021, "domain": "sport", "requirement": {"requirement_text": "G4: Correlation analysis of adjacent tactics. Correlationanalysis is crucial for performance characterizing and rea-soning (R2, R3). A tactic is highly related to the previousand subsequent tactics since it shares two strokes with eachof them (Fig. 2). According to the experts, the subsequentand current tactics can help assess the performance of thecurrent tactic, and the previous and current tactics can helpinterpret the performance of the current tactic (R3)", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "The tactic list illustrates the most details of the selected tactics with glyphs in structure-driven placement for comparative and correlation analyses.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}]}, {"author": "zsz", "index_original": 324, "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches", "pub_year": 2021, "domain": "sport", "requirement": {"requirement_text": "G4: Correlation analysis of adjacent tactics. Correlationanalysis is crucial for performance characterizing and rea-soning (R2, R3). A tactic is highly related to the previousand subsequent tactics since it shares two strokes with eachof them (Fig. 2). According to the experts, the subsequentand current tactics can help assess the performance of thecurrent tactic, and the previous and current tactics can helpinterpret the performance of the current tactic (R3)", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "We provided context information to facilitate comparative and correlation analyses. The point beside each tactic encodes the frequency and scoring rate by its area and transparency, respectively. Additionally, we displayed the distribution of the frequency and scoring rate of a tactic in different conditions defined by the domain experts. The frequency and scoring rate were encoded using the height and luminance of the bar, respectively.", "solution_category": "visualization", "solution_axial": "annotation", "solution_compoent": "bubble+bar", "axial_code": [], "componenet_code": ["bar", "bubble"]}]}, {"author": "zsz", "index_original": 325, "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches", "pub_year": 2021, "domain": "sport", "requirement": {"requirement_text": "G5: Videos sessions of each tactic. Organizing the rawvideos based on the tactic data can help experts ef\ufb01cientlyexamine the video contents they need. Since the analysisobject is tactic, videos clipped based on tactics can helpquickly validate the analysis results of particular tactics (R4)", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.", "data_code": {"tables": 1, "categorical": 1}}, "solution": [{"solution_text": "We provided context information to facilitate comparative and correlation analyses. The point beside each tactic encodes the frequency and scoring rate by its area and transparency, respectively. Additionally, we displayed the distribution of the frequency and scoring rate of a tactic in different conditions defined by the domain experts. The frequency and scoring rate were encoded using the height and luminance of the bar, respectively.", "solution_category": "visualization", "solution_axial": "annotation", "solution_compoent": "bubble+bar", "axial_code": [], "componenet_code": ["bar", "bubble"]}]}, {"author": "zsz", "index_original": 326, "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches", "pub_year": 2021, "domain": "sport", "requirement": {"requirement_text": "G5: Videos sessions of each tactic. Organizing the rawvideos based on the tactic data can help experts ef\ufb01cientlyexamine the video contents they need. Since the analysisobject is tactic, videos clipped based on tactics can helpquickly validate the analysis results of particular tactics (R4)", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.", "data_code": {"tables": 1, "categorical": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The display view provides a replay of each tactic for validation and further investigation. Analysts can click on the button near the context information to unfold the display view, which includes the video clips and animation of a particular tactic. The display view mainly includes a rally list, a display table, and a video widget.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}, {"solution_text": "The display view provides a replay of each tactic for validation and further investigation. Analysts can click on the button near the context information to unfold the display view, which includes the video clips and animation of a particular tactic. The display view mainly includes a rally list, a display table, and a video widget.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "video+animation", "axial_code": [], "componenet_code": ["video", "animation"]}]}, {"author": "zsz", "index_original": 329, "paper_title": "HisVA: A Visual Analytics System for Studying History", "pub_year": 2022, "domain": "education", "requirement": {"requirement_text": "Presents a reduced but effective exploration space.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.", "data_code": {"tables": 1, "textual": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48].", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "zsz", "index_original": 330, "paper_title": "HisVA: A Visual Analytics System for Studying History", "pub_year": 2022, "domain": "education", "requirement": {"requirement_text": "Presents a reduced but effective exploration space.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.", "data_code": {"tables": 1, "textual": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48]. Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]", "solution_category": "data_manipulation", "solution_axial": "Modeling,Excluding", "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.", "axial_code": ["Modeling", "Excluding"], "componenet_code": ["modeling", "excluding"]}, {"solution_text": "The event view presents multiple event charts and a summary chart. We place twenty event charts that are con- structed based on computed topics and time information. The charts are initially ordered as produced by the modeling algorithm, but users can scroll the view, interac- tively adjust the chart order, and hide and show any event chart they choose. Each chart shows 10 topic keywords, which are ordered by their contribution to each topic.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area+bubble", "axial_code": [], "componenet_code": ["bubble", "area"]}, {"solution_text": "The charts are initially ordered as produced by the modeling algorithm, but users can scroll the view, interac- tively adjust the chart order, and hide and show any event chart they choose.", "solution_category": "interaction", "solution_axial": "Reconfigure,Filtering", "solution_compoent": "", "axial_code": ["Reconfigure", "Filtering"], "componenet_code": ["reconfigure", "filtering"]}]}, {"author": "zsz", "index_original": 331, "paper_title": "HisVA: A Visual Analytics System for Studying History", "pub_year": 2022, "domain": "education", "requirement": {"requirement_text": "Presents a reduced but effective exploration space.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.", "data_code": {"tables": 1, "textual": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48]. Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]", "solution_category": "data_manipulation", "solution_axial": "Modeling,Excluding", "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.", "axial_code": ["Modeling", "Excluding"], "componenet_code": ["modeling", "excluding"]}, {"solution_text": "Summary Chart. The summary chart (a line chart at the bottom of the event view, A2) shows the aggregated number of the articles by time to let users see the number of events across time and important events in specific time ranges. For example, the summary chart shows two peaks from 1910 to 1920 and from 1940 to 1945. The peaks mean that, given the document corpus, some events in the two time ranges have the greatest number of articles related to World War I and World War II. In addition, the summary chart provides two vertical gray bars at each side which users use for filtering time ranges. If the bars are at each end, the entire data set is used for computing the number of events by time.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "bar+area", "axial_code": [], "componenet_code": ["bar", "area"]}, {"solution_text": "In addition, the summary chart provides two vertical gray bars at each side which users use for filtering time ranges. If the bars are at each end, the entire data set is used for computing the number of events by time.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 332, "paper_title": "HisVA: A Visual Analytics System for Studying History", "pub_year": 2022, "domain": "education", "requirement": {"requirement_text": "Presents a reduced but effective exploration space.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.", "data_code": {"tables": 1, "textual": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48]. Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]", "solution_category": "data_manipulation", "solution_axial": "Modeling,Excluding", "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.", "axial_code": ["Modeling", "Excluding"], "componenet_code": ["modeling", "excluding"]}, {"solution_text": "Displaying Important Articles. The events with higher importance weights (i.e., topic contribution or page-rank scores) than the threshold value are presented as a pink dot at the top of the event chart. When a series of adja- cent dots occlude each other, they are aggregated into a wide dot (i.e., ellipse). For example, the third event chart in figure shows that the topic of the chart is \u201cItalian,\u201d and there are seven important events and six wide dots which contain more than two important events.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area+bubble", "axial_code": [], "componenet_code": ["bubble", "area"]}]}, {"author": "zsz", "index_original": 333, "paper_title": "HisVA: A Visual Analytics System for Studying History", "pub_year": 2022, "domain": "education", "requirement": {"requirement_text": "Presents a reduced but effective exploration space.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.", "data_code": {"tables": 1, "textual": 1, "clusters_and_sets_and_lists": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48]. Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]", "solution_category": "data_manipulation", "solution_axial": "Modeling,Excluding", "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.", "axial_code": ["Modeling", "Excluding"], "componenet_code": ["modeling", "excluding"]}, {"solution_text": "For efficient exploration, the list view allows users to sort the events by date, important weight, and topic. Events with a importance weight higher than the threshold value are highlighted in gray (e.g., \u201cSecond Balkan War\u201d in figure).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "image+text", "axial_code": [], "componenet_code": ["text", "image"]}, {"solution_text": "For efficient exploration, the list view allows users to sort the events by date, important weight, and topic. Events with a importance weight higher than the threshold value are highlighted in gray (e.g., \u201cSecond Balkan War\u201d in figure).", "solution_category": "interaction", "solution_axial": "Reconfigure,Filtering", "solution_compoent": "", "axial_code": ["Reconfigure", "Filtering"], "componenet_code": ["reconfigure", "filtering"]}]}, {"author": "zsz", "index_original": 334, "paper_title": "HisVA: A Visual Analytics System for Studying History", "pub_year": 2022, "domain": "education", "requirement": {"requirement_text": "Supplies spatial and temporal contexts of historical events.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.", "data_code": {"tables": 1, "textual": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]", "solution_category": "data_manipulation", "solution_axial": "Excluding", "solution_compoent": "Dateandlocationentityextraction.Selectsmostfrequentlyshowninformationasrepresentativetemporalandspatialdata.", "axial_code": ["Excluding"], "componenet_code": ["excluding"]}]}, {"author": "zsz", "index_original": 335, "paper_title": "HisVA: A Visual Analytics System for Studying History", "pub_year": 2022, "domain": "education", "requirement": {"requirement_text": "Supplies spatial and temporal contexts of historical events.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.", "data_code": {"tables": 1, "textual": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48]. Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]", "solution_category": "data_manipulation", "solution_axial": "Modeling,Excluding", "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.", "axial_code": ["Modeling", "Excluding"], "componenet_code": ["modeling", "excluding"]}, {"solution_text": "The event view presents multiple event charts and a summary chart. We place twenty event charts that are con- structed based on computed topics and time information. The charts are initially ordered as produced by the modeling algorithm, but users can scroll the view, interac- tively adjust the chart order, and hide and show any event chart they choose. Each chart shows 10 topic keywords, which are ordered by their contribution to each topic.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area+bubble", "axial_code": [], "componenet_code": ["bubble", "area"]}, {"solution_text": "The charts are initially ordered as produced by the modeling algorithm, but users can scroll the view, interac- tively adjust the chart order, and hide and show any event chart they choose.", "solution_category": "interaction", "solution_axial": "Reconfigure,Filtering", "solution_compoent": "", "axial_code": ["Reconfigure", "Filtering"], "componenet_code": ["reconfigure", "filtering"]}]}, {"author": "zsz", "index_original": 336, "paper_title": "HisVA: A Visual Analytics System for Studying History", "pub_year": 2022, "domain": "education", "requirement": {"requirement_text": "Supplies spatial and temporal contexts of historical events.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.", "data_code": {"tables": 1, "textual": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48]. Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]", "solution_category": "data_manipulation", "solution_axial": "Modeling,Excluding", "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.", "axial_code": ["Modeling", "Excluding"], "componenet_code": ["modeling", "excluding"]}, {"solution_text": "Summary Chart. The summary chart (a line chart at the bottom of the event view, A2) shows the aggregated number of the articles by time to let users see the number of events across time and important events in specific time ranges. For example, the summary chart shows two peaks from 1910 to 1920 and from 1940 to 1945. The peaks mean that, given the document corpus, some events in the two time ranges have the greatest number of articles related to World War I and World War II. In addition, the summary chart provides two vertical gray bars at each side which users use for filtering time ranges. If the bars are at each end, the entire data set is used for computing the number of events by time.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "bar+area", "axial_code": [], "componenet_code": ["bar", "area"]}, {"solution_text": "In addition, the summary chart provides two vertical gray bars at each side which users use for filtering time ranges. If the bars are at each end, the entire data set is used for computing the number of events by time.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 337, "paper_title": "HisVA: A Visual Analytics System for Studying History", "pub_year": 2022, "domain": "education", "requirement": {"requirement_text": "Assists in the acquisition of information for investi- gation of historical events.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.", "data_code": {"tables": 1, "textual": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "We used two methods for recommending importantarticles to help users begin their explorations (R3). The first is topic contribution, which is computed by a topic model-ing algorithm and indicates how much an article contributesto forming each topic. As a second method, we use thearticles\u2019 centrality (i.e., page-rank) in a network, computedfrom the Wikipedia clickstream. The page-rank value of anarticle indicates its popularity, because page-rank valuesare proportional to the article\u2019s number of views. HisVAprovides a toggle button for the selection between contribu-tion-based (TOPIC_REC) and popularity-based recommen-dations, as shown at the top of Fig. 2. In addition to thetoggle, there is the slider bar, which can be used to adjustthreshold values. If an article\u2019s contribution or popularityvalue is greater than the threshold value, it is recommendedas an important article.", "solution_category": "data_manipulation", "solution_axial": "Modeling,AlgorithmicCalculation", "solution_compoent": "Recommendationbytopiccontributionandcentrality.", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "We used two methods for recommending importantarticles to help users begin their explorations (R3). The first is topic contribution, which is computed by a topic model-ing algorithm and indicates how much an article contributesto forming each topic. As a second method, we use thearticles\u2019 centrality (i.e., page-rank) in a network, computedfrom the Wikipedia clickstream. The page-rank value of anarticle indicates its popularity, because page-rank valuesare proportional to the article\u2019s number of views. HisVAprovides a toggle button for the selection between contribu-tion-based (TOPIC_REC) and popularity-based recommen-dations, as shown at the top of Fig. 2. In addition to thetoggle, there is the slider bar, which can be used to adjustthreshold values. If an article\u2019s contribution or popularityvalue is greater than the threshold value, it is recommendedas an important article.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 338, "paper_title": "HisVA: A Visual Analytics System for Studying History", "pub_year": 2022, "domain": "education", "requirement": {"requirement_text": "Assists in the acquisition of information for investi- gation of historical events.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.", "data_code": {"tables": 1, "textual": 1, "clusters_and_sets_and_lists": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48]. Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]", "solution_category": "data_manipulation", "solution_axial": "Modeling,Excluding", "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.", "axial_code": ["Modeling", "Excluding"], "componenet_code": ["modeling", "excluding"]}, {"solution_text": "To help users efficiently investigate historical events with detailed information, HisVA provides a resource view comprising four sub-views\u2014list, result, article, and annotation views. The list view showcases historical events, allowing users to sequentially access a series of events, along with meta information, such as a thumbnail, event date, topic number associated with the article, topic weight, and page- rank value. ", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "image+text", "axial_code": [], "componenet_code": ["text", "image"]}]}, {"author": "zsz", "index_original": 339, "paper_title": "HisVA: A Visual Analytics System for Studying History", "pub_year": 2022, "domain": "education", "requirement": {"requirement_text": "Promotes linking diverse events and finding rela- tionships among them.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.", "data_code": {"tables": 1, "textual": 1, "clusters_and_sets_and_lists": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48]. Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]", "solution_category": "data_manipulation", "solution_axial": "Modeling,Excluding", "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.", "axial_code": ["Modeling", "Excluding"], "componenet_code": ["modeling", "excluding"]}, {"solution_text": "The result view lists articles in response to the user\u2019s search in the search bar. We present articles with the linear structure for easy content navigation in studying [60]. When users click on an event, a new window pops up and presents the clicked event with other events that are most relevant to the clicked event.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "image+text", "axial_code": [], "componenet_code": ["text", "image"]}, {"solution_text": "The result view lists articles in response to the user\u2019s search in the search bar. We present articles with the linear structure for easy content navigation in studying [60]. When users click on an event, a new window pops up and presents the clicked event with other events that are most relevant to the clicked event.", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "zsz", "index_original": 340, "paper_title": "HisVA: A Visual Analytics System for Studying History", "pub_year": 2022, "domain": "education", "requirement": {"requirement_text": "Promotes linking diverse events and finding rela- tionships among them.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.", "data_code": {"tables": 1, "textual": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "To facilitate the task of linking events and finding rela- tionships among them, HisVA utilizes CMVs [11] with quick and easy interactions. For example, when a time range is set in the summary chart, the  historical events in the user-specified time range are pre- sented in all map, article, and event views. When the event\u2019s importance or frequency weight is changed due to user interactions with the filters, the visu- alization and articles in each view are also updated. In the same context, when users select regions by drawing a box or clicking countries in the map view, visualiza- tions and articles in other views are updated accord- ingly. When users select an event from a cluster marker, the article view is auto-scrolled to present the part of the article associated with the selected event. When users hover the mouse over an article in the list view, the cen- ter of the map view is moved to the location associated with the selected article and the cluster marker associ- ated with the selected article changes to the spiral repre- sentation with the clicked article highlighted in red.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 341, "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison", "pub_year": 2022, "domain": "anomaly detection", "requirement": {"requirement_text": "R1. Provide Inspection on the Possible Anomalies From Unla-beled Datasets. Real-world event sequence datasets often con-tain a large number of unlabeled sequences. Users oftenneed to narrow the inspection scope to a smaller group ofsequences that require attention. For example, the medicalexperts commented that they typically filter out problematicpatients based on specific criteria before drilling intodetailed clinical events", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2); After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z; To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal; To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.", "solution_category": "data_manipulation", "solution_axial": "Modeling,SimilarityCalculation,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "SimilarityCalculation"], "componenet_code": ["modeling", "algorithmic_calculation", "similarity_calculation"]}, {"solution_text": "The analysis starts from the anomaly overview, which provides an MDS projection of the latent vectors for all anomalous sequences in the dataset and allows users to select an anoma- lous sequence for subsequent analysis.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "contour+scatter", "axial_code": [], "componenet_code": ["contour", "scatter"]}, {"solution_text": "The analysis starts from the anomaly overview, which provides an MDS projection of the latent vectors for all anomalous sequences in the dataset and allows users to select an anoma- lous sequence for subsequent analysis.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 342, "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison", "pub_year": 2022, "domain": "anomaly detection", "requirement": {"requirement_text": "R1. Provide Inspection on the Possible Anomalies From Unla-beled Datasets. Real-world event sequence datasets often con-tain a large number of unlabeled sequences. Users oftenneed to narrow the inspection scope to a smaller group ofsequences that require attention. For example, the medicalexperts commented that they typically filter out problematicpatients based on specific criteria before drilling intodetailed clinical events", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "Latentvectorsforanomaloussequencesdetection.", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "zsz", "index_original": 343, "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison", "pub_year": 2022, "domain": "anomaly detection", "requirement": {"requirement_text": "R1. Provide Inspection on the Possible Anomalies From Unla-beled Datasets. Real-world event sequence datasets often con-tain a large number of unlabeled sequences. Users oftenneed to narrow the inspection scope to a smaller group ofsequences that require attention. For example, the medicalexperts commented that they typically filter out problematicpatients based on specific criteria before drilling intodetailed clinical events", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2)", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "Sequence-to-SequenceVAEforinterpretableanomalydetection.", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "zsz", "index_original": 344, "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison", "pub_year": 2022, "domain": "anomaly detection", "requirement": {"requirement_text": "R2. Facilitate Interpretation by Identifying Anomalous EventsWithin Abnormal Sequences. The interpretability of the anom-alous sequences highly relies on the analysis of low-levelevents. For example, the clinical path of a patient may bedetected as an anomaly due to a misused medicine, andsoftware may be considered suspicious due to abnormal \ufb01leexecutions. However, real-world event sequences can belong in length and heterogeneous in types, which makes itchallenging to identify anomalous events", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2); After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z; To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal; To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.", "solution_category": "data_manipulation", "solution_axial": "Modeling,SimilarityCalculation,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "SimilarityCalculation"], "componenet_code": ["modeling", "algorithmic_calculation", "similarity_calculation"]}, {"solution_text": "The main panel supports visual interpretation of the selected anomalous sequence via sequence comparison. Specifically, the main view is vertically divided into three major parts, including an anomalous sequence view showing the progression of the selected anomalous sequence, with the type of abnormality being marked out on anoma- lous events.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "bar+glyph", "axial_code": [], "componenet_code": ["bar", "glyph"]}]}, {"author": "zsz", "index_original": 345, "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison", "pub_year": 2022, "domain": "anomaly detection", "requirement": {"requirement_text": "R2. Facilitate Interpretation by Identifying Anomalous EventsWithin Abnormal Sequences. The interpretability of the anom-alous sequences highly relies on the analysis of low-levelevents. For example, the clinical path of a patient may bedetected as an anomaly due to a misused medicine, andsoftware may be considered suspicious due to abnormal \ufb01leexecutions. However, real-world event sequences can belong in length and heterogeneous in types, which makes itchallenging to identify anomalous events", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2)", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "Sequence-to-SequenceVAEforinterpretableanomalydetection.", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "zsz", "index_original": 346, "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison", "pub_year": 2022, "domain": "anomaly detection", "requirement": {"requirement_text": "R2. Facilitate Interpretation by Identifying Anomalous EventsWithin Abnormal Sequences. The interpretability of the anom-alous sequences highly relies on the analysis of low-levelevents. For example, the clinical path of a patient may bedetected as an anomaly due to a misused medicine, andsoftware may be considered suspicious due to abnormal \ufb01leexecutions. However, real-world event sequences can belong in length and heterogeneous in types, which makes itchallenging to identify anomalous events", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "Interpretingsequenceanomalieswithreconstructionprobabilities.", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "zsz", "index_original": 347, "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison", "pub_year": 2022, "domain": "anomaly detection", "requirement": {"requirement_text": "R3. Support Anomaly Analysis in the Context of EntireSequence Progressions. Instead of focusing on the anomalousevents, analyzing anomalies within the context of entiresequence progressions can help illustrate the cause and consequences of the anomalous events. Speci\ufb01cally, themedical experts stated that early prevention could beachieved if we know what led to the occurrence of ananomaly.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2); After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z; To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal; To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.", "solution_category": "data_manipulation", "solution_axial": "Modeling,SimilarityCalculation,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "SimilarityCalculation"], "componenet_code": ["modeling", "algorithmic_calculation", "similarity_calculation"]}, {"solution_text": "The main panel supports visual interpretation of the selected anomalous sequence via sequence comparison. Specifically, the main view is vertically divided into three major parts, including an anomalous sequence view showing the progression of the selected anomalous sequence, with the type of abnormality being marked out on anoma- lous events.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "bar+glyph", "axial_code": [], "componenet_code": ["bar", "glyph"]}]}, {"author": "zsz", "index_original": 348, "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison", "pub_year": 2022, "domain": "anomaly detection", "requirement": {"requirement_text": "R3. Support Anomaly Analysis in the Context of EntireSequence Progressions. Instead of focusing on the anomalousevents, analyzing anomalies within the context of entiresequence progressions can help illustrate the cause and consequences of the anomalous events. Speci\ufb01cally, themedical experts stated that early prevention could beachieved if we know what led to the occurrence of ananomaly.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2); After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z; To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal; To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.", "solution_category": "data_manipulation", "solution_axial": "Modeling,SimilarityCalculation,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "SimilarityCalculation"], "componenet_code": ["modeling", "algorithmic_calculation", "similarity_calculation"]}, {"solution_text": "The selected anomalous sequence is organized intosequence segments aligning with the time slots (Fig. 4 (2)).Events in adjacent time slots are separated by a label of thesequence ID. In addition, events in each time slot are verti-cally aligned with events in the reconstruction view to helpidentify the event labels.We emphasize the event anomalies in the anomaloussequence with a set of glyphs (Figs. 4a, 4b, and 4c) repre-senting the three anomaly types. The glyphs are designedbased on the metaphor of editing symbols, intending to con-vey insights on operations that are required to transform ananomalous sequence into normal. Speci\ufb01cally, we representevent redundancy with a delete symbol (Fig. 4a), event miss-ing with an insert symbol (Fig. 4b), and temporal anomalywith a move arrow (Fig. 4c) pointing from the observed timeslot to the expected time slot. Event missing and the end-point of the temporal anomaly is encoded using a whiterectangular node with a dashed border, indicating that anevent is expected to occur but is not present.Additionally, we retrieve the subgroup of normalsequences for each type of event anomaly that \u201csupport\u201dthe corresponding event to be abnormal as the comparisongroup. For example, in Fig. 3, the comparison group for theevent redundancy in the \ufb01rst slot (\u201c#0\u201d) shall be the top twoclusters in the normal sequence view below where the event isalso not presented. We summarize the abnormality ofevents analyzed by matching the anomalous sequence to allselected normal sequences and integrating all anomalytypes identi\ufb01ed for each event. By default, we select thedominant type of anomaly that has the largest comparisongroup for display. However, users can change their focusby selecting a different subgroup of normal sequences dur-ing analysis for comparison. We calculate a support rate asthe proportion of the selected normal sequences in the com-parison group, and the anomaly score as the average match-ing cost for each event anomaly to help users justify thelevel and the con\ufb01dence of abnormality. The anomaly scoreis displayed with the height of a red peak above, and thesupport rate is encoded with the height of a blue peak underthe anomalous event", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "bar+glyph", "axial_code": [], "componenet_code": ["bar", "glyph"]}, {"solution_text": "However, users can change their focusby selecting a different subgroup of normal sequences dur-ing analysis for comparison. ", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 349, "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison", "pub_year": 2022, "domain": "anomaly detection", "requirement": {"requirement_text": "R4. Allow Case-Based Reasoning to Gain User Trust and HelpExplore Higher-Level Anomalous Patterns. The lack of explain-ability in deep learning models inhibits user trust in theanalysis result. Recent studies tackled this issue throughcase-based reasoning [47], which generates explanationsbased on similar cases in the dataset. Medical expertsexpressed similar interests in following treatment plansunder normal and abnormal circumstances to understandhow the anomalous patient deviates from typical cases.Moreover, comparing normal and abnormal sequences canreveal higher-level anomalous patterns (e.g., anomaloussub-sequences or event ordering) beyond low-level ones.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2); After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z; To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal; To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.", "solution_category": "data_manipulation", "solution_axial": "Modeling,SimilarityCalculation,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "SimilarityCalculation"], "componenet_code": ["modeling", "algorithmic_calculation", "similarity_calculation"]}, {"solution_text": "The similarity view (Fig. 3 (2)) displays the distribution of all normalsequences and their similarities to the selected anomaloussequence, which is derived from the distance of correspond-ing latent vectors. In particular, we measure the sequencesimilarity in two ways: their cost for matching events in thenormal sequences to the selected anomalous sequence (notedas matching cost) and the distance between the latent vectorsof normal sequences and the anomalous sequence (noted assequence distance). Sequences with small matching costs gen-erally have more similar events, while sequences with smalldistances to the anomalous sequence usually contain keyprogression patterns, such as an indicator of a particulartype of disease. From this view, users can switch betweendifferent similarity measurements to inspect different distri-butions and select a group of normal sequences to comparewith the anomalous sequence in the main panel (R4).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "From this view, users can switch betweendifferent similarity measurements to inspect different distri-butions and select a group of normal sequences to comparewith the anomalous sequence in the main panel (R4).", "solution_category": "interaction", "solution_axial": "Filtering,Selecting", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "zsz", "index_original": 350, "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison", "pub_year": 2022, "domain": "anomaly detection", "requirement": {"requirement_text": "R4. Allow Case-Based Reasoning to Gain User Trust and HelpExplore Higher-Level Anomalous Patterns. The lack of explain-ability in deep learning models inhibits user trust in theanalysis result. Recent studies tackled this issue throughcase-based reasoning [47], which generates explanationsbased on similar cases in the dataset. Medical expertsexpressed similar interests in following treatment plansunder normal and abnormal circumstances to understandhow the anomalous patient deviates from typical cases.Moreover, comparing normal and abnormal sequences canreveal higher-level anomalous patterns (e.g., anomaloussub-sequences or event ordering) beyond low-level ones.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "Comparinganomaloussequenceswithnormalonesusingasequencecomparisonmethod.", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}]}, {"author": "zsz", "index_original": 351, "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison", "pub_year": 2022, "domain": "anomaly detection", "requirement": {"requirement_text": "R4. Allow Case-Based Reasoning to Gain User Trust and HelpExplore Higher-Level Anomalous Patterns. The lack of explain-ability in deep learning models inhibits user trust in theanalysis result. Recent studies tackled this issue throughcase-based reasoning [47], which generates explanationsbased on similar cases in the dataset. Medical expertsexpressed similar interests in following treatment plansunder normal and abnormal circumstances to understandhow the anomalous patient deviates from typical cases.Moreover, comparing normal and abnormal sequences canreveal higher-level anomalous patterns (e.g., anomaloussub-sequences or event ordering) beyond low-level ones.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2); After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z; To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal; To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.", "solution_category": "data_manipulation", "solution_axial": "Modeling,SimilarityCalculation,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "SimilarityCalculation"], "componenet_code": ["modeling", "algorithmic_calculation", "similarity_calculation"]}, {"solution_text": "The main panel supports visual interpretation of the selected anomalous sequence via sequence comparison. Specifically, the main view is vertically divided into three major parts, including a normal sequence view summarizing the progression of similar normal sequences that are selected by the user from the similarity view.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "network+bar", "axial_code": [], "componenet_code": ["bar", "network"]}]}, {"author": "zsz", "index_original": 352, "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison", "pub_year": 2022, "domain": "anomaly detection", "requirement": {"requirement_text": "R4. Allow Case-Based Reasoning to Gain User Trust and HelpExplore Higher-Level Anomalous Patterns. The lack of explain-ability in deep learning models inhibits user trust in theanalysis result. Recent studies tackled this issue throughcase-based reasoning [47], which generates explanationsbased on similar cases in the dataset. Medical expertsexpressed similar interests in following treatment plansunder normal and abnormal circumstances to understandhow the anomalous patient deviates from typical cases.Moreover, comparing normal and abnormal sequences canreveal higher-level anomalous patterns (e.g., anomaloussub-sequences or event ordering) beyond low-level ones.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2); After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z; To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal; To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.", "solution_category": "data_manipulation", "solution_axial": "Modeling,SimilarityCalculation,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "SimilarityCalculation"], "componenet_code": ["modeling", "algorithmic_calculation", "similarity_calculation"]}, {"solution_text": "The selected normal sequences are displayed in the normalsequence view (Fig. 3 (5)). To allow comparative analysis atdifferent granularity of sequence aggregation (R5), the nor-mal sequence view is designed to support two visualizationmodes: the sequence mode (Fig. 3 (5B)) for preserving theindividual details of the normal sequences and the clustermode (Fig. 3 (5A)) for enabling inspecting the progressionpaths of a larger number of normal sequences at a time.Users can switch between two visualization mode using thebutton at the top of the main panel (Fig. 3c).Sequence Mode. The sequence mode of the normal sequenceview displays the sequences of normal records individually,aiming to support sequence-to-sequence comparison andef\ufb01cient access to the raw data of normal sequences. Asshown in Fig. 3 (5B), the normal sequences are displayed ina scrollable list. The encoding schema of each individualsequence is kept consistent with the anomalous sequencefor easy comparison. Users can select any individualsequence to compare the anomalous sequence with, and theevent anomalies marked in the anomalous sequence view shallbe updated accordingly. The sequences are ranked from topto bottom with their gradually increasing matching cost orsequence distances to the anomalous sequence in the latentspace, depending on the metric utilized in the selection ofnormal sequences in the similarity view (Fig. 3a). Analystscan focus their comparison to the \ufb01rst few sequences toinvestigate the minimum effort of turning the anomaloussequence into normal, or sequences with similar progres-sion context in order to avoid introducing noisy event com-parison results.Cluster Mode. The cluster mode of the normal sequenceview summarizes the progression of normal sequences intoa \ufb02ow-based visualization by clustering sequence segmentsin each time slot. In particular, the sequence segments ineach slot are clustered using Mean Shift Clustering [55]based on the multi-hot vectors introduced in Section 4. Notethat we include the segments in the anomalous sequence togenerate the clusters, and take the anomalous sequenceaway from the corresponding clusters when displayingonly the normal sequences.The visualization is designed to support comparing theanomalous sequence with subgroups of normal sequencesthat have particular progression patterns. Each cluster isrepresented with a rectangular node (Fig. 4d), consistingof a left-side label displaying the number of sequencesclustered in each node, and the main content depicting theevent occurrence of the clustered sequence segments. Notethat the text in the left-side label is rotated to distinguishfrom the sequence id annotated in the presentation of indi-vidual sequences. The occurrence of each event is encodedwith a vertical bar (Fig. 4e), horizontally aligned with thesame type of event in the reconstruction view and the anoma-lous sequence view. The height of each bar is proportional tothe number of sequences in each cluster having the corre-sponding event occurrence. The color encoding is consis-tent with other views showing the occurrence probability.We set the vertical position of cluster nodes in each timeslot using a layout algorithm introduced in [56] to illus-trate the similarity between clusters and minimize linkcrossing intuitively. Cluster nodes with similar eventoccurrences are grouped together, illustrating a higher-level structure of sequence progression. Cluster nodes inadjacent time slots are connected with light grey links(Fig. 4f) to uncover the transition patterns of sequencesbetween clusters.The cluster nodes can be expanded to show speci\ufb01csequence segments within and contracted back to a summa-rized cluster node in response to a double click (as shown inFig. 3h). This design aims to help users decide the granular-ity of analysis with more \ufb02exibility. The system also sup-ports clustering analysis with the selected anomaloussequence incorporated to help analysts overview how theprogression of anomalous record deviates from the normalgroup. By switching on the overlay button (Fig. 3d), theanomalous sequence segments are added back to the clus-ters. The cluster nodes and transition links that the anoma-lous sequence progress through are highlighted in red (asshown in Fig. 6 (1)) so that users can quickly identify prob-lematic time slots in which the anomalous sequence fall intothe clusters with a small population", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "network+bar", "axial_code": [], "componenet_code": ["bar", "network"]}, {"solution_text": "This design aims to help users decide the granular-ity of analysis with more \ufb02exibility. The system also sup-ports clustering analysis with the selected anomaloussequence incorporated to help analysts overview how theprogression of anomalous record deviates from the normalgroup. By switching on the overlay button (Fig. 3d), theanomalous sequence segments are added back to the clus-ters. The cluster nodes and transition links that the anoma-lous sequence progress through are highlighted in red (asshown in Fig. 6 (1)) so that users can quickly identify prob-lematic time slots in which the anomalous sequence fall intothe clusters with a small population", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 353, "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison", "pub_year": 2022, "domain": "anomaly detection", "requirement": {"requirement_text": "R5. Provide Multi-Level Aggregation and Comparison toExplore the Full Hierarchy and Interpret Various Sequence Anal-ysis Results. Applying different levels of aggregation for agroup of sequences can result in distinct interpretations ofthe result. For example, the anomalous events detected bycomparing an anomalous sequence with an individual nor-mal sequence may be different from the result when com-paring with a subgroup.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2); After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z; To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal; To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.", "solution_category": "data_manipulation", "solution_axial": "Modeling,SimilarityCalculation,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "SimilarityCalculation"], "componenet_code": ["modeling", "algorithmic_calculation", "similarity_calculation"]}, {"solution_text": "The selected normal sequences are displayed in the normalsequence view (Fig. 3 (5)). To allow comparative analysis atdifferent granularity of sequence aggregation (R5), the nor-mal sequence view is designed to support two visualizationmodes: the sequence mode (Fig. 3 (5B)) for preserving theindividual details of the normal sequences and the clustermode (Fig. 3 (5A)) for enabling inspecting the progressionpaths of a larger number of normal sequences at a time.Users can switch between two visualization mode using thebutton at the top of the main panel (Fig. 3c).Sequence Mode. The sequence mode of the normal sequenceview displays the sequences of normal records individually,aiming to support sequence-to-sequence comparison andef\ufb01cient access to the raw data of normal sequences. Asshown in Fig. 3 (5B), the normal sequences are displayed ina scrollable list. The encoding schema of each individualsequence is kept consistent with the anomalous sequencefor easy comparison. Users can select any individualsequence to compare the anomalous sequence with, and theevent anomalies marked in the anomalous sequence view shallbe updated accordingly. The sequences are ranked from topto bottom with their gradually increasing matching cost orsequence distances to the anomalous sequence in the latentspace, depending on the metric utilized in the selection ofnormal sequences in the similarity view (Fig. 3a). Analystscan focus their comparison to the \ufb01rst few sequences toinvestigate the minimum effort of turning the anomaloussequence into normal, or sequences with similar progres-sion context in order to avoid introducing noisy event com-parison results.Cluster Mode. The cluster mode of the normal sequenceview summarizes the progression of normal sequences intoa \ufb02ow-based visualization by clustering sequence segmentsin each time slot. In particular, the sequence segments ineach slot are clustered using Mean Shift Clustering [55]based on the multi-hot vectors introduced in Section 4. Notethat we include the segments in the anomalous sequence togenerate the clusters, and take the anomalous sequenceaway from the corresponding clusters when displayingonly the normal sequences.The visualization is designed to support comparing theanomalous sequence with subgroups of normal sequencesthat have particular progression patterns. Each cluster isrepresented with a rectangular node (Fig. 4d), consistingof a left-side label displaying the number of sequencesclustered in each node, and the main content depicting theevent occurrence of the clustered sequence segments. Notethat the text in the left-side label is rotated to distinguishfrom the sequence id annotated in the presentation of indi-vidual sequences. The occurrence of each event is encodedwith a vertical bar (Fig. 4e), horizontally aligned with thesame type of event in the reconstruction view and the anoma-lous sequence view. The height of each bar is proportional tothe number of sequences in each cluster having the corre-sponding event occurrence. The color encoding is consis-tent with other views showing the occurrence probability.We set the vertical position of cluster nodes in each timeslot using a layout algorithm introduced in [56] to illus-trate the similarity between clusters and minimize linkcrossing intuitively. Cluster nodes with similar eventoccurrences are grouped together, illustrating a higher-level structure of sequence progression. Cluster nodes inadjacent time slots are connected with light grey links(Fig. 4f) to uncover the transition patterns of sequencesbetween clusters.The cluster nodes can be expanded to show speci\ufb01csequence segments within and contracted back to a summa-rized cluster node in response to a double click (as shown inFig. 3h). This design aims to help users decide the granular-ity of analysis with more \ufb02exibility. The system also sup-ports clustering analysis with the selected anomaloussequence incorporated to help analysts overview how theprogression of anomalous record deviates from the normalgroup. By switching on the overlay button (Fig. 3d), theanomalous sequence segments are added back to the clus-ters. The cluster nodes and transition links that the anoma-lous sequence progress through are highlighted in red (asshown in Fig. 6 (1)) so that users can quickly identify prob-lematic time slots in which the anomalous sequence fall intothe clusters with a small population", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "network+bar", "axial_code": [], "componenet_code": ["bar", "network"]}, {"solution_text": "This design aims to help users decide the granular-ity of analysis with more \ufb02exibility. The system also sup-ports clustering analysis with the selected anomaloussequence incorporated to help analysts overview how theprogression of anomalous record deviates from the normalgroup. By switching on the overlay button (Fig. 3d), theanomalous sequence segments are added back to the clus-ters. The cluster nodes and transition links that the anoma-lous sequence progress through are highlighted in red (asshown in Fig. 6 (1)) so that users can quickly identify prob-lematic time slots in which the anomalous sequence fall intothe clusters with a small population", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 354, "paper_title": "DeHumor: Visual Analytics for Decomposing Humor", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "R1: Analyze Text and Audio Simultaneously to Reveal TheirCorrelations. Our experts confirmed that both speech contentand vocal delivery are considered necessary for a humorouseffect. It is difficult to capture both of them by watching thevideos. Therefore, at each level, the system needs to presenttextual and audio features concurrently to help users reasonabout the effective use of words and voice.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "We process the collected data such that (1) the text script and audio are aligned to support multimodal analysis, and (2) the full speech data is segmented into humor snippets to support context and sentence-level analy_x0002_sis. To achieve the alignment, we first detect each word\u2019s starting time and ending time in the transcript using P2FA [45]. Thereafter, we align the audio and text modality together at the word level. As for humor snippet segmenta_x0002_tion, we regard a sentence immediately before a laughter marker as a punchline (i.e., the most important sentence that triggers the audience response). We treat all of the sentences between two punchlines as the candidate context paragraph for the second punchline. The intuition is that all the infor_x0002_mation that occurs after a punchline are potentially useful for building up the next punchline. More concrete context recognition (shown in Section 5.1.3) should come from these candidate sentences. As a result, we split the transcripts at laughter markers. Each resulting humor snippet contains exactly one punchline (i.e., the last sentence of the segment) and its contexts (all the preceding sentences). The audio is clipped correspondingly through the starting and ending times of the sentences. Eventually, we organize the raw speech data into aligned audio and transcripts per snippet, per sentence, and per word; We compute and encode three types of semantic features at the sentence level: incongruity, sentiment, and phonetics. For each feature, a meaningful threshold is used to iden- tify important words or phrases in the sentence, which are annotated with intuitive glyphs. These thresholds can be interactively adjusted by users according to the feature distribution in figure.", "solution_category": "data_manipulation", "solution_axial": "Explainability,Excluding,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding", "Explainability"], "componenet_code": ["algorithmic_calculation", "excluding", "explainability"]}, {"solution_text": "Besides sentence- and context-level, we design an aug- mented time matrix that provides an overview of distribution of humor occurrences and the related features of speech content and vocal delivery at the speech level.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "matrix+bar", "axial_code": [], "componenet_code": ["bar", "matrix"]}]}, {"author": "zsz", "index_original": 355, "paper_title": "DeHumor: Visual Analytics for Decomposing Humor", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "R2: Visualize a Speech Level Overview That Shows Vocal andVerbal Styles of Humor, as well as Their Distribution. At thespeech level, the system should summarize the timing ofhumor-related properties\u2014the humor is injected how fre-quently, under what condition (Or, what topic \ufb02ow), to which part of the speech, and with what verbal and vocalstyles. The visual summary of these properties serves asguidance and should help users \ufb01nd speci\ufb01c humor snip-pets within a speech. For example, a communication coachmight prioritize the very \ufb01rst humorous punchline (when),to show students how to provide an impressive opening(objective) by making small talks or sharing personal lives(e.g., \u201cMy brother\u2019s in prison.\u201d). Besides, as suggested by theexperts, the visual summary of the humor distributionshould be integrated with temporal information, along withthe topic \ufb02ow and verbal feature statistics.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "We process the collected data such that (1) the text script and audio are aligned to support multimodal analysis, and (2) the full speech data is segmented into humor snippets to support context and sentence-level analy_x0002_sis. To achieve the alignment, we first detect each word\u2019s starting time and ending time in the transcript using P2FA [45]. Thereafter, we align the audio and text modality together at the word level. As for humor snippet segmenta_x0002_tion, we regard a sentence immediately before a laughter marker as a punchline (i.e., the most important sentence that triggers the audience response). We treat all of the sentences between two punchlines as the candidate context paragraph for the second punchline. The intuition is that all the infor_x0002_mation that occurs after a punchline are potentially useful for building up the next punchline. More concrete context recognition (shown in Section 5.1.3) should come from these candidate sentences. As a result, we split the transcripts at laughter markers. Each resulting humor snippet contains exactly one punchline (i.e., the last sentence of the segment) and its contexts (all the preceding sentences). The audio is clipped correspondingly through the starting and ending times of the sentences. Eventually, we organize the raw speech data into aligned audio and transcripts per snippet, per sentence, and per word; We compute and encode three types of semantic features at the sentence level: incongruity, sentiment, and phonetics. For each feature, a meaningful threshold is used to iden- tify important words or phrases in the sentence, which are annotated with intuitive glyphs. These thresholds can be interactively adjusted by users according to the feature distribution in figure.", "solution_category": "data_manipulation", "solution_axial": "Explainability,Excluding,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding", "Explainability"], "componenet_code": ["algorithmic_calculation", "excluding", "explainability"]}, {"solution_text": "Visualizing Contextual Repetitions: We design a context linking graph to display the extracted inter-sentence repetition occurrences. As shown in figure, the graph follows a three- stage design, such that it gradually reveals the concrete con- text information to the user and traverse from the context- level to the sentence-level.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "text", "axial_code": [], "componenet_code": ["text"]}, {"solution_text": "Visualizing Contextual Repetitions: We design a context linking graph to display the extracted inter-sentence repetition occurrences. As shown in figure, the graph follows a three- stage design, such that it gradually reveals the concrete con- text information to the user and traverse from the context- level to the sentence-level.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 356, "paper_title": "DeHumor: Visual Analytics for Decomposing Humor", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "R2: Visualize a Speech Level Overview That Shows Vocal andVerbal Styles of Humor, as well as Their Distribution. At thespeech level, the system should summarize the timing ofhumor-related properties\u2014the humor is injected how fre-quently, under what condition (Or, what topic \ufb02ow), to which part of the speech, and with what verbal and vocalstyles. The visual summary of these properties serves asguidance and should help users \ufb01nd speci\ufb01c humor snip-pets within a speech. For example, a communication coachmight prioritize the very \ufb01rst humorous punchline (when),to show students how to provide an impressive opening(objective) by making small talks or sharing personal lives(e.g., \u201cMy brother\u2019s in prison.\u201d). Besides, as suggested by theexperts, the visual summary of the humor distributionshould be integrated with temporal information, along withthe topic \ufb02ow and verbal feature statistics.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "We process the collected data such that (1) the text script and audio are aligned to support multimodal analysis, and (2) the full speech data is segmented into humor snippets to support context and sentence-level analy_x0002_sis. To achieve the alignment, we first detect each word\u2019s starting time and ending time in the transcript using P2FA [45]. Thereafter, we align the audio and text modality together at the word level. As for humor snippet segmenta_x0002_tion, we regard a sentence immediately before a laughter marker as a punchline (i.e., the most important sentence that triggers the audience response). We treat all of the sentences between two punchlines as the candidate context paragraph for the second punchline. The intuition is that all the infor_x0002_mation that occurs after a punchline are potentially useful for building up the next punchline. More concrete context recognition (shown in Section 5.1.3) should come from these candidate sentences. As a result, we split the transcripts at laughter markers. Each resulting humor snippet contains exactly one punchline (i.e., the last sentence of the segment) and its contexts (all the preceding sentences). The audio is clipped correspondingly through the starting and ending times of the sentences. Eventually, we organize the raw speech data into aligned audio and transcripts per snippet, per sentence, and per word; We compute and encode three types of semantic features at the sentence level: incongruity, sentiment, and phonetics. For each feature, a meaningful threshold is used to iden- tify important words or phrases in the sentence, which are annotated with intuitive glyphs. These thresholds can be interactively adjusted by users according to the feature distribution in figure.", "solution_category": "data_manipulation", "solution_axial": "Explainability,Excluding,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding", "Explainability"], "componenet_code": ["algorithmic_calculation", "excluding", "explainability"]}, {"solution_text": "Besides sentence- and context-level, we design an aug- mented time matrix that provides an overview of distribution of humor occurrences and the related features of speech content and vocal delivery at the speech level.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "matrix+bar", "axial_code": [], "componenet_code": ["bar", "matrix"]}]}, {"author": "zsz", "index_original": 357, "paper_title": "DeHumor: Visual Analytics for Decomposing Humor", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "R3: Provide a Context-Level Overview That Shows Build-UpElements of Humor, as well as Their Relationships. Once zoomedin to a speci\ufb01c snippet, context level exploration is necessaryfor evaluating how a humorous story is written (e.g., how thekey concepts in the punchline are \ufb01rst introduced and howthey connect the pieces of humor stories), as well as a sum-mary of delivery skills that are frequently used to help conveythe story. Both researchers and communication coachesviewed the contextual analysis of humor build-ups to be themost demanding. Therefore, our system should primarilysupport users at this level.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "We process the collected data such that (1) the text script and audio are aligned to support multimodal analysis, and (2) the full speech data is segmented into humor snippets to support context and sentence-level analy_x0002_sis. To achieve the alignment, we first detect each word\u2019s starting time and ending time in the transcript using P2FA [45]. Thereafter, we align the audio and text modality together at the word level. As for humor snippet segmenta_x0002_tion, we regard a sentence immediately before a laughter marker as a punchline (i.e., the most important sentence that triggers the audience response). We treat all of the sentences between two punchlines as the candidate context paragraph for the second punchline. The intuition is that all the infor_x0002_mation that occurs after a punchline are potentially useful for building up the next punchline. More concrete context recognition (shown in Section 5.1.3) should come from these candidate sentences. As a result, we split the transcripts at laughter markers. Each resulting humor snippet contains exactly one punchline (i.e., the last sentence of the segment) and its contexts (all the preceding sentences). The audio is clipped correspondingly through the starting and ending times of the sentences. Eventually, we organize the raw speech data into aligned audio and transcripts per snippet, per sentence, and per word; We compute and encode three types of semantic features at the sentence level: incongruity, sentiment, and phonetics. For each feature, a meaningful threshold is used to iden- tify important words or phrases in the sentence, which are annotated with intuitive glyphs. These thresholds can be interactively adjusted by users according to the feature distribution in figure.", "solution_category": "data_manipulation", "solution_axial": "Explainability,Excluding,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding", "Explainability"], "componenet_code": ["algorithmic_calculation", "excluding", "explainability"]}, {"solution_text": "Visualizing Contextual Repetitions: We design a context linking graph to display the extracted inter-sentence repetition occurrences. As shown in figure, the graph follows a three- stage design, such that it gradually reveals the concrete con- text information to the user and traverse from the context- level to the sentence-level.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "text", "axial_code": [], "componenet_code": ["text"]}, {"solution_text": "Visualizing Contextual Repetitions: We design a context linking graph to display the extracted inter-sentence repetition occurrences. As shown in figure, the graph follows a three- stage design, such that it gradually reveals the concrete con- text information to the user and traverse from the context- level to the sentence-level.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 358, "paper_title": "DeHumor: Visual Analytics for Decomposing Humor", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "R4: Highlight the Pairing of Individual Content Words andHumor-Related Verbal Delivery Units. We need to expose theco-occurrence between textual and audio features withineach individual sentence, so to demonstrate the humor strate-gies with relevant concrete examples (e.g., words and utter-ance). Within a snippet, the punchline is the most importantsentence since it immediately triggers laughter.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "We process the collected data such that (1) the text script and audio are aligned to support multimodal analysis, and (2) the full speech data is segmented into humor snippets to support context and sentence-level analy_x0002_sis. To achieve the alignment, we first detect each word\u2019s starting time and ending time in the transcript using P2FA [45]. Thereafter, we align the audio and text modality together at the word level. As for humor snippet segmenta_x0002_tion, we regard a sentence immediately before a laughter marker as a punchline (i.e., the most important sentence that triggers the audience response). We treat all of the sentences between two punchlines as the candidate context paragraph for the second punchline. The intuition is that all the infor_x0002_mation that occurs after a punchline are potentially useful for building up the next punchline. More concrete context recognition (shown in Section 5.1.3) should come from these candidate sentences. As a result, we split the transcripts at laughter markers. Each resulting humor snippet contains exactly one punchline (i.e., the last sentence of the segment) and its contexts (all the preceding sentences). The audio is clipped correspondingly through the starting and ending times of the sentences. Eventually, we organize the raw speech data into aligned audio and transcripts per snippet, per sentence, and per word; We compute and encode three types of semantic features at the sentence level: incongruity, sentiment, and phonetics. For each feature, a meaningful threshold is used to iden- tify important words or phrases in the sentence, which are annotated with intuitive glyphs. These thresholds can be interactively adjusted by users according to the feature distribution in figure.", "solution_category": "data_manipulation", "solution_axial": "Explainability,Excluding,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding", "Explainability"], "componenet_code": ["algorithmic_calculation", "excluding", "explainability"]}]}, {"author": "zsz", "index_original": 359, "paper_title": "DeHumor: Visual Analytics for Decomposing Humor", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "R4: Highlight the Pairing of Individual Content Words andHumor-Related Verbal Delivery Units. We need to expose theco-occurrence between textual and audio features withineach individual sentence, so to demonstrate the humor strate-gies with relevant concrete examples (e.g., words and utter-ance). Within a snippet, the punchline is the most importantsentence since it immediately triggers laughter.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "We process the collected data such that (1) the text script and audio are aligned to support multimodal analysis, and (2) the full speech data is segmented into humor snippets to support context and sentence-level analy_x0002_sis. To achieve the alignment, we first detect each word\u2019s starting time and ending time in the transcript using P2FA [45]. Thereafter, we align the audio and text modality together at the word level. As for humor snippet segmenta_x0002_tion, we regard a sentence immediately before a laughter marker as a punchline (i.e., the most important sentence that triggers the audience response). We treat all of the sentences between two punchlines as the candidate context paragraph for the second punchline. The intuition is that all the infor_x0002_mation that occurs after a punchline are potentially useful for building up the next punchline. More concrete context recognition (shown in Section 5.1.3) should come from these candidate sentences. As a result, we split the transcripts at laughter markers. Each resulting humor snippet contains exactly one punchline (i.e., the last sentence of the segment) and its contexts (all the preceding sentences). The audio is clipped correspondingly through the starting and ending times of the sentences. Eventually, we organize the raw speech data into aligned audio and transcripts per snippet, per sentence, and per word; We compute and encode three types of semantic features at the sentence level: incongruity, sentiment, and phonetics. For each feature, a meaningful threshold is used to iden- tify important words or phrases in the sentence, which are annotated with intuitive glyphs. These thresholds can be interactively adjusted by users according to the feature distribution in figure.", "solution_category": "data_manipulation", "solution_axial": "Explainability,Excluding,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding", "Explainability"], "componenet_code": ["algorithmic_calculation", "excluding", "explainability"]}, {"solution_text": "The graph first provides a context distribution summary, which shows how the sentences are connected to each other through repeated concepts. A rounded gray rectangle repre- sents a sentence, and its horizontal length encodes the sen- tence length. We highlight the most important punchline with a denser gray color. We connect rectangles with arc links if their corresponding sentences share repetitive concepts, and add thin lines below the rectangles to denote the presence of these concept. The combination of the links and the lines help highlight different structures of build-up for humor.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph+arc+text", "axial_code": [], "componenet_code": ["text", "arc", "glyph"]}]}, {"author": "zsz", "index_original": 360, "paper_title": "DeHumor: Visual Analytics for Decomposing Humor", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "R5: Support Intuitive Interactions for Helping Users TraverseAmong Different Levels, and Reveal the Different Level of Details.For example, our communication coaches suggested thatthe original audio and scripts of the speech should also beincluded in the system, in addition to a visual summary ofhumor. The system should allow users to rapidly locate thesegments of interest in the speech and playback the corre-sponding audio clips", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "Details-on-Demand Through Clicking. Once a user clicks a speech of interest in the control panel, the humor exploration will be updated accordingly. When the user clicks on a key-word or in the augmented time matrix, the corresponding humor snippet will be scrolled to the top in the content exploration, and the corresponding sentence in the context linking graph and the transcript will be highlighted.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 361, "paper_title": "DeHumor: Visual Analytics for Decomposing Humor", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "R5: Support Intuitive Interactions for Helping Users TraverseAmong Different Levels, and Reveal the Different Level of Details.For example, our communication coaches suggested thatthe original audio and scripts of the speech should also beincluded in the system, in addition to a visual summary ofhumor. The system should allow users to rapidly locate thesegments of interest in the speech and playback the corre-sponding audio clips", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.", "data_code": {"sequential": 1, "textual": 1, "temporal": 1}}, "solution": [{"solution_text": "Linked Scrolling. When users scroll in the content explora- tion, the time range of the visible humor snippets will be highlighted in the augmented time matrix.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 362, "paper_title": "DeHumor: Visual Analytics for Decomposing Humor", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "R5: Support Intuitive Interactions for Helping Users TraverseAmong Different Levels, and Reveal the Different Level of Details.For example, our communication coaches suggested thatthe original audio and scripts of the speech should also beincluded in the system, in addition to a visual summary ofhumor. The system should allow users to rapidly locate thesegments of interest in the speech and playback the corre-sponding audio clips", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "Active Query Through Searching, Sorting, and Filtering. Users  can search a speech or sort speeches according to a specific cri- terion in the control panel. Also, they can apply filters in the humor focus to find different styles of punchlines. Then, the corresponding humor snippets will be highlighted in the humor exploration view.", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "zsz", "index_original": 363, "paper_title": "MVNet: Multi-Variate Multi-View Brain Network Comparison Over Uncertain Data", "pub_year": 2022, "domain": "brain", "requirement": {"requirement_text": "R1: Eliminate Uncertainty in Brain Connectivity Data. Theneurology doctor in our study described the comparison ofstructural brain connectivity as an infrequent scenario intheir clinical practice. The key reason is attributed to theuncertainty of the brain connectivity data and the follow-upbrain network construction process. The neuroscientist andcomputer scientist echoed the same uncertainty issue. Theydemand analyzing valid brain network data that faithfullyrepresents brain connectivity.", "requirement_code": {"data_filtering": 1}}, "data": {"data_text": "We developed MV2Net system mainly using the data set from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) \u2013 a public Consortium on collecting, validating, and utilizing AD data [61]. The ADNI data set contains struc_x0002_tural MRIs of 202 subjects recruited and scanned at 16 dif_x0002_ferent sites across North America [1]. On each subject, we computed a structural brain network by the segmentation method distributed with the FreeSurfer tool [62]. FreeSurfer applied the Desikan-Killiany parcellation template [63], which defines 70 ROIs (nodes) on the brain cortex of each subject. The default edge weight between two ROIs is set to the number of fibers on the white matter pathway between them, estimated from the DTI data of each subject.", "data_code": {"geometry": 1, "tables": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "On the connectivity features extracted in Section 4.1, it is found that the feature distributions largely follow normal distribution after removing zero values. Therefore, we apply the parameter-free Grubbs\u2019s test as the anomaly detection algorithm to extract zero-quality outlier feature values and compute the QoC measure for the other feature values. The Grubbs\u2019s test adapted to our quality computa_x0002_tion usage is given below. More details of the QoC measure and its computation algorithm can be found in Appendix C, available in the online supplemental material   To address the visualization problem in displaying discrim_x0002_inative connectivity features (P2 and P3), we apply both uni_x0002_variate and multivariate feature selection algorithms in MV2Net to identify connectivity features that are signifi_x0002_cantly different between the brain networks of contrasting subject groups. The univariate algorithm introduces a statis_x0002_tical hypothesis test on each connectivity feature of the net_x0002_work. Under the normality assumption verified in Section 4.2, the Student\u2019s t-test is legitimately used, which computes a p-value on each feature. When the p-value is no greater than a significance level (0.05 by default), the mean feature values of two contrasting groups on the corresponding feature are said to be significantly different. These selected significant features will be visualized in MV2Net for further analysis. Note that when the feature variance of contrasting groups is unequal, the Welch\u2019s t-test can be applied instead of the standard t-test. Users are allowed to choose between the two tests for every comparison. According to clinical researches on white matter disor_x0002_der, there are two major types of brain connectivity destruc_x0002_tion: one on a certain scope of white matter that mostly affects a single brain network connection; the other on a range of cerebral cortex spanning multiple ROIs (mainly grey matter) that could affect all brain fibers connecting to these ROIs. In the latter case, the connectivity features asso_x0002_ciated with affected ROIs will be correlated in the diseased subjects, e.g., with decreased connectivity strength simulta_x0002_neously. Yet, each individual connectivity feature may not be significantly different between the comparing subject group. The univariate feature selection algorithm will not solve the problem in this case because it analyzes each fea_x0002_ture independently. Lowering the p-value threshold also does not work as this will produce too many discriminative features without detecting the feature correlation. We con_x0002_sider the multivariate feature selection method called group lasso (GL), which is a kind of supervised machine learning algorithm. The algorithm aims to leverage the supervision to guide the selection process for a subset of discriminative features, instead of extracting a single feature in the univari_x0002_ate analysis. The method takes both connectivity features and the brain network structure as input. A set of structur_x0002_ally correlated connectivity features (e.g., on the subgraph of brain network) are then detected. When applied collec_x0002_tively, these features can precisely predict the subject label of a brain network, i.e., patient or control. The prediction accuracy is displayed in the interface to demonstrate the performance of the model. To evaluate the significance of each of these multivariate features, we apply univariate sta_x0002_tistical tests on each feature and display the derived p-val_x0002_ues in the interface for the visual analysis of multivariate bio-markers.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "Visual Quality Analysis on Connectivity Features of the Brain Network. To meet requirement using visual analytics techniques, the MV2Net system is designed to conduct built-in quality analysis on brain network connectivity data. A carefully engineered anomaly detection method is applied to compute the pre-defined feature quality mea- sure. Null connections and possibly mismeasured connec- tivity features are assigned zero or low feature quality. On visualization, an interactive data wrangling mechanism is designed and implemented to allow users to eliminate the data uncertainty before the network comparison analysis.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix", "axial_code": [], "componenet_code": ["matrix"]}, {"solution_text": "Visual Quality Analysis on Connectivity Features of the Brain Network. To meet requirement using visual analytics techniques, the MV2Net system is designed to conduct built-in quality analysis on brain network connectivity data. A carefully engineered anomaly detection method is applied to compute the pre-defined feature quality mea- sure. Null connections and possibly mismeasured connec- tivity features are assigned zero or low feature quality. On visualization, an interactive data wrangling mechanism is designed and implemented to allow users to eliminate the data uncertainty before the network comparison analysis.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 364, "paper_title": "MVNet: Multi-Variate Multi-View Brain Network Comparison Over Uncertain Data", "pub_year": 2022, "domain": "brain", "requirement": {"requirement_text": "R2: Analyze Brain Connectivity From Multiple Perspectives.All experts agree that the reconstructed fiber tracts repre-sent full details of brain connectivity, much better than thebrain network with a single edge weight of connectivitystrength. For example, in a typical pathological disorder ofbrain tumors, the tumor in the early stage can cause geomet-ric changes to the brain fiber, but without reducing the fiberconnectivity strength. The analysis of brain connectivityshould be conducted from multiple perspectives, on diffu-sion features at the voxel level and on geometric featuresrepresenting the physical shape of fiber tracts.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "We developed MV2Net system mainly using the data set from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) \u2013 a public Consortium on collecting, validating, and utilizing AD data [61]. The ADNI data set contains struc_x0002_tural MRIs of 202 subjects recruited and scanned at 16 dif_x0002_ferent sites across North America [1]. On each subject, we computed a structural brain network by the segmentation method distributed with the FreeSurfer tool [62]. FreeSurfer applied the Desikan-Killiany parcellation template [63], which defines 70 ROIs (nodes) on the brain cortex of each subject. The default edge weight between two ROIs is set to the number of fibers on the white matter pathway between them, estimated from the DTI data of each subject.", "data_code": {"geometry": 1, "tables": 1, "media": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "On the connectivity features extracted in Section 4.1, it is found that the feature distributions largely follow normal distribution after removing zero values. Therefore, we apply the parameter-free Grubbs\u2019s test as the anomaly detection algorithm to extract zero-quality outlier feature values and compute the QoC measure for the other feature values. The Grubbs\u2019s test adapted to our quality computa_x0002_tion usage is given below. More details of the QoC measure and its computation algorithm can be found in Appendix C, available in the online supplemental material   To address the visualization problem in displaying discrim_x0002_inative connectivity features (P2 and P3), we apply both uni_x0002_variate and multivariate feature selection algorithms in MV2Net to identify connectivity features that are signifi_x0002_cantly different between the brain networks of contrasting subject groups. The univariate algorithm introduces a statis_x0002_tical hypothesis test on each connectivity feature of the net_x0002_work. Under the normality assumption verified in Section 4.2, the Student\u2019s t-test is legitimately used, which computes a p-value on each feature. When the p-value is no greater than a significance level (0.05 by default), the mean feature values of two contrasting groups on the corresponding feature are said to be significantly different. These selected significant features will be visualized in MV2Net for further analysis. Note that when the feature variance of contrasting groups is unequal, the Welch\u2019s t-test can be applied instead of the standard t-test. Users are allowed to choose between the two tests for every comparison. According to clinical researches on white matter disor_x0002_der, there are two major types of brain connectivity destruc_x0002_tion: one on a certain scope of white matter that mostly affects a single brain network connection; the other on a range of cerebral cortex spanning multiple ROIs (mainly grey matter) that could affect all brain fibers connecting to these ROIs. In the latter case, the connectivity features asso_x0002_ciated with affected ROIs will be correlated in the diseased subjects, e.g., with decreased connectivity strength simulta_x0002_neously. Yet, each individual connectivity feature may not be significantly different between the comparing subject group. The univariate feature selection algorithm will not solve the problem in this case because it analyzes each fea_x0002_ture independently. Lowering the p-value threshold also does not work as this will produce too many discriminative features without detecting the feature correlation. We con_x0002_sider the multivariate feature selection method called group lasso (GL), which is a kind of supervised machine learning algorithm. The algorithm aims to leverage the supervision to guide the selection process for a subset of discriminative features, instead of extracting a single feature in the univari_x0002_ate analysis. The method takes both connectivity features and the brain network structure as input. A set of structur_x0002_ally correlated connectivity features (e.g., on the subgraph of brain network) are then detected. When applied collec_x0002_tively, these features can precisely predict the subject label of a brain network, i.e., patient or control. The prediction accuracy is displayed in the interface to demonstrate the performance of the model. To evaluate the significance of each of these multivariate features, we apply univariate sta_x0002_tistical tests on each feature and display the derived p-val_x0002_ues in the interface for the visual analysis of multivariate bio-markers.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "Visualization of Discriminative Connectivity Features Between Two Groups of Brain Networks (Univariate Bio- Markers). To achieve requirement on individual connectivity features, the system integrates univariate statistical tests (e.g., the Student\u2019s t-test) with interactive brain network visualization. Multiple choices of statistical test algorithms and visual comparison modes are supported in the system. Users can directly manipulate the algorithm parameters and visually analyze the outcome. A visual interface for subject group selection is also introduced to specify the comparison setting.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph+network", "axial_code": [], "componenet_code": ["network", "glyph"]}, {"solution_text": "Users can directly manipulate the algorithm parameters and visually analyze the outcome. ", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 365, "paper_title": "MVNet: Multi-Variate Multi-View Brain Network Comparison Over Uncertain Data", "pub_year": 2022, "domain": "brain", "requirement": {"requirement_text": "R2: Analyze Brain Connectivity From Multiple Perspectives.All experts agree that the reconstructed fiber tracts repre-sent full details of brain connectivity, much better than thebrain network with a single edge weight of connectivitystrength. For example, in a typical pathological disorder ofbrain tumors, the tumor in the early stage can cause geomet-ric changes to the brain fiber, but without reducing the fiberconnectivity strength. The analysis of brain connectivityshould be conducted from multiple perspectives, on diffu-sion features at the voxel level and on geometric featuresrepresenting the physical shape of fiber tracts.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "We developed MV2Net system mainly using the data set from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) \u2013 a public Consortium on collecting, validating, and utilizing AD data [61]. The ADNI data set contains struc_x0002_tural MRIs of 202 subjects recruited and scanned at 16 dif_x0002_ferent sites across North America [1]. On each subject, we computed a structural brain network by the segmentation method distributed with the FreeSurfer tool [62]. FreeSurfer applied the Desikan-Killiany parcellation template [63], which defines 70 ROIs (nodes) on the brain cortex of each subject. The default edge weight between two ROIs is set to the number of fibers on the white matter pathway between them, estimated from the DTI data of each subject.", "data_code": {"geometry": 1, "tables": 1, "media": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "On the connectivity features extracted in Section 4.1, it is found that the feature distributions largely follow normal distribution after removing zero values. Therefore, we apply the parameter-free Grubbs\u2019s test as the anomaly detection algorithm to extract zero-quality outlier feature values and compute the QoC measure for the other feature values. The Grubbs\u2019s test adapted to our quality computa_x0002_tion usage is given below. More details of the QoC measure and its computation algorithm can be found in Appendix C, available in the online supplemental material   To address the visualization problem in displaying discrim_x0002_inative connectivity features (P2 and P3), we apply both uni_x0002_variate and multivariate feature selection algorithms in MV2Net to identify connectivity features that are signifi_x0002_cantly different between the brain networks of contrasting subject groups. The univariate algorithm introduces a statis_x0002_tical hypothesis test on each connectivity feature of the net_x0002_work. Under the normality assumption verified in Section 4.2, the Student\u2019s t-test is legitimately used, which computes a p-value on each feature. When the p-value is no greater than a significance level (0.05 by default), the mean feature values of two contrasting groups on the corresponding feature are said to be significantly different. These selected significant features will be visualized in MV2Net for further analysis. Note that when the feature variance of contrasting groups is unequal, the Welch\u2019s t-test can be applied instead of the standard t-test. Users are allowed to choose between the two tests for every comparison. According to clinical researches on white matter disor_x0002_der, there are two major types of brain connectivity destruc_x0002_tion: one on a certain scope of white matter that mostly affects a single brain network connection; the other on a range of cerebral cortex spanning multiple ROIs (mainly grey matter) that could affect all brain fibers connecting to these ROIs. In the latter case, the connectivity features asso_x0002_ciated with affected ROIs will be correlated in the diseased subjects, e.g., with decreased connectivity strength simulta_x0002_neously. Yet, each individual connectivity feature may not be significantly different between the comparing subject group. The univariate feature selection algorithm will not solve the problem in this case because it analyzes each fea_x0002_ture independently. Lowering the p-value threshold also does not work as this will produce too many discriminative features without detecting the feature correlation. We con_x0002_sider the multivariate feature selection method called group lasso (GL), which is a kind of supervised machine learning algorithm. The algorithm aims to leverage the supervision to guide the selection process for a subset of discriminative features, instead of extracting a single feature in the univari_x0002_ate analysis. The method takes both connectivity features and the brain network structure as input. A set of structur_x0002_ally correlated connectivity features (e.g., on the subgraph of brain network) are then detected. When applied collec_x0002_tively, these features can precisely predict the subject label of a brain network, i.e., patient or control. The prediction accuracy is displayed in the interface to demonstrate the performance of the model. To evaluate the significance of each of these multivariate features, we apply univariate sta_x0002_tistical tests on each feature and display the derived p-val_x0002_ues in the interface for the visual analysis of multivariate bio-markers.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "Visualization of Correlated Discriminative Connectivity Features Between Two Groups of Brain Networks (Multivari- ate Bio-Markers). The system also satisfies requirement on correlated connectivity features, e.g., multiple discrimina- tive features on the same brain connection, or the same fea- ture on a subgraph of the brain network. Multivariate feature selection algorithms are applied to extract discrimi- native subgraph features. A high-order com- posite visualization design is introduced to analyze correlated connectivity features on the same brain connection.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "glyph+network", "axial_code": [], "componenet_code": ["network", "glyph"]}]}, {"author": "zsz", "index_original": 366, "paper_title": "MVNet: Multi-Variate Multi-View Brain Network Comparison Over Uncertain Data", "pub_year": 2022, "domain": "brain", "requirement": {"requirement_text": "R3: Detect Brain Connectivity Bio-Markers by StatisticalTests. The clinical practice to detect connectivity bio-markers for disease depends on manually spotting the dif-ference with the naked eye, which is subjective and error-prone. Statistical tests and machine learning algorithms candetect individual connectivity features and a group of corre-lated features that are statistically signi\ufb01cantly differentbetween the comparing subject groups. Domain usersdemand directly looking at the analysis result instead ofanalyzing the raw data themselves.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "We developed MV2Net system mainly using the data set from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) \u2013 a public Consortium on collecting, validating, and utilizing AD data [61]. The ADNI data set contains struc_x0002_tural MRIs of 202 subjects recruited and scanned at 16 dif_x0002_ferent sites across North America [1]. On each subject, we computed a structural brain network by the segmentation method distributed with the FreeSurfer tool [62]. FreeSurfer applied the Desikan-Killiany parcellation template [63], which defines 70 ROIs (nodes) on the brain cortex of each subject. The default edge weight between two ROIs is set to the number of fibers on the white matter pathway between them, estimated from the DTI data of each subject.", "data_code": {"geometry": 1, "tables": 1, "media": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "On the connectivity features extracted in Section 4.1, it is found that the feature distributions largely follow normal distribution after removing zero values. Therefore, we apply the parameter-free Grubbs\u2019s test as the anomaly detection algorithm to extract zero-quality outlier feature values and compute the QoC measure for the other feature values. The Grubbs\u2019s test adapted to our quality computa_x0002_tion usage is given below. More details of the QoC measure and its computation algorithm can be found in Appendix C, available in the online supplemental material   To address the visualization problem in displaying discrim_x0002_inative connectivity features (P2 and P3), we apply both uni_x0002_variate and multivariate feature selection algorithms in MV2Net to identify connectivity features that are signifi_x0002_cantly different between the brain networks of contrasting subject groups. The univariate algorithm introduces a statis_x0002_tical hypothesis test on each connectivity feature of the net_x0002_work. Under the normality assumption verified in Section 4.2, the Student\u2019s t-test is legitimately used, which computes a p-value on each feature. When the p-value is no greater than a significance level (0.05 by default), the mean feature values of two contrasting groups on the corresponding feature are said to be significantly different. These selected significant features will be visualized in MV2Net for further analysis. Note that when the feature variance of contrasting groups is unequal, the Welch\u2019s t-test can be applied instead of the standard t-test. Users are allowed to choose between the two tests for every comparison. According to clinical researches on white matter disor_x0002_der, there are two major types of brain connectivity destruc_x0002_tion: one on a certain scope of white matter that mostly affects a single brain network connection; the other on a range of cerebral cortex spanning multiple ROIs (mainly grey matter) that could affect all brain fibers connecting to these ROIs. In the latter case, the connectivity features asso_x0002_ciated with affected ROIs will be correlated in the diseased subjects, e.g., with decreased connectivity strength simulta_x0002_neously. Yet, each individual connectivity feature may not be significantly different between the comparing subject group. The univariate feature selection algorithm will not solve the problem in this case because it analyzes each fea_x0002_ture independently. Lowering the p-value threshold also does not work as this will produce too many discriminative features without detecting the feature correlation. We con_x0002_sider the multivariate feature selection method called group lasso (GL), which is a kind of supervised machine learning algorithm. The algorithm aims to leverage the supervision to guide the selection process for a subset of discriminative features, instead of extracting a single feature in the univari_x0002_ate analysis. The method takes both connectivity features and the brain network structure as input. A set of structur_x0002_ally correlated connectivity features (e.g., on the subgraph of brain network) are then detected. When applied collec_x0002_tively, these features can precisely predict the subject label of a brain network, i.e., patient or control. The prediction accuracy is displayed in the interface to demonstrate the performance of the model. To evaluate the significance of each of these multivariate features, we apply univariate sta_x0002_tistical tests on each feature and display the derived p-val_x0002_ues in the interface for the visual analysis of multivariate bio-markers.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "Visualization of Discriminative Connectivity Features Between Two Groups of Brain Networks (Univariate Bio- Markers). To achieve requirement on individual connectivity features, the system integrates univariate statistical tests (e.g., the Student\u2019s t-test) with interactive brain network visualization. Multiple choices of statistical test algorithms and visual comparison modes are supported in the system. Users can directly manipulate the algorithm parameters and visually analyze the outcome. A visual interface for subject group selection is also introduced to specify the comparison setting.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph+network", "axial_code": [], "componenet_code": ["network", "glyph"]}, {"solution_text": "Users can directly manipulate the algorithm parameters and visually analyze the outcome. ", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 367, "paper_title": "MVNet: Multi-Variate Multi-View Brain Network Comparison Over Uncertain Data", "pub_year": 2022, "domain": "brain", "requirement": {"requirement_text": "R3: Detect Brain Connectivity Bio-Markers by StatisticalTests. The clinical practice to detect connectivity bio-markers for disease depends on manually spotting the dif-ference with the naked eye, which is subjective and error-prone. Statistical tests and machine learning algorithms candetect individual connectivity features and a group of corre-lated features that are statistically signi\ufb01cantly differentbetween the comparing subject groups. Domain usersdemand directly looking at the analysis result instead ofanalyzing the raw data themselves.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "We developed MV2Net system mainly using the data set from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) \u2013 a public Consortium on collecting, validating, and utilizing AD data [61]. The ADNI data set contains struc_x0002_tural MRIs of 202 subjects recruited and scanned at 16 dif_x0002_ferent sites across North America [1]. On each subject, we computed a structural brain network by the segmentation method distributed with the FreeSurfer tool [62]. FreeSurfer applied the Desikan-Killiany parcellation template [63], which defines 70 ROIs (nodes) on the brain cortex of each subject. The default edge weight between two ROIs is set to the number of fibers on the white matter pathway between them, estimated from the DTI data of each subject.", "data_code": {"geometry": 1, "tables": 1, "media": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "On the connectivity features extracted in Section 4.1, it is found that the feature distributions largely follow normal distribution after removing zero values. Therefore, we apply the parameter-free Grubbs\u2019s test as the anomaly detection algorithm to extract zero-quality outlier feature values and compute the QoC measure for the other feature values. The Grubbs\u2019s test adapted to our quality computa_x0002_tion usage is given below. More details of the QoC measure and its computation algorithm can be found in Appendix C, available in the online supplemental material   To address the visualization problem in displaying discrim_x0002_inative connectivity features (P2 and P3), we apply both uni_x0002_variate and multivariate feature selection algorithms in MV2Net to identify connectivity features that are signifi_x0002_cantly different between the brain networks of contrasting subject groups. The univariate algorithm introduces a statis_x0002_tical hypothesis test on each connectivity feature of the net_x0002_work. Under the normality assumption verified in Section 4.2, the Student\u2019s t-test is legitimately used, which computes a p-value on each feature. When the p-value is no greater than a significance level (0.05 by default), the mean feature values of two contrasting groups on the corresponding feature are said to be significantly different. These selected significant features will be visualized in MV2Net for further analysis. Note that when the feature variance of contrasting groups is unequal, the Welch\u2019s t-test can be applied instead of the standard t-test. Users are allowed to choose between the two tests for every comparison. According to clinical researches on white matter disor_x0002_der, there are two major types of brain connectivity destruc_x0002_tion: one on a certain scope of white matter that mostly affects a single brain network connection; the other on a range of cerebral cortex spanning multiple ROIs (mainly grey matter) that could affect all brain fibers connecting to these ROIs. In the latter case, the connectivity features asso_x0002_ciated with affected ROIs will be correlated in the diseased subjects, e.g., with decreased connectivity strength simulta_x0002_neously. Yet, each individual connectivity feature may not be significantly different between the comparing subject group. The univariate feature selection algorithm will not solve the problem in this case because it analyzes each fea_x0002_ture independently. Lowering the p-value threshold also does not work as this will produce too many discriminative features without detecting the feature correlation. We con_x0002_sider the multivariate feature selection method called group lasso (GL), which is a kind of supervised machine learning algorithm. The algorithm aims to leverage the supervision to guide the selection process for a subset of discriminative features, instead of extracting a single feature in the univari_x0002_ate analysis. The method takes both connectivity features and the brain network structure as input. A set of structur_x0002_ally correlated connectivity features (e.g., on the subgraph of brain network) are then detected. When applied collec_x0002_tively, these features can precisely predict the subject label of a brain network, i.e., patient or control. The prediction accuracy is displayed in the interface to demonstrate the performance of the model. To evaluate the significance of each of these multivariate features, we apply univariate sta_x0002_tistical tests on each feature and display the derived p-val_x0002_ues in the interface for the visual analysis of multivariate bio-markers.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "Visualization of Correlated Discriminative Connectivity Features Between Two Groups of Brain Networks (Multivari- ate Bio-Markers). The system also satisfies requirement on correlated connectivity features, e.g., multiple discrimina- tive features on the same brain connection, or the same fea- ture on a subgraph of the brain network. Multivariate feature selection algorithms are applied to extract discrimi- native subgraph features. A high-order com- posite visualization design is introduced to analyze correlated connectivity features on the same brain connection.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "glyph+network", "axial_code": [], "componenet_code": ["network", "glyph"]}]}, {"author": "zsz", "index_original": 368, "paper_title": "MVNet: Multi-Variate Multi-View Brain Network Comparison Over Uncertain Data", "pub_year": 2022, "domain": "brain", "requirement": {"requirement_text": "R4: Iteratively Compare Multiple Subject Groups and Synthe-size Results From Multiple Comparisons. To identify uniqueconnectivity bio-markers associated with a particular dis-ease type, neurology doctors often not only compare thegroup of patients having this disease type with the healthycontrols, but also compare the patient group having theother type of disease with controls, and compare betweenthe patients with different disease types. The patient groupswith changing severity of disease are also compared withthe healthy controls to understand the disease progression.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "We developed MV2Net system mainly using the data set from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) \u2013 a public Consortium on collecting, validating, and utilizing AD data [61]. The ADNI data set contains struc_x0002_tural MRIs of 202 subjects recruited and scanned at 16 dif_x0002_ferent sites across North America [1]. On each subject, we computed a structural brain network by the segmentation method distributed with the FreeSurfer tool [62]. FreeSurfer applied the Desikan-Killiany parcellation template [63], which defines 70 ROIs (nodes) on the brain cortex of each subject. The default edge weight between two ROIs is set to the number of fibers on the white matter pathway between them, estimated from the DTI data of each subject.", "data_code": {"geometry": 1, "tables": 1, "media": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Visual Analysis of Multiple Group-Based Brain Net- work Comparisons. To meet requirement, MV2Net visualization is designed with multiple comparison views. Both correlation and progressive patterns in these compari- sons can be visually detected. Composite visualizations are also supported to integrate the multiple comparisons into the same view for analysis. To allow users to examine bio- markers on low-level connectivity, a 3D comparison view is provided to drill down to detailed geometric/diffusion fea- tures on the fiber tract level.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "3Dstructure", "axial_code": [], "componenet_code": ["3Dstructure"]}]}, {"author": "zsz", "index_original": 369, "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization", "pub_year": 2022, "domain": "crime patterns ", "requirement": {"requirement_text": "T1. Analyze Street-Level Crime Hotspots. De\ufb01ne and depictcrime hotspots on a street level of detail, aiming to identifylocations with similar crime patterns", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "Illustrated in Fig. 7b, the hotspot scatter view shows the Prob-ability_x0001_Intensity scatter plot. To identify anchor points as hot-spots, one can use the linear discriminant filter functiondescribed in Section 5 by tuning the parameters a and pct (per-centage of anchor points to be considered hotspots), although an interactive brushing mechanism is also available.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "Section 5 by tuning the parameters a and pct (per-centage of anchor points to be considered hotspots), although an interactive brushing mechanism is also available.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 370, "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization", "pub_year": 2022, "domain": "crime patterns ", "requirement": {"requirement_text": "T2. Probability _x0001_ Intensity Crime Hotspots. Enable a mecha-nism capable of identifying hotspots based not only on theabsolute number of crimes but also on the probability oftheir occurrence.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "This view, depicted in Fig. 7a, enables the visualization ofanchor points\u2019 geographical location. Each node\u2019s colordepends on the group it belongs to, which is computed asdescribed in Section 6. The location view is particularly use-ful to overview the spatial distribution of hotspots and theirsimilarity. Besides, it is possible to show additional informa-tion in the background coloring the census units accordingto a given property, for example, socioeconomic and social vulnerability index (this resource will be exploited in thecase studies). Location view provides 2D and 3D visualiza-tion of the hotspots, being possible to change visualizationproperties such as size and opacity of the hotspots. Coloropacity and elevation, the latter only available in the 3Dview, can be set to correspond to the intensity or probabilityof the hotspots.Anchor Point Selection. By a simple clicking in an anchorpoint, Physical View shows the Google Street View photos inthe surroundings of the selected hotspots", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "map+scatter", "axial_code": [], "componenet_code": ["map", "scatter"]}, {"solution_text": "Location view provides 2D and 3D visualiza-tion of the hotspots, being possible to change visualizationproperties such as size and opacity of the hotspots. Coloropacity and elevation, the latter only available in the 3Dview, can be set to correspond to the intensity or probabilityof the hotspots.Anchor Point Selection. By a simple clicking in an anchorpoint, Physical View shows the Google Street View photos inthe surroundings of the selected hotspots", "solution_category": "interaction", "solution_axial": "Encode,OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Encode"], "componenet_code": ["overview_and_explore", "encode"]}]}, {"author": "zsz", "index_original": 371, "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization", "pub_year": 2022, "domain": "crime patterns ", "requirement": {"requirement_text": "T3. Crime Patterns Analysis. Enable the analysis of crimepatterns in a particular hotspot or a group of hotspots", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.", "data_code": {"tables": 1, "categorical": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "This view uses Google Street View to extract and organizephotos of the surroundings of selected hotspots over time(see Fig. 7f). Each photo is a collage of many photosextracted during spatial padding. This padding is accom-plished for each time slice. Physical View helps domainexperts to understand the relationship between crime pat-terns and the urban infrastructure over time.", "solution_category": "visualization", "solution_axial": "annotation", "solution_compoent": "area+image", "axial_code": [], "componenet_code": ["image", "area"]}]}, {"author": "zsz", "index_original": 372, "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization", "pub_year": 2022, "domain": "crime patterns ", "requirement": {"requirement_text": "T4. Analyze the Surroundings of a Hotspot. Scrutinize urbancharacteristics around a hotspot.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "Illustrated in Fig. 7b, the hotspot scatter view shows the Prob-ability_x0001_Intensity scatter plot. To identify anchor points as hot-spots, one can use the linear discriminant filter functiondescribed in Section 5 by tuning the parameters a and pct (per-centage of anchor points to be considered hotspots), although an interactive brushing mechanism is also available.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "Section 5 by tuning the parameters a and pct (per-centage of anchor points to be considered hotspots), although an interactive brushing mechanism is also available.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}, {"solution_text": "Once hotspots have been selected with Hotspot Scatter View,they are grouped according to similarity, and the multidi-mensional projection is performed to reveal their distribu-tion in the feature space.", "solution_category": "data_manipulation", "solution_axial": "Filtering,Clustering&Grouping,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Filtering", "DimensionalityReduction", "Clustering&Grouping"], "componenet_code": ["filtering", "dimensionality_reduction", "clustering_and_grouping"]}, {"solution_text": "Once hotspots have been selected with Hotspot Scatter View,they are grouped according to similarity, and the multidi-mensional projection is performed to reveal their distribu-tion in the feature space. As shown in Fig. 7c, hotspots arecolored according to their groups in the projection. More-over, the legend on the right encodes the label and the num-ber of elements in each group.Group Selection. By clicking in the label of a group, Loca-tion View, Hotspot Scatter View, Between-Group Chart, andWithin-Group Joy Chart are updated to highlight only thedata in the selected group, making it easier for users to focustheir analysis on the selected group of hotspots.Filtering. It is possible to select hotspots using a lasso selec-tion. The \ufb01ltered hotspots are highlighted in the LocationView, Hotspot Scatter View, Between-Group Chart, and Within-Group Joy Chart. Besides, one can analyze individual timeseries of selected hotspots using the Within-Group Joy Chart", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "Group Selection. By clicking in the label of a group, Loca-tion View, Hotspot Scatter View, Between-Group Chart, andWithin-Group Joy Chart are updated to highlight only thedata in the selected group, making it easier for users to focustheir analysis on the selected group of hotspots.Filtering. It is possible to select hotspots using a lasso selec-tion. The \ufb01ltered hotspots are highlighted in the LocationView, Hotspot Scatter View, Between-Group Chart, and Within-Group Joy Chart. Besides, one can analyze individual timeseries of selected hotspots using the Within-Group Joy Chart", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 373, "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization", "pub_year": 2022, "domain": "crime patterns ", "requirement": {"requirement_text": "T4. Analyze the Surroundings of a Hotspot. Scrutinize urbancharacteristics around a hotspot.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "To better visualize the intra-cluster patterns, we create avisual representation that summarizes the crime pattern ineach group (see Fig. 7d). This visualization shows the aver-age time series of each group and the standard deviation ofthe group\u2019s time series. The rectangular glyphs, whose sizere\ufb02ects the number of hotspots in the group, are arrangedto keep the most similar groups closer to each other in thelayout, following the proximity relation observed in theCrime Pattern Projection View. The rectangular glyphsarrangement is computed from an optimization proceduresimilar to the method described in [38]. The Between-GroupChart is useful to understand the crime patterns present inthe data.Group Selection. By clicking in a rectangular glyph, Loca-tion View shows additional information encoded on the geo-map, such as the socioeconomic and social vulnerabilityindex.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area", "axial_code": [], "componenet_code": ["area"]}, {"solution_text": "By clicking in a rectangular glyph, Loca-tion View shows additional information encoded on the geo-map, such as the socioeconomic and social vulnerabilityindex.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 374, "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization", "pub_year": 2022, "domain": "crime patterns ", "requirement": {"requirement_text": "T4. Analyze the Surroundings of a Hotspot. Scrutinize urbancharacteristics around a hotspot.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.", "data_code": {"tables": 1, "categorical": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "This view relies on \ufb01lled line plots (see Fig. 7e) to present, ineach line of the chart, the crime time series of the hotspotsof a speci\ufb01c group. This visualization aims to provide adetailed visualization of the crime pattern in the hotspotgroup.Time Series Selection. By clicking in a particular timeseries, Physical View shows the photos of their surroundings.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area", "axial_code": [], "componenet_code": ["area"]}, {"solution_text": "By clicking in a particular timeseries, Physical View shows the photos of their surroundings.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 375, "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization", "pub_year": 2022, "domain": "crime patterns ", "requirement": {"requirement_text": "T5. Group Similar Hotspots. Group hotspots according tothe similarity of their patterns to visualize the spatial distri-bution of similar hotspots and possible causes for theobserved pattern", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "To better visualize the intra-cluster patterns, we create avisual representation that summarizes the crime pattern ineach group (see Fig. 7d). This visualization shows the aver-age time series of each group and the standard deviation ofthe group\u2019s time series. The rectangular glyphs, whose sizere\ufb02ects the number of hotspots in the group, are arrangedto keep the most similar groups closer to each other in thelayout, following the proximity relation observed in theCrime Pattern Projection View. The rectangular glyphsarrangement is computed from an optimization proceduresimilar to the method described in [38]. The Between-GroupChart is useful to understand the crime patterns present inthe data.Group Selection. By clicking in a rectangular glyph, Loca-tion View shows additional information encoded on the geo-map, such as the socioeconomic and social vulnerabilityindex.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area", "axial_code": [], "componenet_code": ["area"]}, {"solution_text": "By clicking in a rectangular glyph, Loca-tion View shows additional information encoded on the geo-map, such as the socioeconomic and social vulnerabilityindex.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 376, "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization", "pub_year": 2022, "domain": "crime patterns ", "requirement": {"requirement_text": "T5. Group Similar Hotspots. Group hotspots according tothe similarity of their patterns to visualize the spatial distri-bution of similar hotspots and possible causes for theobserved pattern", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.", "data_code": {"tables": 1, "categorical": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "This view relies on \ufb01lled line plots (see Fig. 7e) to present, ineach line of the chart, the crime time series of the hotspotsof a speci\ufb01c group. This visualization aims to provide adetailed visualization of the crime pattern in the hotspotgroup.Time Series Selection. By clicking in a particular timeseries, Physical View shows the photos of their surroundings.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area", "axial_code": [], "componenet_code": ["area"]}, {"solution_text": "By clicking in a particular timeseries, Physical View shows the photos of their surroundings.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 377, "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization", "pub_year": 2022, "domain": "crime patterns ", "requirement": {"requirement_text": "T6. Compare Hotspots\u2019 Groups. Support the comparison ofhotspot groups according, making it possible to analyze thepatterns of different groups and the dispersion of hotspotswithin a group.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "Illustrated in Fig. 7b, the hotspot scatter view shows the Prob-ability_x0001_Intensity scatter plot. To identify anchor points as hot-spots, one can use the linear discriminant filter functiondescribed in Section 5 by tuning the parameters a and pct (per-centage of anchor points to be considered hotspots), although an interactive brushing mechanism is also available.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "Section 5 by tuning the parameters a and pct (per-centage of anchor points to be considered hotspots), although an interactive brushing mechanism is also available.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}, {"solution_text": "Once hotspots have been selected with Hotspot Scatter View,they are grouped according to similarity, and the multidi-mensional projection is performed to reveal their distribu-tion in the feature space.", "solution_category": "data_manipulation", "solution_axial": "Filtering,Clustering&Grouping,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Filtering", "DimensionalityReduction", "Clustering&Grouping"], "componenet_code": ["filtering", "dimensionality_reduction", "clustering_and_grouping"]}, {"solution_text": "Once hotspots have been selected with Hotspot Scatter View,they are grouped according to similarity, and the multidi-mensional projection is performed to reveal their distribu-tion in the feature space. As shown in Fig. 7c, hotspots arecolored according to their groups in the projection. More-over, the legend on the right encodes the label and the num-ber of elements in each group.Group Selection. By clicking in the label of a group, Loca-tion View, Hotspot Scatter View, Between-Group Chart, andWithin-Group Joy Chart are updated to highlight only thedata in the selected group, making it easier for users to focustheir analysis on the selected group of hotspots.Filtering. It is possible to select hotspots using a lasso selec-tion. The \ufb01ltered hotspots are highlighted in the LocationView, Hotspot Scatter View, Between-Group Chart, and Within-Group Joy Chart. Besides, one can analyze individual timeseries of selected hotspots using the Within-Group Joy Chart", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "Group Selection. By clicking in the label of a group, Loca-tion View, Hotspot Scatter View, Between-Group Chart, andWithin-Group Joy Chart are updated to highlight only thedata in the selected group, making it easier for users to focustheir analysis on the selected group of hotspots.Filtering. It is possible to select hotspots using a lasso selec-tion. The \ufb01ltered hotspots are highlighted in the LocationView, Hotspot Scatter View, Between-Group Chart, and Within-Group Joy Chart. Besides, one can analyze individual timeseries of selected hotspots using the Within-Group Joy Chart", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 378, "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization", "pub_year": 2022, "domain": "crime patterns ", "requirement": {"requirement_text": "T6. Compare Hotspots\u2019 Groups. Support the comparison ofhotspot groups according, making it possible to analyze thepatterns of different groups and the dispersion of hotspotswithin a group.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.", "data_code": {"tables": 1, "categorical": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "To better visualize the intra-cluster patterns, we create avisual representation that summarizes the crime pattern ineach group (see Fig. 7d). This visualization shows the aver-age time series of each group and the standard deviation ofthe group\u2019s time series. The rectangular glyphs, whose sizere\ufb02ects the number of hotspots in the group, are arrangedto keep the most similar groups closer to each other in thelayout, following the proximity relation observed in theCrime Pattern Projection View. The rectangular glyphsarrangement is computed from an optimization proceduresimilar to the method described in [38]. The Between-GroupChart is useful to understand the crime patterns present inthe data.Group Selection. By clicking in a rectangular glyph, Loca-tion View shows additional information encoded on the geo-map, such as the socioeconomic and social vulnerabilityindex.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area", "axial_code": [], "componenet_code": ["area"]}, {"solution_text": "By clicking in a rectangular glyph, Loca-tion View shows additional information encoded on the geo-map, such as the socioeconomic and social vulnerabilityindex.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 379, "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization", "pub_year": 2022, "domain": "crime patterns ", "requirement": {"requirement_text": "T6. Compare Hotspots\u2019 Groups. Support the comparison ofhotspot groups according, making it possible to analyze thepatterns of different groups and the dispersion of hotspotswithin a group.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.", "data_code": {"tables": 1, "categorical": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "This view relies on \ufb01lled line plots (see Fig. 7e) to present, ineach line of the chart, the crime time series of the hotspotsof a speci\ufb01c group. This visualization aims to provide adetailed visualization of the crime pattern in the hotspotgroup.Time Series Selection. By clicking in a particular timeseries, Physical View shows the photos of their surroundings.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area", "axial_code": [], "componenet_code": ["area"]}, {"solution_text": "By clicking in a particular timeseries, Physical View shows the photos of their surroundings.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 381, "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models", "pub_year": 2022, "domain": "Fairness in graph mining", "requirement": {"requirement_text": "Which portion of nodes are the most important.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Our modular design enables analysts to freely integrate any graph-based ranking models for use as the target or base model. For demonstration purposes, we apply PageRank as the base model and AttriRank and a debiased PageRank (InFoRM) as the target models.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Our framework is designed to enable a flexible definition of ranks and attributes to be considered when diagnosing fairness. Recent research [8, 35, 37] emphasizes that the top-k elements will receive more attention, and ranking bias is typically explored with respect to the top-k ranks. In our proposed framework, a data setting panel is configured to enable the analyst to select the top-k nodes. This is facilitated by the Ranking Score Density Histogram, which shows the ranking score distribution for the target ranking model. The analyst can interactively modify the number of bins by clicking the gear icon, and the histogram supports brush selection to select a specific ranking range. For example, if the analyst cares about potential biases of nodes who have similar ranking scores, then the analyst can brush a particular bin on the histogram and all the nodes within that ranking score range are selected. If the analyst wishes to select a specific ranking position, a slider is configured to enable the analyst to select nodes from rank m to n. In this way, the analysts can explore how attributes are distributed for any specific range of ranks.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "Our framework is designed to enable a flexible definition of ranks and attributes to be considered when diagnosing fairness. Recent research [8, 35, 37] emphasizes that the top-k elements will receive more attention, and ranking bias is typically explored with respect to the top-k ranks. In our proposed framework, a data setting panel is configured to enable the analyst to select the top-k nodes. This is facilitated by the Ranking Score Density Histogram, which shows the ranking score distribution for the target ranking model. The analyst can interactively modify the number of bins by clicking the gear icon, and the histogram supports brush selection to select a specific ranking range. For example, if the analyst cares about potential biases of nodes who have similar ranking scores, then the analyst can brush a particular bin on the histogram and all the nodes within that ranking score range are selected. If the analyst wishes to select a specific ranking position, a slider is configured to enable the analyst to select nodes from rank m to n. In this way, the analysts can explore how attributes are distributed for any specific range of ranks.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 382, "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models", "pub_year": 2022, "domain": "Fairness in graph mining", "requirement": {"requirement_text": "Which attributes are critically important for fairness.", "requirement_code": {"discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Attributes View. To support the interactive definition of groups (T1.2),we have designed an attribute setting panel (Figure 1.B) and an attributeview panel (Figure 1.C). The attributes view panel employs a paral-lel set where each selected attribute is visualized with multiple bars.Selected nodes are encoded as curves with different widths. Both theheight of bars and the width of the curves encode the number of nodesmapping to a specific attribute value. Additionally, the distribution ofattributes across the selected nodes is visualized with a histogram (Fig-ure 1.C.1). We use a light grey color to show the attribute distributionfor the entire dataset, and the dark grey color histogram shows the dis-tribution of attributes for the selected nodes. The attribute setting panel(Figure 1.B) enables the flexible selection of one or more attributesby clicking on the multiple selection area (Figure 1.B.1). All corre-sponding views including the attributes view (Figure 1.C), the grouptable view (Figure 1.D), the rank mapping view (Figure 1.E), the groupproportion view (Figure 1.E.3) and the group shift view (Figure 1.E.4)are automatically updated as the selected attributes are changed. Sincegroup fairness is most often based on categorical attribute values, wealso include a customization feature that allows analysts to categorizeattributes that may have continuous values. For example, protectedclasses for age are often grouped into ranges, e.g. under 18, 65+, etc..We also provide another histogram (Figure 1.B.3) to facilitate thecomparison of distribution similarities on selected attributes betweenselected nodes and the entire dataset. The metric for measuring dis-tribution similarities can be customized based on the analysts\u2019 needs.Currently, the framework supports Kullback-Leibler divergence fordemonstration purpose. The height of the bars are mapped to the dif-ferences of the between the distributions of the selected nodes and theentire dataset on a speci\ufb01c attribute.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 383, "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models", "pub_year": 2022, "domain": "Fairness in graph mining", "requirement": {"requirement_text": "Which nodes are advantaged/disadvantaged by the model.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Individual Bias. Individual bias represents how the model guarantees that nodes with similar attributes will receive similar rankings. It is important to understand if individual nodes have been \u201csacrificed\u201d or privileged by the model in order to reduce group bias. To help analysts explore the individual biases among selected nodes, we label the selected nodes as advantaged/disadvantage nodes according to their ranking position changes (increase/decrease) between the base and target model.", "solution_category": "data_manipulation", "solution_axial": "Explainability", "solution_compoent": "AnalyseIndividualBiasbylabelingmodel-drivenrankingshifts.", "axial_code": ["Explainability"], "componenet_code": ["explainability"]}]}, {"author": "zsz", "index_original": 384, "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models", "pub_year": 2022, "domain": "Fairness in graph mining", "requirement": {"requirement_text": "Which nodes are advantaged/disadvantaged by the model.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Content Bias. In real-world applications, a full ranking of millions of items simply cannot be displayed, and is typically culled to some top-k rank. In this setting, even the nodes who have the same ranking scores can have a large difference in ranking positions, and this problem is referred to as content bias. For example, imagine a list of items where the second through the seventh item have identical ranking scores. The method of display implies inequality in ranking even though ranks two through seven have equal ranking scores. Here, the implicit ordering can lead to significant differences in their exposure rates. To help analysts explore this phenomenon, we group nodes into clusters based on their ranking scores; Group Bias. Many fairness metrics have been proposed for measuring group fairness [10, 14]. These methods attempt to measure the degree of discrimination or bias [27]. However, there is no single term that universally represents bias. We denote group bias as the bias that reflects the ability of the model to achieve statistical parity between groups, where a group is defined with respect to the analysts\u2019 selected sensitive attributes. The goal of the framework is to enable analysts to audit whether the ranking results of a model exhibit direct or indirect preferences towards one or more groups, resulting in lower ranking scores for the disadvantaged groups. Compared with the content bias, where disadvantages can be due to display constraints, group bias can be mitigated algorithmically. To observe the impact on groups\u2019 ranking between the base and the target model, we formalize the ranking changes for each group by computing the average ranking position change; Individual Bias. Individual bias represents how the model guarantees that nodes with similar attributes will receive similar rankings. It is important to understand if individual nodes have been \u201csacrificed\u201d or privileged by the model in order to reduce group bias. To help analysts explore the individual biases among selected nodes, we label the selected nodes as advantaged/disadvantage nodes according to their ranking position changes (increase/decrease) between the base and target model.", "solution_category": "data_manipulation", "solution_axial": "Excluding,Clustering,AlgorithmicCalculation,Explainability", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding", "Clustering", "Explainability"], "componenet_code": ["algorithmic_calculation", "excluding", "clustering_and_grouping", "explainability"]}, {"solution_text": "From top to bottom, the nodes are ranked from m to n, and in a cluster, the nodes are mapped from left to right according to their rank (high to low). Each cluster from the base model is connected to a corresponding cluster from the target model by a grey line when they share the same node(s), which illustrates how the ranking of this node changes between models.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "matrix+link", "axial_code": [], "componenet_code": ["link", "matrix"]}]}, {"author": "zsz", "index_original": 385, "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models", "pub_year": 2022, "domain": "Fairness in graph mining", "requirement": {"requirement_text": "Which nodes are advantaged/disadvantaged by the model.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Content Bias. In real-world applications, a full ranking of millions of items simply cannot be displayed, and is typically culled to some top-k rank. In this setting, even the nodes who have the same ranking scores can have a large difference in ranking positions, and this problem is referred to as content bias. For example, imagine a list of items where the second through the seventh item have identical ranking scores. The method of display implies inequality in ranking even though ranks two through seven have equal ranking scores. Here, the implicit ordering can lead to significant differences in their exposure rates. To help analysts explore this phenomenon, we group nodes into clusters based on their ranking scores; Group Bias. Many fairness metrics have been proposed for measuring group fairness [10, 14]. These methods attempt to measure the degree of discrimination or bias [27]. However, there is no single term that universally represents bias. We denote group bias as the bias that reflects the ability of the model to achieve statistical parity between groups, where a group is defined with respect to the analysts\u2019 selected sensitive attributes. The goal of the framework is to enable analysts to audit whether the ranking results of a model exhibit direct or indirect preferences towards one or more groups, resulting in lower ranking scores for the disadvantaged groups. Compared with the content bias, where disadvantages can be due to display constraints, group bias can be mitigated algorithmically. To observe the impact on groups\u2019 ranking between the base and the target model, we formalize the ranking changes for each group by computing the average ranking position change; Individual Bias. Individual bias represents how the model guarantees that nodes with similar attributes will receive similar rankings. It is important to understand if individual nodes have been \u201csacrificed\u201d or privileged by the model in order to reduce group bias. To help analysts explore the individual biases among selected nodes, we label the selected nodes as advantaged/disadvantage nodes according to their ranking position changes (increase/decrease) between the base and target model.", "solution_category": "data_manipulation", "solution_axial": "Excluding,Clustering,AlgorithmicCalculation,Explainability", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding", "Clustering", "Explainability"], "componenet_code": ["algorithmic_calculation", "excluding", "clustering_and_grouping", "explainability"]}, {"solution_text": "The group shift view is designed to inspect both group bias and individual bias. For group bias, the bar chart on the left shows the average ranking change of each group. The color of the bar encodes the identity of the group. The bar chart on the right shows the distribution of group members in the base model and target model, and the analyst can diagnose group shifts in selected nodes to understand the corresponding fairness trade-offs between models.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "zsz", "index_original": 386, "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models", "pub_year": 2022, "domain": "Fairness in graph mining", "requirement": {"requirement_text": "Which groups are advantaged/disadvantaged by the model.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Group Bias. Many fairness metrics have been proposed for measuring group fairness [10, 14]. These methods attempt to measure the degree of discrimination or bias [27]. However, there is no single term that universally represents bias. We denote group bias as the bias that reflects the ability of the model to achieve statistical parity between groups, where a group is defined with respect to the analysts\u2019 selected sensitive attributes. The goal of the framework is to enable analysts to audit whether the ranking results of a model exhibit direct or indirect preferences towards one or more groups, resulting in lower ranking scores for the disadvantaged groups. Compared with the content bias, where disadvantages can be due to display constraints, group bias can be mitigated algorithmically. To observe the impact on groups\u2019 ranking between the base and the target model, we formalize the ranking changes for each group by computing the average ranking position change.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "GroupBiasreduction.Computingtheaveragerankingpositionchange.", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "zsz", "index_original": 387, "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models", "pub_year": 2022, "domain": "Fairness in graph mining", "requirement": {"requirement_text": "Which groups are advantaged/disadvantaged by the model.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Content Bias. In real-world applications, a full ranking of millions of items simply cannot be displayed, and is typically culled to some top-k rank. In this setting, even the nodes who have the same ranking scores can have a large difference in ranking positions, and this problem is referred to as content bias. For example, imagine a list of items where the second through the seventh item have identical ranking scores. The method of display implies inequality in ranking even though ranks two through seven have equal ranking scores. Here, the implicit ordering can lead to significant differences in their exposure rates. To help analysts explore this phenomenon, we group nodes into clusters based on their ranking scores; Group Bias. Many fairness metrics have been proposed for measuring group fairness [10, 14]. These methods attempt to measure the degree of discrimination or bias [27]. However, there is no single term that universally represents bias. We denote group bias as the bias that reflects the ability of the model to achieve statistical parity between groups, where a group is defined with respect to the analysts\u2019 selected sensitive attributes. The goal of the framework is to enable analysts to audit whether the ranking results of a model exhibit direct or indirect preferences towards one or more groups, resulting in lower ranking scores for the disadvantaged groups. Compared with the content bias, where disadvantages can be due to display constraints, group bias can be mitigated algorithmically. To observe the impact on groups\u2019 ranking between the base and the target model, we formalize the ranking changes for each group by computing the average ranking position change; Individual Bias. Individual bias represents how the model guarantees that nodes with similar attributes will receive similar rankings. It is important to understand if individual nodes have been \u201csacrificed\u201d or privileged by the model in order to reduce group bias. To help analysts explore the individual biases among selected nodes, we label the selected nodes as advantaged/disadvantage nodes according to their ranking position changes (increase/decrease) between the base and target model.", "solution_category": "data_manipulation", "solution_axial": "Excluding,Clustering,AlgorithmicCalculation,Explainability", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding", "Clustering", "Explainability"], "componenet_code": ["algorithmic_calculation", "excluding", "clustering_and_grouping", "explainability"]}, {"solution_text": "Group Proportion View. The group proportion view is designed to illustrate the target ranking model\u2019s effects on each group\u2019s proportion. The group proportion view consists of two sets of bars and each set shows the composition of selected nodes sorted by both ranking models respectively. To facilitate inspection, we support switching the view mode between the proportion mode and the comparison mode. The proportion mode displays the stacked bars to summarize the overall group distribution of the selected nodes, while the comparison mode supports a direct comparison of group proportions between models. In other words, the comparison mode helps analysts perform pair-wise comparisons of the same group proportions between different models. Analysts can toggle between the proportion and comparison mode by using the switch button on the right side of the title bar of the rank mapping view.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "Analysts can toggle between the proportion and comparison mode by using the switch button on the right side of the title bar of the rank mapping view.", "solution_category": "interaction", "solution_axial": "Encode", "solution_compoent": "", "axial_code": ["Encode"], "componenet_code": ["encode"]}]}, {"author": "zsz", "index_original": 388, "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models", "pub_year": 2022, "domain": "Fairness in graph mining", "requirement": {"requirement_text": "Which groups are advantaged/disadvantaged by the model.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Content Bias. In real-world applications, a full ranking of millions of items simply cannot be displayed, and is typically culled to some top-k rank. In this setting, even the nodes who have the same ranking scores can have a large difference in ranking positions, and this problem is referred to as content bias. For example, imagine a list of items where the second through the seventh item have identical ranking scores. The method of display implies inequality in ranking even though ranks two through seven have equal ranking scores. Here, the implicit ordering can lead to significant differences in their exposure rates. To help analysts explore this phenomenon, we group nodes into clusters based on their ranking scores; Group Bias. Many fairness metrics have been proposed for measuring group fairness [10, 14]. These methods attempt to measure the degree of discrimination or bias [27]. However, there is no single term that universally represents bias. We denote group bias as the bias that reflects the ability of the model to achieve statistical parity between groups, where a group is defined with respect to the analysts\u2019 selected sensitive attributes. The goal of the framework is to enable analysts to audit whether the ranking results of a model exhibit direct or indirect preferences towards one or more groups, resulting in lower ranking scores for the disadvantaged groups. Compared with the content bias, where disadvantages can be due to display constraints, group bias can be mitigated algorithmically. To observe the impact on groups\u2019 ranking between the base and the target model, we formalize the ranking changes for each group by computing the average ranking position change; Individual Bias. Individual bias represents how the model guarantees that nodes with similar attributes will receive similar rankings. It is important to understand if individual nodes have been \u201csacrificed\u201d or privileged by the model in order to reduce group bias. To help analysts explore the individual biases among selected nodes, we label the selected nodes as advantaged/disadvantage nodes according to their ranking position changes (increase/decrease) between the base and target model.", "solution_category": "data_manipulation", "solution_axial": "Excluding,Clustering,AlgorithmicCalculation,Explainability", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding", "Clustering", "Explainability"], "componenet_code": ["algorithmic_calculation", "excluding", "clustering_and_grouping", "explainability"]}, {"solution_text": "The group shift view is designed to inspect both group bias and individual bias. For group bias, the bar chart on the left shows the average ranking change of each group. The color of the bar encodes the identity of the group. The bar chart on the right shows the distribution of group members in the base model and target model, and the analyst can diagnose group shifts in selected nodes to understand the corresponding fairness trade-offs between models.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "zsz", "index_original": 389, "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models", "pub_year": 2022, "domain": "Fairness in graph mining", "requirement": {"requirement_text": "Which nodes have similar relevance (ranking scores).", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Content Bias. In real-world applications, a full ranking of millions of items simply cannot be displayed, and is typically culled to some top-k rank. In this setting, even the nodes who have the same ranking scores can have a large difference in ranking positions, and this problem is referred to as content bias. For example, imagine a list of items where the second through the seventh item have identical ranking scores. The method of display implies inequality in ranking even though ranks two through seven have equal ranking scores. Here, the implicit ordering can lead to significant differences in their exposure rates. To help analysts explore this phenomenon, we group nodes into clusters based on their ranking scores.", "solution_category": "data_manipulation", "solution_axial": "Excluding,Clustering", "solution_compoent": "ExploreContentBias.Clusternodeswithsimilarrankingscores.", "axial_code": ["Excluding", "Clustering"], "componenet_code": ["excluding", "clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 390, "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models", "pub_year": 2022, "domain": "Fairness in graph mining", "requirement": {"requirement_text": "Which nodes have similar relevance (ranking scores).", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Content Bias. In real-world applications, a full ranking of millions of items simply cannot be displayed, and is typically culled to some top-k rank. In this setting, even the nodes who have the same ranking scores can have a large difference in ranking positions, and this problem is referred to as content bias. For example, imagine a list of items where the second through the seventh item have identical ranking scores. The method of display implies inequality in ranking even though ranks two through seven have equal ranking scores. Here, the implicit ordering can lead to significant differences in their exposure rates. To help analysts explore this phenomenon, we group nodes into clusters based on their ranking scores; Group Bias. Many fairness metrics have been proposed for measuring group fairness [10, 14]. These methods attempt to measure the degree of discrimination or bias [27]. However, there is no single term that universally represents bias. We denote group bias as the bias that reflects the ability of the model to achieve statistical parity between groups, where a group is defined with respect to the analysts\u2019 selected sensitive attributes. The goal of the framework is to enable analysts to audit whether the ranking results of a model exhibit direct or indirect preferences towards one or more groups, resulting in lower ranking scores for the disadvantaged groups. Compared with the content bias, where disadvantages can be due to display constraints, group bias can be mitigated algorithmically. To observe the impact on groups\u2019 ranking between the base and the target model, we formalize the ranking changes for each group by computing the average ranking position change; Individual Bias. Individual bias represents how the model guarantees that nodes with similar attributes will receive similar rankings. It is important to understand if individual nodes have been \u201csacrificed\u201d or privileged by the model in order to reduce group bias. To help analysts explore the individual biases among selected nodes, we label the selected nodes as advantaged/disadvantage nodes according to their ranking position changes (increase/decrease) between the base and target model.", "solution_category": "data_manipulation", "solution_axial": "Excluding,Clustering,AlgorithmicCalculation,Explainability", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding", "Clustering", "Explainability"], "componenet_code": ["algorithmic_calculation", "excluding", "clustering_and_grouping", "explainability"]}, {"solution_text": "Rank Mapping View. The rank mapping view consists of two columns of stacked rectangles, where the left column shows the ranking results of the base model, and the right column shows the ranking results of the target model. For each column, small squares that represent nodes of the analyst-defined groups are organized into large rectangles, where each rectangle represents a cluster that contains nodes with similar ranking scores.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "matrix+link", "axial_code": [], "componenet_code": ["link", "matrix"]}]}, {"author": "zsz", "index_original": 391, "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models", "pub_year": 2022, "domain": "Fairness in graph mining", "requirement": {"requirement_text": "What is each node\u2019s position in the ranking result, and how likely is it that content bias has occurred in similar nodes.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Content Bias. In real-world applications, a full ranking of millions of items simply cannot be displayed, and is typically culled to some top-k rank. In this setting, even the nodes who have the same ranking scores can have a large difference in ranking positions, and this problem is referred to as content bias. For example, imagine a list of items where the second through the seventh item have identical ranking scores. The method of display implies inequality in ranking even though ranks two through seven have equal ranking scores. Here, the implicit ordering can lead to significant differences in their exposure rates. To help analysts explore this phenomenon, we group nodes into clusters based on their ranking scores.", "solution_category": "data_manipulation", "solution_axial": "Excluding,Clustering", "solution_compoent": "ExploreContentBias.Clusternodeswithsimilarrankingscores.", "axial_code": ["Excluding", "Clustering"], "componenet_code": ["excluding", "clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 392, "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models", "pub_year": 2022, "domain": "Fairness in graph mining", "requirement": {"requirement_text": "What is each node\u2019s position in the ranking result, and how likely is it that content bias has occurred in similar nodes.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.", "data_code": {"ordinal": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Content Bias. In real-world applications, a full ranking of millions of items simply cannot be displayed, and is typically culled to some top-k rank. In this setting, even the nodes who have the same ranking scores can have a large difference in ranking positions, and this problem is referred to as content bias. For example, imagine a list of items where the second through the seventh item have identical ranking scores. The method of display implies inequality in ranking even though ranks two through seven have equal ranking scores. Here, the implicit ordering can lead to significant differences in their exposure rates. To help analysts explore this phenomenon, we group nodes into clusters based on their ranking scores; Group Bias. Many fairness metrics have been proposed for measuring group fairness [10, 14]. These methods attempt to measure the degree of discrimination or bias [27]. However, there is no single term that universally represents bias. We denote group bias as the bias that reflects the ability of the model to achieve statistical parity between groups, where a group is defined with respect to the analysts\u2019 selected sensitive attributes. The goal of the framework is to enable analysts to audit whether the ranking results of a model exhibit direct or indirect preferences towards one or more groups, resulting in lower ranking scores for the disadvantaged groups. Compared with the content bias, where disadvantages can be due to display constraints, group bias can be mitigated algorithmically. To observe the impact on groups\u2019 ranking between the base and the target model, we formalize the ranking changes for each group by computing the average ranking position change; Individual Bias. Individual bias represents how the model guarantees that nodes with similar attributes will receive similar rankings. It is important to understand if individual nodes have been \u201csacrificed\u201d or privileged by the model in order to reduce group bias. To help analysts explore the individual biases among selected nodes, we label the selected nodes as advantaged/disadvantage nodes according to their ranking position changes (increase/decrease) between the base and target model.", "solution_category": "data_manipulation", "solution_axial": "Excluding,Clustering,AlgorithmicCalculation,Explainability", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding", "Clustering", "Explainability"], "componenet_code": ["algorithmic_calculation", "excluding", "clustering_and_grouping", "explainability"]}, {"solution_text": "Rank Mapping View. The rank mapping view consists of two columns of stacked rectangles, where the left column shows the ranking results of the base model, and the right column shows the ranking results of the target model. For each column, small squares that represent nodes of the analyst-defined groups are organized into large rectangles, where each rectangle represents a cluster that contains nodes with similar ranking scores.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "matrix+link", "axial_code": [], "componenet_code": ["link", "matrix"]}]}, {"author": "zsz", "index_original": 393, "paper_title": "VisQA: X-raying Vision and Language Reasoning in Transformers", "pub_year": 2022, "domain": "Visual Question Answering(HCI)", "requirement": {"requirement_text": "G1 Examine the performances of each instance for a givenmodel. To investigate bias in VQA systems as introduced in sec. 3.3,it is first important to examine the model predictions, along with itsconfidence score with respect to its inputs. In order to be useful, thosepredictions need to be combined with the ground-truth, to estimate ifthe model is wrong, and how frequent is the ground-truth answer andpredictions. Inputs need to be inspected as well as they may conveyambiguities that may be at the beginning of an explanation for a mis-take. Finally, due to a large amount of data available for inspection,experts may prioritize inputs the more likely to be biased, i.e., thosewith infrequent ground-truth answers and frequent predictions.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The GQA [29] standard dataset provides question/image pairs along with their answers, the ground truth of bounding boxes, and semantic descriptions of questions.", "data_code": {"media": 1, "textual": 1}}, "solution": [{"solution_text": "Image ranking-by-feature. In order to ease user exploration over the complete dataset, VISQA displays images in the top bar from left to right based on the likelihood of their questions to be answered using statistical biases. To do so, we classify questions using ground truth answers, as proposed in [32]: top 20% of the most frequent answer will be classified as Head, as opposed to Tail which describes questions with the least frequent answers. We attribute a score to each image based on their Head-questions/Tail-questions ratio. The more an image has Tail-questions over Head-questions, the higher its score is. The underlying hypothesis is that frequent answers will be chosen more likely when a model tends to exploit biases (e.g. \u201cYellow bananas\u201d). Also, frequent questions are harder to analyze since if any bias is exploited by the model, it will answer correctly.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Retrieval", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Retrieval"], "componenet_code": ["algorithmic_calculation", "retrieval"]}, {"solution_text": "Image ranking-by-feature. In order to ease user exploration over the complete dataset, VISQA displays images in the top bar from left to right based on the likelihood of their questions to be answered using statistical biases. To do so, we classify questions using ground truth answers, as proposed in [32]: top 20% of the most frequent answer will be classified as Head, as opposed to Tail which describes questions with the least frequent answers. We attribute a score to each image based on their Head-questions/Tail-questions ratio. The more an image has Tail-questions over Head-questions, the higher its score is. The underlying hypothesis is that frequent answers will be chosen more likely when a model tends to exploit biases (e.g. \u201cYellow bananas\u201d). Also, frequent questions are harder to analyze since if any bias is exploited by the model, it will answer correctly.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "text+image+matrix+bar", "axial_code": [], "componenet_code": ["text", "bar", "image", "matrix"]}]}, {"author": "zsz", "index_original": 394, "paper_title": "VisQA: X-raying Vision and Language Reasoning in Transformers", "pub_year": 2022, "domain": "Visual Question Answering(HCI)", "requirement": {"requirement_text": "G2 Browse the attention of the model for an instance. Analyzingthe attention maps of LXMERT models is crucial for understandingwhat factors in\ufb02uenced its decision, and eventually whether or not themodel attends to both language and vision. While visualizing indi-vidually each attention map is feasible, we aim at improving such anexploration, by contextualizing each attention head with their neighbor-hood (i.e., other attention heads directly connected to it), and positionwithin the model. This is relevant as attention heads get closer to theoutput, they both encapsulate previous attention, and may be more in\ufb02u-ential on models\u2019 decisions. In addition, experts may need to prioritizeheads conveying salient attention, thus those heads need to be summa-rized and/or emphasized. Finally, for in-depth analysis, experts need tovisualize the complete attention map and link each of their elementsto human-understandable information, i.e., words of the question andbounding boxes within the input image.", "requirement_code": {"discover_observation": 1, "collect_evidence": 1}}, "data": {"data_text": "The GQA [29] standard dataset provides question/image pairs along with their answers, the ground truth of bounding boxes, and semantic descriptions of questions.", "data_code": {"media": 1, "textual": 1}}, "solution": [{"solution_text": "Instance view. This view, inspired by VL-Transformer representations illustrated in Fig. 2, is the root of any analysis of attention maps using VISQA. It matches the internal data flow through the internal structure of the model, from left to right: the input image/question pair, layers and heads with intra-modality layers first, and finally the answer output distribution (encoded as horizontal bars). A particular design decision was to display all attention maps at once, using a single-colored rectangle encoding the attention intensity as k-number [47] (see next paragraph for details). In the Instance view, attention heads can be selected with a mouse-over interaction.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix", "axial_code": [], "componenet_code": ["matrix"]}, {"solution_text": "In the Instance view, attention heads can be selected with a mouse-over interaction.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 395, "paper_title": "VisQA: X-raying Vision and Language Reasoning in Transformers", "pub_year": 2022, "domain": "Visual Question Answering(HCI)", "requirement": {"requirement_text": "G3 Link attention to language tasks. Once a relevant attentionhead is observed by an expert, the user should be able to contextualizeit with the rest of the dataset. In particular, experts are interested inevaluating whether or not this head is responsive to certain tasks pro-vided by the semantics of questions (e.g., \ufb01nd a color), or rather if thehead is responsive to certain topics (e.g., clothing). Such informationcan be provided in the VQA training dataset, considered as categories.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The GQA [29] standard dataset provides question/image pairs along with their answers, the ground truth of bounding boxes, and semantic descriptions of questions.", "data_code": {"media": 1, "textual": 1}}, "solution": [{"solution_text": "Image ranking-by-feature. In order to ease user exploration over the complete dataset, VISQA displays images in the top bar from left to right based on the likelihood of their questions to be answered using statistical biases. To do so, we classify questions using ground truth answers, as proposed in [32]: top 20% of the most frequent answer will be classified as Head, as opposed to Tail which describes questions with the least frequent answers. We attribute a score to each image based on their Head-questions/Tail-questions ratio. The more an image has Tail-questions over Head-questions, the higher its score is. The underlying hypothesis is that frequent answers will be chosen more likely when a model tends to exploit biases (e.g. \u201cYellow bananas\u201d). Also, frequent questions are harder to analyze since if any bias is exploited by the model, it will answer correctly.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Retrieval", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Retrieval"], "componenet_code": ["algorithmic_calculation", "retrieval"]}, {"solution_text": "Head Statistics. are represented using three charts. A vertical area (leftmost chart) represents the distribution of k-numbers of the selected head over the complete validation dataset (around 1500 image/question pairs). The vertical axis encodes the values of k-numbers, while the horizontal axis encodes the density of the corresponding k-number. The current k-number, for the selected head, which corresponds to the image/question pair loaded in VISQA, is represented as a horizontal red bar positioned on the vertical axis. This area-chart provides insights such as the detection of useless heads with constant high k-number which can be reduced to calculation on average overall items instead of selecting specific items. In contrast, heads with constant low k-number can be interpreted as conveying key information. More specialized heads, with bi-modal k-number distributions, can also be observed. Two stacked bar-charts represent the k-numbers of the selected head grouped by question operations.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "area+bar", "axial_code": [], "componenet_code": ["bar", "area"]}]}, {"author": "zsz", "index_original": 396, "paper_title": "VisQA: X-raying Vision and Language Reasoning in Transformers", "pub_year": 2022, "domain": "Visual Question Answering(HCI)", "requirement": {"requirement_text": "G4 Explore alternative scenarios. Once cues on how the modeluses its attention to output a decision are gathered, the next step, is totest this knowledge by querying the model on altered input or parame-ters. Ultimately the experts desire to answers questions such as: \u201cwouldthe model have a similar attention if the question were on another objectof the image?\u201d, or \u201cis this head or group of heads relevant for the \ufb01naldecision?\u201d. This can be regrouped into two categories \ufb01rst, the possi-bility to ask free-from questions, and second the possibility to modifythe model\u2019s attention. In order to be usable, and due to the numberof queries an expert may need to execute, both those manipulationsrequire to interact with the model in a reasonable amount of time, e.g.,less than a couple of seconds.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The GQA [29] standard dataset provides question/image pairs along with their answers, the ground truth of bounding boxes, and semantic descriptions of questions.", "data_code": {"textual": 1}}, "solution": [{"solution_text": "Free-form questions. By default, V IS QA loads the GQA dataset [29]to provide images and questions. But at any time, users can type andask free-form open-ended questions (G4). Such an interaction allowsinvestigating the model\u2019s bias exploitation. For instance, when askedthe following question from the GQA dataset \u201cIs this a mirror or asofa\u201d, the model correctly outputs \u201cmirror\u201d. However, when asked thefollowing user-inputted question \u201cIs there a mirror in this image?\u201d, themodel fails and outputs \u201cno\u201d. This suggests that the model might haveexploited biases when it answered the \ufb01rst question, which is supportedby the fact that in the GQA dataset, \u201cmirror\u201d is the correct answer tothe question \u201cIs this a mirror or a sofa\u201d in 85% of all cases.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 397, "paper_title": "VisQA: X-raying Vision and Language Reasoning in Transformers", "pub_year": 2022, "domain": "Visual Question Answering(HCI)", "requirement": {"requirement_text": "G4 Explore alternative scenarios. Once cues on how the modeluses its attention to output a decision are gathered, the next step, is totest this knowledge by querying the model on altered input or parame-ters. Ultimately the experts desire to answers questions such as: \u201cwouldthe model have a similar attention if the question were on another objectof the image?\u201d, or \u201cis this head or group of heads relevant for the \ufb01naldecision?\u201d. This can be regrouped into two categories \ufb01rst, the possi-bility to ask free-from questions, and second the possibility to modifythe model\u2019s attention. In order to be usable, and due to the numberof queries an expert may need to execute, both those manipulationsrequire to interact with the model in a reasonable amount of time, e.g.,less than a couple of seconds.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The GQA [29] standard dataset provides question/image pairs along with their answers, the ground truth of bounding boxes, and semantic descriptions of questions.", "data_code": {"media": 1, "textual": 1}}, "solution": [{"solution_text": "Image ranking-by-feature. In order to ease user exploration over the complete dataset, VISQA displays images in the top bar from left to right based on the likelihood of their questions to be answered using statistical biases. To do so, we classify questions using ground truth answers, as proposed in [32]: top 20% of the most frequent answer will be classified as Head, as opposed to Tail which describes questions with the least frequent answers. We attribute a score to each image based on their Head-questions/Tail-questions ratio. The more an image has Tail-questions over Head-questions, the higher its score is. The underlying hypothesis is that frequent answers will be chosen more likely when a model tends to exploit biases (e.g. \u201cYellow bananas\u201d). Also, frequent questions are harder to analyze since if any bias is exploited by the model, it will answer correctly.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Retrieval", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Retrieval"], "componenet_code": ["algorithmic_calculation", "retrieval"]}, {"solution_text": "Users can select attention heads by clicking on them in the instance view, or by their k-number category. Such a selection can then be used to prune the corresponding head for the next forward of the model. Pruning here means that the attention head does not perform any focused attention, but uniformly distributes attention over the full set of items (objects or words). Each row of a pruned attention map is thus the equivalent of an average calculation. At any time, users can request a new forward pass of the model by clicking on the top left button \u201cask\u201d.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 398, "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce", "pub_year": 2022, "domain": "e-commerce livestreams", "requirement": {"requirement_text": "R1 Highlighting high-risk periods in a video. Browsing a videostream without visual hints may be tedious. Thus, the systemshould automatically detect and highlight critical periods thatmoderators should consider in priority order.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The short videos", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "The data-processing procedure comprises video-audio decomposition and feature extraction. Our framework first separates a video into a sequence of frames and audio clips and then employs state-of-the-art techniques [30, 48, 52, 59] to extract multimodal features; To moderate the sheer volume of videos, we adopt a binary classifier as the filter, which can discover potentially deviant videos in accordance with extracted visual and audio features. However, video moderation policies differ across platforms and applications. Obtaining sufficient data to train a classifier to classify e-commerce livestreaming videos is difficult. Following the idea of \u201clearning with reviewing,\u201d we first define a linear classifier to initiate the framework and then adopt the reviewed videos as the ground truths to improve the classifier iteratively.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "FeatureSelection"], "componenet_code": ["modeling", "feature_selection"]}, {"solution_text": "The video view consists of a video player and a segmented timeline that displays the risk distribution of different time periods. The video player integrates a set of common video tools (e.g., play/stop and fast-forward) to assist the moderators in flexibly browsing the video content. To highlight high-risk periods in a video, we propose three alternative designs, namely, a segmented timeline with risk indicators, a timeline with in-line flow visualization, and a timeline with triangle glyphs. The design with triangular glyphs merely highlights the high-risk moments but lacks the necessary risk context, which may lead to the skipping of potentially deviant frames. To overcome this limitation, we insert a flow chart depicting the risk distribution of an entire video into the timeline.", "solution_category": "visualization", "solution_axial": "large_panel", "solution_compoent": "video+line+matrix", "axial_code": [], "componenet_code": ["matrix", "video", "line"]}]}, {"author": "zsz", "index_original": 399, "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce", "pub_year": 2022, "domain": "e-commerce livestreams", "requirement": {"requirement_text": "R1 Highlighting high-risk periods in a video. Browsing a videostream without visual hints may be tedious. Thus, the systemshould automatically detect and highlight critical periods thatmoderators should consider in priority order.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The short videos", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "The data-processing procedure comprises video-audio decomposition and feature extraction. Our framework first separates a video into a sequence of frames and audio clips and then employs state-of-the-art techniques [30, 48, 52, 59] to extract multimodal features; To moderate the sheer volume of videos, we adopt a binary classifier as the filter, which can discover potentially deviant videos in accordance with extracted visual and audio features. However, video moderation policies differ across platforms and applications. Obtaining sufficient data to train a classifier to classify e-commerce livestreaming videos is difficult. Following the idea of \u201clearning with reviewing,\u201d we first define a linear classifier to initiate the framework and then adopt the reviewed videos as the ground truths to improve the classifier iteratively.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "FeatureSelection"], "componenet_code": ["modeling", "feature_selection"]}, {"solution_text": "Nevertheless, the flow chart is excessively informative to highlight high-risk moments effectively. To balance the two extremes, we adopt a segmented timeline that is divided into different periods, in which the associated risk categories are indicated by color blocks. We encode the risk categories corresponding to the four moderation policies by using a color scheme extracted from ColorBrewer.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix+text", "axial_code": [], "componenet_code": ["text", "matrix"]}]}, {"author": "zsz", "index_original": 400, "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce", "pub_year": 2022, "domain": "e-commerce livestreams", "requirement": {"requirement_text": "R2 Presenting a multilevel overview of frames. Long videos containnumerous frames that can hinder the exploration of video con-tent. Hence, the system should provide a scalable and compactsummarization of frames to support an effective overview.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The short videos", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "The data-processing procedure comprises video-audio decomposition and feature extraction. Our framework first separates a video into a sequence of frames and audio clips and then employs state-of-the-art techniques [30, 48, 52, 59] to extract multimodal features; To moderate the sheer volume of videos, we adopt a binary classifier as the filter, which can discover potentially deviant videos in accordance with extracted visual and audio features. However, video moderation policies differ across platforms and applications. Obtaining sufficient data to train a classifier to classify e-commerce livestreaming videos is difficult. Following the idea of \u201clearning with reviewing,\u201d we first define a linear classifier to initiate the framework and then adopt the reviewed videos as the ground truths to improve the classifier iteratively.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "FeatureSelection"], "componenet_code": ["modeling", "feature_selection"]}, {"solution_text": "The frame comprises a multilevel overview of the video frames and a risk-aware circular glyph. To provide a concrete video summarization, we employ a well-established narrative model [40, 44] that characterizes video content at scene, shot, and frame levels. A shot refers to a set of similar consecutive frames, and a scene refers to a list of similar shots.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "image+donut", "axial_code": [], "componenet_code": ["donut", "image"]}]}, {"author": "zsz", "index_original": 401, "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce", "pub_year": 2022, "domain": "e-commerce livestreams", "requirement": {"requirement_text": "R3 Displaying a multifaceted overview of audio content. The lin-ear reading habit makes searching audio clips time consuming.Therefore, the system should provide a multifaceted overview toenable the efficient exploration of audio content.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The short videos", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "The data-processing procedure comprises video-audio decomposition and feature extraction. Our framework first separates a video into a sequence of frames and audio clips and then employs state-of-the-art techniques [30, 48, 52, 59] to extract multimodal features; To moderate the sheer volume of videos, we adopt a binary classifier as the filter, which can discover potentially deviant videos in accordance with extracted visual and audio features. However, video moderation policies differ across platforms and applications. Obtaining sufficient data to train a classifier to classify e-commerce livestreaming videos is difficult. Following the idea of \u201clearning with reviewing,\u201d we first define a linear classifier to initiate the framework and then adopt the reviewed videos as the ground truths to improve the classifier iteratively.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "FeatureSelection"], "componenet_code": ["modeling", "feature_selection"]}, {"solution_text": "The audio view provides a multifaceted overview of streamers\u2019 speech, which consists of a horizontal histogram and a storyline-based visualization. The horizontal histogram is designed to demonstrate the frequency distribution of the risk words extracted from the audio content, in which each bar refers to a word. The height of a bar represents the count of the word mentioned in the speech, and its color represents the risk category, which is the same as the circular glyph. We rank the risk words in descending order to enable moderators to review the risky content from highest to lowest. However, moderating audio content by only reviewing the risk word summaries is insufficient, because moderators cannot make a reliable decision without the temporal context. Thus, we adopt storyline-based visualization to reveal the temporal concurrence of risk word.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "bar+line+donut+text", "axial_code": [], "componenet_code": ["donut", "bar", "text", "line"]}]}, {"author": "zsz", "index_original": 402, "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce", "pub_year": 2022, "domain": "e-commerce livestreams", "requirement": {"requirement_text": "R4 Contextualizing risk information with multimodal content. Riskinformation is extracted from video or audio content. The sys-tem should combine multimodal content with associated risks toprovide concrete visual representations.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The short videos", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "The data-processing procedure comprises video-audio decomposition and feature extraction. Our framework first separates a video into a sequence of frames and audio clips and then employs state-of-the-art techniques [30, 48, 52, 59] to extract multimodal features; To moderate the sheer volume of videos, we adopt a binary classifier as the filter, which can discover potentially deviant videos in accordance with extracted visual and audio features. However, video moderation policies differ across platforms and applications. Obtaining sufficient data to train a classifier to classify e-commerce livestreaming videos is difficult. Following the idea of \u201clearning with reviewing,\u201d we first define a linear classifier to initiate the framework and then adopt the reviewed videos as the ground truths to improve the classifier iteratively.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "FeatureSelection"], "componenet_code": ["modeling", "feature_selection"]}, {"solution_text": "The frame comprises a multilevel overview of the video frames and a risk-aware circular glyph. To provide a concrete video summarization, we employ a well-established narrative model [40, 44] that characterizes video content at scene, shot, and frame levels. A shot refers to a set of similar consecutive frames, and a scene refers to a list of similar shots.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "image+donut", "axial_code": [], "componenet_code": ["donut", "image"]}]}, {"author": "zsz", "index_original": 403, "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce", "pub_year": 2022, "domain": "e-commerce livestreams", "requirement": {"requirement_text": "R4 Contextualizing risk information with multimodal content. Riskinformation is extracted from video or audio content. The sys-tem should combine multimodal content with associated risks toprovide concrete visual representations.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The short videos", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "The data-processing procedure comprises video-audio decomposition and feature extraction. Our framework first separates a video into a sequence of frames and audio clips and then employs state-of-the-art techniques [30, 48, 52, 59] to extract multimodal features; To moderate the sheer volume of videos, we adopt a binary classifier as the filter, which can discover potentially deviant videos in accordance with extracted visual and audio features. However, video moderation policies differ across platforms and applications. Obtaining sufficient data to train a classifier to classify e-commerce livestreaming videos is difficult. Following the idea of \u201clearning with reviewing,\u201d we first define a linear classifier to initiate the framework and then adopt the reviewed videos as the ground truths to improve the classifier iteratively.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "FeatureSelection"], "componenet_code": ["modeling", "feature_selection"]}, {"solution_text": "Moreover, we integrate the circular designs [19] with arepresentative frame whose risk score is the highest in the scene (R4).The radial bar chart visualizes risk tags by using bar sectors and the barheight to encode the risk score, which is easy to interpret. The donutchart encodes the risk score by using the sector angle, which is scalableto sparse risk information. The rose diagram encodes the risk scorewith the sector area, which highlights extreme risk scores effectively.Considering that the three designs demonstrate their strengths in differ-ent tasks, we provide a radio button to enable users to select differentdesigns in accordance with their requirements.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "image+donut", "axial_code": [], "componenet_code": ["donut", "image"]}]}, {"author": "zsz", "index_original": 404, "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce", "pub_year": 2022, "domain": "e-commerce livestreams", "requirement": {"requirement_text": "R5 Linking associated risk-aware visualizations in coordinated views.A gap exists in the risk information extracted from multimodalcontent. The system should visually connect risk-aware visualiza-tions in different views to facilitate quick labeling.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The short videos", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "Locating high-risk periods. Moderators can locate high-risk periods by using the segmented timeline in figure. The system will update the video content during the located period when moderators click on the blocks.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 405, "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce", "pub_year": 2022, "domain": "e-commerce livestreams", "requirement": {"requirement_text": "R5 Linking associated risk-aware visualizations in coordinated views.A gap exists in the risk information extracted from multimodalcontent. The system should visually connect risk-aware visualiza-tions in different views to facilitate quick labeling.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The short videos", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "Linking frames with risk tags. Moderators can link video frames with associated risk tags to review deviant content quickly. In figure, the system will automatically enlarge the associated frames when moderators hover over the sectors of the risk glyph. Moreover, moderators can update video content by clicking on a frame to locate deviant content promptly.", "solution_category": "interaction", "solution_axial": "OverviewandExplore,Filtering", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Filtering"], "componenet_code": ["overview_and_explore", "filtering"]}]}, {"author": "zsz", "index_original": 406, "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce", "pub_year": 2022, "domain": "e-commerce livestreams", "requirement": {"requirement_text": "R5 Linking associated risk-aware visualizations in coordinated views.A gap exists in the risk information extracted from multimodalcontent. The system should visually connect risk-aware visualiza-tions in different views to facilitate quick labeling.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The short videos", "data_code": {"sequential": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "Exploring audio context. Moderators can effectively explore audio context by clicking on a risk word or a histogram bar in figure. The system will enlarge the links to highlight the associated temporal context. Moreover, moderators can click on a word cloud to specify the period they want to review further.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 407, "paper_title": "ThreadStates: State-based Visual Analysis of Disease Progression", "pub_year": 2022, "domain": "Disease Progression", "requirement": {"requirement_text": "T1. Characterize value distribution over multiple features at eachstate. States indicate patient statuses and are represented by the valuedistributions of multiple timepoint features. For example, cancer pa-tients may exhibit different genomic mutations at the first cancer di-agnosis and a later cancer relapse. Visualizing the timepoint featuresof each state enables users to interpret the identified states, comparedifferent states, and refine states based on their analysis needs. As aresult, users are able to understand disease progression through theevolution of theses states and their timepoint feature values (G1)", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The synthetic COVID-19 dataset has relatively long sequences with dense timepoint features. For these patients, a daily measurement for a set of indicators (such as heart rate, white blood cell count, etc.) had been generated using a model-based approach based on real data to protect patient privacy; The real cytogenetically normal acute myeloid leukemia (CN-AML) dataset has relatively short se_x0002_quences with sparse timepoint features. This dataset provides the somatic genomic alterations of 50 patients at three critical timepoints of CN_x0002_AML progression (i.e., diagnosis, remission, relapse).", "data_code": {"tables": 1, "sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Algorithm Choice. In ThreadStates, we treat state identification as an unsupervised clustering task. Each input item is a high dimensional vector Oi, j = (x1,x2,...,xm). Oi, j indicates the observation of patient i at timepoint j. (x1, x2...,xm) indicate the value of the m types of observations, such as body temperature, antibody level. The task is to find clusters with similar timepoint observations. For state-based visual analysis of disease progression, previous studies have employed Hidden Markov Models (HMM) to identify states and state transitions synchronously [21]. The synchronous identification of states and transi_x0002_tions enables efficient pattern mining but provides little support for user participation and state refinement. In this study, we use a hierarchical agglomerative clustering method [32] to provide users more flexibility in the process of state identification. For missing values, we replace them with non-missing values at the nearest timepoint of the same patient, based on the assumption that clinicians are more likely to skip an observation that has no dramatic change. The clustering results are influenced by the selected timepoint fea_x0002_tures. The selection of timepoint features will influence the state iden_x0002_tification results, therefore affecting the transition summarization and the analysis conclusions. The integration of users\u2019 domain knowledge can help select appropriate features and identify clinically meaningful states. In our earlier work [15], we proposed a Feature Manager based on Lineup [9]. This Feature Manager calculates a set of variability scores to highlight features that are of potential clinical interest.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Glyph Matrix that explains the characteristics of states based on their value distribution on different features. As shown in figure, the Glyph Matrix employs a small multiples design [33]. Each small chart depicts the value distribution of one state (column) on one feature (row). This Glyph Matrix design is similar to the matrix design proposed in DPVis. However, DPVis only provides statistic summaries (e.g., mean, standard deviation) for the feature distribution. The occurrence frequency (i.e., number of records) of each state is not presented to users in DPVis.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "matrix+glyph", "axial_code": [], "componenet_code": ["glyph", "matrix"]}]}, {"author": "zsz", "index_original": 408, "paper_title": "ThreadStates: State-based Visual Analysis of Disease Progression", "pub_year": 2022, "domain": "Disease Progression", "requirement": {"requirement_text": "T2. Summarize and compare the occurrence of each state. Usersmight be interested in the occurrence patterns of a state, such as whendoes a state normally occur and how frequently does this state occurin the target cohort. Since disease progression is depicted in terms ofstate transitions, summarizing the occurrence patterns of each state canoffer users a more comprehensive understanding of states, thereforefacilitating the interpretation of disease progressions (G1)", "requirement_code": {"compare_entities": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "The synthetic COVID-19 dataset has relatively long sequences with dense timepoint features. For these patients, a daily measurement for a set of indicators (such as heart rate, white blood cell count, etc.) had been generated using a model-based approach based on real data to protect patient privacy; The real cytogenetically normal acute myeloid leukemia (CN-AML) dataset has relatively short se_x0002_quences with sparse timepoint features. This dataset provides the somatic genomic alterations of 50 patients at three critical timepoints of CN_x0002_AML progression (i.e., diagnosis, remission, relapse).", "data_code": {"tables": 1, "sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Algorithm Choice. In ThreadStates, we treat state identification as an unsupervised clustering task. Each input item is a high dimensional vector Oi, j = (x1,x2,...,xm). Oi, j indicates the observation of patient i at timepoint j. (x1, x2...,xm) indicate the value of the m types of observations, such as body temperature, antibody level. The task is to find clusters with similar timepoint observations. For state-based visual analysis of disease progression, previous studies have employed Hidden Markov Models (HMM) to identify states and state transitions synchronously [21]. The synchronous identification of states and transi_x0002_tions enables efficient pattern mining but provides little support for user participation and state refinement. In this study, we use a hierarchical agglomerative clustering method [32] to provide users more flexibility in the process of state identification. For missing values, we replace them with non-missing values at the nearest timepoint of the same patient, based on the assumption that clinicians are more likely to skip an observation that has no dramatic change. The clustering results are influenced by the selected timepoint fea_x0002_tures. The selection of timepoint features will influence the state iden_x0002_tification results, therefore affecting the transition summarization and the analysis conclusions. The integration of users\u2019 domain knowledge can help select appropriate features and identify clinically meaningful states. In our earlier work [15], we proposed a Feature Manager based on Lineup [9]. This Feature Manager calculates a set of variability scores to highlight features that are of potential clinical interest.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Glyph Matrix that explains the characteristics of states based on their value distribution on different features. As shown in figure, the Glyph Matrix employs a small multiples design [33]. Each small chart depicts the value distribution of one state (column) on one feature (row). This Glyph Matrix design is similar to the matrix design proposed in DPVis. However, DPVis only provides statistic summaries (e.g., mean, standard deviation) for the feature distribution. The occurrence frequency (i.e., number of records) of each state is not presented to users in DPVis.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "matrix+glyph", "axial_code": [], "componenet_code": ["glyph", "matrix"]}]}, {"author": "zsz", "index_original": 409, "paper_title": "ThreadStates: State-based Visual Analysis of Disease Progression", "pub_year": 2022, "domain": "Disease Progression", "requirement": {"requirement_text": "T2. Summarize and compare the occurrence of each state. Usersmight be interested in the occurrence patterns of a state, such as whendoes a state normally occur and how frequently does this state occurin the target cohort. Since disease progression is depicted in terms ofstate transitions, summarizing the occurrence patterns of each state canoffer users a more comprehensive understanding of states, thereforefacilitating the interpretation of disease progressions (G1)", "requirement_code": {"compare_entities": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "The synthetic COVID-19 dataset has relatively long sequences with dense timepoint features. For these patients, a daily measurement for a set of indicators (such as heart rate, white blood cell count, etc.) had been generated using a model-based approach based on real data to protect patient privacy; The real cytogenetically normal acute myeloid leukemia (CN-AML) dataset has relatively short se_x0002_quences with sparse timepoint features. This dataset provides the somatic genomic alterations of 50 patients at three critical timepoints of CN_x0002_AML progression (i.e., diagnosis, remission, relapse).", "data_code": {"tables": 1, "sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Algorithm Choice. In ThreadStates, we treat state identification as an unsupervised clustering task. Each input item is a high dimensional vector Oi, j = (x1,x2,...,xm). Oi, j indicates the observation of patient i at timepoint j. (x1, x2...,xm) indicate the value of the m types of observations, such as body temperature, antibody level. The task is to find clusters with similar timepoint observations. For state-based visual analysis of disease progression, previous studies have employed Hidden Markov Models (HMM) to identify states and state transitions synchronously [21]. The synchronous identification of states and transi_x0002_tions enables efficient pattern mining but provides little support for user participation and state refinement. In this study, we use a hierarchical agglomerative clustering method [32] to provide users more flexibility in the process of state identification. For missing values, we replace them with non-missing values at the nearest timepoint of the same patient, based on the assumption that clinicians are more likely to skip an observation that has no dramatic change. The clustering results are influenced by the selected timepoint fea_x0002_tures. The selection of timepoint features will influence the state iden_x0002_tification results, therefore affecting the transition summarization and the analysis conclusions. The integration of users\u2019 domain knowledge can help select appropriate features and identify clinically meaningful states. In our earlier work [15], we proposed a Feature Manager based on Lineup [9]. This Feature Manager calculates a set of variability scores to highlight features that are of potential clinical interest.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "ThreadStates also provides areas to present the temporal distribution of each state. As shown in figure, users can easily compare the temporal distributions of states to better understand their role in disease progression.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area", "axial_code": [], "componenet_code": ["area"]}]}, {"author": "zsz", "index_original": 410, "paper_title": "ThreadStates: State-based Visual Analysis of Disease Progression", "pub_year": 2022, "domain": "Disease Progression", "requirement": {"requirement_text": "T2. Summarize and compare the occurrence of each state. Usersmight be interested in the occurrence patterns of a state, such as whendoes a state normally occur and how frequently does this state occurin the target cohort. Since disease progression is depicted in terms ofstate transitions, summarizing the occurrence patterns of each state canoffer users a more comprehensive understanding of states, thereforefacilitating the interpretation of disease progressions (G1)", "requirement_code": {"compare_entities": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "The synthetic COVID-19 dataset has relatively long sequences with dense timepoint features. For these patients, a daily measurement for a set of indicators (such as heart rate, white blood cell count, etc.) had been generated using a model-based approach based on real data to protect patient privacy; The real cytogenetically normal acute myeloid leukemia (CN-AML) dataset has relatively short se_x0002_quences with sparse timepoint features. This dataset provides the somatic genomic alterations of 50 patients at three critical timepoints of CN_x0002_AML progression (i.e., diagnosis, remission, relapse).", "data_code": {"tables": 1, "sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Algorithm Choice. In ThreadStates, we treat state identification as an unsupervised clustering task. Each input item is a high dimensional vector Oi, j = (x1,x2,...,xm). Oi, j indicates the observation of patient i at timepoint j. (x1, x2...,xm) indicate the value of the m types of observations, such as body temperature, antibody level. The task is to find clusters with similar timepoint observations. For state-based visual analysis of disease progression, previous studies have employed Hidden Markov Models (HMM) to identify states and state transitions synchronously [21]. The synchronous identification of states and transi_x0002_tions enables efficient pattern mining but provides little support for user participation and state refinement. In this study, we use a hierarchical agglomerative clustering method [32] to provide users more flexibility in the process of state identification. For missing values, we replace them with non-missing values at the nearest timepoint of the same patient, based on the assumption that clinicians are more likely to skip an observation that has no dramatic change. The clustering results are influenced by the selected timepoint fea_x0002_tures. The selection of timepoint features will influence the state iden_x0002_tification results, therefore affecting the transition summarization and the analysis conclusions. The integration of users\u2019 domain knowledge can help select appropriate features and identify clinically meaningful states. In our earlier work [15], we proposed a Feature Manager based on Lineup [9]. This Feature Manager calculates a set of variability scores to highlight features that are of potential clinical interest.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "For example, as shown in (1), it is hard to examine and compare state B and state C. The histograms may also lead to the overlooking of certain value distributions (2). Meanwhile, when using histograms and bar charts, it is ineffective to compare the occurrence frequency of states, which is important for the proposed analysis. Users can only estimate the occurrence frequency of each state by summing up the height of all rectangles. To address this issue, we proposed a novel glyph for the Glyph Matrix (c). This glyph reflected a design trade-off between the ease of reading certain important information and the familiarity of the visualization. We further refined the design by removing glyph color to facilitate comparison of glyphs in the same row, i.e., comparing states across one feature.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "matrix+glyph", "axial_code": [], "componenet_code": ["glyph", "matrix"]}]}, {"author": "zsz", "index_original": 411, "paper_title": "ThreadStates: State-based Visual Analysis of Disease Progression", "pub_year": 2022, "domain": "Disease Progression", "requirement": {"requirement_text": "T3. Search, rank, and filter state transition patterns. Capturingimportant state transition patterns is crucial for the analysis of diseaseprogression. It enables users to comprehend the evolution of diseases(G1) and find distinctive groups of patients with similar interestedtransition patterns (G2). Sequential pattern mining algorithms cangenerate long lists of patterns, many of which are redundant and do notcontribute to the understanding of disease progression. Therefore, thetool should provide flexible support for users to quickly search, rank,and locate interested patterns", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The synthetic COVID-19 dataset has relatively long sequences with dense timepoint features. For these patients, a daily measurement for a set of indicators (such as heart rate, white blood cell count, etc.) had been generated using a model-based approach based on real data to protect patient privacy; The real cytogenetically normal acute myeloid leukemia (CN-AML) dataset has relatively short se_x0002_quences with sparse timepoint features. This dataset provides the somatic genomic alterations of 50 patients at three critical timepoints of CN_x0002_AML progression (i.e., diagnosis, remission, relapse).", "data_code": {"tables": 1, "sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "In phase 2, we aim to identify 1) frequent state transition patterns and 2) patient groups with similar transition patterns. Note that the current version of ThreadStates fo_x0002_cuses on the analysis of sequences and does not consider the interval between timepoints. Event features are considered based on their rela_x0002_tive positions to these timepoints rather than their actual timestamps. Algorithm Choice. Previous studies have proposed a wide range of novel algorithms to tackle various challenges in the visual analytics of sequence data. Instead of proposing additional algorithms, we focus on experimenting and selecting a suitable algorithm from existing ones based on the analysis needs in disease progression.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Apart from the Sankey-based visualization, ThreadStates provides two tables to assist the analysis of different patient groups. The first table enables users to search, sort, filter the frequent patterns of each patient group to better understand the transition patterns.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table", "axial_code": [], "componenet_code": ["table"]}, {"solution_text": "The first table enables users to search, sort, filter the frequent patterns of each patient group to better understand the transition patterns.", "solution_category": "interaction", "solution_axial": "Extractionoffeatures,Reconfigure,Filtering", "solution_compoent": "", "axial_code": ["Reconfigure", "Extractionoffeatures", "Filtering"], "componenet_code": ["reconfigure", "extraction_of_features", "filtering"]}]}, {"author": "zsz", "index_original": 412, "paper_title": "ThreadStates: State-based Visual Analysis of Disease Progression", "pub_year": 2022, "domain": "Disease Progression", "requirement": {"requirement_text": "T4. Group state transition patterns. Progression patterns of thesame disease vary across patients and across treatments. Discoveringand disentangling patterns from a target cohort allow researchers to ob-tain a comprehensive understanding of the disease. Grouping differenttransition patterns also provides an effective means to group patientsand find patient subgroups with similar transition patterns (G2).", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The synthetic COVID-19 dataset has relatively long sequences with dense timepoint features. For these patients, a daily measurement for a set of indicators (such as heart rate, white blood cell count, etc.) had been generated using a model-based approach based on real data to protect patient privacy; The real cytogenetically normal acute myeloid leukemia (CN-AML) dataset has relatively short se_x0002_quences with sparse timepoint features. This dataset provides the somatic genomic alterations of 50 patients at three critical timepoints of CN_x0002_AML progression (i.e., diagnosis, remission, relapse).", "data_code": {"tables": 1, "sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "N-gram based methods have exhibited great performance in previous event sequence analysis studies [2, 36]. Based on those previous studies, we implemented the algorithm in ThreadStates as below. Each patient Pi can be represented by a set of N-grams, i.e., N consecutive elements in the state sequence (s1,s2,...,sk). We denote the Ngram set of patient Pi as TN(Pi), which can be represented as TN(Pi) = {(sjsj+1...sj+N\u22121)| j \u2208 [1, k +1\u2212N]}, where k is the number of timepoints, N is the number of consecutive elements. The distance between patient i and patient j is calculated as the Jaccard distance between the two N-gram sets: dist(Pi,Pj) = TN(Pi)\u2229TN(Pj)/TN(Pi)\u222aTN(Pj). Patients are then grouped through hierarchical agglomerative clustering [32] based on their pairwise distances. In other words, patients with similar state transition patterns will be grouped together.", "solution_category": "data_manipulation", "solution_axial": "Wrangling,SimilarityCalculation,Clustering&Grouping", "solution_compoent": "", "axial_code": ["SimilarityCalculation", "Clustering&Grouping", "Wrangling"], "componenet_code": ["similarity_calculation", "clustering_and_grouping", "wrangling"]}]}, {"author": "zsz", "index_original": 413, "paper_title": "ThreadStates: State-based Visual Analysis of Disease Progression", "pub_year": 2022, "domain": "Disease Progression", "requirement": {"requirement_text": "T5. Reveal the association between patient groups and other fea-tures. Usually, users not only want to summarize disease progressionpatterns but are also interested in revealing the association betweenprogression patterns and other features (G3). These associations cangenerate valuable insights for a number of clinical tasks, such as earlydisease prediction and treatment selection. Since we group patientsbased on progression patterns, the association between progressions andother features can be revealed by analyzing the associations betweenpatient groups and these features", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The synthetic COVID-19 dataset has relatively long sequences with dense timepoint features. For these patients, a daily measurement for a set of indicators (such as heart rate, white blood cell count, etc.) had been generated using a model-based approach based on real data to protect patient privacy; The real cytogenetically normal acute myeloid leukemia (CN-AML) dataset has relatively short se_x0002_quences with sparse timepoint features. This dataset provides the somatic genomic alterations of 50 patients at three critical timepoints of CN_x0002_AML progression (i.e., diagnosis, remission, relapse).", "data_code": {"tables": 1, "sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "In phase 2, we aim to identify 1) frequent state transition patterns and 2) patient groups with similar transition patterns. Note that the current version of ThreadStates fo_x0002_cuses on the analysis of sequences and does not consider the interval between timepoints. Event features are considered based on their rela_x0002_tive positions to these timepoints rather than their actual timestamps. Algorithm Choice. Previous studies have proposed a wide range of novel algorithms to tackle various challenges in the visual analytics of sequence data. Instead of proposing additional algorithms, we focus on experimenting and selecting a suitable algorithm from existing ones based on the analysis needs in disease progression.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Apart from the Sankey-based visualization, ThreadStates provides two tables to assist the analysis of different patient groups. The second table summarizes the patient-features of each group to reveal the correlation between state transition patterns and patient features. In each table cell, the patient feature visualization is represented using the same glyph as the one used in the Glyph Matrix to reduce the learning burden.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+glyph", "axial_code": [], "componenet_code": ["glyph", "table"]}]}, {"author": "zsz", "index_original": 414, "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports", "pub_year": 2022, "domain": "sports", "requirement": {"requirement_text": "T1 Identify the tactics used by a player. Given that each player haspersonal tactics, domain experts usually specify a player of inter-est and analyze his/her personal tactics. Moreover, domain ex-perts usually select many similar opponents (e.g., all left-handedopponents) to know the player\u2019s tactics for competing againstsuch opponents comprehensively. Our system should mine andvisualize the tactics of a certain player.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.", "data_code": {"tables": 1, "ordinal": 1}}, "solution": [{"solution_text": "The Control Bar allows users to select a dataset from one of several racket sports, one player of interest (IP), and multiple opponents (OPs) in order to analyze the tactics of the IP against these OPs. The system filters the rallies and displays the number of filtered rallies on the right. Throughout the user interface, we distinguish the IP from the OPs by hue because it is the most effective way to differentiate among a small number of categories [43]. We chose orange for the IP because it is an energetic and attention-getting color that domain experts preferred. We chose blue to stand for the OPs because it contrasts with orange.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 415, "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports", "pub_year": 2022, "domain": "sports", "requirement": {"requirement_text": "T1 Identify the tactics used by a player. Given that each player haspersonal tactics, domain experts usually specify a player of inter-est and analyze his/her personal tactics. Moreover, domain ex-perts usually select many similar opponents (e.g., all left-handedopponents) to know the player\u2019s tactics for competing againstsuch opponents comprehensively. Our system should mine andvisualize the tactics of a certain player.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.", "data_code": {"tables": 1, "ordinal": 1}}, "solution": [{"solution_text": "Our algorithm is highly correlated with our visualizations. Considering that there exist two core analysis targets: the tactics and the tactic progression, we implement our algorithm in two steps: identifying the tactics and discovering the tactic progression. Detailed implementa_x0002_tion of these two steps is as follows. In this step, we introduce a multivariate pattern mining algorithm to summarize a small set of frequent tactics from hundreds of original sequences. For example, in Fig. 4, we mine three tactics (t1 to t3) from four sequences (s1 to s4). Inspired by Ditto [2], we propose a generic multivariate pattern mining algorithm based on MDL. The core idea is similar to the Generative Adversarial Network (GAN), where the algorithm consists of two parts: a pattern generator and a pattern discriminator. The generator continuously generates new candidate patterns based on the current pattern set. The discriminator provides a measure based on MDL to judge whether a pattern is beneficial to sum_x0002_marizing the dataset. The algorithm iteratively adds the good patterns to the pattern set and removes the bad ones until the pattern set no longer changes. We adopt this idea due to two reasons: 1) The generator can selectively generate tactics instead of searching the ample space of versatile tactics, reducing the running time. 2) The discriminator can define a measure flexibly based on domain requirements to filter the tactics, which are meaningful to domain analysis. This step is designed for highly abstracting the complex original se_x0002_quences into a clear Sankey diagram that reveals the tactic progression, based on the mined tactics (Fig. 4(A and B \u2192 D)). Given a pattern set P and original sequences S, the algorithm generates a directed acyclic graph G (i.e., the Sankey diagram).", "solution_category": "data_manipulation", "solution_axial": "Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "The Flow View uses a Sankey diagram to show a ever-changing tactic progression. The diagram is in chronological order from left to right. Each node indicates a tactic along with certain contexts (i.e., specific pre-tactics). Multiple nodes may share the same tactic but have different contexts.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "sankey+glyph", "axial_code": [], "componenet_code": ["sankey", "glyph"]}]}, {"author": "zsz", "index_original": 416, "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports", "pub_year": 2022, "domain": "sports", "requirement": {"requirement_text": "T1 Identify the tactics used by a player. Given that each player haspersonal tactics, domain experts usually specify a player of inter-est and analyze his/her personal tactics. Moreover, domain ex-perts usually select many similar opponents (e.g., all left-handedopponents) to know the player\u2019s tactics for competing againstsuch opponents comprehensively. Our system should mine andvisualize the tactics of a certain player.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.", "data_code": {"tables": 1, "ordinal": 1}}, "solution": [{"solution_text": "In this step, we introduce a multivariate pattern mining algorithm to summarize a small set of frequent tactics from hundreds of original sequences. For example, in Fig. 4, we mine three tactics (t1 to t3) from four sequences (s1 to s4). Inspired by Ditto [2], we propose a generic multivariate pattern mining algorithm based on MDL. The core idea is similar to the Generative Adversarial Network (GAN), where the algorithm consists of two parts: a pattern generator and a pattern discriminator. The generator continuously generates new candidate patterns based on the current pattern set. The discriminator provides a measure based on MDL to judge whether a pattern is beneficial to sum_x0002_marizing the dataset. The algorithm iteratively adds the good patterns to the pattern set and removes the bad ones until the pattern set no longer changes. We adopt this idea due to two reasons: 1) The generator can selectively generate tactics instead of searching the ample space of versatile tactics, reducing the running time. 2) The discriminator can define a measure flexibly based on domain requirements to filter the tactics, which are meaningful to domain analysis; The Projection View reveals similarities among tactics. Each tactic is represented by a point, and the distance between two points encodes the similarity between the two tactics. For each point, the area encodes the frequency of the tactic. The hue and the saturation encode the winning rate after using the tactic, consistent with the tail of our glyph design in the Flow View.", "solution_category": "data_manipulation", "solution_axial": "Modeling,SimilarityCalculation,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Modeling", "DimensionalityReduction", "SimilarityCalculation"], "componenet_code": ["modeling", "dimensionality_reduction", "similarity_calculation"]}, {"solution_text": "The Projection View reveals similarities among tactics. Each tactic is represented by a point, and the distance between two points encodes the similarity between the two tactics. For each point, the area encodes the frequency of the tactic. The hue and the saturation encode the winning rate after using the tactic, consistent with the tail of our glyph design in the Flow View.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}]}, {"author": "zsz", "index_original": 417, "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports", "pub_year": 2022, "domain": "sports", "requirement": {"requirement_text": "T1 Identify the tactics used by a player. Given that each player haspersonal tactics, domain experts usually specify a player of inter-est and analyze his/her personal tactics. Moreover, domain ex-perts usually select many similar opponents (e.g., all left-handedopponents) to know the player\u2019s tactics for competing againstsuch opponents comprehensively. Our system should mine andvisualize the tactics of a certain player.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.", "data_code": {"tables": 1, "ordinal": 1}}, "solution": [{"solution_text": "In this step, we introduce a multivariate pattern mining algorithm to summarize a small set of frequent tactics from hundreds of original sequences. For example, in Fig. 4, we mine three tactics (t1 to t3) from four sequences (s1 to s4). Inspired by Ditto [2], we propose a generic multivariate pattern mining algorithm based on MDL. The core idea is similar to the Generative Adversarial Network (GAN), where the algorithm consists of two parts: a pattern generator and a pattern discriminator. The generator continuously generates new candidate patterns based on the current pattern set. The discriminator provides a measure based on MDL to judge whether a pattern is beneficial to sum_x0002_marizing the dataset. The algorithm iteratively adds the good patterns to the pattern set and removes the bad ones until the pattern set no longer changes. We adopt this idea due to two reasons: 1) The generator can selectively generate tactics instead of searching the ample space of versatile tactics, reducing the running time. 2) The discriminator can define a measure flexibly based on domain requirements to filter the tactics, which are meaningful to domain analysis.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "When users click on the glyph or point of a tactic, the Tactic View can visualize the multivariate tactic in tabular form (T1), where each row represents one attribute, and each column represents one event. Each obround indicates the value of an attribute within an event, while the hue of the background encodes who hits the ball. For each value within the tactic, we directly display this value as text because there are so many possibilities that it does not make sense to design different encodings or to expect users to remember them. For values where the player has multiple choices for applying the tactic, we visualize the frequency of each choice through a bar chart, one bar for one choice. The bar chart helps users understand how the tactic is used in detail by including values ignored by the pattern mining algorithm.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "table+text+bar", "axial_code": [], "componenet_code": ["text", "bar", "table"]}, {"solution_text": "When users click on the glyph or point of a tactic, the Tactic View can visualize the multivariate tactic in tabular form (T1), where each row represents one attribute, and each column represents one event. Each obround indicates the value of an attribute within an event, while the hue of the background encodes who hits the ball. For each value within the tactic, we directly display this value as text because there are so many possibilities that it does not make sense to design different encodings or to expect users to remember them. For values where the player has multiple choices for applying the tactic, we visualize the frequency of each choice through a bar chart, one bar for one choice. The bar chart helps users understand how the tactic is used in detail by including values ignored by the pattern mining algorithm.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 418, "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports", "pub_year": 2022, "domain": "sports", "requirement": {"requirement_text": "T2 Reveal the tactic progression. When finding a tactic of interest,domain experts need to know what tactics it comes from and what tactics it may progress into. By understanding the various changes,experts can know when a tactic is used and the results that thetactic can lead to. Our system should reveal the progressiverelationships among versatile tactics.", "requirement_code": {"identify_main_cause_aggregate": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.", "data_code": {"tables": 1, "ordinal": 1}}, "solution": [{"solution_text": "Our algorithm is highly correlated with our visualizations. Considering that there exist two core analysis targets: the tactics and the tactic progression, we implement our algorithm in two steps: identifying the tactics and discovering the tactic progression. Detailed implementa_x0002_tion of these two steps is as follows. In this step, we introduce a multivariate pattern mining algorithm to summarize a small set of frequent tactics from hundreds of original sequences. For example, in Fig. 4, we mine three tactics (t1 to t3) from four sequences (s1 to s4). Inspired by Ditto [2], we propose a generic multivariate pattern mining algorithm based on MDL. The core idea is similar to the Generative Adversarial Network (GAN), where the algorithm consists of two parts: a pattern generator and a pattern discriminator. The generator continuously generates new candidate patterns based on the current pattern set. The discriminator provides a measure based on MDL to judge whether a pattern is beneficial to sum_x0002_marizing the dataset. The algorithm iteratively adds the good patterns to the pattern set and removes the bad ones until the pattern set no longer changes. We adopt this idea due to two reasons: 1) The generator can selectively generate tactics instead of searching the ample space of versatile tactics, reducing the running time. 2) The discriminator can define a measure flexibly based on domain requirements to filter the tactics, which are meaningful to domain analysis. This step is designed for highly abstracting the complex original se_x0002_quences into a clear Sankey diagram that reveals the tactic progression, based on the mined tactics (Fig. 4(A and B \u2192 D)). Given a pattern set P and original sequences S, the algorithm generates a directed acyclic graph G (i.e., the Sankey diagram).", "solution_category": "data_manipulation", "solution_axial": "Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation"], "componenet_code": ["modeling", "algorithmic_calculation"]}, {"solution_text": "The Flow View uses a Sankey diagram to show a ever-changing tactic progression. The diagram is in chronological order from left to right. Each node indicates a tactic along with certain contexts (i.e., specific pre-tactics). Multiple nodes may share the same tactic but have different contexts.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "sankey+glyph", "axial_code": [], "componenet_code": ["sankey", "glyph"]}]}, {"author": "zsz", "index_original": 419, "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports", "pub_year": 2022, "domain": "sports", "requirement": {"requirement_text": "T3 Point out the tactics worth analyzing across a full progression.A player may adopt versatile tactics to compete against opponents.Instead of analyzing each tactic and exploring what comes beforeand after, domain experts prefer to analyze tactics that can affectthe overall progression signi\ufb01cantly (e.g., a tactic that can directlyscore a point without tactics coming after it). Our system shouldvisualize the tactics with multiple levels of detail, where thevisualizations at the highest level should help users quickly \ufb01ndthe tactics worth analyzing.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.", "data_code": {"tables": 1, "ordinal": 1}}, "solution": [{"solution_text": "In this step, we introduce a multivariate pattern mining algorithm to summarize a small set of frequent tactics from hundreds of original sequences. For example, in Fig. 4, we mine three tactics (t1 to t3) from four sequences (s1 to s4). Inspired by Ditto [2], we propose a generic multivariate pattern mining algorithm based on MDL. The core idea is similar to the Generative Adversarial Network (GAN), where the algorithm consists of two parts: a pattern generator and a pattern discriminator. The generator continuously generates new candidate patterns based on the current pattern set. The discriminator provides a measure based on MDL to judge whether a pattern is beneficial to sum_x0002_marizing the dataset. The algorithm iteratively adds the good patterns to the pattern set and removes the bad ones until the pattern set no longer changes. We adopt this idea due to two reasons: 1) The generator can selectively generate tactics instead of searching the ample space of versatile tactics, reducing the running time. 2) The discriminator can define a measure flexibly based on domain requirements to filter the tactics, which are meaningful to domain analysis.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Multiple nodes may share the same tactic but have different contexts. We introduce a glyph-based design to encode the tactic and its context because glyphs can visualize multidimensional data with intuitive visual metaphors [3], preferred by domain experts. Each flow from one node to another indicates that IP first uses a tactic and then changes into another tactic without other tactics in between, where the width encodes the number of sequences.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}]}, {"author": "zsz", "index_original": 420, "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports", "pub_year": 2022, "domain": "sports", "requirement": {"requirement_text": "T3 Point out the tactics worth analyzing across a full progression.A player may adopt versatile tactics to compete against opponents.Instead of analyzing each tactic and exploring what comes beforeand after, domain experts prefer to analyze tactics that can affectthe overall progression signi\ufb01cantly (e.g., a tactic that can directlyscore a point without tactics coming after it). Our system shouldvisualize the tactics with multiple levels of detail, where thevisualizations at the highest level should help users quickly \ufb01ndthe tactics worth analyzing.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.", "data_code": {"tables": 1, "ordinal": 1}}, "solution": [{"solution_text": "In this step, we introduce a multivariate pattern mining algorithm to summarize a small set of frequent tactics from hundreds of original sequences. For example, in Fig. 4, we mine three tactics (t1 to t3) from four sequences (s1 to s4). Inspired by Ditto [2], we propose a generic multivariate pattern mining algorithm based on MDL. The core idea is similar to the Generative Adversarial Network (GAN), where the algorithm consists of two parts: a pattern generator and a pattern discriminator. The generator continuously generates new candidate patterns based on the current pattern set. The discriminator provides a measure based on MDL to judge whether a pattern is beneficial to sum_x0002_marizing the dataset. The algorithm iteratively adds the good patterns to the pattern set and removes the bad ones until the pattern set no longer changes. We adopt this idea due to two reasons: 1) The generator can selectively generate tactics instead of searching the ample space of versatile tactics, reducing the running time. 2) The discriminator can define a measure flexibly based on domain requirements to filter the tactics, which are meaningful to domain analysis; The Projection View reveals similarities among tactics. Each tactic is represented by a point, and the distance between two points encodes the similarity between the two tactics. For each point, the area encodes the frequency of the tactic. The hue and the saturation encode the winning rate after using the tactic, consistent with the tail of our glyph design in the Flow View.", "solution_category": "data_manipulation", "solution_axial": "Modeling,SimilarityCalculation,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Modeling", "DimensionalityReduction", "SimilarityCalculation"], "componenet_code": ["modeling", "dimensionality_reduction", "similarity_calculation"]}, {"solution_text": "The Projection View reveals similarities among tactics. Each tactic is represented by a point, and the distance between two points encodes the similarity between the two tactics. For each point, the area encodes the frequency of the tactic. The hue and the saturation encode the winning rate after using the tactic, consistent with the tail of our glyph design in the Flow View.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}]}, {"author": "zsz", "index_original": 421, "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports", "pub_year": 2022, "domain": "sports", "requirement": {"requirement_text": "T4 Compare two multivariate tactics. Comparison is essential fordata analysis of sequential data [22, 26, 38]. For two tactics,domain experts usually want to know which is better and why it isbetter than the other. Especially for two similar ones, experts needto know which characteristics make them perform differently. Oursystem should support comparison between two tactics.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.", "data_code": {"tables": 1, "ordinal": 1}}, "solution": [{"solution_text": "In this step, we introduce a multivariate pattern mining algorithm to summarize a small set of frequent tactics from hundreds of original sequences. For example, in Fig. 4, we mine three tactics (t1 to t3) from four sequences (s1 to s4). Inspired by Ditto [2], we propose a generic multivariate pattern mining algorithm based on MDL. The core idea is similar to the Generative Adversarial Network (GAN), where the algorithm consists of two parts: a pattern generator and a pattern discriminator. The generator continuously generates new candidate patterns based on the current pattern set. The discriminator provides a measure based on MDL to judge whether a pattern is beneficial to sum_x0002_marizing the dataset. The algorithm iteratively adds the good patterns to the pattern set and removes the bad ones until the pattern set no longer changes. We adopt this idea due to two reasons: 1) The generator can selectively generate tactics instead of searching the ample space of versatile tactics, reducing the running time. 2) The discriminator can define a measure flexibly based on domain requirements to filter the tactics, which are meaningful to domain analysis.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "When users click on the glyph or point of a tactic, the Tactic View can visualize the multivariate tactic in tabular form (T1), where each row represents one attribute, and each column represents one event. Each obround indicates the value of an attribute within an event, while the hue of the background encodes who hits the ball. For each value within the tactic, we directly display this value as text because there are so many possibilities that it does not make sense to design different encodings or to expect users to remember them. For values where the player has multiple choices for applying the tactic, we visualize the frequency of each choice through a bar chart, one bar for one choice. The bar chart helps users understand how the tactic is used in detail by including values ignored by the pattern mining algorithm.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "table+text+bar", "axial_code": [], "componenet_code": ["text", "bar", "table"]}, {"solution_text": "When users click on the glyph or point of a tactic, the Tactic View can visualize the multivariate tactic in tabular form (T1), where each row represents one attribute, and each column represents one event. Each obround indicates the value of an attribute within an event, while the hue of the background encodes who hits the ball. For each value within the tactic, we directly display this value as text because there are so many possibilities that it does not make sense to design different encodings or to expect users to remember them. For values where the player has multiple choices for applying the tactic, we visualize the frequency of each choice through a bar chart, one bar for one choice. The bar chart helps users understand how the tactic is used in detail by including values ignored by the pattern mining algorithm.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}, {"solution_text": "When users then right click on another tactic, this view can enable a comparison mode to compare the two tactics. To reduce users\u2019 learning costs, we simply divide each obround into two parts and juxtapose the corresponding values of the two tactics for a one-to-one comparison [23]. To ensure that similar hits are aligned, we also propose a greedy alignment strategy based on the relative indices and which player made each hit.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "text+bar", "axial_code": [], "componenet_code": ["text", "bar"]}]}, {"author": "zsz", "index_original": 422, "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports", "pub_year": 2022, "domain": "sports", "requirement": {"requirement_text": "T5 Display the raw sequences. Experts need to examine a tacticin the context of a rally in order to know what happened in realgames so that they can communicate each rally to the player. Oursystem should display the raw data for data exploration in detail.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.", "data_code": {"tables": 1, "ordinal": 1, "media": 1}}, "solution": [{"solution_text": "In this step, we introduce a multivariate pattern mining algorithm to summarize a small set of frequent tactics from hundreds of original sequences. For example, in Fig. 4, we mine three tactics (t1 to t3) from four sequences (s1 to s4). Inspired by Ditto [2], we propose a generic multivariate pattern mining algorithm based on MDL. The core idea is similar to the Generative Adversarial Network (GAN), where the algorithm consists of two parts: a pattern generator and a pattern discriminator. The generator continuously generates new candidate patterns based on the current pattern set. The discriminator provides a measure based on MDL to judge whether a pattern is beneficial to sum_x0002_marizing the dataset. The algorithm iteratively adds the good patterns to the pattern set and removes the bad ones until the pattern set no longer changes. We adopt this idea due to two reasons: 1) The generator can selectively generate tactics instead of searching the ample space of versatile tactics, reducing the running time. 2) The discriminator can define a measure flexibly based on domain requirements to filter the tactics, which are meaningful to domain analysis; The Projection View reveals similarities among tactics. Each tactic is represented by a point, and the distance between two points encodes the similarity between the two tactics. For each point, the area encodes the frequency of the tactic. The hue and the saturation encode the winning rate after using the tactic, consistent with the tail of our glyph design in the Flow View.", "solution_category": "data_manipulation", "solution_axial": "Modeling,SimilarityCalculation,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Modeling", "DimensionalityReduction", "SimilarityCalculation"], "componenet_code": ["modeling", "dimensionality_reduction", "similarity_calculation"]}, {"solution_text": "The Rally View lists all the rallies that contain a tactic selected by users, one per row. The view is divided into two parts according to the outcome of each rally so that users can compare rallies that IP won (the upper parts, indicated by a W on the left) and that he/she lost (the lower part, indicated by an L on the left). The top-right corner of each part displays the total number of wins and losses. For each row, circles on the right represent the hits in the rally, where the hue encodes whether IP or OPs hit the ball, and the number is the index of the hit. We highlight hits within the tactic with a solid circle so that users can know where the tactic is applied. When the user clicks on a rally, the exact values are expanded into a table. Users can also hover over a row to find a video button on the right, which triggers a video of the appropriate segment of the rally.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+circle", "axial_code": [], "componenet_code": ["circle", "table"]}, {"solution_text": "When the user clicks on a rally, the exact values are expanded into a table. Users can also hover over a row to find a video button on the right, which triggers a video of the appropriate segment of the rally.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 423, "paper_title": "VITALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers & Visual Analytics", "pub_year": 2022, "domain": "academic literature", "requirement": {"requirement_text": "Serendipity: Enable serendipitous identification of semantically related articles that do not necessarily have shared keywords through visual exploration.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The dataset has 8 attributes (columns) and 59,232 papers (rows). ", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "Embed: We next curated a dataframe of Title, Abstract, Au_x0002_thors, Source, Year, and Keywords and created the GloVe [50] and Specter [14] document embeddings. To create the document embed_x0002_dings for GloVe, we used TF-IDF weightings (instead of mean vectors) and SIF weightings that have been shown to remove noise through PCA reduction [3]. We used the public API to create the Specter em_x0002_beddings [14]. With these document embeddings, we used UMAP to construct 2-D document representations used in the Visualization Canvas (see Figure 4).", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,DimensionalityReduction", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "DimensionalityReduction"], "componenet_code": ["algorithmic_calculation", "dimensionality_reduction"]}, {"solution_text": "Similarity Search View shows options to find papers similar to one or more input papers. VITALITY supports setting the dimensions (2-Dimensional, n-Dimensional), number of similar papers to return, and the word embedding approach (e.g., Specter) to compute similarity.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+text", "axial_code": [], "componenet_code": ["text", "table"]}, {"solution_text": "Similarity Search View shows options to find papers similar to one or more input papers. VITALITY supports setting the dimensions (2-Dimensional, n-Dimensional), number of similar papers to return, and the word embedding approach (e.g., Specter) to compute similarity.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 424, "paper_title": "VITALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers & Visual Analytics", "pub_year": 2022, "domain": "academic literature", "requirement": {"requirement_text": "Familiarity: Facilitate a familiar search functionality to what users are currently accustomed to, such as keyword and author search.", "requirement_code": {"data_filtering": 1}}, "data": {"data_text": "The dataset has 8 attributes (columns) and 59,232 papers (rows). ", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "Embed: We next curated a dataframe of Title, Abstract, Au_x0002_thors, Source, Year, and Keywords and created the GloVe [50] and Specter [14] document embeddings. To create the document embed_x0002_dings for GloVe, we used TF-IDF weightings (instead of mean vectors) and SIF weightings that have been shown to remove noise through PCA reduction [3]. We used the public API to create the Specter em_x0002_beddings [14].", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Paper Collection View shows the entire corpus of papers in an interactive tabular layout. (1) shows an overview (number of visible papers) and UI controls to perform a global search, show hidden columns, add all papers to the input list of papers in the Similarity Search table, and save all papers to the \u201ccart\u201d in the Saved Papers View. (2) shows the attributes along with UI controls to filter (range sliders for Quantitative attributes, multiselect dropdowns for Nominal attributes), hide a column, and define a column on hover. (3) shows an interactive table of all papers with options to see detail, locate in the UMAP, add to the input list of papers for similarity search, and save to the \u201ccart\u201d. Search and filter capabilities are designed to be an intuitive entry-point into the dataset of academic articles.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+text", "axial_code": [], "componenet_code": ["text", "table"]}, {"solution_text": "Paper Collection View shows the entire corpus of papers in an interactive tabular layout. (1) shows an overview (number of visible papers) and UI controls to perform a global search, show hidden columns, add all papers to the input list of papers in the Similarity Search table, and save all papers to the \u201ccart\u201d in the Saved Papers View. (2) shows the attributes along with UI controls to filter (range sliders for Quantitative attributes, multiselect dropdowns for Nominal attributes), hide a column, and define a column on hover. (3) shows an interactive table of all papers with options to see detail, locate in the UMAP, add to the input list of papers for similarity search, and save to the \u201ccart\u201d. Search and filter capabilities are designed to be an intuitive entry-point into the dataset of academic articles.", "solution_category": "interaction", "solution_axial": "Selecting,Filtering", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "zsz", "index_original": 425, "paper_title": "VITALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers & Visual Analytics", "pub_year": 2022, "domain": "academic literature", "requirement": {"requirement_text": "Novelty: Afford users to find semantically related articles by searching based on the author\u2019s own ideas in the form of unpublished sentences / abstract.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The dataset has 8 attributes (columns) and 59,232 papers (rows). ", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "Embed: We next curated a dataframe of Title, Abstract, Au_x0002_thors, Source, Year, and Keywords and created the GloVe [50] and Specter [14] document embeddings. To create the document embed_x0002_dings for GloVe, we used TF-IDF weightings (instead of mean vectors) and SIF weightings that have been shown to remove noise through PCA reduction [3]. We used the public API to create the Specter em_x0002_beddings [14]. With these document embeddings, we used UMAP to construct 2-D document representations used in the Visualization Canvas (see Figure 4).", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,DimensionalityReduction", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "DimensionalityReduction"], "componenet_code": ["algorithmic_calculation", "dimensionality_reduction"]}, {"solution_text": "Similarity Search View shows options to find papers similar to one or more input papers. VITALITY supports setting the dimensions (2-Dimensional, n-Dimensional), number of similar papers to return, and the word embedding approach (e.g., Specter) to compute similarity.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+text", "axial_code": [], "componenet_code": ["text", "table"]}, {"solution_text": "Similarity Search View shows options to find papers similar to one or more input papers. VITALITY supports setting the dimensions (2-Dimensional, n-Dimensional), number of similar papers to return, and the word embedding approach (e.g., Specter) to compute similarity.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 426, "paper_title": "VITALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers & Visual Analytics", "pub_year": 2022, "domain": "academic literature", "requirement": {"requirement_text": "Overview: Enable users to interact with a visual overview of a group of papers.", "requirement_code": {"discover_observation": 1, "interactivity": 1}}, "data": {"data_text": "The dataset has 8 attributes (columns) and 59,232 papers (rows). ", "data_code": {"tables": 1, "categorical": 1, "textual": 1}}, "solution": [{"solution_text": "Embed: We next curated a dataframe of Title, Abstract, Au_x0002_thors, Source, Year, and Keywords and created the GloVe [50] and Specter [14] document embeddings. To create the document embed_x0002_dings for GloVe, we used TF-IDF weightings (instead of mean vectors) and SIF weightings that have been shown to remove noise through PCA reduction [3]. We used the public API to create the Specter em_x0002_beddings [14]. With these document embeddings, we used UMAP to construct 2-D document representations used in the Visualization Canvas (see Figure 4).", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,DimensionalityReduction", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "DimensionalityReduction"], "componenet_code": ["algorithmic_calculation", "dimensionality_reduction"]}, {"solution_text": "Visualization Canvas shows a 2-D UMAP projection of the embedding space of the entire paper collection: hovering on a point highlights it, shows the corresponding title in a fixed tooltip below, and automatically scrolls the collection (table) to bring the corresponding paper (row) into the viewport; pressing Shift enables lasso-mode to select multiple points using a free-form lasso operation; zooming and panning support helps navigate the UMAP to specific regions; By default, each point in the UMAP is colored based on the state of the corresponding paper (\u201cDefault\u201d): Unfiltered (unfiltered and visible in the main paper collection table; dark-grey), Filtered (filtered out and not visible in the paper collection table; light-grey), Similarity Input (added to the By Papers section in the Similarity Search View; pink), Similarity Output (in the Output Similar table; orange), and Saved (added to the Saved Papers table; red). Other options to color include Source, Year, CitationCounts, and Similarity Score.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "Visualization Canvas shows a 2-D UMAP projection of the embedding space of the entire paper collection: hovering on a point highlights it, shows the corresponding title in a fixed tooltip below, and automatically scrolls the collection (table) to bring the corresponding paper (row) into the viewport; pressing Shift enables lasso-mode to select multiple points using a free-form lasso operation; zooming and panning support helps navigate the UMAP to specific regions; By default, each point in the UMAP is colored based on the state of the corresponding paper (\u201cDefault\u201d): Unfiltered (unfiltered and visible in the main paper collection table; dark-grey), Filtered (filtered out and not visible in the paper collection table; light-grey), Similarity Input (added to the By Papers section in the Similarity Search View; pink), Similarity Output (in the Output Similar table; orange), and Saved (added to the Saved Papers table; red). Other options to color include Source, Year, CitationCounts, and Similarity Score.", "solution_category": "interaction", "solution_axial": "Selecting,Filtering", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "zsz", "index_original": 427, "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers", "pub_year": 2022, "domain": "academic career", "requirement": {"requirement_text": "T1 How does a factor influence career success over time? For lon-gitudinal comparison, the expert wishes to know how the impact ofa specific factor on career success develops over time. This couldbe related to the development of specific academic fields.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].", "data_code": {"tables": 1, "ordinal": 1, "textual": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).", "solution_category": "data_manipulation", "solution_axial": "UserInput,Clustering&Grouping", "solution_compoent": "", "axial_code": ["UserInput", "Clustering&Grouping"], "componenet_code": ["user_input", "clustering_and_grouping"]}, {"solution_text": "The Horizon Chart Group summarizes the trends of factor impacts and supports the comparison among factors. Description: Each horizon chart represents a factor which is extended from a line chart. The x-axis encodes the time and the y-axis represents the explanatory power of a factor. The line chart is divided into layered bands with uniform ranges. The y value is encoded by a gradient color scheme in blue. The darker the color, the higher the value. Then the bands are shifted to the center and distributed within a fixed height. Two y scales are provided: a unified scale for impact comparison across factors and an independent scale for temporal inspection within a factor. Users can choose a factor for further analysis. Justification: Initially, we used line charts with two modes. However, a multi-line graph that includes all the factors in one coordinate suffered great visual clutters. Small multiples where each line chart represented a factor were also not appropriate, since the height of each line chart was too narrow to show the temporal trends, let alone the comparison among factors. Thus, we chose the horizon chart to show the impact of multiple factors in a compact way.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area", "axial_code": [], "componenet_code": ["area"]}]}, {"author": "zsz", "index_original": 428, "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers", "pub_year": 2022, "domain": "academic career", "requirement": {"requirement_text": "T2 How do multiple factors differ in their impacts on career suc-cess? The expert wants to know the effects of different factorsat a speci\ufb01c time as a cross-sectional comparison to determinethe dominant factors in\ufb02uencing career success. Speci\ufb01cally, it isinteresting to compare the impacts of individual and social factors.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].", "data_code": {"tables": 1, "ordinal": 1, "textual": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).", "solution_category": "data_manipulation", "solution_axial": "UserInput,Clustering&Grouping", "solution_compoent": "", "axial_code": ["UserInput", "Clustering&Grouping"], "componenet_code": ["user_input", "clustering_and_grouping"]}, {"solution_text": "The Horizon Chart Group summarizes the trends of factor impacts and supports the comparison among factors. Description: Each horizon chart represents a factor which is extended from a line chart. The x-axis encodes the time and the y-axis represents the explanatory power of a factor. The line chart is divided into layered bands with uniform ranges. The y value is encoded by a gradient color scheme in blue. The darker the color, the higher the value. Then the bands are shifted to the center and distributed within a fixed height. Two y scales are provided: a unified scale for impact comparison across factors and an independent scale for temporal inspection within a factor. Users can choose a factor for further analysis. Justification: Initially, we used line charts with two modes. However, a multi-line graph that includes all the factors in one coordinate suffered great visual clutters. Small multiples where each line chart represented a factor were also not appropriate, since the height of each line chart was too narrow to show the temporal trends, let alone the comparison among factors. Thus, we chose the horizon chart to show the impact of multiple factors in a compact way.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area", "axial_code": [], "componenet_code": ["area"]}]}, {"author": "zsz", "index_original": 429, "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers", "pub_year": 2022, "domain": "academic career", "requirement": {"requirement_text": "T3 How does a category within a factor change over time to affectcareer success? The impact of a category can change at differentperiods. It re\ufb02ects the change of the roles of this category.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].", "data_code": {"tables": 1, "ordinal": 1, "textual": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).", "solution_category": "data_manipulation", "solution_axial": "UserInput,Clustering&Grouping", "solution_compoent": "", "axial_code": ["UserInput", "Clustering&Grouping"], "componenet_code": ["user_input", "clustering_and_grouping"]}, {"solution_text": "The Horizon Chart Group summarizes the trends of factor impacts and supports the comparison among factors. Description: Each horizon chart represents a factor which is extended from a line chart. The x-axis encodes the time and the y-axis represents the explanatory power of a factor. The line chart is divided into layered bands with uniform ranges. The y value is encoded by a gradient color scheme in blue. The darker the color, the higher the value. Then the bands are shifted to the center and distributed within a fixed height. Two y scales are provided: a unified scale for impact comparison across factors and an independent scale for temporal inspection within a factor. Users can choose a factor for further analysis. Justification: Initially, we used line charts with two modes. However, a multi-line graph that includes all the factors in one coordinate suffered great visual clutters. Small multiples where each line chart represented a factor were also not appropriate, since the height of each line chart was too narrow to show the temporal trends, let alone the comparison among factors. Thus, we chose the horizon chart to show the impact of multiple factors in a compact way.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area", "axial_code": [], "componenet_code": ["area"]}, {"solution_text": "After specifying the factor in the Horizon Chart Group, users can use the Impact Timeline to study the detailed impacts of the factor obtained from the regression model.Users can observe the temporal trends of the impact and compare the effects of different categories within the factor through intuitive interactions.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}, {"solution_text": "Description: The Impact Timeline consists of two parts: a MatrixLine showing the detailed impact of a time window and a Navigator revealing the impact evolvement over time. Justification: We designed two line charts for two values as the navigator while it was space-wasting. We then assembled them into a dual-y chart with an area encoding the p-value. However, after trying the system, our expert suggested that the ranges of the p-values were more practical to learn the statistical significance than the raw values. Thus, we adopted a stepped line graph to fulfill the requirement.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "pie+bar+matrix+line", "axial_code": [], "componenet_code": ["bar", "pie", "line", "matrix"]}]}, {"author": "zsz", "index_original": 430, "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers", "pub_year": 2022, "domain": "academic career", "requirement": {"requirement_text": "T3 How does a category within a factor change over time to affectcareer success? The impact of a category can change at differentperiods. It re\ufb02ects the change of the roles of this category.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].", "data_code": {"tables": 1, "ordinal": 1, "textual": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).", "solution_category": "data_manipulation", "solution_axial": "UserInput,Clustering&Grouping", "solution_compoent": "", "axial_code": ["UserInput", "Clustering&Grouping"], "componenet_code": ["user_input", "clustering_and_grouping"]}, {"solution_text": "Navigator. A Navigator provides a temporal overview of two values(i.e., p-value and coefficient) of a selected category mentioned aboveor the numerical independent variables (i.e., IV10 \u2212 IV12 ) (T3). It isa timeline with a dual y-axis that represents p-value and coefficient,respectively. A stepped line graph in green shows the p-values and theline chart in black reveals the coefficients. The color of the circle on the line chart encodes the positive and negative values with the same colorscheme in the matrix. Users can quickly \ufb01nd the period of interest anddrag to focus the time window in MatrixLine. For the numerical factors,users can directly use Navigator to study the time-varying impacts.", "solution_category": "visualization", "solution_axial": "large_panel", "solution_compoent": "bar+line", "axial_code": [], "componenet_code": ["bar", "line"]}, {"solution_text": "Users can quickly \ufb01nd the period of interest anddrag to focus the time window in MatrixLine. For the numerical factors,users can directly use Navigator to study the time-varying impacts.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 431, "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers", "pub_year": 2022, "domain": "academic career", "requirement": {"requirement_text": "T4 How do the categories within a factor differ from affecting ca-reer success? For each factor, different categories may have differ-ent sizes of impacts. Figuring out those with high impacts can helpexplain and provide guidelines for academic career development.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].", "data_code": {"tables": 1, "ordinal": 1, "textual": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).", "solution_category": "data_manipulation", "solution_axial": "UserInput,Clustering&Grouping", "solution_compoent": "", "axial_code": ["UserInput", "Clustering&Grouping"], "componenet_code": ["user_input", "clustering_and_grouping"]}, {"solution_text": "The Horizon Chart Group summarizes the trends of factor impacts and supports the comparison among factors. Description: Each horizon chart represents a factor which is extended from a line chart. The x-axis encodes the time and the y-axis represents the explanatory power of a factor. The line chart is divided into layered bands with uniform ranges. The y value is encoded by a gradient color scheme in blue. The darker the color, the higher the value. Then the bands are shifted to the center and distributed within a fixed height. Two y scales are provided: a unified scale for impact comparison across factors and an independent scale for temporal inspection within a factor. Users can choose a factor for further analysis. Justification: Initially, we used line charts with two modes. However, a multi-line graph that includes all the factors in one coordinate suffered great visual clutters. Small multiples where each line chart represented a factor were also not appropriate, since the height of each line chart was too narrow to show the temporal trends, let alone the comparison among factors. Thus, we chose the horizon chart to show the impact of multiple factors in a compact way.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area", "axial_code": [], "componenet_code": ["area"]}, {"solution_text": "After specifying the factor in the Horizon Chart Group, users can use the Impact Timeline to study the detailed impacts of the factor obtained from the regression model.Users can observe the temporal trends of the impact and compare the effects of different categories within the factor through intuitive interactions.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}, {"solution_text": "Description: The Impact Timeline consists of two parts: a MatrixLine showing the detailed impact of a time window and a Navigator revealing the impact evolvement over time. Justification: We designed two line charts for two values as the navigator while it was space-wasting. We then assembled them into a dual-y chart with an area encoding the p-value. However, after trying the system, our expert suggested that the ranges of the p-values were more practical to learn the statistical significance than the raw values. Thus, we adopted a stepped line graph to fulfill the requirement.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "pie+bar+matrix+line", "axial_code": [], "componenet_code": ["bar", "pie", "line", "matrix"]}]}, {"author": "zsz", "index_original": 432, "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers", "pub_year": 2022, "domain": "academic career", "requirement": {"requirement_text": "T4 How do the categories within a factor differ from affecting ca-reer success? For each factor, different categories may have differ-ent sizes of impacts. Figuring out those with high impacts can helpexplain and provide guidelines for academic career development.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].", "data_code": {"tables": 1, "ordinal": 1, "textual": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).", "solution_category": "data_manipulation", "solution_axial": "UserInput,Clustering&Grouping", "solution_compoent": "", "axial_code": ["UserInput", "Clustering&Grouping"], "componenet_code": ["user_input", "clustering_and_grouping"]}, {"solution_text": "MatrixLine. Each matrix shows the MIA model results of a timewindow (T4). The design targets independent variables related to se-quence clustering results (i.e., IV1 \u2212 IV9 in Section 4.3). To show theimpacts of different categories (i.e., clusters) in individual\u2019s (IV1 \u2212 IV3 )and his top collaborator\u2019s factors (IV4 \u2212 IV6 ), our MIA model trans-forms these categories into dummy variables and uses post-hoc analysisto produce pairwise category comparisons. The output is an n \u2217 n ma-trix for coef\ufb01cients and p-values in the regression as mentioned inSection 4.3. Thus, in Fig. 4-A, we divide the matrix into an uppertriangle and a lower triangle to show the pairwise p-values and coef-\ufb01cients, respectively. In the upper triangle, each cell represents thepairwise p-value. The p-value is partitioned to four ranges (i.e., [0,0.001), [0.001, 0.01), [0.01, 0.05), [0.05, 1]) based on the traditionalsocial science approaches [11, 57] and our expert\u2019s suggestions to showthe statistical signi\ufb01cance. We use white for range [0.05, 1] (i.e., notstatistically signi\ufb01cant) and green in different saturations for the otherthree ranges to distinguish the statistical signi\ufb01cance at different levels.The smaller the p-values, the darker the green. In the lower triangle,each cell shows the coef\ufb01cient of pairwise categories. Our expert em-phasized the importance of studying both the absolute values and thepositive and negative of the coef\ufb01cients. Thus, we use blue and red toencode the positive and negative values, respectively. The saturationencodes the absolute values. The larger the absolute value, the darkerthe color. The bar charts and pie charts above the matrix summarizethe population and proportions of individuals in each category (i.e.,sequence cluster) respectively. The dark grey area in the pie chartrepresents the proportion of this cluster.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "pie+bar+matrix", "axial_code": [], "componenet_code": ["bar", "pie", "matrix"]}, {"solution_text": "To inspect the time-varying impact of a category, users can \ufb01rstspecify a reference category (Fig. 4-A1) to align all the matrices (Sec-tion 4.3) across time. It will skip matrices without the reference cate-gory. Then they can choose a target category by clicking its pie chart.Two cells showing the p-value and coef\ufb01cient will be highlighted acrosstime. The Navigator will also be updated to summarize the temporaltrends. The target category will be added to the Cluster View for furtheranalysis. Users can customize the number of clusters, the length of thetime window and the window moving step to adjust the MIA model.To show the impact of collaboration strength of each collaborator\u2019scategory of a social factor (i.e., IV7 \u2212 IV9 ), we transform the n \u2217 n matrixinto an n\u22172 one (Fig. 4-B). Each row represents a category of this factorand two columns represent the p-value and coef\ufb01cient respectively.Users can align a category to study its time-varying impact.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 433, "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers", "pub_year": 2022, "domain": "academic career", "requirement": {"requirement_text": "T4 How do the categories within a factor differ from affecting ca-reer success? For each factor, different categories may have differ-ent sizes of impacts. Figuring out those with high impacts can helpexplain and provide guidelines for academic career development.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].", "data_code": {"tables": 1, "ordinal": 1, "textual": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).", "solution_category": "data_manipulation", "solution_axial": "UserInput,Clustering&Grouping", "solution_compoent": "", "axial_code": ["UserInput", "Clustering&Grouping"], "componenet_code": ["user_input", "clustering_and_grouping"]}, {"solution_text": "The Cluster View (Fig. 3-B) presents a list of categories (i.e., sequence clusters) chosen from the Impact Timeline. Three glyphs (Fig. 5-A, B, C) are designed to summarize the cluster of different sequences (T4). Description: Each row shows a sequence cluster of a time window with a title listing the window, the cluster label, and the number of researchers within the cluster. Each glyph of the row summarizes the distribution of researchers within the cluster at one year. We have designed three glyphs to show the cluster summary of three types of sequences (Fig. 5-A, B, C). The career glyph uses a sunburst struc_x0002_ture with two levels of hierarchy to show the career sequence event distribution. The inner ring shows three title ranks (i.e., junior, interme_x0002_diate, and senior) with purple in three saturation categories (Fig. 3-C). The outer ring encodes five citation ranks in black at five saturation categories. The domain glyph is a doughnut chart that records the dis_x0002_tribution of individuals\u2019 most frequently published paper venue types in the cluster. We highlight the visualization venues as pink and others as grey to show visualization researchers\u2019 domain diversity (Fig. 3-C). The sector glyph is also a doughnut chart showing the social sector (i.e., academia, industry, and government) distribution: green for the industry, blue for the academia, and yellow for the government agency (Fig. 3-C). The histogram on the right shows the citation distribution of the following year (i.e., DVs) of the sequence window. Users can compare the distribution of different clusters using the sequence list and choose a cluster of interest for further analysis.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph+area", "axial_code": [], "componenet_code": ["area", "glyph"]}]}, {"author": "zsz", "index_original": 434, "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers", "pub_year": 2022, "domain": "academic career", "requirement": {"requirement_text": "T5 How does a researcher\u2019s career path change over time? Theexpert wishes to identify different individuals from the data andstudy the multi-factor effects on career success from a micro per-spective. Revealing the career changes of a researcher over timecan help understand the different academic stages he goes over.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].", "data_code": {"tables": 1, "ordinal": 1, "textual": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).", "solution_category": "data_manipulation", "solution_axial": "UserInput,Clustering&Grouping", "solution_compoent": "", "axial_code": ["UserInput", "Clustering&Grouping"], "componenet_code": ["user_input", "clustering_and_grouping"]}, {"solution_text": "The Cluster View (Fig. 3-B) presents a list of categories (i.e., sequence clusters) chosen from the Impact Timeline. Three glyphs (Fig. 5-A, B, C) are designed to summarize the cluster of different sequences (T4). Description: Each row shows a sequence cluster of a time window with a title listing the window, the cluster label, and the number of researchers within the cluster. Each glyph of the row summarizes the distribution of researchers within the cluster at one year. We have designed three glyphs to show the cluster summary of three types of sequences (Fig. 5-A, B, C). The career glyph uses a sunburst struc_x0002_ture with two levels of hierarchy to show the career sequence event distribution. The inner ring shows three title ranks (i.e., junior, interme_x0002_diate, and senior) with purple in three saturation categories (Fig. 3-C). The outer ring encodes five citation ranks in black at five saturation categories. The domain glyph is a doughnut chart that records the dis_x0002_tribution of individuals\u2019 most frequently published paper venue types in the cluster. We highlight the visualization venues as pink and others as grey to show visualization researchers\u2019 domain diversity (Fig. 3-C). The sector glyph is also a doughnut chart showing the social sector (i.e., academia, industry, and government) distribution: green for the industry, blue for the academia, and yellow for the government agency (Fig. 3-C). The histogram on the right shows the citation distribution of the following year (i.e., DVs) of the sequence window. Users can compare the distribution of different clusters using the sequence list and choose a cluster of interest for further analysis.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph+area", "axial_code": [], "componenet_code": ["area", "glyph"]}, {"solution_text": "After choosing a cluster from the Cluster View, we use a CareerLine tovisualize an individual\u2019s careers and the effects of factors with foldedand unfolded modes in the Person View (Fig. 3-C, T5, T6).", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}, {"solution_text": "Description: In the folded mode (Fig. 6-A), the color of each circleshows the career title rank and the middle strip of the CareerLine represents the social sectors of the researcher, all with the same encoding inglyphs. Two flat gray areas distributed above and below the sector stripdepict the collaborator and domain diversity scores of the researcher,respectively. The outer light blue area shows the predicted citationsof the researcher based on the regression model. When hovering on acircle, a tooltip summarizing the researcher\u2019s domain diversity is shown(Fig. 3-C4). The outer ring is a doughnut chart summarizing the paperdistribution in different domains. The inner word cloud is generatedbased on the paper venue names from the bibliographic data. We breakthe venue names into separate words and count the frequency statisticsof each word to reflect the research topics. The word size encodes thenumber of papers.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "line+glyph+text", "axial_code": [], "componenet_code": ["text", "glyph", "line"]}, {"solution_text": "Users can unfold the area of collaborator diversityscore to study details about the researcher\u2019s collaborators (Fig. 6-B). Amulti-line graph shows the collaborators\u2019 different diversity scores (i.e.,career, domain, and sector). They can click one of the lines (i.e., diversi-ties) to show the summary of the collaborators\u2019 population distributionvia corresponding glyphs (Fig. 5-A, B, C). We have provided sortingand filtering for users to choose researchers of interest. Users can rankthem by average predicted citations or the collaborator diversity scores.", "solution_category": "interaction", "solution_axial": "OverviewandExplore,Reconfigure,Filtering", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Filtering", "Reconfigure"], "componenet_code": ["overview_and_explore", "filtering", "reconfigure"]}]}, {"author": "zsz", "index_original": 435, "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers", "pub_year": 2022, "domain": "academic career", "requirement": {"requirement_text": "T6 How do the different factors of a researcher change over time?Showing the evolution of career factors over time is essential tointerpret the career development of a researcher and further validateexisting rules or generate new hypotheses.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].", "data_code": {"tables": 1, "ordinal": 1, "textual": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).", "solution_category": "data_manipulation", "solution_axial": "UserInput,Clustering&Grouping", "solution_compoent": "", "axial_code": ["UserInput", "Clustering&Grouping"], "componenet_code": ["user_input", "clustering_and_grouping"]}, {"solution_text": "The Cluster View (Fig. 3-B) presents a list of categories (i.e., sequence clusters) chosen from the Impact Timeline. Three glyphs (Fig. 5-A, B, C) are designed to summarize the cluster of different sequences (T4). Description: Each row shows a sequence cluster of a time window with a title listing the window, the cluster label, and the number of researchers within the cluster. Each glyph of the row summarizes the distribution of researchers within the cluster at one year. We have designed three glyphs to show the cluster summary of three types of sequences (Fig. 5-A, B, C). The career glyph uses a sunburst struc_x0002_ture with two levels of hierarchy to show the career sequence event distribution. The inner ring shows three title ranks (i.e., junior, interme_x0002_diate, and senior) with purple in three saturation categories (Fig. 3-C). The outer ring encodes five citation ranks in black at five saturation categories. The domain glyph is a doughnut chart that records the dis_x0002_tribution of individuals\u2019 most frequently published paper venue types in the cluster. We highlight the visualization venues as pink and others as grey to show visualization researchers\u2019 domain diversity (Fig. 3-C). The sector glyph is also a doughnut chart showing the social sector (i.e., academia, industry, and government) distribution: green for the industry, blue for the academia, and yellow for the government agency (Fig. 3-C). The histogram on the right shows the citation distribution of the following year (i.e., DVs) of the sequence window. Users can compare the distribution of different clusters using the sequence list and choose a cluster of interest for further analysis.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph+area", "axial_code": [], "componenet_code": ["area", "glyph"]}, {"solution_text": "After choosing a cluster from the Cluster View, we use a CareerLine tovisualize an individual\u2019s careers and the effects of factors with foldedand unfolded modes in the Person View (Fig. 3-C, T5, T6).", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}, {"solution_text": "Description: In the folded mode (Fig. 6-A), the color of each circleshows the career title rank and the middle strip of the CareerLine represents the social sectors of the researcher, all with the same encoding inglyphs. Two flat gray areas distributed above and below the sector stripdepict the collaborator and domain diversity scores of the researcher,respectively. The outer light blue area shows the predicted citationsof the researcher based on the regression model. When hovering on acircle, a tooltip summarizing the researcher\u2019s domain diversity is shown(Fig. 3-C4). The outer ring is a doughnut chart summarizing the paperdistribution in different domains. The inner word cloud is generatedbased on the paper venue names from the bibliographic data. We breakthe venue names into separate words and count the frequency statisticsof each word to reflect the research topics. The word size encodes thenumber of papers.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "line+glyph+text", "axial_code": [], "componenet_code": ["text", "glyph", "line"]}, {"solution_text": "Users can unfold the area of collaborator diversityscore to study details about the researcher\u2019s collaborators (Fig. 6-B). Amulti-line graph shows the collaborators\u2019 different diversity scores (i.e.,career, domain, and sector). They can click one of the lines (i.e., diversi-ties) to show the summary of the collaborators\u2019 population distributionvia corresponding glyphs (Fig. 5-A, B, C). We have provided sortingand filtering for users to choose researchers of interest. Users can rankthem by average predicted citations or the collaborator diversity scores.", "solution_category": "interaction", "solution_axial": "OverviewandExplore,Reconfigure,Filtering", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Filtering", "Reconfigure"], "componenet_code": ["overview_and_explore", "filtering", "reconfigure"]}]}, {"author": "zsz", "index_original": 436, "paper_title": "Real-Time Visual Analysis of High-Volume Social Media Posts", "pub_year": 2022, "domain": "Social media", "requirement": {"requirement_text": "Overview: Gain a continuous overview of major themes people currently talk about on social media.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The 20 Newsgroups data set. It contains nearly 20,000 posts spread across 20 different newsgroups, and the corresponding newsgroup of each post serves as a class label.", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "textual": 1}}, "solution": [{"solution_text": "We establish two parallel and independent clustering processes with different levels of granularity (i.e., different thresholds for the maximum number of clusters). By default, the first, coarse-grained clustering does not extract more than 10 clusters to provide analysts with an interactive topical overview. The second process powers the diverse stream of representative posts with not more than 100 clusters per default. We set an upper limit of 10 for the number of main clusters so that we do not exceed the usual capacity of the analyst\u2019s short term memory, but both thresholds are adjustable.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "Parallelclusterpoststogetdifferentlevelsofgranularity.", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "zsz", "index_original": 437, "paper_title": "Real-Time Visual Analysis of High-Volume Social Media Posts", "pub_year": 2022, "domain": "Social media", "requirement": {"requirement_text": "Details: Learn more about specific interesting themes", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The 20 Newsgroups data set. It contains nearly 20,000 posts spread across 20 different newsgroups, and the corresponding newsgroup of each post serves as a class label.", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "textual": 1}}, "solution": [{"solution_text": "We also call the coarse-grained clusters topics and the more finegrained ones subtopics. It should be noted, however, that topics and subtopics do not form a classical hierarchy since both clustering processes are independent from each other. For each subtopic, we find its representative item, that is, the post closest to the respective centroid. Each post, therefore, has two cluster associations, one fine- and one coarse-grained, so each extracted representative item is also associated with exactly one topic. Analysts can select one or more topics to retrieve additional details, including a stream of representative posts that are associated with the selection and extracted relevant keyphrases. Such a selection of topics can be added as a new filter, which will create a new session layer that operates on the filtered stream. Hence, with our layered approach analysts can interactively increase the resolution and adapt the specificity of their analysis.", "solution_category": "data_manipulation", "solution_axial": "Retrieval", "solution_compoent": "Layeredapproachwithdifferentlevelsofgranularityincreaseresolution.", "axial_code": ["Retrieval"], "componenet_code": ["retrieval"]}]}, {"author": "zsz", "index_original": 438, "paper_title": "Real-Time Visual Analysis of High-Volume Social Media Posts", "pub_year": 2022, "domain": "Social media", "requirement": {"requirement_text": "Monitoring: Constantly monitor specific themes to keep track of new developments", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The 20 Newsgroups data set. It contains nearly 20,000 posts spread across 20 different newsgroups, and the corresponding newsgroup of each post serves as a class label.", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "textual": 1}}, "solution": [{"solution_text": "On every update, the frequent phrases will be updated and new representative posts may be added. If a new subtopic appears or the representative item of a subtopic changes and is sufficiently different, the post will be added to the stream of representative items. The number of new items per update is limited because it correlates with the total number of subtopics. These items offer a diverse view of what is currently being posted since they originated from different clusters.", "solution_category": "data_manipulation", "solution_axial": "Real-TimeInput", "solution_compoent": "UpdatesforRepresentativePosts", "axial_code": ["Real-TimeInput"], "componenet_code": ["real_time_input"]}]}, {"author": "zsz", "index_original": 439, "paper_title": "Real-Time Visual Analysis of High-Volume Social Media Posts", "pub_year": 2022, "domain": "Social media", "requirement": {"requirement_text": "Dive-in: Make specific themes the center of the analysis and increase resolution", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The 20 Newsgroups data set. It contains nearly 20,000 posts spread across 20 different newsgroups, and the corresponding newsgroup of each post serves as a class label.", "data_code": {"clusters_and_sets_and_lists": 1, "ordinal": 1, "textual": 1}}, "solution": [{"solution_text": "We also call the coarse-grained clusters topics and the more finegrained ones subtopics. It should be noted, however, that topics and subtopics do not form a classical hierarchy since both clustering processes are independent from each other. For each subtopic, we find its representative item, that is, the post closest to the respective centroid. Each post, therefore, has two cluster associations, one fine- and one coarse-grained, so each extracted representative item is also associated with exactly one topic. Analysts can select one or more topics to retrieve additional details, including a stream of representative posts that are associated with the selection and extracted relevant keyphrases. Such a selection of topics can be added as a new filter, which will create a new session layer that operates on the filtered stream. Hence, with our layered approach analysts can interactively increase the resolution and adapt the specificity of their analysis.", "solution_category": "data_manipulation", "solution_axial": "Retrieval", "solution_compoent": "Layeredapproachwithdifferentlevelsofgranularityincreaseresolution.", "axial_code": ["Retrieval"], "componenet_code": ["retrieval"]}]}, {"author": "zsz", "index_original": 450, "paper_title": "Interactive Visual Pattern Search on Graph Data via Graph Representation Learning", "pub_year": 2022, "domain": "pattern in Graphs ", "requirement": {"requirement_text": "T1 Browse/search the graph database. To start the query process,the user needs to be able to select from hundreds to thousandsof graphs. Therefore, the system should provide graph searchand filtering functionalities based on the category, the name, orgraph statistics such as the number of nodes/links. Besides that,a visualization showing an overview of all graphs in the databasewill be useful to help locate interesting graphs or clusters.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We evaluate the performance of the proposed system on 4 graph datasets in various domains: program workflow dataset (vehicle diagnostic), MSRC-21 (image processing), COX2 (chemistry) and Enzymes (biology). The workflow dataset contains \u223c500 individual workflow graphs with the number of nodes ranging from 5 to 150\u223c20 different types of nodes correspond to different diagnostic procedures. MSRC-21 [62] contains natural scene images with 21 object semantic labels. After the super-pixel extraction and processing steps as described in Section 5.2 and Fig. 7, the resulting graph dataset includes 544 graphs with 11 to 31 nodes. COX2 [46, 66] consists of 467 chemical molecule graphs with the number of nodes ranging from 32 to 56. Enzymes dataset [46, 58] contains 600 graphs of protein tertiary structure with 3 to 96 nodes. The last 3 datasets are public.", "data_code": {"clusters_and_sets_and_lists": 1, "media": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "We first define the subgraph matching problem and describe our overall framework to resolve it. We then describe NeuroMatch and NeuroAlign, the two GNNs as the core components of the framework. Finally, we introduce an improved inference method and a simple extension to support approximate query matching", "solution_category": "data_manipulation", "solution_axial": "Modeling,SimilarityCalculation", "solution_compoent": "", "axial_code": ["Modeling", "SimilarityCalculation"], "componenet_code": ["modeling", "similarity_calculation"]}, {"solution_text": "Overview and filters. In the overview panel the system displays the distribution of key graph statistics such as the number of the nodes/edges as well as domain-specific attributes such as the category of the graph. Both univariate distributions and bivariate distributions can be displayed as histograms or scatterplots.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "scatter+bar+table", "axial_code": [], "componenet_code": ["bar", "scatter", "table"]}, {"solution_text": "Users can brush the charts and select a subset of graphs to create example-based query patterns. To provide an overview of the graph structural information and help users navigate and select a graph to start the query, we further precom- pute the graph editing distance [23] which roughly captures the struc- tural similarities between all pairs of graphs. A 2-D projection coordi- nates of the graph can then be precomputed using t-SNE [73] based on the distance matrix and stored as additional graph attributes.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 451, "paper_title": "Interactive Visual Pattern Search on Graph Data via Graph Representation Learning", "pub_year": 2022, "domain": "pattern in Graphs ", "requirement": {"requirement_text": "T2 Interactively construct the query pattern by selecting on agraph visualization. To minimize user effort, the system shouldsupport both bulk selection mechanisms such as brushing thegraph regions as well as query refinement methods to add/deleteindividual nodes/edges from the pattern.", "requirement_code": {"data_filtering": 1, "interactivity": 1}}, "data": {"data_text": "We evaluate the performance of the proposed system on 4 graph datasets in various domains: program workflow dataset (vehicle diagnostic), MSRC-21 (image processing), COX2 (chemistry) and Enzymes (biology). The workflow dataset contains \u223c500 individual workflow graphs with the number of nodes ranging from 5 to 150\u223c20 different types of nodes correspond to different diagnostic procedures. MSRC-21 [62] contains natural scene images with 21 object semantic labels. After the super-pixel extraction and processing steps as described in Section 5.2 and Fig. 7, the resulting graph dataset includes 544 graphs with 11 to 31 nodes. COX2 [46, 66] consists of 467 chemical molecule graphs with the number of nodes ranging from 32 to 56. Enzymes dataset [46, 58] contains 600 graphs of protein tertiary structure with 3 to 96 nodes. The last 3 datasets are public.", "data_code": {"clusters_and_sets_and_lists": 1, "media": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "We first define the subgraph matching problem and describe our overall framework to resolve it. We then describe NeuroMatch and NeuroAlign, the two GNNs as the core components of the framework. Finally, we introduce an improved inference method and a simple extension to support approximate query matching", "solution_category": "data_manipulation", "solution_axial": "Modeling,SimilarityCalculation", "solution_compoent": "", "axial_code": ["Modeling", "SimilarityCalculation"], "componenet_code": ["modeling", "similarity_calculation"]}, {"solution_text": "Graph query panel. In the graph query panel, the user can interactively select from a graph instance to construct the query pattern. The color of the nodes encodes the key node attribute to be matched in the subgraph pattern query. The system currently supports categorical node attributes. This can be extended to numerical attributes by quantizing the values. Additional node attributes are displayed in attachment to the nodes or in tooltips. We need to support fast, interactive query construction. In this panel, the user can quickly select a group of nodes and the subgraph they induce by brushing a rectangular area on the visualization. They can also construct the pattern in a more precise manner by clicking the + and - button on the top right corner of each node. A minimap on the bottom right of the panel allows the user to easily navigate and explore graphs of larger size. The layout of the graph is computed with existing layout algorithms, such as the algorithm described in [22] for directed graphs. When the nodes have inherent spatial locations, they are used directly for display.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "network+text", "axial_code": [], "componenet_code": ["text", "network"]}, {"solution_text": "Graph query panel. In the graph query panel, the user can interactively select from a graph instance to construct the query pattern. The color of the nodes encodes the key node attribute to be matched in the subgraph pattern query. The system currently supports categorical node attributes. This can be extended to numerical attributes by quantizing the values. Additional node attributes are displayed in attachment to the nodes or in tooltips. We need to support fast, interactive query construction. In this panel, the user can quickly select a group of nodes and the subgraph they induce by brushing a rectangular area on the visualization. They can also construct the pattern in a more precise manner by clicking the + and - button on the top right corner of each node. A minimap on the bottom right of the panel allows the user to easily navigate and explore graphs of larger size. The layout of the graph is computed with existing layout algorithms, such as the algorithm described in [22] for directed graphs. When the nodes have inherent spatial locations, they are used directly for display.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 452, "paper_title": "Interactive Visual Pattern Search on Graph Data via Graph Representation Learning", "pub_year": 2022, "domain": "pattern in Graphs ", "requirement": {"requirement_text": "T3 Interpret and validate the matched graphs via highlightedsimilarities and differences. To help users interpret the matchingresults, the node correspondences, as well as differences inthe query results, should be highlighted. Furthermore, sincethe subgraph matching and node correspondence calculationalgorithms are not 100% accurate, the results need to be presentedin a meaningful way for easy verification.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "We evaluate the performance of the proposed system on 4 graph datasets in various domains: program workflow dataset (vehicle diagnostic), MSRC-21 (image processing), COX2 (chemistry) and Enzymes (biology). The workflow dataset contains \u223c500 individual workflow graphs with the number of nodes ranging from 5 to 150\u223c20 different types of nodes correspond to different diagnostic procedures. MSRC-21 [62] contains natural scene images with 21 object semantic labels. After the super-pixel extraction and processing steps as described in Section 5.2 and Fig. 7, the resulting graph dataset includes 544 graphs with 11 to 31 nodes. COX2 [46, 66] consists of 467 chemical molecule graphs with the number of nodes ranging from 32 to 56. Enzymes dataset [46, 58] contains 600 graphs of protein tertiary structure with 3 to 96 nodes. The last 3 datasets are public.", "data_code": {"clusters_and_sets_and_lists": 1, "media": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "We first define the subgraph matching problem and describe our overall framework to resolve it. We then describe NeuroMatch and NeuroAlign, the two GNNs as the core components of the framework. Finally, we introduce an improved inference method and a simple extension to support approximate query matching", "solution_category": "data_manipulation", "solution_axial": "Modeling,SimilarityCalculation", "solution_compoent": "", "axial_code": ["Modeling", "SimilarityCalculation"], "componenet_code": ["modeling", "similarity_calculation"]}, {"solution_text": "Graph query panel. In the graph query panel, the user can interactively select from a graph instance to construct the query pattern. The color of the nodes encodes the key node attribute to be matched in the subgraph pattern query. The system currently supports categorical node attributes. This can be extended to numerical attributes by quantizing the values. Additional node attributes are displayed in attachment to the nodes or in tooltips. We need to support fast, interactive query construction. In this panel, the user can quickly select a group of nodes and the subgraph they induce by brushing a rectangular area on the visualization. They can also construct the pattern in a more precise manner by clicking the + and - button on the top right corner of each node. A minimap on the bottom right of the panel allows the user to easily navigate and explore graphs of larger size. The layout of the graph is computed with existing layout algorithms, such as the algorithm described in [22] for directed graphs. When the nodes have inherent spatial locations, they are used directly for display.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "network+text", "axial_code": [], "componenet_code": ["text", "network"]}, {"solution_text": "Graph query panel. In the graph query panel, the user can interactively select from a graph instance to construct the query pattern. The color of the nodes encodes the key node attribute to be matched in the subgraph pattern query. The system currently supports categorical node attributes. This can be extended to numerical attributes by quantizing the values. Additional node attributes are displayed in attachment to the nodes or in tooltips. We need to support fast, interactive query construction. In this panel, the user can quickly select a group of nodes and the subgraph they induce by brushing a rectangular area on the visualization. They can also construct the pattern in a more precise manner by clicking the + and - button on the top right corner of each node. A minimap on the bottom right of the panel allows the user to easily navigate and explore graphs of larger size. The layout of the graph is computed with existing layout algorithms, such as the algorithm described in [22] for directed graphs. When the nodes have inherent spatial locations, they are used directly for display.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}, {"solution_text": "Query results. After the sub-graph pattern matching results are returned, the query results panel will be updated to display all the matching graphs as a small multiples display. Since the number of returned results could be large, the system sup- ports sorting the returned graphs with graph attribute values such as the number of nodes. To support requirement, the matching nodes are highlighted based on the results returned by the node alignment module. The graphs can be displayed either in a node-link diagram with the same layout as the graph in the query panel or in a thumbnail visualization designed to display the graph in a more compact manner. In particular, we use topological sorting of the nodes for directed acyclic graphs to order the nodes, layout them vertically, and route the links on the right to obtain a compact view.", "solution_category": "data_manipulation", "solution_axial": "Retrieval", "solution_compoent": "", "axial_code": ["Retrieval"], "componenet_code": ["retrieval"]}, {"solution_text": "Query results. After the sub-graph pattern matching results are returned, the query results panel will be updated to display all the matching graphs as a small multiples display. Since the number of returned results could be large, the system sup- ports sorting the returned graphs with graph attribute values such as the number of nodes. To support requirement, the matching nodes are highlighted based on the results returned by the node alignment module. The graphs can be displayed either in a node-link diagram with the same layout as the graph in the query panel or in a thumbnail visualization designed to display the graph in a more compact manner. In particular, we use topological sorting of the nodes for directed acyclic graphs to order the nodes, layout them vertically, and route the links on the right to obtain a compact view.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "network", "axial_code": [], "componenet_code": ["network"]}]}, {"author": "zsz", "index_original": 453, "paper_title": "Interactive Visual Pattern Search on Graph Data via Graph Representation Learning", "pub_year": 2022, "domain": "pattern in Graphs ", "requirement": {"requirement_text": "T3 Interpret and validate the matched graphs via highlightedsimilarities and differences. To help users interpret the matchingresults, the node correspondences, as well as differences inthe query results, should be highlighted. Furthermore, sincethe subgraph matching and node correspondence calculationalgorithms are not 100% accurate, the results need to be presentedin a meaningful way for easy verification.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "We evaluate the performance of the proposed system on 4 graph datasets in various domains: program workflow dataset (vehicle diagnostic), MSRC-21 (image processing), COX2 (chemistry) and Enzymes (biology). The workflow dataset contains \u223c500 individual workflow graphs with the number of nodes ranging from 5 to 150\u223c20 different types of nodes correspond to different diagnostic procedures. MSRC-21 [62] contains natural scene images with 21 object semantic labels. After the super-pixel extraction and processing steps as described in Section 5.2 and Fig. 7, the resulting graph dataset includes 544 graphs with 11 to 31 nodes. COX2 [46, 66] consists of 467 chemical molecule graphs with the number of nodes ranging from 32 to 56. Enzymes dataset [46, 58] contains 600 graphs of protein tertiary structure with 3 to 96 nodes. The last 3 datasets are public.", "data_code": {"clusters_and_sets_and_lists": 1, "media": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "We first define the subgraph matching problem and describe our overall framework to resolve it. We then describe NeuroMatch and NeuroAlign, the two GNNs as the core components of the framework. Finally, we introduce an improved inference method and a simple extension to support approximate query matching", "solution_category": "data_manipulation", "solution_axial": "Modeling,SimilarityCalculation", "solution_compoent": "", "axial_code": ["Modeling", "SimilarityCalculation"], "componenet_code": ["modeling", "similarity_calculation"]}, {"solution_text": "Comparison view. To support requirement, we further visualize the query and selected matching graphs side-by-side in a popup window. The user can click on the zoom-in button on each small multiple to bring out the comparison view and review each matching graph in detail. The matched nodes are highlighted for verification.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "network", "axial_code": [], "componenet_code": ["network"]}, {"solution_text": "The user can click on the zoom-in button on each small multiple to bring out the comparison view and review each matching graph in detail. The matched nodes are highlighted for verification.", "solution_category": "interaction", "solution_axial": "OverviewandExplore,Filtering", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Filtering"], "componenet_code": ["overview_and_explore", "filtering"]}]}, {"author": "zsz", "index_original": 454, "paper_title": "Interactive Visual Pattern Search on Graph Data via Graph Representation Learning", "pub_year": 2022, "domain": "pattern in Graphs ", "requirement": {"requirement_text": "T4 Explore the distribution of the matching instances. After thematched graphs are returned, the system should indicate howfrequently the query pattern occurs in the entire database, andprovide the distribution of the pattern among different categoriesof graphs in the database.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "We evaluate the performance of the proposed system on 4 graph datasets in various domains: program workflow dataset (vehicle diagnostic), MSRC-21 (image processing), COX2 (chemistry) and Enzymes (biology). The workflow dataset contains \u223c500 individual workflow graphs with the number of nodes ranging from 5 to 150\u223c20 different types of nodes correspond to different diagnostic procedures. MSRC-21 [62] contains natural scene images with 21 object semantic labels. After the super-pixel extraction and processing steps as described in Section 5.2 and Fig. 7, the resulting graph dataset includes 544 graphs with 11 to 31 nodes. COX2 [46, 66] consists of 467 chemical molecule graphs with the number of nodes ranging from 32 to 56. Enzymes dataset [46, 58] contains 600 graphs of protein tertiary structure with 3 to 96 nodes. The last 3 datasets are public.", "data_code": {"clusters_and_sets_and_lists": 1, "media": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "We first define the subgraph matching problem and describe our overall framework to resolve it. We then describe NeuroMatch and NeuroAlign, the two GNNs as the core components of the framework. Finally, we introduce an improved inference method and a simple extension to support approximate query matching", "solution_category": "data_manipulation", "solution_axial": "Modeling,SimilarityCalculation", "solution_compoent": "", "axial_code": ["Modeling", "SimilarityCalculation"], "componenet_code": ["modeling", "similarity_calculation"]}, {"solution_text": "After the query result is obtained, the charts will be updated to provide a contextual view of how the subgraph pattern occurs in the database. For example, the user can observe whether the pattern occur- rence concentrate on a small subset of graph categories or it is a generic pattern that appears in many different categories", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "zsz", "index_original": 455, "paper_title": "Interactive Visual Pattern Search on Graph Data via Graph Representation Learning", "pub_year": 2022, "domain": "pattern in Graphs ", "requirement": {"requirement_text": "T5 Refine query results. A flexible query system should furthersupport query refinement mechanism where the users can applytheir domain knowledge to filter the results with additionalconstraints, such as matching additional node attributes or limitingthe results to a certain category of graphs.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "We evaluate the performance of the proposed system on 4 graph datasets in various domains: program workflow dataset (vehicle diagnostic), MSRC-21 (image processing), COX2 (chemistry) and Enzymes (biology). The workflow dataset contains \u223c500 individual workflow graphs with the number of nodes ranging from 5 to 150\u223c20 different types of nodes correspond to different diagnostic procedures. MSRC-21 [62] contains natural scene images with 21 object semantic labels. After the super-pixel extraction and processing steps as described in Section 5.2 and Fig. 7, the resulting graph dataset includes 544 graphs with 11 to 31 nodes. COX2 [46, 66] consists of 467 chemical molecule graphs with the number of nodes ranging from 32 to 56. Enzymes dataset [46, 58] contains 600 graphs of protein tertiary structure with 3 to 96 nodes. The last 3 datasets are public.", "data_code": {"clusters_and_sets_and_lists": 1, "media": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "We first define the subgraph matching problem and describe our overall framework to resolve it. We then describe NeuroMatch and NeuroAlign, the two GNNs as the core components of the framework. Finally, we introduce an improved inference method and a simple extension to support approximate query matching", "solution_category": "data_manipulation", "solution_axial": "Modeling,SimilarityCalculation", "solution_compoent": "", "axial_code": ["Modeling", "SimilarityCalculation"], "componenet_code": ["modeling", "similarity_calculation"]}, {"solution_text": "Comparison view. To support requirement, we further visualize the query and selected matching graphs side-by-side in a popup window. The user can click on the zoom-in button on each small multiple to bring out the comparison view and review each matching graph in detail. The matched nodes are highlighted for verification.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "network", "axial_code": [], "componenet_code": ["network"]}, {"solution_text": "The user can click on the zoom-in button on each small multiple to bring out the comparison view and review each matching graph in detail. The matched nodes are highlighted for verification.", "solution_category": "interaction", "solution_axial": "OverviewandExplore,Filtering", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Filtering"], "componenet_code": ["overview_and_explore", "filtering"]}]}, {"author": "zsz", "index_original": 456, "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines", "pub_year": 2022, "domain": "manufacturing", "requirement": {"requirement_text": "T1 Gain overview of engines: By taking the role of explorers, en-gineers need a structured overview of the data, in our case of enginesrepresented by their acoustic signals. Grouping engines by the similar-ity of acoustic signals using clustering algorithms would be desirable.In addition, interactive grouping to adapt to the information need ofindividual engineers would be useful, e.g., based on different steeringparameters (such as the number of groups) or filtered subsets of engines.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.", "data_code": {"tables": 1}}, "solution": [{"solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}, {"solution_text": "In (A), all clusters of similar Hypermatrices are displayed. Each Hypermtarix in the grid is represented by the mean aggregation of all Hypermatrices in a cluster of engines. The grid view serves to get an overview over all clusters and their individual properties and to support the decision of which cluster to select.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix+pie+glyph", "axial_code": [], "componenet_code": ["pie", "glyph", "matrix"]}]}, {"author": "zsz", "index_original": 457, "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines", "pub_year": 2022, "domain": "manufacturing", "requirement": {"requirement_text": "T1 Gain overview of engines: By taking the role of explorers, en-gineers need a structured overview of the data, in our case of enginesrepresented by their acoustic signals. Grouping engines by the similar-ity of acoustic signals using clustering algorithms would be desirable.In addition, interactive grouping to adapt to the information need ofindividual engineers would be useful, e.g., based on different steeringparameters (such as the number of groups) or filtered subsets of engines.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.", "data_code": {"tables": 1}}, "solution": [{"solution_text": "First, they give an additional overview over groups of engines (T1 ), because clusterswhich are already completely labeled are less interesting for an analysis.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 458, "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines", "pub_year": 2022, "domain": "manufacturing", "requirement": {"requirement_text": "T2 Drill-down to engines: Engineers exploring large numbers ofengines need support for the drill-down to engines of interest. Theinformation need may differ between clusters of engines, the labelingstatus, or the degree of the anomaly of engines.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.", "data_code": {"tables": 1}}, "solution": [{"solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}, {"solution_text": "In (A), all clusters of similar Hypermatrices are displayed. Each Hypermtarix in the grid is represented by the mean aggregation of all Hypermatrices in a cluster of engines. The grid view serves to get an overview over all clusters and their individual properties and to support the decision of which cluster to select.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix+pie+glyph", "axial_code": [], "componenet_code": ["pie", "glyph", "matrix"]}]}, {"author": "zsz", "index_original": 459, "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines", "pub_year": 2022, "domain": "manufacturing", "requirement": {"requirement_text": "T3 Identify engine of interest: The work\ufb02ow of both explorers andcon\ufb01rmers includes the identi\ufb01cation of single engines as an entry pointinto an in-depth analysis. Identi\ufb01cation may be through knowledgeabout a particular engine, by special or even unique acoustic signatures,or by traversing several (similar) engines in the exploration process.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.", "data_code": {"tables": 1}}, "solution": [{"solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}, {"solution_text": "(B) shows engines in a cluster, the Hypermatrix of each engine as small multiple, and the aggregated deviation of the signature of a single engine to all other engines. This view is designed to get an overview of the engine\u2019s properties and compare them to other engines in the cluster. Thus, it supports the user in selecting an engine.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix+text+glyph", "axial_code": [], "componenet_code": ["text", "glyph", "matrix"]}, {"solution_text": "This view is designed to get an overview of the engine\u2019s properties and compare them to other engines in the cluster. Thus, it supports the user in selecting an engine.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 460, "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines", "pub_year": 2022, "domain": "manufacturing", "requirement": {"requirement_text": "T3 Identify engine of interest: The work\ufb02ow of both explorers andcon\ufb01rmers includes the identi\ufb01cation of single engines as an entry pointinto an in-depth analysis. Identi\ufb01cation may be through knowledgeabout a particular engine, by special or even unique acoustic signatures,or by traversing several (similar) engines in the exploration process.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Second, they support the selection of an engine (T3 ), since engines thatcontain a label are also less probable to be selected", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 461, "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines", "pub_year": 2022, "domain": "manufacturing", "requirement": {"requirement_text": "T4 Analyze single engine: Engineers need to analyze individualengines in detail for being able to assign labels on a profound basis.Single engines are both represented by their acoustic signature and theraw acoustic measurements. The analysis is also supported by storeddomain knowledge from the systems knowledge base.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.", "data_code": {"tables": 1}}, "solution": [{"solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}, {"solution_text": "In (C), the detailed Hypermatrix of a selected engine is shown. Additional information about single selected cells in the matrix is displayed in the upper left triangle. This view is designed to support the user in the analysis of a selected engine. A matrix representation is adequate to represent the relation between pairs of sub-components (e.g. Gear and rotor shaft). The regions of sub-components in the Hypermatrix are marked with additional black lines. The same color scheme as in the cluster view for Hypermatrices is applied. The selection of a cell in the Hypermatrix is supported by additional lines and triangles (dark and light grey) for each axis.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix", "axial_code": [], "componenet_code": ["matrix"]}]}, {"author": "zsz", "index_original": 462, "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines", "pub_year": 2022, "domain": "manufacturing", "requirement": {"requirement_text": "T4 Analyze single engine: Engineers need to analyze individualengines in detail for being able to assign labels on a profound basis.Single engines are both represented by their acoustic signature and theraw acoustic measurements. The analysis is also supported by storeddomain knowledge from the systems knowledge base.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.", "data_code": {"tables": 1}}, "solution": [{"solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}, {"solution_text": "(D) visualizes a spectrogram of the selected engine. This type of visualization is the same one, engineers analyze during their daily routines and is used to support the analysis of a single engine. Here, a diverging color scale is used. Blueish colors represent acoustic measurements, which are more quiet compared to all other engines, and reddish colors louder ones. This kind of color scale is appropriate since all values spread around 0 and deviate in different directions.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix", "axial_code": [], "componenet_code": ["matrix"]}]}, {"author": "zsz", "index_original": 463, "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines", "pub_year": 2022, "domain": "manufacturing", "requirement": {"requirement_text": "T4 Analyze single engine: Engineers need to analyze individualengines in detail for being able to assign labels on a profound basis.Single engines are both represented by their acoustic signature and theraw acoustic measurements. The analysis is also supported by storeddomain knowledge from the systems knowledge base.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.", "data_code": {"tables": 1}}, "solution": [{"solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}, {"solution_text": "(F) shows three different views. First, the line chart displays two orders from the spectrogram across their rpm values. Second, the scatterplot shows the correlation of the selected order pair. Third, the bar chart shows the aggregated deviations for a region in the Hypermatrix to all other regions as indicated in Figure 6. These views are designed to facilitate the analysis of an engine, where their input data are displayed when hovering over a cell in (C). In our application domain, deviations above and below three tend to be reasons for errors in the selected part. Thus, lines above and below this limit are marked as red for (+3) and blue for (-3). The purpose of the scatterplot is to provide additional information on how the two selected order lines correlate with each other. An additional overview about the five most deviating sub-component pairs in (B) is provided as a bar chart view in (E). To be consistent with our use of colors, red bars represent aggregated deviations greater than zero and blue ones lower zero.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "line+scatter+bar", "axial_code": [], "componenet_code": ["bar", "scatter", "line"]}]}, {"author": "zsz", "index_original": 464, "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines", "pub_year": 2022, "domain": "manufacturing", "requirement": {"requirement_text": "T4 Analyze single engine: Engineers need to analyze individualengines in detail for being able to assign labels on a profound basis.Single engines are both represented by their acoustic signature and theraw acoustic measurements. The analysis is also supported by storeddomain knowledge from the systems knowledge base.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Third, they help inthe analysis of a selected engine (T4 ). This is because they are immediately displayed in the engine list view in (B) and thus give hints on the probability of a label for the selected engine", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 465, "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines", "pub_year": 2022, "domain": "manufacturing", "requirement": {"requirement_text": "T4 Analyze single engine: Engineers need to analyze individualengines in detail for being able to assign labels on a profound basis.Single engines are both represented by their acoustic signature and theraw acoustic measurements. The analysis is also supported by storeddomain knowledge from the systems knowledge base.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.", "data_code": {"tables": 1}}, "solution": [{"solution_text": "First, users are able to review annotations from previous analysesof other engineers. By selecting a label from the bar chart view inFigure 1-E all other annotations for the same label are displayed in thespectrogram. This supports the user in the analysis of an engine (T4 ).", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 466, "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines", "pub_year": 2022, "domain": "manufacturing", "requirement": {"requirement_text": "T5 Assign label to engine: Engineers need to assign labels of errorcategories. In close collaboration, we formed a default alphabet oflabel categories as follows: Error/Electromagnetic-Field, Error/FirstGear, Error/Second Gear, Error/A-Bearing, Error/B-Bearing,Error/C-Bearing, No error. In addition, engineers also expressed theneed to leave the label alphabet open for modi\ufb01cations, as knowledgeabout error variations will constantly grow as IRVINE is used. Finally,to enhance ef\ufb01ciency in combination with T2 , it would be desirableto label single engines but also multiple similar engines at a glance.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.", "data_code": {"tables": 1}}, "solution": [{"solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}, {"solution_text": "(E) shows the distribution of already assigned labels as a bar chart. The main purpose of the view is to support the labeling of (selections of) engines, as outlined in detail in Section 6.3. To show label distributions, a bar chart is an obvious choice. It would have been possible to use a pie chart, but for the labels that would have broken the guideline that there should be no more than six segments [24]. In the example in figure, all but one engine in the cluster are labeled as B-Bearing error.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "bar+table", "axial_code": [], "componenet_code": ["bar", "table"]}]}, {"author": "zsz", "index_original": 467, "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines", "pub_year": 2022, "domain": "manufacturing", "requirement": {"requirement_text": "T5 Assign label to engine: Engineers need to assign labels of errorcategories. In close collaboration, we formed a default alphabet oflabel categories as follows: Error/Electromagnetic-Field, Error/FirstGear, Error/Second Gear, Error/A-Bearing, Error/B-Bearing,Error/C-Bearing, No error. In addition, engineers also expressed theneed to leave the label alphabet open for modi\ufb01cations, as knowledgeabout error variations will constantly grow as IRVINE is used. Finally,to enhance ef\ufb01ciency in combination with T2 , it would be desirableto label single engines but also multiple similar engines at a glance.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.", "data_code": {"tables": 1}}, "solution": [{"solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}, {"solution_text": "(E) shows the distribution of already assigned labels as a bar chart. The main purpose of the view is to support the labeling of (selections of) engines, as outlined in detail in Section 6.3. To show label distributions, a bar chart is an obvious choice. It would have been possible to use a pie chart, but for the labels that would have broken the guideline that there should be no more than six segments [24]. In the example in figure, all but one engine in the cluster are labeled as B-Bearing error.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "bar+table", "axial_code": [], "componenet_code": ["bar", "table"]}, {"solution_text": "After an analysis is complete, the user can assign a label to the com-ponent in (E) with the two list views (T5 ).", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 468, "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines", "pub_year": 2022, "domain": "manufacturing", "requirement": {"requirement_text": "T6 Annotate acoustic measurements: The cause of a labeled enginecan be annotated by marking the respective region inside of the acousticraw data of an engine (the spectrogram) serving two purposes. First,annotations can be used to review how similar labels were annotated bydifferent users. Second, a suf\ufb01cient amount of annotations for a givenlabel will allow building thresholds for semi-automatic error detection.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.", "data_code": {"tables": 1}}, "solution": [{"solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Clustering&Grouping", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}, {"solution_text": "In (C), the detailed Hypermatrix of a selected engine is shown. Additional information about single selected cells in the matrix is displayed in the upper left triangle. This view is designed to support the user in the analysis of a selected engine. A matrix representation is adequate to represent the relation between pairs of sub-components (e.g. Gear and rotor shaft). The regions of sub-components in the Hypermatrix are marked with additional black lines. The same color scheme as in the cluster view for Hypermatrices is applied. The selection of a cell in the Hypermatrix is supported by additional lines and triangles (dark and light grey) for each axis.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix", "axial_code": [], "componenet_code": ["matrix"]}, {"solution_text": "By hovering over a cell in (C) the according orders of a pair of sub-components are displayed in the spectrogram. In the example in Figure 1, the sub-component pair (B-Bearing and A-Bearing) is selected. The former is shown with a triangle in dark grey and the latter with light grey. An engine can be annotated by clicking on \u201cAdd Annotation\u201d.", "solution_category": "interaction", "solution_axial": "Filtering,Participation/Collaboration", "solution_compoent": "", "axial_code": ["Filtering", "Participation/Collaboration"], "componenet_code": ["filtering", "participation_collaboration"]}]}, {"author": "zsz", "index_original": 481, "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "pub_year": 2022, "domain": "Life Sciences", "requirement": {"requirement_text": "For each approach, show similar patients, based on symptom severity at a specific time point.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We first organized the symptom ratings into a patient-symptom matrix for the selected time point, where each element (i, j) corresponds to the score given to symptom j by patient i at that time point. Prior research in symptom cluster for HNC [34] had applied hierarchical clustering using Ward\u2019s method [48] with Euclidean distance on the patient-symptom matrix to group patients based on their raw symptom ratings. After alternative clustering with complete and average linkages, we found that Ward\u2019s method generated larger, more informative groups of high symptom patients, which made sense to the clinicians. We identified two patient groups with high and low symptom burden (T1.1). This two-group clustering was preferred by clinicians, who found it easier to compare two groups instead of more. The axes of the scatterplot correspond to the first two components obtained by applying PCA to the patient-symptom matrix. Clusters for a specific time point are extracted and displayed, while clusters for different timepoints can be investigated via the time slider, which will update the scatterplot.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping,DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction", "Clustering&Grouping"], "componenet_code": ["dimensionality_reduction", "clustering_and_grouping"]}, {"solution_text": "The scatterplot was customized to separately capture acute and late symptom burden distribution as identified by the symptom clusters, and to reflect via marker color, shape, and size the therapeutic combination administered to each patient, their gender, and their disease stage.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "scatter+glyph", "axial_code": [], "componenet_code": ["scatter", "glyph"]}, {"solution_text": "The data can be filtered by attributes, and filtering operations update the other views. A filtering control panel serves double duty, by also providing the plot legend. This customized scatterplot encoding effectively captured the symptom distribution across the patient population, patient outliers, and therapeutic distribution across the data.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 482, "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "pub_year": 2022, "domain": "Life Sciences", "requirement": {"requirement_text": "For each approach, detect correlations among symptoms, during and after treatment.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.", "data_code": {"tables": 1, "quantitative": 1, "textual": 1}}, "solution": [{"solution_text": "We applied ARM to each of the acute stage and the late stage, and empirically chose to illustrate the top 20 rules yielded by this approach, because only a small number of rules were of clinical interest. We chose minimum values for the support and lift metrics that were suitable for frequent and interdependent symptoms.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "From the many possible encodings of ARMs [38], we selected a node-link representation (Fig. 1.A), which was deemed by clinicians to be more friendly to broader audiences (A3), and a good fit for the rela_x0002_tively small number of nodes. Graphs are laid out using a force-directed layout algorithm based on statistical multidimensional scaling [39, 79], which results in nodes with high degree being placed centrally.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Network+bubble+text", "axial_code": [], "componenet_code": ["text", "network", "bubble"]}]}, {"author": "zsz", "index_original": 483, "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "pub_year": 2022, "domain": "Life Sciences", "requirement": {"requirement_text": "For each approach, detect correlations among symptoms, during and after treatment.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Designing an appropriate encoding for the symptom longitudinal data(A2) turned out to be particularly challenging, primarily due to thenature and richness of the temporal data, the acknowledged variabilityin ratings across patients, and the missing or uneven time points, whichwere expected in this context. The design process explored a widerange of possible temporal encodings, many of which suffered fromscalability issues and, after several sessions, focused on a promising encoding called a \u201dtendril plot\u201d [51]. A tendril plot is a visual summaryof the incidence, significance, and temporal aspects of adverse eventsin clinical trials, in which individual temporal threads, one per eachpatient, emanate from a common root and shoot upwards and curl eitherto the left or to the right depending on whether the next event in thetimeline was adverse or an improvement. For clinical trial data, tendrilswere shown to create beautiful, compact, naturally clustering pathlinesillustrating the positive or negative evolution of each group of patients.The clinicians had also seen this representation and thought it couldwork (T1.2, T2.2). Whereas promising on paper, unfortunately, thetendril implementation did not yield similarly clean illustrations forthe symptom data, because of the much smaller number of time points,the variability in therapeutic sequences, and the variability in patientoutcomes, which are not typical of clinical trials.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "zsz", "index_original": 484, "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "pub_year": 2022, "domain": "Life Sciences", "requirement": {"requirement_text": "For each approach, detect correlations among symptoms, during and after treatment.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The percentile heatmap (Fig. 1.D) is a custom representation show_x0002_ing the rating distribution of individual symptoms over time, for the entire patient cohort (T2.3). We arrived at this representation after exploring a variety of alternatives such as stacked line plots, parallel coordinates plots, and radar charts, guided by feedback from collabora_x0002_tors. We settled on a matrix-based layout due to its compactness and to its ability to support small multiple plots. Each row corresponds to a symptom, with rows grouped by symptom category, and each column corresponds to a time point. Each cell in this matrix is a horizontal bar graph showing via shade the percentage of patients reporting within a specific range (0, 1-5, 6-9, or 10) for that symptom, at that time point. The bar height maps the percentage of individuals from the entire co_x0002_hort who reported the symptom ratings at that time point. The current patient is indicated in this heatmap by cross markers (Fig. 4.C) (T3.1). This encoding proved to be an intuitive way of showing what symptoms produce a higher burden for patients, and when, as well as to indicate how many patients were affected by these symptoms from the entire cohort (T1.2, T2.3)", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "matrix+bar+heatmap", "axial_code": [], "componenet_code": ["heatmap", "bar", "matrix"]}]}, {"author": "zsz", "index_original": 485, "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "pub_year": 2022, "domain": "Life Sciences", "requirement": {"requirement_text": "For each approach, detect correlations among symptoms, during and after treatment.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.", "data_code": {"tables": 1, "textual": 1, "quantitative": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Additionally, a compact correlation matrix along with the percentile heatmap, supports requirement, by showing the strength of the correlation between a selected symptom and all other symptoms, with circles encoding Spearman\u2019s coefficient via color and size.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "matrix+text+circle", "axial_code": [], "componenet_code": ["text", "circle", "matrix"]}]}, {"author": "zsz", "index_original": 486, "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "pub_year": 2022, "domain": "Life Sciences", "requirement": {"requirement_text": "For each approach, detect patient outliers and trends.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We first organized the symptom ratings into a patient-symptom matrix for the selected time point, where each element (i, j) corresponds to the score given to symptom j by patient i at that time point. Prior research in symptom cluster for HNC [34] had applied hierarchical clustering using Ward\u2019s method [48] with Euclidean distance on the patient-symptom matrix to group patients based on their raw symptom ratings. After alternative clustering with complete and average linkages, we found that Ward\u2019s method generated larger, more informative groups of high symptom patients, which made sense to the clinicians. We identified two patient groups with high and low symptom burden (T1.1). This two-group clustering was preferred by clinicians, who found it easier to compare two groups instead of more. The axes of the scatterplot correspond to the first two components obtained by applying PCA to the patient-symptom matrix. Clusters for a specific time point are extracted and displayed, while clusters for different timepoints can be investigated via the time slider, which will update the scatterplot.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping,DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction", "Clustering&Grouping"], "componenet_code": ["dimensionality_reduction", "clustering_and_grouping"]}, {"solution_text": "The scatterplot was customized to separately capture acute and late symptom burden distribution as identified by the symptom clusters, and to reflect via marker color, shape, and size the therapeutic combination administered to each patient, their gender, and their disease stage.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "scatter+glyph", "axial_code": [], "componenet_code": ["scatter", "glyph"]}, {"solution_text": "The data can be filtered by attributes, and filtering operations update the other views. A filtering control panel serves double duty, by also providing the plot legend. This customized scatterplot encoding effectively captured the symptom distribution across the patient population, patient outliers, and therapeutic distribution across the data.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 487, "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "pub_year": 2022, "domain": "Life Sciences", "requirement": {"requirement_text": "Analyze the patient symptom trajectories as a whole, by therapy type, and by stage.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "In order to better support activities A1 and A2, an additional option uses the same filament encoding, this time with the color mapped to the therapy type, to capture the mean trajectory per each therapeutic combination. Since in the therapy case the symptom mean ratings across the population bear meaning, the filaments are spread out according to the mean ratings per therapy. This therapy-analysis option helps estimate what treatment plans are less symptomatic, or on the contrary, conduct to high symptom burden.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "zsz", "index_original": 488, "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "pub_year": 2022, "domain": "Life Sciences", "requirement": {"requirement_text": "T2.2. Compare symptom trajectories by therapy type", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Designing an appropriate encoding for the symptom longitudinal data(A2) turned out to be particularly challenging, primarily due to thenature and richness of the temporal data, the acknowledged variabilityin ratings across patients, and the missing or uneven time points, whichwere expected in this context. The design process explored a widerange of possible temporal encodings, many of which suffered fromscalability issues and, after several sessions, focused on a promising encoding called a \u201dtendril plot\u201d [51]. A tendril plot is a visual summaryof the incidence, significance, and temporal aspects of adverse eventsin clinical trials, in which individual temporal threads, one per eachpatient, emanate from a common root and shoot upwards and curl eitherto the left or to the right depending on whether the next event in thetimeline was adverse or an improvement. For clinical trial data, tendrilswere shown to create beautiful, compact, naturally clustering pathlinesillustrating the positive or negative evolution of each group of patients.The clinicians had also seen this representation and thought it couldwork (T1.2, T2.2). Whereas promising on paper, unfortunately, thetendril implementation did not yield similarly clean illustrations forthe symptom data, because of the much smaller number of time points,the variability in therapeutic sequences, and the variability in patientoutcomes, which are not typical of clinical trials.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "zsz", "index_original": 489, "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "pub_year": 2022, "domain": "Life Sciences", "requirement": {"requirement_text": "Summarize symptom ratings for the entire cohort, by stage", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We first organized the symptom ratings into a patient-symptom matrix for the selected time point, where each element (i, j) corresponds to the score given to symptom j by patient i at that time point. Prior research in symptom cluster for HNC [34] had applied hierarchical clustering using Ward\u2019s method [48] with Euclidean distance on the patient-symptom matrix to group patients based on their raw symptom ratings. After alternative clustering with complete and average linkages, we found that Ward\u2019s method generated larger, more informative groups of high symptom patients, which made sense to the clinicians. We identified two patient groups with high and low symptom burden (T1.1). This two-group clustering was preferred by clinicians, who found it easier to compare two groups instead of more. The axes of the scatterplot correspond to the first two components obtained by applying PCA to the patient-symptom matrix. Clusters for a specific time point are extracted and displayed, while clusters for different timepoints can be investigated via the time slider, which will update the scatterplot.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping,DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction", "Clustering&Grouping"], "componenet_code": ["dimensionality_reduction", "clustering_and_grouping"]}, {"solution_text": "The scatterplot was customized to separately capture acute and late symptom burden distribution as identified by the symptom clusters, and to reflect via marker color, shape, and size the therapeutic combination administered to each patient, their gender, and their disease stage.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "scatter+glyph", "axial_code": [], "componenet_code": ["scatter", "glyph"]}, {"solution_text": "The data can be filtered by attributes, and filtering operations update the other views. A filtering control panel serves double duty, by also providing the plot legend. This customized scatterplot encoding effectively captured the symptom distribution across the patient population, patient outliers, and therapeutic distribution across the data.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 490, "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "pub_year": 2022, "domain": "Life Sciences", "requirement": {"requirement_text": "Summarize symptom ratings for the entire cohort, by stage", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The percentile heatmap is a custom representation showing the rating distribution of individual symptoms over time, for the entire patient cohort. We arrived at this representation after exploring a variety of alternatives such as stacked line plots, parallel coordinates plots, and radars, guided by feedback from collaborators.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "matrix+bar+heatmap", "axial_code": [], "componenet_code": ["heatmap", "bar", "matrix"]}]}, {"author": "zsz", "index_original": 491, "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "pub_year": 2022, "domain": "Life Sciences", "requirement": {"requirement_text": "Show an individual patient in the context of the cohort", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.", "data_code": {"tables": 1, "textual": 1, "quantitative": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "For missing data during the observation period, the associated pointsare not represented, and we consider no rating change from the previoustime points; the surveillance period is represented on each filamentuntil the last recorded time point for each patient. We account for thetime ratio between the acute (1 week) and late (months) stages, so thedistances illustrated for the acute time points are smaller as opposedto the late time points. Hovering over a filament greys out all theother filaments in the plot. This interaction helps in the comparison ofsymptom trajectories for the same patient, and via brushing and linkingwith the other views, in highlighting the additional patient data (T3.1).", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 492, "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "pub_year": 2022, "domain": "Life Sciences", "requirement": {"requirement_text": "Show an individual patient in the context of the cohort", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The percentile heatmap (Fig. 1.D) is a custom representation show_x0002_ing the rating distribution of individual symptoms over time, for the entire patient cohort (T2.3). We arrived at this representation after exploring a variety of alternatives such as stacked line plots, parallel coordinates plots, and radar charts, guided by feedback from collabora_x0002_tors. We settled on a matrix-based layout due to its compactness and to its ability to support small multiple plots. Each row corresponds to a symptom, with rows grouped by symptom category, and each column corresponds to a time point. Each cell in this matrix is a horizontal bar graph showing via shade the percentage of patients reporting within a specific range (0, 1-5, 6-9, or 10) for that symptom, at that time point. The bar height maps the percentage of individuals from the entire co_x0002_hort who reported the symptom ratings at that time point. The current patient is indicated in this heatmap by cross markers (Fig. 4.C) (T3.1). This encoding proved to be an intuitive way of showing what symptoms produce a higher burden for patients, and when, as well as to indicate how many patients were affected by these symptoms from the entire cohort (T1.2, T2.3)", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "matrix+bar+heatmap", "axial_code": [], "componenet_code": ["heatmap", "bar", "matrix"]}]}, {"author": "zsz", "index_original": 493, "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "pub_year": 2022, "domain": "Life Sciences", "requirement": {"requirement_text": "Display demographic and diagnostic patient data, and indicate patients with similar diagnostic attributes", "requirement_code": {"discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We first organized the symptom ratings into a patient-symptom matrix for the selected time point, where each element (i, j) corresponds to the score given to symptom j by patient i at that time point. Prior research in symptom cluster for HNC [34] had applied hierarchical clustering using Ward\u2019s method [48] with Euclidean distance on the patient-symptom matrix to group patients based on their raw symptom ratings. After alternative clustering with complete and average linkages, we found that Ward\u2019s method generated larger, more informative groups of high symptom patients, which made sense to the clinicians. We identified two patient groups with high and low symptom burden (T1.1). This two-group clustering was preferred by clinicians, who found it easier to compare two groups instead of more. The axes of the scatterplot correspond to the first two components obtained by applying PCA to the patient-symptom matrix. Clusters for a specific time point are extracted and displayed, while clusters for different timepoints can be investigated via the time slider, which will update the scatterplot.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping,DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction", "Clustering&Grouping"], "componenet_code": ["dimensionality_reduction", "clustering_and_grouping"]}, {"solution_text": "The scatterplot was customized to separately capture acute and late symptom burden distribution as identified by the symptom clusters, and to reflect via marker color, shape, and size the therapeutic combination administered to each patient, their gender, and their disease stage.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "scatter+glyph", "axial_code": [], "componenet_code": ["scatter", "glyph"]}, {"solution_text": "The data can be filtered by attributes, and filtering operations update the other views. A filtering control panel serves double duty, by also providing the plot legend. This customized scatterplot encoding effectively captured the symptom distribution across the patient population, patient outliers, and therapeutic distribution across the data.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 494, "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "pub_year": 2022, "domain": "Life Sciences", "requirement": {"requirement_text": "Display the anatomical locations affected by a symptom", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.", "data_code": {"tables": 1, "quantitative": 1}}, "solution": [{"solution_text": "Finally, because a discussion of task revealed that patients tend to point to the location of their symptoms, an anatomical sketch supports visual anchoring based on anatomy. Regions in the head and neck affected by the selected symptoms are highlighted in this sketch.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}]}, {"author": "zsz", "index_original": 495, "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "pub_year": 2022, "domain": "Life Sciences", "requirement": {"requirement_text": "Filter a patient\u2019s symptoms by association rule", "requirement_code": {"data_filtering": 1}}, "data": {"data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.", "data_code": {"tables": 1, "quantitative": 1, "textual": 1}}, "solution": [{"solution_text": "We applied ARM to each of the acute stage and the late stage, and empirically chose to illustrate the top 20 rules yielded by this approach, because only a small number of rules were of clinical interest. We chose minimum values for the support and lift metrics that were suitable for frequent and interdependent symptoms.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "From the many possible encodings of ARMs [38], we selected a node-link representation (Fig. 1.A), which was deemed by clinicians to be more friendly to broader audiences (A3), and a good fit for the rela_x0002_tively small number of nodes. Graphs are laid out using a force-directed layout algorithm based on statistical multidimensional scaling [39, 79], which results in nodes with high degree being placed centrally.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Network+bubble+text", "axial_code": [], "componenet_code": ["text", "network", "bubble"]}]}, {"author": "zsz", "index_original": 496, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.", "requirement_code": {"describe_observation_item": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"media": 1, "geometry": 1, "textual": 1}}, "solution": [{"solution_text": "A geographical analysis of factors is provided in view (D). By clicking a country on the map, the speakers belonging to the country will be highlighted, so users can analyze the regional difference between countries.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "image+map+text", "axial_code": [], "componenet_code": ["text", "map", "image"]}]}, {"author": "zsz", "index_original": 497, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.", "requirement_code": {"describe_observation_item": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27].", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "FeatureSelection"], "componenet_code": ["modeling", "feature_selection"]}, {"solution_text": "How speech script content relates to key speech delivery information is the subject of much literature on speech giving. E-script allows fine grained understanding of how multi-modal speech emotion, word speed, and pauses relate to the timing of each word of a speech.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "text+glyph", "axial_code": [], "componenet_code": ["text", "glyph"]}]}, {"author": "zsz", "index_original": 498, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.", "requirement_code": {"describe_observation_item": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "As discussed in the interview section, an open question for speech experts is the role of the categories of emotions in speeches, as well as the temporal distribution of emotions. E-type allows continuous valence and arousal data to be compared to discrete emotional type data across time in speeches. We supposed that a linear visualization might be more intuitively understandable to non-expert audiences.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "bar+line", "axial_code": [], "componenet_code": ["bar", "line"]}]}, {"author": "zsz", "index_original": 499, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.", "requirement_code": {"describe_observation_item": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "Upon selection of a factor in (A) the user is presented with many dots, with each dot positioned horizontally according to the cumulative amount of the factor in a speech. The speeches are sorted vertically by the level of the speech.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 500, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.", "requirement_code": {"describe_observation_item": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "As shown in figure, the light blue rectangle for each level covers the middle 50% distribution of the speeches and the dark blue line indicates the median of each level.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "scatter+rectangle", "axial_code": [], "componenet_code": ["scatter", "rectangle"]}]}, {"author": "zsz", "index_original": 501, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.", "requirement_code": {"describe_observation_item": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "DimensionalityReduction", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "dimensionality_reduction", "feature_selection"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "radar", "axial_code": [], "componenet_code": ["radar"]}]}, {"author": "zsz", "index_original": 502, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.", "requirement_code": {"describe_observation_item": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "We therefore created a new form of spiral that shows the emotional twists and turns in a visually dramatic way. Clockwise and counterclockwise turns in the spiral indicate shifting negative and positive emotions, with sharp angles of the visualization showing the emotional turning points. Due to the compact structure, large scale comparison is possible, supporting comparison and navigation between speeches and within a speech.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bubble", "axial_code": [], "componenet_code": ["bubble"]}]}, {"author": "zsz", "index_original": 503, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.", "requirement_code": {"describe_observation_item": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "DimensionalityReduction", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "dimensionality_reduction", "feature_selection"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}, {"solution_text": "In order to better understand this relation, a radar displays the five most significant factors, and a given speech\u2019s estimated level based on the amount of each of the factors", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "radar", "axial_code": [], "componenet_code": ["radar"]}]}, {"author": "zsz", "index_original": 504, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.", "requirement_code": {"describe_observation_item": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "We designed E-distribution in order to show how the effect of each factor changes the calculated probability of each level, which can be interpreted as a metric of effectiveness. We obtained the probability of the five levels with respect to each factor. The five lines in the graph represent the distribution of probability of the five levels of the contest, with the same color encoding used as in E-Similarity. For example for the factor arousal mean we can observe larger values to the right of E-distribution result in higher probabilities of the darker line, or final level of the contest.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "table+text+area", "axial_code": [], "componenet_code": ["area", "text", "table"]}]}, {"author": "zsz", "index_original": 505, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"media": 1, "geometry": 1, "textual": 1}}, "solution": [{"solution_text": "A geographical analysis of factors is provided in view (D). By clicking a country on the map, the speakers belonging to the country will be highlighted, so users can analyze the regional difference between countries.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "image+map+text", "axial_code": [], "componenet_code": ["text", "map", "image"]}]}, {"author": "zsz", "index_original": 506, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27].", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "FeatureSelection"], "componenet_code": ["modeling", "feature_selection"]}, {"solution_text": "How speech script content relates to key speech delivery information is the subject of much literature on speech giving. E-script allows fine grained understanding of how multi-modal speech emotion, word speed, and pauses relate to the timing of each word of a speech.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "text+glyph", "axial_code": [], "componenet_code": ["text", "glyph"]}]}, {"author": "zsz", "index_original": 507, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "As discussed in the interview section, an open question for speech experts is the role of the categories of emotions in speeches, as well as the temporal distribution of emotions. E-type allows continuous valence and arousal data to be compared to discrete emotional type data across time in speeches. We supposed that a linear visualization might be more intuitively understandable to non-expert audiences.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "bar+line", "axial_code": [], "componenet_code": ["bar", "line"]}]}, {"author": "zsz", "index_original": 508, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "Upon selection of a factor in (A) the user is presented with many dots, with each dot positioned horizontally according to the cumulative amount of the factor in a speech. The speeches are sorted vertically by the level of the speech.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 509, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "As shown in figure, the light blue rectangle for each level covers the middle 50% distribution of the speeches and the dark blue line indicates the median of each level.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "scatter+rectangle", "axial_code": [], "componenet_code": ["scatter", "rectangle"]}]}, {"author": "zsz", "index_original": 510, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "DimensionalityReduction", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "dimensionality_reduction", "feature_selection"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "radar", "axial_code": [], "componenet_code": ["radar"]}]}, {"author": "zsz", "index_original": 511, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "With the help of E-spiral, as shown in figure, we can clearly see the changes of emotion during the speech via the turning spiral. Interaction of rapidly skipping to the video frame of the selected speech by clicking the circle on the spiral supports rapid browsing with emotional cues.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bubble", "axial_code": [], "componenet_code": ["bubble"]}]}, {"author": "zsz", "index_original": 512, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27].", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "FeatureSelection"], "componenet_code": ["modeling", "feature_selection"]}, {"solution_text": "How speech script content relates to key speech delivery information is the subject of much literature on speech giving. E-script allows fine grained understanding of how multi-modal speech emotion, word speed, and pauses relate to the timing of each word of a speech.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "text+glyph", "axial_code": [], "componenet_code": ["text", "glyph"]}]}, {"author": "zsz", "index_original": 513, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "We therefore created a new form of spiral that shows the emotional twists and turns in a visually dramatic way. Clockwise and counterclockwise turns in the spiral indicate shifting negative and positive emotions, with sharp angles of the visualization showing the emotional turning points. Due to the compact structure, large scale comparison is possible, supporting comparison and navigation between speeches and within a speech.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bubble", "axial_code": [], "componenet_code": ["bubble"]}]}, {"author": "zsz", "index_original": 514, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"media": 1, "geometry": 1, "textual": 1}}, "solution": [{"solution_text": "A geographical analysis of factors is provided in view (D). By clicking a country on the map, the speakers belonging to the country will be highlighted, so users can analyze the regional difference between countries.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "image+map+text", "axial_code": [], "componenet_code": ["text", "map", "image"]}]}, {"author": "zsz", "index_original": 515, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27].", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "FeatureSelection"], "componenet_code": ["modeling", "feature_selection"]}, {"solution_text": "How speech script content relates to key speech delivery information is the subject of much literature on speech giving. E-script allows fine grained understanding of how multi-modal speech emotion, word speed, and pauses relate to the timing of each word of a speech.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "text+glyph", "axial_code": [], "componenet_code": ["text", "glyph"]}]}, {"author": "zsz", "index_original": 516, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "As discussed in the interview section, an open question for speech experts is the role of the categories of emotions in speeches, as well as the temporal distribution of emotions. E-type allows continuous valence and arousal data to be compared to discrete emotional type data across time in speeches. We supposed that a linear visualization might be more intuitively understandable to non-expert audiences.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "bar+line", "axial_code": [], "componenet_code": ["bar", "line"]}]}, {"author": "zsz", "index_original": 517, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "Upon selection of a factor in (A) the user is presented with many dots, with each dot positioned horizontally according to the cumulative amount of the factor in a speech. The speeches are sorted vertically by the level of the speech.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 518, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "With the help of E-spiral, as shown in figure, we can clearly see the changes of emotion during the speech via the turning spiral. Interaction of rapidly skipping to the video frame of the selected speech by clicking the circle on the spiral supports rapid browsing with emotional cues.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bubble", "axial_code": [], "componenet_code": ["bubble"]}]}, {"author": "zsz", "index_original": 519, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27].", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "FeatureSelection"], "componenet_code": ["modeling", "feature_selection"]}, {"solution_text": "How speech script content relates to key speech delivery information is the subject of much literature on speech giving. E-script allows fine grained understanding of how multi-modal speech emotion, word speed, and pauses relate to the timing of each word of a speech.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "text+glyph", "axial_code": [], "componenet_code": ["text", "glyph"]}]}, {"author": "zsz", "index_original": 520, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "We therefore created a new form of spiral that shows the emotional twists and turns in a visually dramatic way. Clockwise and counterclockwise turns in the spiral indicate shifting negative and positive emotions, with sharp angles of the visualization showing the emotional turning points. Due to the compact structure, large scale comparison is possible, supporting comparison and navigation between speeches and within a speech.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bubble", "axial_code": [], "componenet_code": ["bubble"]}]}, {"author": "zsz", "index_original": 521, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "DimensionalityReduction", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "dimensionality_reduction", "feature_selection"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}, {"solution_text": "In order to better understand this relation, a radar displays the five most significant factors, and a given speech\u2019s estimated level based on the amount of each of the factors", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "radar", "axial_code": [], "componenet_code": ["radar"]}]}, {"author": "zsz", "index_original": 522, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "We designed E-distribution in order to show how the effect of each factor changes the calculated probability of each level, which can be interpreted as a metric of effectiveness. We obtained the probability of the five levels with respect to each factor. The five lines in the graph represent the distribution of probability of the five levels of the contest, with the same color encoding used as in E-Similarity. For example for the factor arousal mean we can observe larger values to the right of E-distribution result in higher probabilities of the darker line, or final level of the contest.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "table+text+area", "axial_code": [], "componenet_code": ["area", "text", "table"]}]}, {"author": "zsz", "index_original": 523, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"media": 1, "geometry": 1, "textual": 1}}, "solution": [{"solution_text": "A geographical analysis of factors is provided in view (D). By clicking a country on the map, the speakers belonging to the country will be highlighted, so users can analyze the regional difference between countries.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "image+map+text", "axial_code": [], "componenet_code": ["text", "map", "image"]}]}, {"author": "zsz", "index_original": 524, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27].", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "FeatureSelection"], "componenet_code": ["modeling", "feature_selection"]}, {"solution_text": "How speech script content relates to key speech delivery information is the subject of much literature on speech giving. E-script allows fine grained understanding of how multi-modal speech emotion, word speed, and pauses relate to the timing of each word of a speech.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "text+glyph", "axial_code": [], "componenet_code": ["text", "glyph"]}]}, {"author": "zsz", "index_original": 525, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "As discussed in the interview section, an open question for speech experts is the role of the categories of emotions in speeches, as well as the temporal distribution of emotions. E-type allows continuous valence and arousal data to be compared to discrete emotional type data across time in speeches. We supposed that a linear visualization might be more intuitively understandable to non-expert audiences.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "bar+line", "axial_code": [], "componenet_code": ["bar", "line"]}]}, {"author": "zsz", "index_original": 526, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "As shown in figure, the light blue rectangle for each level covers the middle 50% distribution of the speeches and the dark blue line indicates the median of each level.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "scatter+rectangle", "axial_code": [], "componenet_code": ["scatter", "rectangle"]}]}, {"author": "zsz", "index_original": 527, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "DimensionalityReduction", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "dimensionality_reduction", "feature_selection"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "radar", "axial_code": [], "componenet_code": ["radar"]}]}, {"author": "zsz", "index_original": 528, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "DimensionalityReduction", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "dimensionality_reduction", "feature_selection"]}, {"solution_text": "According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches. To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. The closer two speeches are to each other, the more similar the two speeches are.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}]}, {"author": "zsz", "index_original": 529, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "We therefore created a new form of spiral that shows the emotional twists and turns in a visually dramatic way. Clockwise and counterclockwise turns in the spiral indicate shifting negative and positive emotions, with sharp angles of the visualization showing the emotional turning points. Due to the compact structure, large scale comparison is possible, supporting comparison and navigation between speeches and within a speech.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bubble", "axial_code": [], "componenet_code": ["bubble"]}]}, {"author": "zsz", "index_original": 530, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "DimensionalityReduction", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "dimensionality_reduction", "feature_selection"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}, {"solution_text": "In order to better understand this relation, a radar displays the five most significant factors, and a given speech\u2019s estimated level based on the amount of each of the factors", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "radar", "axial_code": [], "componenet_code": ["radar"]}]}, {"author": "zsz", "index_original": 531, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "We designed E-distribution in order to show how the effect of each factor changes the calculated probability of each level, which can be interpreted as a metric of effectiveness. We obtained the probability of the five levels with respect to each factor. The five lines in the graph represent the distribution of probability of the five levels of the contest, with the same color encoding used as in E-Similarity. For example for the factor arousal mean we can observe larger values to the right of E-distribution result in higher probabilities of the darker line, or final level of the contest.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "table+text+area", "axial_code": [], "componenet_code": ["area", "text", "table"]}]}, {"author": "zsz", "index_original": 532, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"media": 1, "geometry": 1, "textual": 1}}, "solution": [{"solution_text": "A geographical analysis of factors is provided in view (D). By clicking a country on the map, the speakers belonging to the country will be highlighted, so users can analyze the regional difference between countries.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "image+map+text", "axial_code": [], "componenet_code": ["text", "map", "image"]}]}, {"author": "zsz", "index_original": 533, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27].", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "FeatureSelection"], "componenet_code": ["modeling", "feature_selection"]}, {"solution_text": "How speech script content relates to key speech delivery information is the subject of much literature on speech giving. E-script allows fine grained understanding of how multi-modal speech emotion, word speed, and pauses relate to the timing of each word of a speech.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "text+glyph", "axial_code": [], "componenet_code": ["text", "glyph"]}]}, {"author": "zsz", "index_original": 534, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "As discussed in the interview section, an open question for speech experts is the role of the categories of emotions in speeches, as well as the temporal distribution of emotions. E-type allows continuous valence and arousal data to be compared to discrete emotional type data across time in speeches. We supposed that a linear visualization might be more intuitively understandable to non-expert audiences.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "bar+line", "axial_code": [], "componenet_code": ["bar", "line"]}]}, {"author": "zsz", "index_original": 535, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "As shown in figure, the light blue rectangle for each level covers the middle 50% distribution of the speeches and the dark blue line indicates the median of each level.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "scatter+rectangle", "axial_code": [], "componenet_code": ["scatter", "rectangle"]}]}, {"author": "zsz", "index_original": 536, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "DimensionalityReduction", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "dimensionality_reduction", "feature_selection"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "radar", "axial_code": [], "componenet_code": ["radar"]}]}, {"author": "zsz", "index_original": 537, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "DimensionalityReduction", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "dimensionality_reduction", "feature_selection"]}, {"solution_text": "According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches. To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. The closer two speeches are to each other, the more similar the two speeches are.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}]}, {"author": "zsz", "index_original": 538, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "With the help of E-spiral, as shown in figure, we can clearly see the changes of emotion during the speech via the turning spiral. Interaction of rapidly skipping to the video frame of the selected speech by clicking the circle on the spiral supports rapid browsing with emotional cues.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bubble", "axial_code": [], "componenet_code": ["bubble"]}]}, {"author": "zsz", "index_original": 539, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27].", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling", "solution_compoent": "", "axial_code": ["Modeling", "FeatureSelection"], "componenet_code": ["modeling", "feature_selection"]}, {"solution_text": "How speech script content relates to key speech delivery information is the subject of much literature on speech giving. E-script allows fine grained understanding of how multi-modal speech emotion, word speed, and pauses relate to the timing of each word of a speech.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "text+glyph", "axial_code": [], "componenet_code": ["text", "glyph"]}]}, {"author": "zsz", "index_original": 540, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "We therefore created a new form of spiral that shows the emotional twists and turns in a visually dramatic way. Clockwise and counterclockwise turns in the spiral indicate shifting negative and positive emotions, with sharp angles of the visualization showing the emotional turning points. Due to the compact structure, large scale comparison is possible, supporting comparison and navigation between speeches and within a speech.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bubble", "axial_code": [], "componenet_code": ["bubble"]}]}, {"author": "zsz", "index_original": 541, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "DimensionalityReduction", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "dimensionality_reduction", "feature_selection"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}, {"solution_text": "In order to better understand this relation, a radar displays the five most significant factors, and a given speech\u2019s estimated level based on the amount of each of the factors", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "radar", "axial_code": [], "componenet_code": ["radar"]}]}, {"author": "zsz", "index_original": 542, "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches", "pub_year": 2022, "domain": "speaking", "requirement": {"requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;", "data_code": {"sequential": 1, "textual": 1, "temporal": 1, "media": 1}}, "solution": [{"solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["Modeling", "AlgorithmicCalculation", "FeatureSelection"], "componenet_code": ["modeling", "algorithmic_calculation", "feature_selection"]}, {"solution_text": "We designed E-distribution in order to show how the effect of each factor changes the calculated probability of each level, which can be interpreted as a metric of effectiveness. We obtained the probability of the five levels with respect to each factor. The five lines in the graph represent the distribution of probability of the five levels of the contest, with the same color encoding used as in E-Similarity. For example for the factor arousal mean we can observe larger values to the right of E-distribution result in higher probabilities of the darker line, or final level of the contest.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "table+text+area", "axial_code": [], "componenet_code": ["area", "text", "table"]}]}, {"author": "zsz", "index_original": 544, "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy", "pub_year": 2022, "domain": "Bitcoin", "requirement": {"requirement_text": "AF1: Bitcoin regulation and the evolution of mining pools. An excessive concentration of pool market shares is dangerous to Bitcoin\u2019s operations and raises questions about potential mining regulations. The economist wanted to analyze factors that pose a risk to pool concentra_x0002_tion and detect critical periods in Bitcoin mining. Periods characterized by significant variations of bitcoin values (e.g., halving days, peak in bitcoin value) impact mining pools and should be analyzed. Further more, external data such as news about Bitcoin might give insights to better understand the impact of regulatory changes in some countries.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "D1: The mining pool distribution dataset gives information about the evolution of mining pools over time. It was derived from identifying mining pools that received the reward for each individual block in the Bitcoin blockchain. We extended the initial dataset compiled by Romiti et al. [49] (data up to: 2018\u201312) to extract the mining pool that mined each block. Blocks that could not be attributed to any known mining pool were marked as \u201cunknown\u201d. Our final dataset at the time of writing includes data until 2020-09 and contains the mining power of each mining pool for each month. We computed multiple quantitative measures related to mining power: hash rate, market share (normalized hash rate), and total reward received from mining (in BTC and USD). Additionally, we calculated the estimated electricity consumption of each mining pool based on the CBECI index [4] to allow to judge the externality of mining on the environment.", "data_code": {"sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Visual Encoding: We chose a temporal ribbon chart that allows analysts to see both the changing rank and a quantitative metric related to each mining pool. The ribbon chart is filtered to the time period selected in V1 and users can choose among multiple statistical measures and how to group and color the pools. The chart\u2019s multiple stacked bars sorted by rank show data aggregated and displayed by months. By default mining pools are sorted from the highest value at the top of the stack to the lowest one at the bottom per month. The same mining pool is connected across months with a ribbon to highlight its rank changing. We encode the top-10 mining pools for the selected time in distinct colors, while the remaining pools are colored in grey. Fig. 2 shows examples of different configurations of the ribbon chart. Analysts can select the mining power measure, e .g., market share, hash rate, total reward, and transaction fees. They can also chose mining pool characteristics to display in different color hue scales, i .e., the name of the pool, its payout scheme, and location. Using the coloring mechanism, analysts can see patterns for the characteristics of the top mining pool. Furthermore, analysts can group those mining pools by the same characteristics to see if any characteristics correlate with the mining growth and domination in the market. Within each group, mining pools are sorted by the selected measure.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "sankey", "axial_code": [], "componenet_code": ["sankey"]}, {"solution_text": "Interaction: Besides the measures, group-by, and color-by selectors, we provide two ways to highlight mining pools or their characteristics. First, analysts can click on the left side labels which will increase transparency of the unselected pools and consequently highlight those that fall into the selection. The selection also affects and filters the mining pools displayed on the mining pool details view (V3). Second, analysts can draw a brush on the ribbon chart to filter both mining pools and highlight a specific time range. The highlighted timeframe is also represented in views V3\u2013V6. Finally, like in all other views, detail-on_x0002_demand is available on hover via tooltips that show the mining pool name and exact value of the selected measure for this pool.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 545, "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy", "pub_year": 2022, "domain": "Bitcoin", "requirement": {"requirement_text": "AF1: Bitcoin regulation and the evolution of mining pools. An excessive concentration of pool market shares is dangerous to Bitcoin\u2019s operations and raises questions about potential mining regulations. The economist wanted to analyze factors that pose a risk to pool concentra_x0002_tion and detect critical periods in Bitcoin mining. Periods characterized by significant variations of bitcoin values (e.g., halving days, peak in bitcoin value) impact mining pools and should be analyzed. Further more, external data such as news about Bitcoin might give insights to better understand the impact of regulatory changes in some countries.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "D1: The mining pool distribution dataset gives information about the evolution of mining pools over time. It was derived from identifying mining pools that received the reward for each individual block in the Bitcoin blockchain. We extended the initial dataset compiled by Romiti et al. [49] (data up to: 2018\u201312) to extract the mining pool that mined each block. Blocks that could not be attributed to any known mining pool were marked as \u201cunknown\u201d. Our final dataset at the time of writing includes data until 2020-09 and contains the mining power of each mining pool for each month. We computed multiple quantitative measures related to mining power: hash rate, market share (normalized hash rate), and total reward received from mining (in BTC and USD). Additionally, we calculated the estimated electricity consumption of each mining pool based on the CBECI index [4] to allow to judge the externality of mining on the environment; D2: The mining pool characteristics dataset includes external data about mining pools that we obtained from public sources [6, 7] and manually cleaned. The attributes include the primary location of the pool (nominal), payout scheme (nominal), pool fee (quantitative), and whether the pool kept transaction fees or shared it to miners (binary). We also extracted the wiki history to track attribute changes as mining pools adapt their payout policies to compete with other pools; D3: The Bitcoin network statistics dataset contains multiple quan_x0002_titative measurements of Bitcoin network\u2019s state over time. We col_x0002_lected daily statistics from Blockchain.info [10] and averaged those measures to a monthly time window. Examples of network statistics for each month include the market price, total hash rate, total block reward, total transaction fees, mining difficulty, the number of transactions, electricity consumption [4], and the global energy price index [27].", "data_code": {"sequential": 1, "temporal": 1, "geometry": 1, "categorical": 1, "tables": 1}}, "solution": [{"solution_text": "Visual Encoding: The time series shows a selected network statis_x0002_tic measure for the entire history of Bitcoin mining. By default, the timeline shows the mining pool concentration index as an indication to which extent the mining pool distribution risks being dominated by just a few pools [60]. Analysts can select other Bitcoin statistics measures (e.g., total reward, market price, mining difficulty) from a dropdown menu and switch to a log scale. The time series highlights halving days as important events related to mining rewards.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}, {"solution_text": "Interaction: Analysts can filter all remaining views (V2\u2013V6) to a specific time interval by brushing on the time axis or specifying a range with the calendar inputs. The selection will then trigger the other views to filter the information to the specified time interval.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 546, "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy", "pub_year": 2022, "domain": "Bitcoin", "requirement": {"requirement_text": "AF1: Bitcoin regulation and the evolution of mining pools. An excessive concentration of pool market shares is dangerous to Bitcoin\u2019s operations and raises questions about potential mining regulations. The economist wanted to analyze factors that pose a risk to pool concentra_x0002_tion and detect critical periods in Bitcoin mining. Periods characterized by significant variations of bitcoin values (e.g., halving days, peak in bitcoin value) impact mining pools and should be analyzed. Further more, external data such as news about Bitcoin might give insights to better understand the impact of regulatory changes in some countries.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "D1: The mining pool distribution dataset gives information about the evolution of mining pools over time. It was derived from identifying mining pools that received the reward for each individual block in the Bitcoin blockchain. We extended the initial dataset compiled by Romiti et al. [49] (data up to: 2018\u201312) to extract the mining pool that mined each block. Blocks that could not be attributed to any known mining pool were marked as \u201cunknown\u201d. Our final dataset at the time of writing includes data until 2020-09 and contains the mining power of each mining pool for each month. We computed multiple quantitative measures related to mining power: hash rate, market share (normalized hash rate), and total reward received from mining (in BTC and USD). Additionally, we calculated the estimated electricity consumption of each mining pool based on the CBECI index [4] to allow to judge the externality of mining on the environment; D2: The mining pool characteristics dataset includes external data about mining pools that we obtained from public sources [6, 7] and manually cleaned. The attributes include the primary location of the pool (nominal), payout scheme (nominal), pool fee (quantitative), and whether the pool kept transaction fees or shared it to miners (binary). We also extracted the wiki history to track attribute changes as mining pools adapt their payout policies to compete with other pools; D3: The Bitcoin network statistics dataset contains multiple quan_x0002_titative measurements of Bitcoin network\u2019s state over time. We col_x0002_lected daily statistics from Blockchain.info [10] and averaged those measures to a monthly time window. Examples of network statistics for each month include the market price, total hash rate, total block reward, total transaction fees, mining difficulty, the number of transactions, electricity consumption [4], and the global energy price index [27].", "data_code": {"sequential": 1, "temporal": 1, "geometry": 1, "categorical": 1, "tables": 1}}, "solution": [{"solution_text": "Visual Encoding: We chose a temporal ribbon chart that allows analysts to see both the changing rank and a quantitative metric related to each mining pool. The ribbon chart is filtered to the time period selected in V1 and users can choose among multiple statistical measures and how to group and color the pools. The chart\u2019s multiple stacked bars sorted by rank show data aggregated and displayed by months. By default mining pools are sorted from the highest value at the top of the stack to the lowest one at the bottom per month. The same mining pool is connected across months with a ribbon to highlight its rank changing. We encode the top-10 mining pools for the selected time in distinct colors, while the remaining pools are colored in grey. Fig. 2 shows examples of different configurations of the ribbon chart. Analysts can select the mining power measure, e .g., market share, hash rate, total reward, and transaction fees. They can also chose mining pool characteristics to display in different color hue scales, i .e., the name of the pool, its payout scheme, and location. Using the coloring mechanism, analysts can see patterns for the characteristics of the top mining pool. Furthermore, analysts can group those mining pools by the same characteristics to see if any characteristics correlate with the mining growth and domination in the market. Within each group, mining pools are sorted by the selected measure.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "sankey", "axial_code": [], "componenet_code": ["sankey"]}, {"solution_text": "Interaction: Besides the measures, group-by, and color-by selectors, we provide two ways to highlight mining pools or their characteristics. First, analysts can click on the left side labels which will increase transparency of the unselected pools and consequently highlight those that fall into the selection. The selection also affects and filters the mining pools displayed on the mining pool details view (V3). Second, analysts can draw a brush on the ribbon chart to filter both mining pools and highlight a specific time range. The highlighted timeframe is also represented in views V3\u2013V6. Finally, like in all other views, detail-on_x0002_demand is available on hover via tooltips that show the mining pool name and exact value of the selected measure for this pool.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 547, "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy", "pub_year": 2022, "domain": "Bitcoin", "requirement": {"requirement_text": "AF1: Bitcoin regulation and the evolution of mining pools. An excessive concentration of pool market shares is dangerous to Bitcoin\u2019s operations and raises questions about potential mining regulations. The economist wanted to analyze factors that pose a risk to pool concentra_x0002_tion and detect critical periods in Bitcoin mining. Periods characterized by significant variations of bitcoin values (e.g., halving days, peak in bitcoin value) impact mining pools and should be analyzed. Further more, external data such as news about Bitcoin might give insights to better understand the impact of regulatory changes in some countries.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "D2: The mining pool characteristics dataset includes external data about mining pools that we obtained from public sources [6, 7] and manually cleaned. The attributes include the primary location of the pool (nominal), payout scheme (nominal), pool fee (quantitative), and whether the pool kept transaction fees or shared it to miners (binary). We also extracted the wiki history to track attribute changes as mining pools adapt their payout policies to compete with other pools.", "data_code": {"geometry": 1, "categorical": 1, "tables": 1}}, "solution": [{"solution_text": "Visual Encoding: The Bitcoin statistics view (Fig. 4) is located in the same position as V3 reachable via a toggle bar. We encode 17 different factors each as a gray temporal area chart including Bitcoin_x0002_internal statistics such as the total number of blocks mined, the total hash rate, the median confirmation time for a block, total rewards paid out, or the total transaction fees. In addition, we calculated statistics with external data such as fees converted to USD according to the current market price or the trade volume in USD. As such, the list of Bitcoin statistics provides information about the status of the Bitcoin network. For example, the number of transactions implies the demand of users; the amount of Bitcoins in transactions means the supply of currency circulating in the market; and the average waiting time indicates the network capacity to verify transactions.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area", "axial_code": [], "componenet_code": ["area"]}, {"solution_text": "Interaction: The view offers details-on-demand via a tooltip.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 548, "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy", "pub_year": 2022, "domain": "Bitcoin", "requirement": {"requirement_text": "AF1: Bitcoin regulation and the evolution of mining pools. An excessive concentration of pool market shares is dangerous to Bitcoin\u2019s operations and raises questions about potential mining regulations. The economist wanted to analyze factors that pose a risk to pool concentra_x0002_tion and detect critical periods in Bitcoin mining. Periods characterized by significant variations of bitcoin values (e.g., halving days, peak in bitcoin value) impact mining pools and should be analyzed. Further more, external data such as news about Bitcoin might give insights to better understand the impact of regulatory changes in some countries.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "D4: The Bitcoin news dataset lists headlines from the Press forum in Bitcointalk.org [8], where users posted links to news articles related to Bitcoin. Each news contains the date when the news was published by its source, the news headline, and the number of replies and views. We chose to use the number of views and replies as an attractiveness indicator of the news from the Bitcointalk\u2019s members. We derived an importance score of each news items as views\u00d7(replies+1).", "data_code": {"categorical": 1, "textual": 1, "temporal": 1}}, "solution": [{"solution_text": "The news we collected covered various Bitcoin-related topics. To group related articles, we used the Word Network Topic Model method [71] that proved to be simple and effective for short texts. We extracted 15 topics and the top-10 keywords for each topic.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Visual Encoding: We use a swarm plot to display each individual news item in a compact fashion across a timeline. It trades off accurate position across the timeline for an overlap-free layout. Each circle represents one news article and its size corresponds to the calculated importance score. The larger the size, the more frequently the posting was read or commented on. We use color to indicate topic membership. The news topics are encoded in different colors, and the list of keywords listed on the left. The numbers behind the topic label indicate the number of news shown in the swam plot versus the total news.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "bubble+bar", "axial_code": [], "componenet_code": ["bar", "bubble"]}, {"solution_text": "Interaction: We demonstrate an example use case for the view in Fig. 5. Analysts can browse the news by hovering circles to see tooltips with news headlines on the top-left of the chart. When the analyst clicks on the circle, it will open a new tab to the news source. On the top panel, the list of news can also be narrowed down by keyword search. Analysts can use a slider to specify the number of news showing in the chart. The panel also shows the number of total news and the percentage of news that the chart currently displays. Finally, analysts can use the topics panel on the left to filter news by topic.", "solution_category": "interaction", "solution_axial": "OverviewandExplore,Filtering", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Filtering"], "componenet_code": ["overview_and_explore", "filtering"]}]}, {"author": "zsz", "index_original": 549, "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy", "pub_year": 2022, "domain": "Bitcoin", "requirement": {"requirement_text": "AF2: Pool managers\u2019 behavior and the competition to attract miners. Mining pools have different payout schemes [50] that define which and how much miners are paid. Payouts are reduced by pool fees [5] a pool may keep to pay for operating costs. We would expect that pools with lower fees pay more constant income to miners for a given payout scheme. These pools may therefore attract miners and exhibit increasing mining power. Pool competition might be visible by convergence toward a limited number of (best) payout schemes and a decrease in pool fees. Such information is crucial for economic debates around pool viability as these fees correspond to a significant part of their income. An analysis of the data might provide additional evidence of the importance of payout schemes and pool fees in attracting miners.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "D1: The mining pool distribution dataset gives information about the evolution of mining pools over time. It was derived from identifying mining pools that received the reward for each individual block in the Bitcoin blockchain. We extended the initial dataset compiled by Romiti et al. [49] (data up to: 2018\u201312) to extract the mining pool that mined each block. Blocks that could not be attributed to any known mining pool were marked as \u201cunknown\u201d. Our final dataset at the time of writing includes data until 2020-09 and contains the mining power of each mining pool for each month. We computed multiple quantitative measures related to mining power: hash rate, market share (normalized hash rate), and total reward received from mining (in BTC and USD). Additionally, we calculated the estimated electricity consumption of each mining pool based on the CBECI index [4] to allow to judge the externality of mining on the environment; D3: The Bitcoin network statistics dataset contains multiple quan_x0002_titative measurements of Bitcoin network\u2019s state over time. We col_x0002_lected daily statistics from Blockchain.info [10] and averaged those measures to a monthly time window. Examples of network statistics for each month include the market price, total hash rate, total block reward, total transaction fees, mining difficulty, the number of transactions, electricity consumption [4], and the global energy price index [27].", "data_code": {"sequential": 1, "temporal": 1, "tables": 1, "categorical": 1}}, "solution": [{"solution_text": "Visual Encoding: We chose a temporal ribbon chart that allows analysts to see both the changing rank and a quantitative metric related to each mining pool. The ribbon chart is filtered to the time period selected in V1 and users can choose among multiple statistical measures and how to group and color the pools. The chart\u2019s multiple stacked bars sorted by rank show data aggregated and displayed by months. By default mining pools are sorted from the highest value at the top of the stack to the lowest one at the bottom per month. The same mining pool is connected across months with a ribbon to highlight its rank changing. We encode the top-10 mining pools for the selected time in distinct colors, while the remaining pools are colored in grey. Fig. 2 shows examples of different configurations of the ribbon chart. Analysts can select the mining power measure, e .g., market share, hash rate, total reward, and transaction fees. They can also chose mining pool characteristics to display in different color hue scales, i .e., the name of the pool, its payout scheme, and location. Using the coloring mechanism, analysts can see patterns for the characteristics of the top mining pool. Furthermore, analysts can group those mining pools by the same characteristics to see if any characteristics correlate with the mining growth and domination in the market. Within each group, mining pools are sorted by the selected measure.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "sankey", "axial_code": [], "componenet_code": ["sankey"]}, {"solution_text": "Interaction: Besides the measures, group-by, and color-by selectors, we provide two ways to highlight mining pools or their characteristics. First, analysts can click on the left side labels which will increase transparency of the unselected pools and consequently highlight those that fall into the selection. The selection also affects and filters the mining pools displayed on the mining pool details view (V3). Second, analysts can draw a brush on the ribbon chart to filter both mining pools and highlight a specific time range. The highlighted timeframe is also represented in views V3\u2013V6. Finally, like in all other views, detail-on_x0002_demand is available on hover via tooltips that show the mining pool name and exact value of the selected measure for this pool.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 550, "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy", "pub_year": 2022, "domain": "Bitcoin", "requirement": {"requirement_text": "AF2: Pool managers\u2019 behavior and the competition to attract miners. Mining pools have different payout schemes [50] that define which and how much miners are paid. Payouts are reduced by pool fees [5] a pool may keep to pay for operating costs. We would expect that pools with lower fees pay more constant income to miners for a given payout scheme. These pools may therefore attract miners and exhibit increasing mining power. Pool competition might be visible by convergence toward a limited number of (best) payout schemes and a decrease in pool fees. Such information is crucial for economic debates around pool viability as these fees correspond to a significant part of their income. An analysis of the data might provide additional evidence of the importance of payout schemes and pool fees in attracting miners.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "D1: The mining pool distribution dataset gives information about the evolution of mining pools over time. It was derived from identifying mining pools that received the reward for each individual block in the Bitcoin blockchain. We extended the initial dataset compiled by Romiti et al. [49] (data up to: 2018\u201312) to extract the mining pool that mined each block. Blocks that could not be attributed to any known mining pool were marked as \u201cunknown\u201d. Our final dataset at the time of writing includes data until 2020-09 and contains the mining power of each mining pool for each month. We computed multiple quantitative measures related to mining power: hash rate, market share (normalized hash rate), and total reward received from mining (in BTC and USD). Additionally, we calculated the estimated electricity consumption of each mining pool based on the CBECI index [4] to allow to judge the externality of mining on the environment; D3: The Bitcoin network statistics dataset contains multiple quan_x0002_titative measurements of Bitcoin network\u2019s state over time. We col_x0002_lected daily statistics from Blockchain.info [10] and averaged those measures to a monthly time window. Examples of network statistics for each month include the market price, total hash rate, total block reward, total transaction fees, mining difficulty, the number of transactions, electricity consumption [4], and the global energy price index [27].", "data_code": {"sequential": 1, "temporal": 1, "tables": 1, "categorical": 1}}, "solution": [{"solution_text": "Visual Encoding: We use a temporal bar chart to encode aggregated (per month) mining pool measures (Fig. 3 (B)). Each bar chart is nor_x0002_malized to the maximum measure of each pool instead of a maximum across all pools. This helps to see the mining power of small pools more clearly. The color of each bar corresponds to the one in V2 to help cross-comparison of these two views. In order to address T2.3, we added two additional visual encodings to the bar chart: the pool fees kept by the pool (quantitative), and whether the mining pool shares the transaction fee to its miners (nominal). Due to limited screen space we chose a dual-axis encoding instead of an additional chart and show the pool fee as a line chart (Fig. 3 (C)). As the pool fee may vary according to the payout scheme, we use different line colors for different schemes. This dual-axis allows analysts to estimate a possible correlation between pool measures and pool fees. The information about the share of the transaction fee (binary) is encoded as the background color (Fig. 3 (D)). It helps to see when a mining pool changed its policy.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "bar+line", "axial_code": [], "componenet_code": ["bar", "line"]}, {"solution_text": "Interaction: The charts in V3 are mostly controlled by selections made in V2 as they are meant as accompanying detail. Analysts can click the info icon for a text description of the pool\u2019s characteristics (Fig. 3 (A)). Additional interactions are tooltips for detail-in-demand.", "solution_category": "interaction", "solution_axial": "Filtering,OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Filtering"], "componenet_code": ["overview_and_explore", "filtering"]}]}, {"author": "zsz", "index_original": 551, "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy", "pub_year": 2022, "domain": "Bitcoin", "requirement": {"requirement_text": "AF2: Pool managers\u2019 behavior and the competition to attract miners. Mining pools have different payout schemes [50] that define which and how much miners are paid. Payouts are reduced by pool fees [5] a pool may keep to pay for operating costs. We would expect that pools with lower fees pay more constant income to miners for a given payout scheme. These pools may therefore attract miners and exhibit increasing mining power. Pool competition might be visible by convergence toward a limited number of (best) payout schemes and a decrease in pool fees. Such information is crucial for economic debates around pool viability as these fees correspond to a significant part of their income. An analysis of the data might provide additional evidence of the importance of payout schemes and pool fees in attracting miners.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "D2: The mining pool characteristics dataset includes external data about mining pools that we obtained from public sources [6, 7] and manually cleaned. The attributes include the primary location of the pool (nominal), payout scheme (nominal), pool fee (quantitative), and whether the pool kept transaction fees or shared it to miners (binary). We also extracted the wiki history to track attribute changes as mining pools adapt their payout policies to compete with other pools; D3: The Bitcoin network statistics dataset contains multiple quantitative measurements of Bitcoin network\u2019s state over time. We collected daily statistics from Blockchain.info [10] and averaged those measures to a monthly time window. Examples of network statistics for each month include the market price, total hash rate, total block reward, total transaction fees, mining difficulty, the number of transactions, electricity consumption [4], and the global energy price index [27].", "data_code": {"geometry": 1, "categorical": 1, "tables": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Visual Encoding: We chose a temporal ribbon chart that allows analysts to see both the changing rank and a quantitative metric related to each mining pool. The ribbon chart is filtered to the time period selected in V1 and users can choose among multiple statistical measures and how to group and color the pools. The chart\u2019s multiple stacked bars sorted by rank show data aggregated and displayed by months. By default mining pools are sorted from the highest value at the top of the stack to the lowest one at the bottom per month. The same mining pool is connected across months with a ribbon to highlight its rank changing. We encode the top-10 mining pools for the selected time in distinct colors, while the remaining pools are colored in grey. Fig. 2 shows examples of different configurations of the ribbon chart. Analysts can select the mining power measure, e .g., market share, hash rate, total reward, and transaction fees. They can also chose mining pool characteristics to display in different color hue scales, i .e., the name of the pool, its payout scheme, and location. Using the coloring mechanism, analysts can see patterns for the characteristics of the top mining pool. Furthermore, analysts can group those mining pools by the same characteristics to see if any characteristics correlate with the mining growth and domination in the market. Within each group, mining pools are sorted by the selected measure.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "sankey", "axial_code": [], "componenet_code": ["sankey"]}, {"solution_text": "Interaction: Besides the measures, group-by, and color-by selectors, we provide two ways to highlight mining pools or their characteristics. First, analysts can click on the left side labels which will increase transparency of the unselected pools and consequently highlight those that fall into the selection. The selection also affects and filters the mining pools displayed on the mining pool details view (V3). Second, analysts can draw a brush on the ribbon chart to filter both mining pools and highlight a specific time range. The highlighted timeframe is also represented in views V3\u2013V6. Finally, like in all other views, detail-on_x0002_demand is available on hover via tooltips that show the mining pool name and exact value of the selected measure for this pool.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 552, "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy", "pub_year": 2022, "domain": "Bitcoin", "requirement": {"requirement_text": "AF2: Pool managers\u2019 behavior and the competition to attract miners. Mining pools have different payout schemes [50] that define which and how much miners are paid. Payouts are reduced by pool fees [5] a pool may keep to pay for operating costs. We would expect that pools with lower fees pay more constant income to miners for a given payout scheme. These pools may therefore attract miners and exhibit increasing mining power. Pool competition might be visible by convergence toward a limited number of (best) payout schemes and a decrease in pool fees. Such information is crucial for economic debates around pool viability as these fees correspond to a significant part of their income. An analysis of the data might provide additional evidence of the importance of payout schemes and pool fees in attracting miners.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "D2: The mining pool characteristics dataset includes external data about mining pools that we obtained from public sources [6, 7] and manually cleaned. The attributes include the primary location of the pool (nominal), payout scheme (nominal), pool fee (quantitative), and whether the pool kept transaction fees or shared it to miners (binary). We also extracted the wiki history to track attribute changes as mining pools adapt their payout policies to compete with other pools; D3: The Bitcoin network statistics dataset contains multiple quantitative measurements of Bitcoin network\u2019s state over time. We collected daily statistics from Blockchain.info [10] and averaged those measures to a monthly time window. Examples of network statistics for each month include the market price, total hash rate, total block reward, total transaction fees, mining difficulty, the number of transactions, electricity consumption [4], and the global energy price index [27].", "data_code": {"geometry": 1, "categorical": 1, "tables": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Visual Encoding: We use a temporal bar chart to encode aggregated (per month) mining pool measures (Fig. 3 (B)). Each bar chart is nor_x0002_malized to the maximum measure of each pool instead of a maximum across all pools. This helps to see the mining power of small pools more clearly. The color of each bar corresponds to the one in V2 to help cross-comparison of these two views. In order to address T2.3, we added two additional visual encodings to the bar chart: the pool fees kept by the pool (quantitative), and whether the mining pool shares the transaction fee to its miners (nominal). Due to limited screen space we chose a dual-axis encoding instead of an additional chart and show the pool fee as a line chart (Fig. 3 (C)). As the pool fee may vary according to the payout scheme, we use different line colors for different schemes. This dual-axis allows analysts to estimate a possible correlation between pool measures and pool fees. The information about the share of the transaction fee (binary) is encoded as the background color (Fig. 3 (D)). It helps to see when a mining pool changed its policy.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "bar+line", "axial_code": [], "componenet_code": ["bar", "line"]}, {"solution_text": "Interaction: The charts in V3 are mostly controlled by selections made in V2 as they are meant as accompanying detail. Analysts can click the info icon for a text description of the pool\u2019s characteristics (Fig. 3 (A)). Additional interactions are tooltips for detail-in-demand.", "solution_category": "interaction", "solution_axial": "Filtering,OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Filtering"], "componenet_code": ["overview_and_explore", "filtering"]}]}, {"author": "zsz", "index_original": 553, "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy", "pub_year": 2022, "domain": "Bitcoin", "requirement": {"requirement_text": "AF2: Pool managers\u2019 behavior and the competition to attract miners. Mining pools have different payout schemes [50] that define which and how much miners are paid. Payouts are reduced by pool fees [5] a pool may keep to pay for operating costs. We would expect that pools with lower fees pay more constant income to miners for a given payout scheme. These pools may therefore attract miners and exhibit increasing mining power. Pool competition might be visible by convergence toward a limited number of (best) payout schemes and a decrease in pool fees. Such information is crucial for economic debates around pool viability as these fees correspond to a significant part of their income. An analysis of the data might provide additional evidence of the importance of payout schemes and pool fees in attracting miners.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "D2: The mining pool characteristics dataset includes external data about mining pools that we obtained from public sources [6, 7] and manually cleaned. The attributes include the primary location of the pool (nominal), payout scheme (nominal), pool fee (quantitative), and whether the pool kept transaction fees or shared it to miners (binary). We also extracted the wiki history to track attribute changes as mining pools adapt their payout policies to compete with other pools; D3: The Bitcoin network statistics dataset contains multiple quantitative measurements of Bitcoin network\u2019s state over time. We collected daily statistics from Blockchain.info [10] and averaged those measures to a monthly time window. Examples of network statistics for each month include the market price, total hash rate, total block reward, total transaction fees, mining difficulty, the number of transactions, electricity consumption [4], and the global energy price index [27].", "data_code": {"geometry": 1, "categorical": 1, "tables": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Visual Encoding: The Bitcoin statistics view (Fig. 4) is located in the same position as V3 reachable via a toggle bar. We encode 17 different factors each as a gray temporal area chart including Bitcoin_x0002_internal statistics such as the total number of blocks mined, the total hash rate, the median confirmation time for a block, total rewards paid out, or the total transaction fees. In addition, we calculated statistics with external data such as fees converted to USD according to the current market price or the trade volume in USD. As such, the list of Bitcoin statistics provides information about the status of the Bitcoin network. For example, the number of transactions implies the demand of users; the amount of Bitcoins in transactions means the supply of currency circulating in the market; and the average waiting time indicates the network capacity to verify transactions.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area", "axial_code": [], "componenet_code": ["area"]}, {"solution_text": "Interaction: The view offers details-on-demand via a tooltip.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 554, "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy", "pub_year": 2022, "domain": "Bitcoin", "requirement": {"requirement_text": "AF2: Pool managers\u2019 behavior and the competition to attract miners. Mining pools have different payout schemes [50] that define which and how much miners are paid. Payouts are reduced by pool fees [5] a pool may keep to pay for operating costs. We would expect that pools with lower fees pay more constant income to miners for a given payout scheme. These pools may therefore attract miners and exhibit increasing mining power. Pool competition might be visible by convergence toward a limited number of (best) payout schemes and a decrease in pool fees. Such information is crucial for economic debates around pool viability as these fees correspond to a significant part of their income. An analysis of the data might provide additional evidence of the importance of payout schemes and pool fees in attracting miners.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "D3: The Bitcoin network statistics dataset contains multiple quan_x0002_titative measurements of Bitcoin network\u2019s state over time. We col_x0002_lected daily statistics from Blockchain.info [10] and averaged those measures to a monthly time window. Examples of network statistics for each month include the market price, total hash rate, total block reward, total transaction fees, mining difficulty, the number of transactions, electricity consumption [4], and the global energy price index [27].", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Visual Encoding: We use a temporal bar chart to encode aggregated (per month) mining pool measures (Fig. 3 (B)). Each bar chart is nor_x0002_malized to the maximum measure of each pool instead of a maximum across all pools. This helps to see the mining power of small pools more clearly. The color of each bar corresponds to the one in V2 to help cross-comparison of these two views. In order to address T2.3, we added two additional visual encodings to the bar chart: the pool fees kept by the pool (quantitative), and whether the mining pool shares the transaction fee to its miners (nominal). Due to limited screen space we chose a dual-axis encoding instead of an additional chart and show the pool fee as a line chart (Fig. 3 (C)). As the pool fee may vary according to the payout scheme, we use different line colors for different schemes. This dual-axis allows analysts to estimate a possible correlation between pool measures and pool fees. The information about the share of the transaction fee (binary) is encoded as the background color (Fig. 3 (D)). It helps to see when a mining pool changed its policy.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "bar+line", "axial_code": [], "componenet_code": ["bar", "line"]}, {"solution_text": "Interaction: The charts in V3 are mostly controlled by selections made in V2 as they are meant as accompanying detail. Analysts can click the info icon for a text description of the pool\u2019s characteristics (Fig. 3 (A)). Additional interactions are tooltips for detail-in-demand.", "solution_category": "interaction", "solution_axial": "Filtering,OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Filtering"], "componenet_code": ["overview_and_explore", "filtering"]}]}, {"author": "zsz", "index_original": 555, "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy", "pub_year": 2022, "domain": "Bitcoin", "requirement": {"requirement_text": "AF3: Miners\u2019 decision to join or leave a pool and its impact on mining pool market shares. Miners are economic agents who con_x0002_sider the cost and benefit of mining. In this respect, Bitcoin value, pay_x0002_out schemes, and pool fees are major determinants of miners\u2019 expected income. Miners\u2019 migration data (i.e., pool hopping and cross-pooling) helps to test assumptions and better understand the drivers of miners\u2019 pool choice. Such analyses are critical to understand a pool\u2019s growth or decline and to which extent miners behave as rational economic agents.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "D5: The miners\u2019 migration dataset describes miners\u2019 migration between mining pools over months. We detect miners who participate in more than one pool (cross-pooling) within a month and calculate the number of miners\u2019 addresses and the total reward those miners received from each pool. Besides, we also detect the flow of miners that join a mining pool for the first time (enter), move from a pool to another (pool hopping), and quit the mining pool (exit) between months. The process of obtaining this dataset is complex and covered in one of our previous articles focused on pool hopping [62].", "data_code": {"sequential": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "Visual Encoding: The cross-pooling miners during a selected time interval can be considered as a flow of miners between mining pools. Fig. 5 (B) shows the visual encoding of the view. We used a chord dia_x0002_gram to display a metric related to miners crossing between pools; the total amount of miners\u2019 rewards (default) or the total number of miner addresses. The diagram shows the metric encoded as the outside arcs\u2019 length. The flow between mining pools represents the total amount of the metric for those miners who cross-pooled. We represent the average percentage of the miner\u2019s migration statistics per month (i.e., new, exit, hopping in, hopping out, cross pooling) as stacked bar charts around the outer circle. For each pool, three rows represent the percentage of miners incoming (new and hopping in), outgoing (dropout and hopping out), and cross-pooling with the mining pool, respectively.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "chord+bar", "axial_code": [], "componenet_code": ["bar", "chord"]}, {"solution_text": "Interaction: Analysts can hover over the flow or stacked bars to see the exact value. They can also change the metric from the total reward to the number of miner addresses. The total reward is a weighted average that considers the impact of large players in the pool. The measure is more robust than just the number of miners.", "solution_category": "interaction", "solution_axial": "OverviewandExplore,Filtering", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Filtering"], "componenet_code": ["overview_and_explore", "filtering"]}]}, {"author": "zsz", "index_original": 556, "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy", "pub_year": 2022, "domain": "Bitcoin", "requirement": {"requirement_text": "AF3: Miners\u2019 decision to join or leave a pool and its impact on mining pool market shares. Miners are economic agents who con_x0002_sider the cost and benefit of mining. In this respect, Bitcoin value, pay_x0002_out schemes, and pool fees are major determinants of miners\u2019 expected income. Miners\u2019 migration data (i.e., pool hopping and cross-pooling) helps to test assumptions and better understand the drivers of miners\u2019 pool choice. Such analyses are critical to understand a pool\u2019s growth or decline and to which extent miners behave as rational economic agents.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "D2: The mining pool characteristics dataset includes external data about mining pools that we obtained from public sources [6, 7] and manually cleaned. The attributes include the primary location of the pool (nominal), payout scheme (nominal), pool fee (quantitative), and whether the pool kept transaction fees or shared it to miners (binary). We also extracted the wiki history to track attribute changes as mining pools adapt their payout policies to compete with other pools; D5: The miners\u2019 migration dataset describes miners\u2019 migration between mining pools over months. We detect miners who participate in more than one pool (cross-pooling) within a month and calculate the number of miners\u2019 addresses and the total reward those miners received from each pool. Besides, we also detect the flow of miners that join a mining pool for the first time (enter), move from a pool to another (pool hopping), and quit the mining pool (exit) between months. The process of obtaining this dataset is complex and covered in one of our previous articles focused on pool hopping [62].", "data_code": {"geometry": 1, "categorical": 1, "tables": 1, "sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Visual Encoding: We use a temporal bar chart to encode aggregated (per month) mining pool measures (Fig. 3 (B)). Each bar chart is nor_x0002_malized to the maximum measure of each pool instead of a maximum across all pools. This helps to see the mining power of small pools more clearly. The color of each bar corresponds to the one in V2 to help cross-comparison of these two views. In order to address T2.3, we added two additional visual encodings to the bar chart: the pool fees kept by the pool (quantitative), and whether the mining pool shares the transaction fee to its miners (nominal). Due to limited screen space we chose a dual-axis encoding instead of an additional chart and show the pool fee as a line chart (Fig. 3 (C)). As the pool fee may vary according to the payout scheme, we use different line colors for different schemes. This dual-axis allows analysts to estimate a possible correlation between pool measures and pool fees. The information about the share of the transaction fee (binary) is encoded as the background color (Fig. 3 (D)). It helps to see when a mining pool changed its policy.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "bar+line", "axial_code": [], "componenet_code": ["bar", "line"]}, {"solution_text": "Interaction: The charts in V3 are mostly controlled by selections made in V2 as they are meant as accompanying detail. Analysts can click the info icon for a text description of the pool\u2019s characteristics (Fig. 3 (A)). Additional interactions are tooltips for detail-in-demand.", "solution_category": "interaction", "solution_axial": "Filtering,OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Filtering"], "componenet_code": ["overview_and_explore", "filtering"]}]}, {"author": "zsz", "index_original": 557, "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy", "pub_year": 2022, "domain": "Bitcoin", "requirement": {"requirement_text": "AF3: Miners\u2019 decision to join or leave a pool and its impact on mining pool market shares. Miners are economic agents who con_x0002_sider the cost and benefit of mining. In this respect, Bitcoin value, pay_x0002_out schemes, and pool fees are major determinants of miners\u2019 expected income. Miners\u2019 migration data (i.e., pool hopping and cross-pooling) helps to test assumptions and better understand the drivers of miners\u2019 pool choice. Such analyses are critical to understand a pool\u2019s growth or decline and to which extent miners behave as rational economic agents.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "D2: The mining pool characteristics dataset includes external data about mining pools that we obtained from public sources [6, 7] and manually cleaned. The attributes include the primary location of the pool (nominal), payout scheme (nominal), pool fee (quantitative), and whether the pool kept transaction fees or shared it to miners (binary). We also extracted the wiki history to track attribute changes as mining pools adapt their payout policies to compete with other pools; D5: The miners\u2019 migration dataset describes miners\u2019 migration between mining pools over months. We detect miners who participate in more than one pool (cross-pooling) within a month and calculate the number of miners\u2019 addresses and the total reward those miners received from each pool. Besides, we also detect the flow of miners that join a mining pool for the first time (enter), move from a pool to another (pool hopping), and quit the mining pool (exit) between months. The process of obtaining this dataset is complex and covered in one of our previous articles focused on pool hopping [62].", "data_code": {"geometry": 1, "categorical": 1, "tables": 1, "sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Visual Encoding: The cross-pooling miners during a selected time interval can be considered as a flow of miners between mining pools. Fig. 5 (B) shows the visual encoding of the view. We used a chord dia_x0002_gram to display a metric related to miners crossing between pools; the total amount of miners\u2019 rewards (default) or the total number of miner addresses. The diagram shows the metric encoded as the outside arcs\u2019 length. The flow between mining pools represents the total amount of the metric for those miners who cross-pooled. We represent the average percentage of the miner\u2019s migration statistics per month (i.e., new, exit, hopping in, hopping out, cross pooling) as stacked bar charts around the outer circle. For each pool, three rows represent the percentage of miners incoming (new and hopping in), outgoing (dropout and hopping out), and cross-pooling with the mining pool, respectively.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "chord+bar", "axial_code": [], "componenet_code": ["bar", "chord"]}, {"solution_text": "Interaction: Analysts can hover over the flow or stacked bars to see the exact value. They can also change the metric from the total reward to the number of miner addresses. The total reward is a weighted average that considers the impact of large players in the pool. The measure is more robust than just the number of miners.", "solution_category": "interaction", "solution_axial": "OverviewandExplore,Filtering", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Filtering"], "componenet_code": ["overview_and_explore", "filtering"]}]}, {"author": "zsz", "index_original": 558, "paper_title": "Compass: Towards Better Causal Analysis of Urban Time Series", "pub_year": 2022, "domain": "urban causality", "requirement": {"requirement_text": "R1 Summarize causal graphs across the time (WA). First, the ex-perts require the system to summarize all causal graphs detected indifferent time windows and thereby grasp the brief patterns. Forexample, which sensors have strong causal relations across thewhole period? What are the causal directions between them?", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The air pollution dataset we used comprises the hourly readings of the PM2.5 2 concentration from 448 major air quality sensors in China between January 8 and March 23, 2018. Each sensor comprises a time series with 1800 recorded values. This dataset has a total of 806,400 records and a size of 2.99 MB. We constructed a neighbor index based on the sensors\u2019 spatial proximity (Sec. 4.2.1); The traffic dataset we used was from the released Q_x0002_Traffic dataset [39]. This dataset includes subparts of the road network in Beijing and the traffic speed for every 15 minutes for every road segment. We first sampled 511 high-level roads, such as highways. Each road segment is regarded as a road sensor. We also sampled the readings from May 9, 2017 to May 18, 2017. Finally, this dataset totally comprises 490,560 records and has a size of 1.89 MB.", "data_code": {"sequential": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "First, we pre-process the data in space and time dimensions. Indexing neighbors. A sensor has causal relations only with its neighbor sensors. This neighbor relationship depends on specific ap_x0002_plication scenarios. Two concrete examples are provided below. A road sensor can be influenced only by those sensors encountered first along forward the traffic direction because traffic congestions propagate backward. An air quality sensor has causal relations only with those closest sensors around it because air pollutants spread continuously in space. For every sensor, we index its neighbors with potential causality and categorize them into downstream and upstream neighbors based on actual applications. The center sensor is denoted as an ego sensor se. The downstream and upstream sensors are denoted as Egod(se) and Egou(se), respectively. Fig. 4a illustrates Egod(sx) and Fig. 4b illustrates Egou(sy). Partitioning time. Two strategies for time partitioning can be used based on whether the time series is periodic. For periodic time series, they can be directly partitioned by their period. For example, traffic time series can be naturally partitioned by 24 hours. Otherwise, the peaks of the time series will be extracted automatically, and thereby the time windows can be identified based on these peaks. This proce_x0002_dure is accomplished using Python find peaks imported from a well known package scipy.signal [32]. The find peaks has four important parameters: height, prominence, wlen, and distance. The height and prominence limit peaks\u2019 minimum heights and prominences. The wlen indicates the maximum duration of windows. The distance means the minimum horizon distance between peaks. Given these parameters and a time series, find peaks returns the intervals where peaks are located.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Retrieval", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Retrieval"], "componenet_code": ["algorithmic_calculation", "retrieval"]}, {"solution_text": "Visualizing spatiotemporal causal graph (5f). The causal graphs across the windows can be summarized as a spatiotemporal causal graph on the map. The causal directions across multiple graphs are first aggregated according to the edge. The aggregation is then encoded with a revisited compass glyph (5e). For example, 5e summarizes the causal directions in the bottom-right edge across the three causal graphs. In a revisited compass glyph, arrows still convey the causal directions within a spatial context. The glyph is revised to incorporate the temporal information as follows. The arrow\u2019s size encodes the frequency of the causal links with this direction. An offset between the two opposite arrows can be observed if bi-directional relations exist. The overlapping part of the two arrows encodes the frequency of bi-directional relations.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "network+glyph", "axial_code": [], "componenet_code": ["network", "glyph"]}]}, {"author": "zsz", "index_original": 559, "paper_title": "Compass: Towards Better Causal Analysis of Urban Time Series", "pub_year": 2022, "domain": "urban causality", "requirement": {"requirement_text": "R2 Explore causal graphs along the time (WA). Multiple graphsin different time windows constitute a dynamic graph where thestructure can change over time [8]. The experts need to relate thecausal graphs to the time for effective time-oriented exploration [69,70,77]. They also want to learn the temporal variations of the causalstructures, such as periodicity and stability. Therefore, the systemneeds to couple a timeline-based organization with a structure-aware representation to visualize these causal graphs.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The air pollution dataset we used comprises the hourly readings of the PM2.5 2 concentration from 448 major air quality sensors in China between January 8 and March 23, 2018. Each sensor comprises a time series with 1800 recorded values. This dataset has a total of 806,400 records and a size of 2.99 MB. We constructed a neighbor index based on the sensors\u2019 spatial proximity (Sec. 4.2.1); The traffic dataset we used was from the released Q_x0002_Traffic dataset [39]. This dataset includes subparts of the road network in Beijing and the traffic speed for every 15 minutes for every road segment. We first sampled 511 high-level roads, such as highways. Each road segment is regarded as a road sensor. We also sampled the readings from May 9, 2017 to May 18, 2017. Finally, this dataset totally comprises 490,560 records and has a size of 1.89 MB.", "data_code": {"sequential": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "First, we pre-process the data in space and time dimensions. Indexing neighbors. A sensor has causal relations only with its neighbor sensors. This neighbor relationship depends on specific ap_x0002_plication scenarios. Two concrete examples are provided below. A road sensor can be influenced only by those sensors encountered first along forward the traffic direction because traffic congestions propagate backward. An air quality sensor has causal relations only with those closest sensors around it because air pollutants spread continuously in space. For every sensor, we index its neighbors with potential causality and categorize them into downstream and upstream neighbors based on actual applications. The center sensor is denoted as an ego sensor se. The downstream and upstream sensors are denoted as Egod(se) and Egou(se), respectively. Fig. 4a illustrates Egod(sx) and Fig. 4b illustrates Egou(sy). Partitioning time. Two strategies for time partitioning can be used based on whether the time series is periodic. For periodic time series, they can be directly partitioned by their period. For example, traffic time series can be naturally partitioned by 24 hours. Otherwise, the peaks of the time series will be extracted automatically, and thereby the time windows can be identified based on these peaks. This proce_x0002_dure is accomplished using Python find peaks imported from a well known package scipy.signal [32]. The find peaks has four important parameters: height, prominence, wlen, and distance. The height and prominence limit peaks\u2019 minimum heights and prominences. The wlen indicates the maximum duration of windows. The distance means the minimum horizon distance between peaks. Given these parameters and a time series, find peaks returns the intervals where peaks are located.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Retrieval", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Retrieval"], "componenet_code": ["algorithmic_calculation", "retrieval"]}, {"solution_text": "Visualizing multiple causal graphs. The system should expose the temporal variation of causalities and support the time-oriented exploration. Thus, all graph bands are placed along the time view according to the time window in which the graph is detected (5d). The bands repel each other to avoid occlusion while keeping their positions along the timeline as much as possible. To further enhance the spatial context, we add minimaps on the left side of this view. Each minimap (5d-2) contains the same ego-graph structure as in the map view but removes detailed information such as geographic background, colors, and directions. The bold edge indicates where the glyphs on the right are located. This visualization presents time-varying causal relations in an unobstructed way. Take figure as an example. The causal relations of the bottom left edge exist in all time windows. These existences may be hidden by the scattered arrows in 5c but are clearly revealed in the third row in 5d.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "network+glyph", "axial_code": [], "componenet_code": ["network", "glyph"]}]}, {"author": "zsz", "index_original": 560, "paper_title": "Compass: Towards Better Causal Analysis of Urban Time Series", "pub_year": 2022, "domain": "urban causality", "requirement": {"requirement_text": "R3 Learn influence propagation via causal graphs (WA). The ex-perts also need to drill down individual graphs and learn the influ-ence propagation during a specific period. Specifically, they aim toestablish where the stimulus is from and how it influences the urbanspace. Therefore, in the system, every individual graph should beaccessible in the spatial context.", "requirement_code": {"describe_observation_item": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "The air pollution dataset we used comprises the hourly readings of the PM2.5 2 concentration from 448 major air quality sensors in China between January 8 and March 23, 2018. Each sensor comprises a time series with 1800 recorded values. This dataset has a total of 806,400 records and a size of 2.99 MB. We constructed a neighbor index based on the sensors\u2019 spatial proximity (Sec. 4.2.1); The traffic dataset we used was from the released Q_x0002_Traffic dataset [39]. This dataset includes subparts of the road network in Beijing and the traffic speed for every 15 minutes for every road segment. We first sampled 511 high-level roads, such as highways. Each road segment is regarded as a road sensor. We also sampled the readings from May 9, 2017 to May 18, 2017. Finally, this dataset totally comprises 490,560 records and has a size of 1.89 MB.", "data_code": {"sequential": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "First, we pre-process the data in space and time dimensions. Indexing neighbors. A sensor has causal relations only with its neighbor sensors. This neighbor relationship depends on specific ap_x0002_plication scenarios. Two concrete examples are provided below. A road sensor can be influenced only by those sensors encountered first along forward the traffic direction because traffic congestions propagate backward. An air quality sensor has causal relations only with those closest sensors around it because air pollutants spread continuously in space. For every sensor, we index its neighbors with potential causality and categorize them into downstream and upstream neighbors based on actual applications. The center sensor is denoted as an ego sensor se. The downstream and upstream sensors are denoted as Egod(se) and Egou(se), respectively. Fig. 4a illustrates Egod(sx) and Fig. 4b illustrates Egou(sy). Partitioning time. Two strategies for time partitioning can be used based on whether the time series is periodic. For periodic time series, they can be directly partitioned by their period. For example, traffic time series can be naturally partitioned by 24 hours. Otherwise, the peaks of the time series will be extracted automatically, and thereby the time windows can be identified based on these peaks. This proce_x0002_dure is accomplished using Python find peaks imported from a well known package scipy.signal [32]. The find peaks has four important parameters: height, prominence, wlen, and distance. The height and prominence limit peaks\u2019 minimum heights and prominences. The wlen indicates the maximum duration of windows. The distance means the minimum horizon distance between peaks. Given these parameters and a time series, find peaks returns the intervals where peaks are located.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Retrieval", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Retrieval"], "componenet_code": ["algorithmic_calculation", "retrieval"]}, {"solution_text": "Visualizing spatial causal graph (5c-1). Each causal graph detected in a time window can be displayed on the map as a spatial causal graph. The question mark will be replaced with the arrows that denote the causal relations between sensors. Each arrow represents a causal link and points from the cause sensor to the effect sensor. The arrows are colored according to the cause sensor. The arrow opacity encodes the causal strength. Moreover, we follow a compass metaphor to place the arrows (5b). We call such a design compass glyph. Each glyph denotes a causal relation. An edge is colored red, blue, or gray according to its direction indicated by the arrows. Sensors are removed to reduce clutter if they have no causality with the ego sensor.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "network+glyph", "axial_code": [], "componenet_code": ["network", "glyph"]}]}, {"author": "zsz", "index_original": 561, "paper_title": "Compass: Towards Better Causal Analysis of Urban Time Series", "pub_year": 2022, "domain": "urban causality", "requirement": {"requirement_text": "R3 Learn influence propagation via causal graphs (WA). The ex-perts also need to drill down individual graphs and learn the influ-ence propagation during a specific period. Specifically, they aim toestablish where the stimulus is from and how it influences the urbanspace. Therefore, in the system, every individual graph should beaccessible in the spatial context.", "requirement_code": {"describe_observation_item": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "The air pollution dataset we used comprises the hourly readings of the PM2.5 2 concentration from 448 major air quality sensors in China between January 8 and March 23, 2018. Each sensor comprises a time series with 1800 recorded values. This dataset has a total of 806,400 records and a size of 2.99 MB. We constructed a neighbor index based on the sensors\u2019 spatial proximity (Sec. 4.2.1); The traffic dataset we used was from the released Q_x0002_Traffic dataset [39]. This dataset includes subparts of the road network in Beijing and the traffic speed for every 15 minutes for every road segment. We first sampled 511 high-level roads, such as highways. Each road segment is regarded as a road sensor. We also sampled the readings from May 9, 2017 to May 18, 2017. Finally, this dataset totally comprises 490,560 records and has a size of 1.89 MB.", "data_code": {"sequential": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "First, we pre-process the data in space and time dimensions. Indexing neighbors. A sensor has causal relations only with its neighbor sensors. This neighbor relationship depends on specific ap_x0002_plication scenarios. Two concrete examples are provided below. A road sensor can be influenced only by those sensors encountered first along forward the traffic direction because traffic congestions propagate backward. An air quality sensor has causal relations only with those closest sensors around it because air pollutants spread continuously in space. For every sensor, we index its neighbors with potential causality and categorize them into downstream and upstream neighbors based on actual applications. The center sensor is denoted as an ego sensor se. The downstream and upstream sensors are denoted as Egod(se) and Egou(se), respectively. Fig. 4a illustrates Egod(sx) and Fig. 4b illustrates Egou(sy). Partitioning time. Two strategies for time partitioning can be used based on whether the time series is periodic. For periodic time series, they can be directly partitioned by their period. For example, traffic time series can be naturally partitioned by 24 hours. Otherwise, the peaks of the time series will be extracted automatically, and thereby the time windows can be identified based on these peaks. This proce_x0002_dure is accomplished using Python find peaks imported from a well known package scipy.signal [32]. The find peaks has four important parameters: height, prominence, wlen, and distance. The height and prominence limit peaks\u2019 minimum heights and prominences. The wlen indicates the maximum duration of windows. The distance means the minimum horizon distance between peaks. Given these parameters and a time series, find peaks returns the intervals where peaks are located.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Retrieval", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Retrieval"], "componenet_code": ["algorithmic_calculation", "retrieval"]}, {"solution_text": "Visualizing single causal graph (R3). Simply presenting spatialcausal graphs results in low scalability because the maps are spaceineffective (Fig. 5c). To this end, we compact each causal graph (e.g.,Fig. 5c-1) into a graph band (e.g., Fig. 5d-1). The compass glyphsfrom top to bottom in a graph band correspond to that of the edgesin a clockwise order starting from the west direction. These compassglyphs preserve the spatial context for each causal relation.To further integrate the spatial context of causal graphs, heatmap-based spatial summaries for every causal graph are provided and placedabove the bands (Fig. 1c-1). The heatmap is generated from the spatialdistribution of the influences of the ego and neighbor sensors. Thedarker red (or blue) indicates that the area receives more influencesfrom the ego sensor (or the neighbor sensors). To do so, we firstgenerate a red heatmap based on the spatial distribution of the redcausal links. A blue heatmap is generated in the same way. We subtractthe blue heatmap from the red heatmap and finally derive the summary.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}]}, {"author": "zsz", "index_original": 562, "paper_title": "Compass: Towards Better Causal Analysis of Urban Time Series", "pub_year": 2022, "domain": "urban causality", "requirement": {"requirement_text": "R4 Interpret and validate causal relations (WA, WI). Causal inter-pretation and validation are required. The experts hope to interpretwhy there are causalities between sensors and learn what the ef-fects are based on the involved time series. Furthermore, validationquestions may be asked, for instance, are causal relations includ-ing their directions and time lags reasonable? So, causal relationsmust be fully encoded regarding the occurrence frequency, involvedtime series, and comprised multidimensional attributes.", "requirement_code": {"identify_main_cause_aggregate": 1}}, "data": {"data_text": "The air pollution dataset we used comprises the hourly readings of the PM2.5 2 concentration from 448 major air quality sensors in China between January 8 and March 23, 2018. Each sensor comprises a time series with 1800 recorded values. This dataset has a total of 806,400 records and a size of 2.99 MB. We constructed a neighbor index based on the sensors\u2019 spatial proximity (Sec. 4.2.1); The traffic dataset we used was from the released Q_x0002_Traffic dataset [39]. This dataset includes subparts of the road network in Beijing and the traffic speed for every 15 minutes for every road segment. We first sampled 511 high-level roads, such as highways. Each road segment is regarded as a road sensor. We also sampled the readings from May 9, 2017 to May 18, 2017. Finally, this dataset totally comprises 490,560 records and has a size of 1.89 MB.", "data_code": {"sequential": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "First, we pre-process the data in space and time dimensions. Indexing neighbors. A sensor has causal relations only with its neighbor sensors. This neighbor relationship depends on specific ap_x0002_plication scenarios. Two concrete examples are provided below. A road sensor can be influenced only by those sensors encountered first along forward the traffic direction because traffic congestions propagate backward. An air quality sensor has causal relations only with those closest sensors around it because air pollutants spread continuously in space. For every sensor, we index its neighbors with potential causality and categorize them into downstream and upstream neighbors based on actual applications. The center sensor is denoted as an ego sensor se. The downstream and upstream sensors are denoted as Egod(se) and Egou(se), respectively. Fig. 4a illustrates Egod(sx) and Fig. 4b illustrates Egou(sy). Partitioning time. Two strategies for time partitioning can be used based on whether the time series is periodic. For periodic time series, they can be directly partitioned by their period. For example, traffic time series can be naturally partitioned by 24 hours. Otherwise, the peaks of the time series will be extracted automatically, and thereby the time windows can be identified based on these peaks. This proce_x0002_dure is accomplished using Python find peaks imported from a well known package scipy.signal [32]. The find peaks has four important parameters: height, prominence, wlen, and distance. The height and prominence limit peaks\u2019 minimum heights and prominences. The wlen indicates the maximum duration of windows. The distance means the minimum horizon distance between peaks. Given these parameters and a time series, find peaks returns the intervals where peaks are located.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Retrieval", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Retrieval"], "componenet_code": ["algorithmic_calculation", "retrieval"]}, {"solution_text": "Visualizing the causal relations of a graph. The graphs need to be unfolded to interpret and validate the causal relations further. Figure shows an unfolded graph band. First, the compass glyphs are shrunk, leaving a ring space to encode the lags with arcs. The arc behind an arrow corresponds to the causal link denoted by the arrow and has the same color as the arrow. The arc length encodes the lag of the causal link. Second, a canvas expands from the right. For every causal relation, the two involved time series are sliced according to the time window and displayed together in the same line chart on the glyph\u2019s right side. The lines are also colored according to the sensor. Users can easily compare the time series of all sensors involved in the graph because these glyphs are aligned vertically in a graph band.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph+line", "axial_code": [], "componenet_code": ["line", "glyph"]}]}, {"author": "zsz", "index_original": 563, "paper_title": "Compass: Towards Better Causal Analysis of Urban Time Series", "pub_year": 2022, "domain": "urban causality", "requirement": {"requirement_text": "R4 Interpret and validate causal relations (WA, WI). Causal inter-pretation and validation are required. The experts hope to interpretwhy there are causalities between sensors and learn what the ef-fects are based on the involved time series. Furthermore, validationquestions may be asked, for instance, are causal relations includ-ing their directions and time lags reasonable? So, causal relationsmust be fully encoded regarding the occurrence frequency, involvedtime series, and comprised multidimensional attributes.", "requirement_code": {"identify_main_cause_aggregate": 1}}, "data": {"data_text": "The air pollution dataset we used comprises the hourly readings of the PM2.5 2 concentration from 448 major air quality sensors in China between January 8 and March 23, 2018. Each sensor comprises a time series with 1800 recorded values. This dataset has a total of 806,400 records and a size of 2.99 MB. We constructed a neighbor index based on the sensors\u2019 spatial proximity (Sec. 4.2.1); The traffic dataset we used was from the released Q_x0002_Traffic dataset [39]. This dataset includes subparts of the road network in Beijing and the traffic speed for every 15 minutes for every road segment. We first sampled 511 high-level roads, such as highways. Each road segment is regarded as a road sensor. We also sampled the readings from May 9, 2017 to May 18, 2017. Finally, this dataset totally comprises 490,560 records and has a size of 1.89 MB.", "data_code": {"sequential": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "First, we pre-process the data in space and time dimensions. Indexing neighbors. A sensor has causal relations only with its neighbor sensors. This neighbor relationship depends on specific ap_x0002_plication scenarios. Two concrete examples are provided below. A road sensor can be influenced only by those sensors encountered first along forward the traffic direction because traffic congestions propagate backward. An air quality sensor has causal relations only with those closest sensors around it because air pollutants spread continuously in space. For every sensor, we index its neighbors with potential causality and categorize them into downstream and upstream neighbors based on actual applications. The center sensor is denoted as an ego sensor se. The downstream and upstream sensors are denoted as Egod(se) and Egou(se), respectively. Fig. 4a illustrates Egod(sx) and Fig. 4b illustrates Egou(sy). Partitioning time. Two strategies for time partitioning can be used based on whether the time series is periodic. For periodic time series, they can be directly partitioned by their period. For example, traffic time series can be naturally partitioned by 24 hours. Otherwise, the peaks of the time series will be extracted automatically, and thereby the time windows can be identified based on these peaks. This proce_x0002_dure is accomplished using Python find peaks imported from a well known package scipy.signal [32]. The find peaks has four important parameters: height, prominence, wlen, and distance. The height and prominence limit peaks\u2019 minimum heights and prominences. The wlen indicates the maximum duration of windows. The distance means the minimum horizon distance between peaks. Given these parameters and a time series, find peaks returns the intervals where peaks are located.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Retrieval", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Retrieval"], "componenet_code": ["algorithmic_calculation", "retrieval"]}, {"solution_text": "The relation view shows the multidimensional details of causal relations. Causality suspiciousness is visually encoded to help identify and correct suspicious causal links and relations.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Network+bubble", "axial_code": [], "componenet_code": ["network", "bubble"]}]}, {"author": "zsz", "index_original": 564, "paper_title": "Compass: Towards Better Causal Analysis of Urban Time Series", "pub_year": 2022, "domain": "urban causality", "requirement": {"requirement_text": "R5 Modify incorrect causal relations (WI). The experts commentedthat detection results are not completely reliable. Causal interpre-tation and validation can help identify incorrect causal relations.Afterward, the experts require modifying them interactively.", "requirement_code": {"ensuring_data_quality": 1}}, "data": {"data_text": "The air pollution dataset we used comprises the hourly readings of the PM2.5 2 concentration from 448 major air quality sensors in China between January 8 and March 23, 2018. Each sensor comprises a time series with 1800 recorded values. This dataset has a total of 806,400 records and a size of 2.99 MB. We constructed a neighbor index based on the sensors\u2019 spatial proximity (Sec. 4.2.1); The traffic dataset we used was from the released Q_x0002_Traffic dataset [39]. This dataset includes subparts of the road network in Beijing and the traffic speed for every 15 minutes for every road segment. We first sampled 511 high-level roads, such as highways. Each road segment is regarded as a road sensor. We also sampled the readings from May 9, 2017 to May 18, 2017. Finally, this dataset totally comprises 490,560 records and has a size of 1.89 MB.", "data_code": {"sequential": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "First, we pre-process the data in space and time dimensions. Indexing neighbors. A sensor has causal relations only with its neighbor sensors. This neighbor relationship depends on specific ap_x0002_plication scenarios. Two concrete examples are provided below. A road sensor can be influenced only by those sensors encountered first along forward the traffic direction because traffic congestions propagate backward. An air quality sensor has causal relations only with those closest sensors around it because air pollutants spread continuously in space. For every sensor, we index its neighbors with potential causality and categorize them into downstream and upstream neighbors based on actual applications. The center sensor is denoted as an ego sensor se. The downstream and upstream sensors are denoted as Egod(se) and Egou(se), respectively. Fig. 4a illustrates Egod(sx) and Fig. 4b illustrates Egou(sy). Partitioning time. Two strategies for time partitioning can be used based on whether the time series is periodic. For periodic time series, they can be directly partitioned by their period. For example, traffic time series can be naturally partitioned by 24 hours. Otherwise, the peaks of the time series will be extracted automatically, and thereby the time windows can be identified based on these peaks. This proce_x0002_dure is accomplished using Python find peaks imported from a well known package scipy.signal [32]. The find peaks has four important parameters: height, prominence, wlen, and distance. The height and prominence limit peaks\u2019 minimum heights and prominences. The wlen indicates the maximum duration of windows. The distance means the minimum horizon distance between peaks. Given these parameters and a time series, find peaks returns the intervals where peaks are located.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation,Retrieval", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Retrieval"], "componenet_code": ["algorithmic_calculation", "retrieval"]}, {"solution_text": "The relation view shows the multidimensional details of causal relations. Causality suspiciousness is visually encoded to help identify and correct suspicious causal links and relations.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Network+bubble", "axial_code": [], "componenet_code": ["network", "bubble"]}]}, {"author": "zsz", "index_original": 565, "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse", "pub_year": 2022, "domain": "E-Commerce", "requirement": {"requirement_text": "M1 Facilitate the inspection of delayed orders. Orders that havebeen processed or hung up for a long time should be preliminarilyhighlighted for users to ef\ufb01ciently locate the possibly abnormalprocedure and delayed orders. Thus, the proposed system shouldbe able to highlight the possible delayed orders. Furthermore,the highlighted orders need to be more evident for a progressivewarning based on more cost duration.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.", "data_code": {"sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The monitoring view is motivated by the requirements for real-time delay detection and unpredictable data stream. This view presents the processing status of incoming online orders updated in real time.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "zsz", "index_original": 566, "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse", "pub_year": 2022, "domain": "E-Commerce", "requirement": {"requirement_text": "M1 Facilitate the inspection of delayed orders. Orders that havebeen processed or hung up for a long time should be preliminarilyhighlighted for users to ef\ufb01ciently locate the possibly abnormalprocedure and delayed orders. Thus, the proposed system shouldbe able to highlight the possible delayed orders. Furthermore,the highlighted orders need to be more evident for a progressivewarning based on more cost duration.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.", "data_code": {"sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The slots are set based on integer multiples of the threshold, which increase from left to right. There is a boundary between these slots, indicating the border of normal and delayed processed orders. Within each slot, the darkness of color encodes the time cost and the width of the bar represents the number of orders in the slot.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}]}, {"author": "zsz", "index_original": 567, "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse", "pub_year": 2022, "domain": "E-Commerce", "requirement": {"requirement_text": "M2 Scale with the unpredictable volumes of incoming orders.The system should be capable of an efficient scalability to presentthe incoming data, due to the uncertain volumes of streaming data.Moreover, the presentations of real-time data change need smoothtransitions to ensure readability.", "requirement_code": {"flexibility_and_scalability": 1}}, "data": {"data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.", "data_code": {"sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The monitoring view is motivated by the requirements for real-time delay detection and unpredictable data stream. This view presents the processing status of incoming online orders updated in real time.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "zsz", "index_original": 568, "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse", "pub_year": 2022, "domain": "E-Commerce", "requirement": {"requirement_text": "M3 Present parallel operation status. As a part of order processingdata in an e-commerce warehouse, the status of parallel taskoperators is crucial for managers to know the operation ef\ufb01ciencyand engagement. Thus, it is necessary to provide a design topresent these parallel task data in the system.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.", "data_code": {"sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Considering that the order-processing data have a hierarchical struc-ture (M3) and parallel tasks (M4), we propose a tailored design forthese data features. Specially, for such procedures as Picking & Sortingand Packing, extra visual designs are integrated into the order sedimen-tation metaphor. In the Picking & Sorting procedure, the falling objectschange from individual orders to picking lists because the processed ob-jects have changed. Within this procedure, multiple tokens are attachedto represent orders in one picking list (Fig. 3 (d)). The attached designillustrates the hierarchical structure of aggregated individual orders andthe picking list item. As for the Packing & Weighting procedure, wepresent parallel tasks in the parallel drop charts (Fig. 3 (e)). Each dropchart represents one individual working operator or workstation, withinwhich order items fall and accumulate independently. In the proceduresof Preprocessing, Aggregating, and Weighting, the processed timesare nearly zero because operations in these procedures are executedby the computer system. Therefore, we only leave processed layersin these procedures. In the Outbound procedure, the processed ordersare outbound, such that no waiting or delay issues occur, which makesweighting the last procedure in the monitoring view.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}]}, {"author": "zsz", "index_original": 569, "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse", "pub_year": 2022, "domain": "E-Commerce", "requirement": {"requirement_text": "M4 Support the visualization of hierarchical order processingdata. To support a comprehensive presentation of the order pro-cessing data content, such as the picking list, the system shouldconsider the hierarchical data structures and enable the real-timepresentations of these data, including individual orders and pick-ing lists.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.", "data_code": {"sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Considering that the order-processing data have a hierarchical struc-ture (M3) and parallel tasks (M4), we propose a tailored design forthese data features. Specially, for such procedures as Picking & Sortingand Packing, extra visual designs are integrated into the order sedimen-tation metaphor. In the Picking & Sorting procedure, the falling objectschange from individual orders to picking lists because the processed ob-jects have changed. Within this procedure, multiple tokens are attachedto represent orders in one picking list (Fig. 3 (d)). The attached designillustrates the hierarchical structure of aggregated individual orders andthe picking list item. As for the Packing & Weighting procedure, wepresent parallel tasks in the parallel drop charts (Fig. 3 (e)). Each dropchart represents one individual working operator or workstation, withinwhich order items fall and accumulate independently. In the proceduresof Preprocessing, Aggregating, and Weighting, the processed timesare nearly zero because operations in these procedures are executedby the computer system. Therefore, we only leave processed layersin these procedures. In the Outbound procedure, the processed ordersare outbound, such that no waiting or delay issues occur, which makesweighting the last procedure in the monitoring view.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}]}, {"author": "zsz", "index_original": 570, "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse", "pub_year": 2022, "domain": "E-Commerce", "requirement": {"requirement_text": "A1 Visualize the historical processing records of orders. Bychecking the processing timeline, users can judge whether anorder is delayed from the comprehensive historical overview,including the time cost on processing and waiting, and the rela-tionship of picking lists with the aggregated orders. Therefore,the system should support the visualization of these historicalrecords for in-depth analysis.", "requirement_code": {"collect_evidence": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.", "data_code": {"sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The goal of the analyzing view is to allow users to check the historical processing record of delayed orders detected by time thresholds. This kind of checking enables users to know the source of delayed orders to solve the delay problems efficiently.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "gantt+marey\u2019sgraph", "axial_code": [], "componenet_code": ["line", "bar"]}]}, {"author": "zsz", "index_original": 571, "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse", "pub_year": 2022, "domain": "E-Commerce", "requirement": {"requirement_text": "A1 Visualize the historical processing records of orders. Bychecking the processing timeline, users can judge whether anorder is delayed from the comprehensive historical overview,including the time cost on processing and waiting, and the rela-tionship of picking lists with the aggregated orders. Therefore,the system should support the visualization of these historicalrecords for in-depth analysis.", "requirement_code": {"collect_evidence": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.", "data_code": {"sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Our final design, named order processing lifeline, is built on thecombination of Marey\u2019s graph and a Gantt chart (Fig. 4 (d)), with theadvantages of both. In each procedure, the horizontal axis encodestime, which increases from left to right. We employ a componentconsisting of a Gantt chart unit visualizing the processed time and aMarey\u2019s graph unit presenting the blocked time (A1). In the Gantt chart unit, we make each row represent parallel operators or workstations,where individual processing tasks are shown in a temporal sequence.However, this way is unscalable when many operators exist. Also, we\ufb01nd real-world data are sparse among individual operators. Therefore,we provide an overview of the parallel task status at the same timeby compressing the Gantt chart units in the dimension of individualoperators (Fig. 4 (c)). The overview is obtained through a dynamicprogramming algorithm, which makes the rectangles in the Gantt chartarranged in as few rows as possible. Through the overview, users canaccess the parallel task density in a speci\ufb01c time.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "gantt+marey\u2019sgraph", "axial_code": [], "componenet_code": ["line", "bar"]}]}, {"author": "zsz", "index_original": 572, "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse", "pub_year": 2022, "domain": "E-Commerce", "requirement": {"requirement_text": "A2 Provide related information about processing history. Re-lated information is necessary for analysis. For example, par-allel task density can assist warehouse managers in accessing theprocessing load degree. Statistical information such as each pro-cedures\u2019 time cost distribution can also make users aware of thenormal and abnormal time cost. The system should also provide\ufb02exible interactions such as selecting or \ufb01ltering speci\ufb01c ordersfor further analysis and priority evaluations.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.", "data_code": {"sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The goal of the analyzing view is to allow users to check the historical processing record of delayed orders detected by time thresholds. This kind of checking enables users to know the source of delayed orders to solve the delay problems efficiently.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "gantt+marey\u2019sgraph", "axial_code": [], "componenet_code": ["line", "bar"]}]}, {"author": "zsz", "index_original": 573, "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse", "pub_year": 2022, "domain": "E-Commerce", "requirement": {"requirement_text": "E1 Present handling priority ranking. Several factors decide thehandling priorities. For example, orders close to the delivery dead-line have a high priority to be handled. Thus, the system shouldintegrate a ranking method considering the trade-off between allkinds of delay-handling factors. Moreover, users should be al-lowed to adjust the weight of these factors for a proper rankingresult under practical real-time conditions.", "requirement_code": {"discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.", "data_code": {"sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "To help users decide which delayed orders should be handled first, we provide an evaluating view. In this view, users can set a series of weights on the factors that determine the dealing priority of delayed orders and access details about each order, including operator information and goods details. Through the above information, users can identify whether orders should be handled.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "area+bar+table", "axial_code": [], "componenet_code": ["bar", "table", "area"]}]}, {"author": "zsz", "index_original": 574, "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse", "pub_year": 2022, "domain": "E-Commerce", "requirement": {"requirement_text": "E2 Show order details. To facilitate the further exploration of se-lected orders, users need to know the details of the orders, suchas the goods SKUs, goods attributes, and order ids. So the systemshould present the detail information on users\u2019 demand.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.", "data_code": {"sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The evaluating view also contains detail panels that show detailedinformation about selected orders in the ranking component (E2). Bydiscussing with the domain experts, we select some key attributes thatthey concern about, including the SKUs, quantity, fragility, and retailerof the goods (Fig. 5 (f1-f4)). For example, if an order contains fragilegoods, the operators will do more packing operations than those for theorders with other goods. Moreover, within each order, a bar chart showsthe time distribution of the processed procedures (Fig. 5 (f5)). Fromthe details presented, managers can further judge the delaying issues onthe selected orders. Moreover, this panel also supports users to untagsome wrongly detected orders on demand (E3) through a button (Fig. 5(e1)). The untagged orders will be removed from the ranking list.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+bar", "axial_code": [], "componenet_code": ["bar", "table"]}, {"solution_text": "Moreover, this panel also supports users to untagsome wrongly detected orders on demand (E3) through a button (Fig. 5(e1)). The untagged orders will be removed from the ranking list.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 575, "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse", "pub_year": 2022, "domain": "E-Commerce", "requirement": {"requirement_text": "E2 Show order details. To facilitate the further exploration of se-lected orders, users need to know the details of the orders, suchas the goods SKUs, goods attributes, and order ids. So the systemshould present the detail information on users\u2019 demand.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.", "data_code": {"sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "To help users decide which delayed orders should be handled first, we provide an evaluating view. In this view, users can set a series of weights on the factors that determine the dealing priority of delayed orders and access details about each order, including operator information and goods details. Through the above information, users can identify whether orders should be handled.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "area+bar+table", "axial_code": [], "componenet_code": ["bar", "table", "area"]}]}, {"author": "zsz", "index_original": 576, "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse", "pub_year": 2022, "domain": "E-Commerce", "requirement": {"requirement_text": "E3 Enable interactive identi\ufb01cation of delayed orders. The initialdelay detection results according to the time interval thresholdsare rough, within which, experts may \ufb01nd several incorrectly de-tected delayed orders or potential delayed orders based on visualexploration and their domain knowledge, Under this condition,the system should support smooth user interactions to identifyand tag delayed orders.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.", "data_code": {"sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The evaluating view also contains detail panels that show detailedinformation about selected orders in the ranking component (E2). Bydiscussing with the domain experts, we select some key attributes thatthey concern about, including the SKUs, quantity, fragility, and retailerof the goods (Fig. 5 (f1-f4)). For example, if an order contains fragilegoods, the operators will do more packing operations than those for theorders with other goods. Moreover, within each order, a bar chart showsthe time distribution of the processed procedures (Fig. 5 (f5)). Fromthe details presented, managers can further judge the delaying issues onthe selected orders. Moreover, this panel also supports users to untagsome wrongly detected orders on demand (E3) through a button (Fig. 5(e1)). The untagged orders will be removed from the ranking list.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+bar", "axial_code": [], "componenet_code": ["bar", "table"]}, {"solution_text": "Moreover, this panel also supports users to untagsome wrongly detected orders on demand (E3) through a button (Fig. 5(e1)). The untagged orders will be removed from the ranking list.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 577, "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse", "pub_year": 2022, "domain": "E-Commerce", "requirement": {"requirement_text": "E3 Enable interactive identi\ufb01cation of delayed orders. The initialdelay detection results according to the time interval thresholdsare rough, within which, experts may \ufb01nd several incorrectly de-tected delayed orders or potential delayed orders based on visualexploration and their domain knowledge, Under this condition,the system should support smooth user interactions to identifyand tag delayed orders.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.", "data_code": {"sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "To help users decide which delayed orders should be handled first, we provide an evaluating view. In this view, users can set a series of weights on the factors that determine the dealing priority of delayed orders and access details about each order, including operator information and goods details. Through the above information, users can identify whether orders should be handled.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "area+bar+table", "axial_code": [], "componenet_code": ["bar", "table", "area"]}]}, {"author": "zsz", "index_original": 578, "paper_title": "TIVEE: Visual Exploration and Explanation of Badminton Tactics in Immersive Visualizations", "pub_year": 2022, "domain": "sports", "requirement": {"requirement_text": "R1: Summarizing the usage of tactics. Badminton tactics are di-verse, and a player will use various tactics in a match. Experts areinterested in analyzing a player\u2019s commonly used tactics, which helps to outline his/her playing style. In addition, knowing thescoring rates of different tactics helps to deepen the understand-ing of the player\u2019s strength and provides clear guidance for thefollowing analysis.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The data of each badminton game is provided as hundreds of strokes. Each stroke has a variety of attributes. The detailed explanation of each stroke attribute is as follows: Splayer: The player giving the stroke. Stechnique: The technique used by the player to give the stroke. According to kinematic features, stroke techniques can be di_x0002_vided into three categories [60]: 1) offensive technique (including smash, net shot, and cut smash), 2) control technique (including clear, drop, chop, push, hook shot, and drive), and 3) defensive technique (including lob and block). Tplayer: The 2D movement trajectory of the player. For each stroke, we record two 2D positions of the player \u2014 the one when the opponent hit the shuttle at the previous stroke (Pstart) and the one when he/she hit the shuttle at the current stroke (Pend) \u2014 to reveal his/her movement to return the shuttle. Tshuttle: The 3D flying trajectory of the shuttle. For each stroke, we record three key points of the shuttle\u2019s flying trajectory \u2014 the start position (Pstart), the highest position (Phighest), and the end position (Pend) \u2014 to reconstruct the whole trajectory based on the shuttle\u2019s kinematic features [41]. Tfield: The fields where the shuttle comes from and falls into. Instead of analyzing the exact 3D positions, experts tend to divide the 3D court space into multiple fields and analyze the fields of the start/end position of the shuttle [37]. Following experts\u2019 requirements, we divide a half-court into 3\u00d72\u00d73 fields (Fig. 3). According to the distance to the net, the court can be divided into fore-court, middle-court, and back-court. The player can hit the shuttle in their forehand area or backhand area. The height can be divided into three levels: low-space (0-1.55m), middle-space (1.55-2.5m), and high-space (2.5-4m). According to the category of the technique used in the last stroke, we further consider three types of tactics: the offensive tactics, the control tactics, and the defensive tactics [58].", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Hence, the number of tactics could be large and it is hard to present all the tactics in the VR environment. To address this issue, we aggregate similar tactics into groups to reduce the number of visual items. Two tactics will be aggregated into one group if they have the same sequence of Stechnique.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The tactic overview is provided to show the corresponding statistical data of badminton tactics and the relation between consecutive stroke sequences in tactics (R1, R2). Users are able to explore the scene in a third-person perspective with flying navigation mode. Considering the effectiveness of small multiples in VR environments [30], We design a multi-court layout (Fig. 5) to visually summarize the tactics and prevent the issue of visual overwhelming when the tactic number grows. Each column in the layout (Fig. 5) represents a tactic group that consists of multiple similar tactics. Each row (Fig. 5) in a group shows the strokes of tactics in this group using a virtual court. We rank the tactic group from left to right (Fig. 5) according to the tactic score (i.e., a summarization of the usage rate and the scoring rate of each tactic group). This can help users focus on the analysis of important tactics. Visualization of a tactic group. According to Figure 2, a tactic is characterized by a sequence of consecutive strokes. Hence, the number of tactics could be large and it is hard to present all the tactics in the VR environment. To address this issue, we aggregate similar tactics into groups to reduce the number of visual items. Two tactics will be aggregated into one group if they have the same sequence of Stechnique. In each tactic group, we use 2n\u22121 courts to visualize the strokes of tactics as well as the reaction strokes of opponents where n is the tactic length (n = 2 in Fig. 5). The first stroke of tactics is placed at the top court and the last stroke is placed at the bottom court (Fig. 5). Each court serves as a small multiple in three dimensions that displays the shuttle trajectories with the same stroke ordinal in one tactic. The thickness of strokes encodes the usage rate of the corresponding tactic (Rusage) and the color encodes the scoring rate (Rscoring) of the corresponding tactic. Rusage is computed as the ratio of the number of rallies with this tactic to the number of all rallies. Rscoring is computed as the ratio of the number of winning rallies with this tactic to the number of all rallies with this tactic. We use a discrete palette to encode the scoring rate of the strokes (Fig. 1a). Similar to Rusage and Rscoring, we further compute the usage rate and the scoring rate of each tactic group and use waffle charts at the top-back of each tactic group (Fig. 1a) to encode the two indicators. The left waffle chart shows the usage rate while the right one shows the scoring rate.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}, {"solution_text": "Interactions in the tactic overview are as follows. \u2022 Changing the viewing angle of small multiples. Experts can use the trackpad of the controller to get close to the tactic they are interested in or fly far away to observe as many tactics as possible at the same time. \u2022 Adjusting the order of small multiples. Experts can click the virtual function menu to see the slider of the weight of usage rates and scoring rates. They can further use raycasting to change the weights to recompute the tactic score and the order of small multiples (tactic groups) will be changed accordingly. \u2022 Decomposing a tactic group. Experts can use the trackpad to select a tactic group and decompose the tactic group into multiple subgroups according to the hit point and the drop point. For example, when using the hit point to decompose the tactic group, the tactic overview will be updated to show the subgroups and strokes in a subgroup will share the same field of the hit point.", "solution_category": "interaction", "solution_axial": "OverviewandExplore,Participation/Collaboration", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Participation/Collaboration"], "componenet_code": ["overview_and_explore", "participation_collaboration"]}]}, {"author": "zsz", "index_original": 579, "paper_title": "TIVEE: Visual Exploration and Explanation of Badminton Tactics in Immersive Visualizations", "pub_year": 2022, "domain": "sports", "requirement": {"requirement_text": "R2: Presenting the similarity between tactics. Although tactics aredifferent in terms of the kinematic features, such as the 3D po-sition of the hit point and the technique used, a set of tacticswould be considered as similar to each other due to the similartactical aim (e.g., playing defensive tactics to wait for mistakes).Showing a category of tactics can help the experts learn differentoffensive/defensive tactics and conduct a detailed analysis.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The data of each badminton game is provided as hundreds of strokes. Each stroke has a variety of attributes. The detailed explanation of each stroke attribute is as follows: Splayer: The player giving the stroke. Stechnique: The technique used by the player to give the stroke. According to kinematic features, stroke techniques can be di_x0002_vided into three categories [60]: 1) offensive technique (including smash, net shot, and cut smash), 2) control technique (including clear, drop, chop, push, hook shot, and drive), and 3) defensive technique (including lob and block). Tplayer: The 2D movement trajectory of the player. For each stroke, we record two 2D positions of the player \u2014 the one when the opponent hit the shuttle at the previous stroke (Pstart) and the one when he/she hit the shuttle at the current stroke (Pend) \u2014 to reveal his/her movement to return the shuttle. Tshuttle: The 3D flying trajectory of the shuttle. For each stroke, we record three key points of the shuttle\u2019s flying trajectory \u2014 the start position (Pstart), the highest position (Phighest), and the end position (Pend) \u2014 to reconstruct the whole trajectory based on the shuttle\u2019s kinematic features [41]. Tfield: The fields where the shuttle comes from and falls into. Instead of analyzing the exact 3D positions, experts tend to divide the 3D court space into multiple fields and analyze the fields of the start/end position of the shuttle [37]. Following experts\u2019 requirements, we divide a half-court into 3\u00d72\u00d73 fields (Fig. 3). According to the distance to the net, the court can be divided into fore-court, middle-court, and back-court. The player can hit the shuttle in their forehand area or backhand area. The height can be divided into three levels: low-space (0-1.55m), middle-space (1.55-2.5m), and high-space (2.5-4m). According to the category of the technique used in the last stroke, we further consider three types of tactics: the offensive tactics, the control tactics, and the defensive tactics [58].", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Hence, the number of tactics could be large and it is hard to present all the tactics in the VR environment. To address this issue, we aggregate similar tactics into groups to reduce the number of visual items. Two tactics will be aggregated into one group if they have the same sequence of Stechnique.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The tactic overview is provided to show the corresponding statistical data of badminton tactics and the relation between consecutive stroke sequences in tactics (R1, R2). Users are able to explore the scene in a third-person perspective with flying navigation mode. Considering the effectiveness of small multiples in VR environments [30], We design a multi-court layout (Fig. 5) to visually summarize the tactics and prevent the issue of visual overwhelming when the tactic number grows. Each column in the layout (Fig. 5) represents a tactic group that consists of multiple similar tactics. Each row (Fig. 5) in a group shows the strokes of tactics in this group using a virtual court. We rank the tactic group from left to right (Fig. 5) according to the tactic score (i.e., a summarization of the usage rate and the scoring rate of each tactic group). This can help users focus on the analysis of important tactics. Visualization of a tactic group. According to Figure 2, a tactic is characterized by a sequence of consecutive strokes. Hence, the number of tactics could be large and it is hard to present all the tactics in the VR environment. To address this issue, we aggregate similar tactics into groups to reduce the number of visual items. Two tactics will be aggregated into one group if they have the same sequence of Stechnique. In each tactic group, we use 2n\u22121 courts to visualize the strokes of tactics as well as the reaction strokes of opponents where n is the tactic length (n = 2 in Fig. 5). The first stroke of tactics is placed at the top court and the last stroke is placed at the bottom court (Fig. 5). Each court serves as a small multiple in three dimensions that displays the shuttle trajectories with the same stroke ordinal in one tactic. The thickness of strokes encodes the usage rate of the corresponding tactic (Rusage) and the color encodes the scoring rate (Rscoring) of the corresponding tactic. Rusage is computed as the ratio of the number of rallies with this tactic to the number of all rallies. Rscoring is computed as the ratio of the number of winning rallies with this tactic to the number of all rallies with this tactic. We use a discrete palette to encode the scoring rate of the strokes (Fig. 1a). Similar to Rusage and Rscoring, we further compute the usage rate and the scoring rate of each tactic group and use waffle charts at the top-back of each tactic group (Fig. 1a) to encode the two indicators. The left waffle chart shows the usage rate while the right one shows the scoring rate.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}, {"solution_text": "Interactions in the tactic overview are as follows. \u2022 Changing the viewing angle of small multiples. Experts can use the trackpad of the controller to get close to the tactic they are interested in or fly far away to observe as many tactics as possible at the same time. \u2022 Adjusting the order of small multiples. Experts can click the virtual function menu to see the slider of the weight of usage rates and scoring rates. They can further use raycasting to change the weights to recompute the tactic score and the order of small multiples (tactic groups) will be changed accordingly. \u2022 Decomposing a tactic group. Experts can use the trackpad to select a tactic group and decompose the tactic group into multiple subgroups according to the hit point and the drop point. For example, when using the hit point to decompose the tactic group, the tactic overview will be updated to show the subgroups and strokes in a subgroup will share the same field of the hit point.", "solution_category": "interaction", "solution_axial": "OverviewandExplore,Participation/Collaboration", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Participation/Collaboration"], "componenet_code": ["overview_and_explore", "participation_collaboration"]}]}, {"author": "zsz", "index_original": 580, "paper_title": "TIVEE: Visual Exploration and Explanation of Badminton Tactics in Immersive Visualizations", "pub_year": 2022, "domain": "sports", "requirement": {"requirement_text": "R3: Identifying the relation between tactics and game situations.After obtaining an overall picture of a player\u2019s tactics, the expertsneed to know how the player uses different tactics to cope withdifferent game situations. Correctly applying a tactic in an ap-propriate situation is the key to increase the winning rate. Hence,knowing the usage of tactics under different situations can helpplayers identify valuable usages and existing weaknesses.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "The data of each badminton game is provided as hundreds of strokes. Each stroke has a variety of attributes. The detailed explanation of each stroke attribute is as follows: Splayer: The player giving the stroke. Stechnique: The technique used by the player to give the stroke. According to kinematic features, stroke techniques can be di_x0002_vided into three categories [60]: 1) offensive technique (including smash, net shot, and cut smash), 2) control technique (including clear, drop, chop, push, hook shot, and drive), and 3) defensive technique (including lob and block). Tplayer: The 2D movement trajectory of the player. For each stroke, we record two 2D positions of the player \u2014 the one when the opponent hit the shuttle at the previous stroke (Pstart) and the one when he/she hit the shuttle at the current stroke (Pend) \u2014 to reveal his/her movement to return the shuttle. Tshuttle: The 3D flying trajectory of the shuttle. For each stroke, we record three key points of the shuttle\u2019s flying trajectory \u2014 the start position (Pstart), the highest position (Phighest), and the end position (Pend) \u2014 to reconstruct the whole trajectory based on the shuttle\u2019s kinematic features [41]. Tfield: The fields where the shuttle comes from and falls into. Instead of analyzing the exact 3D positions, experts tend to divide the 3D court space into multiple fields and analyze the fields of the start/end position of the shuttle [37]. Following experts\u2019 requirements, we divide a half-court into 3\u00d72\u00d73 fields (Fig. 3). According to the distance to the net, the court can be divided into fore-court, middle-court, and back-court. The player can hit the shuttle in their forehand area or backhand area. The height can be divided into three levels: low-space (0-1.55m), middle-space (1.55-2.5m), and high-space (2.5-4m). According to the category of the technique used in the last stroke, we further consider three types of tactics: the offensive tactics, the control tactics, and the defensive tactics [58].", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "In addition to having an overview of general tactics, the expert wants to identify tactics that the player commonly uses in specific game situations to better tailor strategies. Users can use the virtual menu to jump to the tactic customization and set the context game scenario.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 581, "paper_title": "TIVEE: Visual Exploration and Explanation of Badminton Tactics in Immersive Visualizations", "pub_year": 2022, "domain": "sports", "requirement": {"requirement_text": "R4: Revealing the characteristics of a tactic. When focusing on an-alyzing a specific tactic, experts will develop an interest in howthe player performs such a tactic with strokes. Different executionstyles can reflect the athletic or physiological characteristics ofthe player; for example, different players may have various dis-tributions of height at the hit point when performing the tactic oflob-smash due to height issues. Revealing these details can helpexperts understand the efficient way of performing a tactic andestablish corresponding coping tactics.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The data of each badminton game is provided as hundreds of strokes. Each stroke has a variety of attributes. The detailed explanation of each stroke attribute is as follows: Splayer: The player giving the stroke. Stechnique: The technique used by the player to give the stroke. According to kinematic features, stroke techniques can be di_x0002_vided into three categories [60]: 1) offensive technique (including smash, net shot, and cut smash), 2) control technique (including clear, drop, chop, push, hook shot, and drive), and 3) defensive technique (including lob and block). Tplayer: The 2D movement trajectory of the player. For each stroke, we record two 2D positions of the player \u2014 the one when the opponent hit the shuttle at the previous stroke (Pstart) and the one when he/she hit the shuttle at the current stroke (Pend) \u2014 to reveal his/her movement to return the shuttle. Tshuttle: The 3D flying trajectory of the shuttle. For each stroke, we record three key points of the shuttle\u2019s flying trajectory \u2014 the start position (Pstart), the highest position (Phighest), and the end position (Pend) \u2014 to reconstruct the whole trajectory based on the shuttle\u2019s kinematic features [41]. Tfield: The fields where the shuttle comes from and falls into. Instead of analyzing the exact 3D positions, experts tend to divide the 3D court space into multiple fields and analyze the fields of the start/end position of the shuttle [37]. Following experts\u2019 requirements, we divide a half-court into 3\u00d72\u00d73 fields (Fig. 3). According to the distance to the net, the court can be divided into fore-court, middle-court, and back-court. The player can hit the shuttle in their forehand area or backhand area. The height can be divided into three levels: low-space (0-1.55m), middle-space (1.55-2.5m), and high-space (2.5-4m). According to the category of the technique used in the last stroke, we further consider three types of tactics: the offensive tactics, the control tactics, and the defensive tactics [58].", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "After rounds of decomposition of tactic groups in the tactic overview, users can select a tactic of interest and jump to the tactic explanation to perceive the tactic from a first-person perspective. With the first-person perspective, users can watch the animation of the selected tactic to replicate the real scenario. This can help users more clearly see the detailed kinematic characteristics of the tactic (which is important for justifying the performance) and more easily learn the tactical purpose. Specifically, a tactic can be summarized and demonstrated by many use cases that appeared in real games. Here we present the summary of a tactic and allow users to use several forms of visualizations to explain the tactic\u2019s practicality.", "solution_category": "interaction", "solution_axial": "Gamification", "solution_compoent": "", "axial_code": ["Gamification"], "componenet_code": ["gamification"]}]}, {"author": "zsz", "index_original": 582, "paper_title": "TIVEE: Visual Exploration and Explanation of Badminton Tactics in Immersive Visualizations", "pub_year": 2022, "domain": "sports", "requirement": {"requirement_text": "R5: Explaining the effect of a tactic. The effectiveness of a tactic isdetermined by multiple factors, including how the player deploysthis tactic and how the opponent responds to this tactic. The suc-cess of this tactic may be because the player found the opponent\u2019sgap or the player forced the opponent, whose weakness is a back-hand catch, to return the ball backhand. Therefore, the systemshould support the correlation analysis of different attributes andassist experts in \ufb01nding the key to success.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "The data of each badminton game is provided as hundreds of strokes. Each stroke has a variety of attributes. The detailed explanation of each stroke attribute is as follows: Splayer: The player giving the stroke. Stechnique: The technique used by the player to give the stroke. According to kinematic features, stroke techniques can be di_x0002_vided into three categories [60]: 1) offensive technique (including smash, net shot, and cut smash), 2) control technique (including clear, drop, chop, push, hook shot, and drive), and 3) defensive technique (including lob and block). Tplayer: The 2D movement trajectory of the player. For each stroke, we record two 2D positions of the player \u2014 the one when the opponent hit the shuttle at the previous stroke (Pstart) and the one when he/she hit the shuttle at the current stroke (Pend) \u2014 to reveal his/her movement to return the shuttle. Tshuttle: The 3D flying trajectory of the shuttle. For each stroke, we record three key points of the shuttle\u2019s flying trajectory \u2014 the start position (Pstart), the highest position (Phighest), and the end position (Pend) \u2014 to reconstruct the whole trajectory based on the shuttle\u2019s kinematic features [41]. Tfield: The fields where the shuttle comes from and falls into. Instead of analyzing the exact 3D positions, experts tend to divide the 3D court space into multiple fields and analyze the fields of the start/end position of the shuttle [37]. Following experts\u2019 requirements, we divide a half-court into 3\u00d72\u00d73 fields (Fig. 3). According to the distance to the net, the court can be divided into fore-court, middle-court, and back-court. The player can hit the shuttle in their forehand area or backhand area. The height can be divided into three levels: low-space (0-1.55m), middle-space (1.55-2.5m), and high-space (2.5-4m). According to the category of the technique used in the last stroke, we further consider three types of tactics: the offensive tactics, the control tactics, and the defensive tactics [58].", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "After rounds of decomposition of tactic groups in the tactic overview, users can select a tactic of interest and jump to the tactic explanation to perceive the tactic from a first-person perspective. With the first-person perspective, users can watch the animation of the selected tactic to replicate the real scenario. This can help users more clearly see the detailed kinematic characteristics of the tactic (which is important for justifying the performance) and more easily learn the tactical purpose. Specifically, a tactic can be summarized and demonstrated by many use cases that appeared in real games. Here we present the summary of a tactic and allow users to use several forms of visualizations to explain the tactic\u2019s practicality.", "solution_category": "interaction", "solution_axial": "Gamification", "solution_compoent": "", "axial_code": ["Gamification"], "componenet_code": ["gamification"]}]}, {"author": "zsz", "index_original": 583, "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences", "pub_year": 2022, "domain": "Temporal event sequence", "requirement": {"requirement_text": "T1. Explore common and deviating pathways: help users to ex-plore and discover which clusterings summarize better the mostcommon (and deviating) pathways in the data. Clusters will groupsequences that share a set of event types, regardless of their order.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.", "data_code": {"ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The final height of a cluster is proportional to the number of records it contains, however, sometimes clusters might contain too few records in proportion to the whole dataset ending up not visible. In such cases the height is scaled up by a constant number of pixels and the cluster is surrounded by a dotted line, allowing users to identify deviating pathways.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "matrix+bar", "axial_code": [], "componenet_code": ["bar", "matrix"]}]}, {"author": "zsz", "index_original": 584, "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences", "pub_year": 2022, "domain": "Temporal event sequence", "requirement": {"requirement_text": "T1. Explore common and deviating pathways: help users to ex-plore and discover which clusterings summarize better the mostcommon (and deviating) pathways in the data. Clusters will groupsequences that share a set of event types, regardless of their order.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.", "data_code": {"ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Figure shows the two sliders used to transform the level-of-detail of the overview: the cluster slider and the information score slider. The cluster slider transforms the vertical level-of-detail by changing the number of clusters k in the range 1 \u2264 k \u2264 N, being N the number of input unique sequences. A combobox next to the cluster slider shows the current number of clusters and contains the list of alternative optimal number of clusters, to guide users in finding a set of pathways that best summarize the data. Alternatively, users can break down a selected cluster into its two child sub-clusters, and so on, until a cluster with a single sequence is reached. The information score slider transforms the horizontal level-of-detail by changing the information score threshold I\u03c4 in the range.", "solution_category": "interaction", "solution_axial": "Abstract/Elaborate,Participation/Collaboration", "solution_compoent": "", "axial_code": ["Abstract/Elaborate", "Participation/Collaboration"], "componenet_code": ["abstract_elaborate", "participation_collaboration"]}]}, {"author": "zsz", "index_original": 585, "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences", "pub_year": 2022, "domain": "Temporal event sequence", "requirement": {"requirement_text": "T2. Interpret the sequences that constitute a cluster: the visual-ization should allow users to compare the most common eventorderings (and permutations) within and across clusters usingsequence alignment.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.", "data_code": {"ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Figure shows how each of the elements in the representation matrix contains either one or multiple event types, where an element with multiple event types corresponds to the row wise merged sub-sequences in the Simplify step. Sub-sequences contained in a single element are represented using a box divided by colored bars, where each bar is colored by event type and ordered as per the sub-sequence. This visual encoding allows to derive the original sequences forming a cluster. To reduce visual clutter, when the number of events in the merged sub-sequence increases, bars can be ordered by event type to show proportion, or colored in gray to show the number of merged records.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "matrix+bar", "axial_code": [], "componenet_code": ["bar", "matrix"]}]}, {"author": "zsz", "index_original": 586, "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences", "pub_year": 2022, "domain": "Temporal event sequence", "requirement": {"requirement_text": "T2. Interpret the sequences that constitute a cluster: the visual-ization should allow users to compare the most common eventorderings (and permutations) within and across clusters usingsequence alignment.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.", "data_code": {"ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "This view shows the individual sequences in the selected clusters, grouped by unique sequence. The sequences are visually encoded as an ordered sequence of boxes arranged horizontally and colored by event type, along with their identifier and frequency. Sequences are shown without any simplification allowing the inspection of the full sequences in the selected clusters.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table", "axial_code": [], "componenet_code": ["table"]}, {"solution_text": "These sequences can be sorted by frequency or similarity, or aligned by a selected event.", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "zsz", "index_original": 587, "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences", "pub_year": 2022, "domain": "Temporal event sequence", "requirement": {"requirement_text": "T3. Focus the analysis on a selected set of records: allow queriesin the dataset to focus on sequences with specific characteristics.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.", "data_code": {"ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The distribution of a data attribute can be analysed for a selected set of records, or compared amongst clusters and unique sequences. This view shows one stacked bar chart per attribute in the dataset, where a chart contains one vertical bar per value, each bar is divided in sub-bars representing series, and series are identified by a unique color. Series can be interactively hidden to focus on only one or compare a reduced number of series. Three types of charts are provided: 1) Selected data: compares the selected data against the rest of the records in the dataset. For a given attribute, this type of bar chart shows one series colored in red for the records contained in the selected clusters or unique sequences, and another series (in grey) for the rest of data. 2) Sequence: it plots one series for each unique sequence shown in the unique sequence view. 3) Cluster: it compares all clusters in the overview, and assigns one series per cluster.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "zsz", "index_original": 588, "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences", "pub_year": 2022, "domain": "Temporal event sequence", "requirement": {"requirement_text": "T3. Focus the analysis on a selected set of records: allow queriesin the dataset to focus on sequences with specific characteristics.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.", "data_code": {"ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "Records can be removed from the overview by applying filters basedon data attributes, frequency, date range, event occurrence; includingfilters by day of the week, month, or year (T3). A filter is specifiedby an attribute, operator, and value. For example, the filter event = Atranslates to \u201cshow only sequences that contain event A at least once\u201d.Users can select sections of a cluster, such as events and sub-sequences,or sequences in the unique sequence view by drawing a square with themouse. These selections are added to the unique sequence view andindividual sequence view, and are plotted in the attribute analysis view(T4).", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 589, "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences", "pub_year": 2022, "domain": "Temporal event sequence", "requirement": {"requirement_text": "T4. Obtain details on demand: provide coordinated views so thatusers can request \ufb01ner details of interesting items in the overview.Users should be able to go from the highest level of aggregation(i.e. clusters), passing through sequences grouped by their uniquesequence, to individual sequences and their raw data includingevent timestamps and duration.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.", "data_code": {"ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "This view shows the individual sequences in the selected clusters, grouped by unique sequence. The sequences are visually encoded as an ordered sequence of boxes arranged horizontally and colored by event type, along with their identifier and frequency. Sequences are shown without any simplification allowing the inspection of the full sequences in the selected clusters.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table", "axial_code": [], "componenet_code": ["table"]}, {"solution_text": "These sequences can be sorted by frequency or similarity, or aligned by a selected event.", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "zsz", "index_original": 590, "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences", "pub_year": 2022, "domain": "Temporal event sequence", "requirement": {"requirement_text": "T4. Obtain details on demand: provide coordinated views so thatusers can request \ufb01ner details of interesting items in the overview.Users should be able to go from the highest level of aggregation(i.e. clusters), passing through sequences grouped by their uniquesequence, to individual sequences and their raw data includingevent timestamps and duration.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.", "data_code": {"ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "This view shows the individual sequences of the selections in the unique sequence view and the overview, along with their temporal information and raw data attributes. Following a Gantt chart approach, each individual sequence is visualized as a horizontal sequence of events, positioned along the horizontal axis according to their timestamp. A table of attributes is displayed next to the Gantt chart, where each column represents a data attribute at either individual sequence level or individual event level.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "text+gantt", "axial_code": [], "componenet_code": ["text", "bar"]}]}, {"author": "zsz", "index_original": 591, "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences", "pub_year": 2022, "domain": "Temporal event sequence", "requirement": {"requirement_text": "T4. Obtain details on demand: provide coordinated views so thatusers can request \ufb01ner details of interesting items in the overview.Users should be able to go from the highest level of aggregation(i.e. clusters), passing through sequences grouped by their uniquesequence, to individual sequences and their raw data includingevent timestamps and duration.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.", "data_code": {"ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The distribution of a data attribute can be analysed for a selected set of records, or compared amongst clusters and unique sequences. This view shows one stacked bar chart per attribute in the dataset, where a chart contains one vertical bar per value, each bar is divided in sub-bars representing series, and series are identified by a unique color. Series can be interactively hidden to focus on only one or compare a reduced number of series. Three types of charts are provided: 1) Selected data: compares the selected data against the rest of the records in the dataset. For a given attribute, this type of bar chart shows one series colored in red for the records contained in the selected clusters or unique sequences, and another series (in grey) for the rest of data. 2) Sequence: it plots one series for each unique sequence shown in the unique sequence view. 3) Cluster: it compares all clusters in the overview, and assigns one series per cluster.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "zsz", "index_original": 592, "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences", "pub_year": 2022, "domain": "Temporal event sequence", "requirement": {"requirement_text": "T5. Aggregate and compare context information for selectedgroups of records: the system should allow to aggregate andcompare data attributes (e.g. age, gender, country) for selectedclusters, unique sequences, or individual sequences.", "requirement_code": {"compare_entities": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.", "data_code": {"ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "This view shows the individual sequences of the selections in the unique sequence view and the overview, along with their temporal information and raw data attributes. Following a Gantt chart approach, each individual sequence is visualized as a horizontal sequence of events, positioned along the horizontal axis according to their timestamp. A table of attributes is displayed next to the Gantt chart, where each column represents a data attribute at either individual sequence level or individual event level.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "text+gantt", "axial_code": [], "componenet_code": ["text", "bar"]}]}, {"author": "zsz", "index_original": 593, "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences", "pub_year": 2022, "domain": "Temporal event sequence", "requirement": {"requirement_text": "T5. Aggregate and compare context information for selectedgroups of records: the system should allow to aggregate andcompare data attributes (e.g. age, gender, country) for selectedclusters, unique sequences, or individual sequences.", "requirement_code": {"compare_entities": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.", "data_code": {"ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The distribution of a data attribute can be analysed for a selected set of records, or compared amongst clusters and unique sequences. This view shows one stacked bar chart per attribute in the dataset, where a chart contains one vertical bar per value, each bar is divided in sub-bars representing series, and series are identified by a unique color. Series can be interactively hidden to focus on only one or compare a reduced number of series. Three types of charts are provided: 1) Selected data: compares the selected data against the rest of the records in the dataset. For a given attribute, this type of bar chart shows one series colored in red for the records contained in the selected clusters or unique sequences, and another series (in grey) for the rest of data. 2) Sequence: it plots one series for each unique sequence shown in the unique sequence view. 3) Cluster: it compares all clusters in the overview, and assigns one series per cluster.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "zsz", "index_original": 594, "paper_title": "Visual Evaluation for Autonomous Driving", "pub_year": 2022, "domain": "Autonomous Driving", "requirement": {"requirement_text": "R1: Module developers should be able to use the system to assess the overall performance of an autonomous driving system with a score as general feedback.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Autonomous driving data can be obtained from the real car tests or simulation tests. Due to the limitations of the real testing environment, we use the autonomous driving data records obtained from the LGSVL simulator in this work. Based on the official classification [3] and the inputs from our collaborators, we divide our data into five mod_x0002_ules: perception, planning, prediction, control, and comfort. In each module, there are several measurement factors, i.e., the attributes captured by the sensors, that are related to its performance. The mea_x0002_surement factors of the perception module are from the perception component, and include signal detection accuracy, obstacle detection accuracy, as well as the accuracy of distance between vehicle and obstacles. With the perceived information, the autonomous driving sys_x0002_tem needs to predict the behaviors of obstacles and traffic participants. The prediction module from the decision making component can be measured with the accuracy of the predicted trajectory of obstacles. For the planning module with the understanding of the environment and obstacles, which is also from the decision making component, au_x0002_tonomous driving model developers are interested in such measures as the differences between predicted vehicle speed and the actual speed at a given time, the differences between predicted position and actual po_x0002_sition, etc. The control module from the control component includes those factors related to the control of throttle, brake, steering wheel, etc. The comfort module, which concerns the feelings of the driver, uses measures like the acceleration rate of the vehicle, turning angle, etc. Table 1 summarizes these modules, involved factors in each module, their descriptions, and their evaluation criteria.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "In our case, we divided the autonomous driving process into five modules and selected relevant evaluation factors in each module. We combine AHP and TOPSIS in calculating the overall evaluation score for autonomous driving, because there are no officially defined eval_x0002_uation criteria for autonomous driving. Therefore, with the help of TOPSIS, for a driving record over a period of time and a given eval_x0002_uation factor, a time series of evaluation results for the factor can be obtained. In this way, we can obtain the time series of evaluation re_x0002_sults for all factors involved in the evaluation of autonomous driving over a period of time. Because there are no standard rules for ranking the importance of the factors of autonomous driving, AHP is used to determine the weight of each evaluation factor. Initially the weights are determined by module developers to equalize every evaluation module weight. With our visual evaluation system, they can determine the importance of each factor for different needs by themselves and the system can calculate the parameters that meet their needs with the help of AHP; With the input of the autonomous driving data record, we first calculatethe scores for each factor based on our model described in Sect. 4.1.The total score, module score and factor score range from 0 to 1. Thescore of each module is averaged by the scores of all the factors fromthis module at each time period. The score of each time period isaveraged by the scores of all the modules at this time period. Thetotal score is calculated by the average of all the scores in every timestep.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Module developers first read the total score in Fig. 1-b (R1) andexamine the timeline visualization (R2, Fig. 1-c) to check the trendof the scores along the time. They can identify the outlier momentswith the low scores and check details in the radar view (Fig. 1-b). Wechoose the radar because we have five modules which is a suitablenumber of axes for choosing the radar. Users can easily identifythe good and bad performance scores in different modules.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "radar", "axial_code": [], "componenet_code": ["radar"]}]}, {"author": "zsz", "index_original": 595, "paper_title": "Visual Evaluation for Autonomous Driving", "pub_year": 2022, "domain": "Autonomous Driving", "requirement": {"requirement_text": "R2: The system should allow module developers to evaluate the performance of a system in different time periods of the whole driving process and to identify the time periods with bad performance.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Autonomous driving data can be obtained from the real car tests or simulation tests. Due to the limitations of the real testing environment, we use the autonomous driving data records obtained from the LGSVL simulator in this work. Based on the official classification [3] and the inputs from our collaborators, we divide our data into five mod_x0002_ules: perception, planning, prediction, control, and comfort. In each module, there are several measurement factors, i.e., the attributes captured by the sensors, that are related to its performance. The mea_x0002_surement factors of the perception module are from the perception component, and include signal detection accuracy, obstacle detection accuracy, as well as the accuracy of distance between vehicle and obstacles. With the perceived information, the autonomous driving sys_x0002_tem needs to predict the behaviors of obstacles and traffic participants. The prediction module from the decision making component can be measured with the accuracy of the predicted trajectory of obstacles. For the planning module with the understanding of the environment and obstacles, which is also from the decision making component, au_x0002_tonomous driving model developers are interested in such measures as the differences between predicted vehicle speed and the actual speed at a given time, the differences between predicted position and actual po_x0002_sition, etc. The control module from the control component includes those factors related to the control of throttle, brake, steering wheel, etc. The comfort module, which concerns the feelings of the driver, uses measures like the acceleration rate of the vehicle, turning angle, etc. Table 1 summarizes these modules, involved factors in each module, their descriptions, and their evaluation criteria.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "In our case, we divided the autonomous driving process into five modules and selected relevant evaluation factors in each module. We combine AHP and TOPSIS in calculating the overall evaluation score for autonomous driving, because there are no officially defined eval_x0002_uation criteria for autonomous driving. Therefore, with the help of TOPSIS, for a driving record over a period of time and a given eval_x0002_uation factor, a time series of evaluation results for the factor can be obtained. In this way, we can obtain the time series of evaluation re_x0002_sults for all factors involved in the evaluation of autonomous driving over a period of time. Because there are no standard rules for ranking the importance of the factors of autonomous driving, AHP is used to determine the weight of each evaluation factor. Initially the weights are determined by module developers to equalize every evaluation module weight. With our visual evaluation system, they can determine the importance of each factor for different needs by themselves and the system can calculate the parameters that meet their needs with the help of AHP; With the input of the autonomous driving data record, we first calculatethe scores for each factor based on our model described in Sect. 4.1.The total score, module score and factor score range from 0 to 1. Thescore of each module is averaged by the scores of all the factors fromthis module at each time period. The score of each time period isaveraged by the scores of all the modules at this time period. Thetotal score is calculated by the average of all the scores in every timestep.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Module developers first read the total score in Fig. 1-b (R1) andexamine the timeline visualization (R2, Fig. 1-c) to check the trendof the scores along the time. They can identify the outlier momentswith the low scores.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "zsz", "index_original": 596, "paper_title": "Visual Evaluation for Autonomous Driving", "pub_year": 2022, "domain": "Autonomous Driving", "requirement": {"requirement_text": "R3: The system should provide methods for the evaluation of the five modules (perception, prediction, planning, control and comfort) so that module developers can tell the performances of individual modules.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Autonomous driving data can be obtained from the real car tests or simulation tests. Due to the limitations of the real testing environment, we use the autonomous driving data records obtained from the LGSVL simulator in this work. Based on the official classification [3] and the inputs from our collaborators, we divide our data into five mod_x0002_ules: perception, planning, prediction, control, and comfort. In each module, there are several measurement factors, i.e., the attributes captured by the sensors, that are related to its performance. The mea_x0002_surement factors of the perception module are from the perception component, and include signal detection accuracy, obstacle detection accuracy, as well as the accuracy of distance between vehicle and obstacles. With the perceived information, the autonomous driving sys_x0002_tem needs to predict the behaviors of obstacles and traffic participants. The prediction module from the decision making component can be measured with the accuracy of the predicted trajectory of obstacles. For the planning module with the understanding of the environment and obstacles, which is also from the decision making component, au_x0002_tonomous driving model developers are interested in such measures as the differences between predicted vehicle speed and the actual speed at a given time, the differences between predicted position and actual po_x0002_sition, etc. The control module from the control component includes those factors related to the control of throttle, brake, steering wheel, etc. The comfort module, which concerns the feelings of the driver, uses measures like the acceleration rate of the vehicle, turning angle, etc. Table 1 summarizes these modules, involved factors in each module, their descriptions, and their evaluation criteria.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "In our case, we divided the autonomous driving process into five modules and selected relevant evaluation factors in each module. We combine AHP and TOPSIS in calculating the overall evaluation score for autonomous driving, because there are no officially defined eval_x0002_uation criteria for autonomous driving. Therefore, with the help of TOPSIS, for a driving record over a period of time and a given eval_x0002_uation factor, a time series of evaluation results for the factor can be obtained. In this way, we can obtain the time series of evaluation re_x0002_sults for all factors involved in the evaluation of autonomous driving over a period of time. Because there are no standard rules for ranking the importance of the factors of autonomous driving, AHP is used to determine the weight of each evaluation factor. Initially the weights are determined by module developers to equalize every evaluation module weight. With our visual evaluation system, they can determine the importance of each factor for different needs by themselves and the system can calculate the parameters that meet their needs with the help of AHP; With the input of the autonomous driving data record, we first calculatethe scores for each factor based on our model described in Sect. 4.1.The total score, module score and factor score range from 0 to 1. Thescore of each module is averaged by the scores of all the factors fromthis module at each time period. The score of each time period isaveraged by the scores of all the modules at this time period. Thetotal score is calculated by the average of all the scores in every timestep.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "From the radar view, the score distributions of five modules, including perception, planning, prediction, control, and comfort are visualized. We can compare and analyze each module to observe the performance of different autonomous driving evaluation modules. The radar view shows the overall performance of the five modules at the initial moment and the module scores at specific moments.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "radar", "axial_code": [], "componenet_code": ["radar"]}, {"solution_text": "It can be dynamically updated along with the timeline. We can find the time period with poor or unbalanced evaluation scores of each module.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 597, "paper_title": "Visual Evaluation for Autonomous Driving", "pub_year": 2022, "domain": "Autonomous Driving", "requirement": {"requirement_text": "R4: The system should allow module developers to observe the performances of individual module at any time period and to identify those factors that contribute to the observed performances.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Autonomous driving data can be obtained from the real car tests or simulation tests. Due to the limitations of the real testing environment, we use the autonomous driving data records obtained from the LGSVL simulator in this work. Based on the official classification [3] and the inputs from our collaborators, we divide our data into five mod_x0002_ules: perception, planning, prediction, control, and comfort. In each module, there are several measurement factors, i.e., the attributes captured by the sensors, that are related to its performance. The mea_x0002_surement factors of the perception module are from the perception component, and include signal detection accuracy, obstacle detection accuracy, as well as the accuracy of distance between vehicle and obstacles. With the perceived information, the autonomous driving sys_x0002_tem needs to predict the behaviors of obstacles and traffic participants. The prediction module from the decision making component can be measured with the accuracy of the predicted trajectory of obstacles. For the planning module with the understanding of the environment and obstacles, which is also from the decision making component, au_x0002_tonomous driving model developers are interested in such measures as the differences between predicted vehicle speed and the actual speed at a given time, the differences between predicted position and actual po_x0002_sition, etc. The control module from the control component includes those factors related to the control of throttle, brake, steering wheel, etc. The comfort module, which concerns the feelings of the driver, uses measures like the acceleration rate of the vehicle, turning angle, etc. Table 1 summarizes these modules, involved factors in each module, their descriptions, and their evaluation criteria.", "data_code": {"quantitative": 1, "tables": 1}}, "solution": [{"solution_text": "In our case, we divided the autonomous driving process into five modules and selected relevant evaluation factors in each module. We combine AHP and TOPSIS in calculating the overall evaluation score for autonomous driving, because there are no officially defined eval_x0002_uation criteria for autonomous driving. Therefore, with the help of TOPSIS, for a driving record over a period of time and a given eval_x0002_uation factor, a time series of evaluation results for the factor can be obtained. In this way, we can obtain the time series of evaluation re_x0002_sults for all factors involved in the evaluation of autonomous driving over a period of time. Because there are no standard rules for ranking the importance of the factors of autonomous driving, AHP is used to determine the weight of each evaluation factor. Initially the weights are determined by module developers to equalize every evaluation module weight. With our visual evaluation system, they can determine the importance of each factor for different needs by themselves and the system can calculate the parameters that meet their needs with the help of AHP; With the input of the autonomous driving data record, we first calculatethe scores for each factor based on our model described in Sect. 4.1.The total score, module score and factor score range from 0 to 1. Thescore of each module is averaged by the scores of all the factors fromthis module at each time period. The score of each time period isaveraged by the scores of all the modules at this time period. Thetotal score is calculated by the average of all the scores in every timestep.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Therefore, a view of parallel coordinates is used to show the situation of each factor involved in evaluation. As shown in the figure, the top axis of parallel coordinates is the score axis, which shows the evaluation result of each time point. The polylines in parallel coordinates are color encoded by the total score at a specific moment. Under the score axis are factor axes, which indicate the evaluation result of each factor at each time point. With the help of parallel coordinates, users can easily see the connection of between overall scores and the values of individual factor, and observe the influence of the evaluation factors on the modules.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "parallelcoordinates", "axial_code": [], "componenet_code": ["parallelcoordinates"]}]}, {"author": "zsz", "index_original": 598, "paper_title": "Visual Evaluation for Autonomous Driving", "pub_year": 2022, "domain": "Autonomous Driving", "requirement": {"requirement_text": "R5: The system should allow module developers to customize the ranking of importance of each module based on their different goals and contexts in analysis.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Autonomous driving data can be obtained from the real car tests or simulation tests. Due to the limitations of the real testing environment, we use the autonomous driving data records obtained from the LGSVL simulator in this work. Based on the official classification [3] and the inputs from our collaborators, we divide our data into five mod_x0002_ules: perception, planning, prediction, control, and comfort. In each module, there are several measurement factors, i.e., the attributes captured by the sensors, that are related to its performance. The mea_x0002_surement factors of the perception module are from the perception component, and include signal detection accuracy, obstacle detection accuracy, as well as the accuracy of distance between vehicle and obstacles. With the perceived information, the autonomous driving sys_x0002_tem needs to predict the behaviors of obstacles and traffic participants. The prediction module from the decision making component can be measured with the accuracy of the predicted trajectory of obstacles. For the planning module with the understanding of the environment and obstacles, which is also from the decision making component, au_x0002_tonomous driving model developers are interested in such measures as the differences between predicted vehicle speed and the actual speed at a given time, the differences between predicted position and actual po_x0002_sition, etc. The control module from the control component includes those factors related to the control of throttle, brake, steering wheel, etc. The comfort module, which concerns the feelings of the driver, uses measures like the acceleration rate of the vehicle, turning angle, etc. Table 1 summarizes these modules, involved factors in each module, their descriptions, and their evaluation criteria.", "data_code": {"quantitative": 1, "tables": 1}}, "solution": [{"solution_text": "In our case, we divided the autonomous driving process into five modules and selected relevant evaluation factors in each module. We combine AHP and TOPSIS in calculating the overall evaluation score for autonomous driving, because there are no officially defined eval_x0002_uation criteria for autonomous driving. Therefore, with the help of TOPSIS, for a driving record over a period of time and a given eval_x0002_uation factor, a time series of evaluation results for the factor can be obtained. In this way, we can obtain the time series of evaluation re_x0002_sults for all factors involved in the evaluation of autonomous driving over a period of time. Because there are no standard rules for ranking the importance of the factors of autonomous driving, AHP is used to determine the weight of each evaluation factor. Initially the weights are determined by module developers to equalize every evaluation module weight. With our visual evaluation system, they can determine the importance of each factor for different needs by themselves and the system can calculate the parameters that meet their needs with the help of AHP; With the input of the autonomous driving data record, we first calculatethe scores for each factor based on our model described in Sect. 4.1.The total score, module score and factor score range from 0 to 1. Thescore of each module is averaged by the scores of all the factors fromthis module at each time period. The score of each time period isaveraged by the scores of all the modules at this time period. Thetotal score is calculated by the average of all the scores in every timestep.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The weights of all evaluation factors are equally distributed in the initial state. Because module developers with different background and interest may view different factors with different priorities. Users can drag the bars on the factor ranking visualization to reorder their own priority. Dragging the factors that users consider important and moving the unimportant factors to the bottom can achieve the priority customization and update the evaluation modelling. In this way, we can get the new relative importance r between factors according to the relative positions of those bars, construct a new judgment matrix R, and calculate the new evaluation results according to AHP.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "text", "axial_code": [], "componenet_code": ["text"]}]}, {"author": "zsz", "index_original": 599, "paper_title": "Scalable Comparative Visualization of Ensembles of Call Graphs", "pub_year": 2023, "domain": "Optimizing the performance of large-scale parallel codes", "requirement": {"requirement_text": "A1. Compare Calling Contexts (T1) Across Runs (R1, R3).Due to potential differences in calling contexts of differentexecutions, it is essential to highlight any structural differ-ences, i.e., missing nodes/edges in the graph.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "Here, we are given an ensemble of multiprocess perfor_x0002_mance profiles to study weak scaling of an application across eight execution parameters: 1, 8, 27, 64, 125, 216, 343, and 512 processes. In particular, the application is LULESH [64], a hydrodynamics mini-application that uses both MPI and OpenMP to achieve parallelism. Recently, a similar case study was conducted by Bhatele et al. [52] to study run-to-run performance differences for identifying the most time-consuming regions of the code. We use a similar collection of profiles and showcase the advantages of visual analytics for such exploration using EnsembleCallFlow.", "data_code": {"quantitative": 1}}, "solution": [{"solution_text": "In CallFlow, the performance profiles are converted into GraphFrames using Hatchet [52], an open-source profile analysis tool. A GraphFrame (G) is the key data source for CallFlow and consists of two data structures: a directed acyclic graph (G) that represents the CCT or call graph, and a Pandas [57] DataFrame (D) that stores the associated perfor_x0002_mance metrics. ", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Icicle plot places the call sites of the selected library based on their depth inside the supernode hierarchy from top to bottom. Each call site in the supernode hierarchy is visual- ized as horizontal rectangular bars. As with the ensemble- Sankey, ensemble gradients and runtime borders are used to encode the ensemble distribution and the runtime distri- bution for the call site, respectively.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "icicle", "axial_code": [], "componenet_code": ["icicle"]}]}, {"author": "zsz", "index_original": 600, "paper_title": "Scalable Comparative Visualization of Ensembles of Call Graphs", "pub_year": 2023, "domain": "Optimizing the performance of large-scale parallel codes", "requirement": {"requirement_text": "A1. Compare Calling Contexts (T1) Across Runs (R1, R3).Due to potential differences in calling contexts of differentexecutions, it is essential to highlight any structural differ-ences, i.e., missing nodes/edges in the graph.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "Here, we are given an ensemble of multiprocess perfor_x0002_mance profiles to study weak scaling of an application across eight execution parameters: 1, 8, 27, 64, 125, 216, 343, and 512 processes. In particular, the application is LULESH [64], a hydrodynamics mini-application that uses both MPI and OpenMP to achieve parallelism. Recently, a similar case study was conducted by Bhatele et al. [52] to study run-to-run performance differences for identifying the most time-consuming regions of the code. We use a similar collection of profiles and showcase the advantages of visual analytics for such exploration using EnsembleCallFlow.", "data_code": {"quantitative": 1}}, "solution": [{"solution_text": "In CallFlow, the performance profiles are converted into GraphFrames using Hatchet [52], an open-source profile analysis tool. A GraphFrame (G) is the key data source for CallFlow and consists of two data structures: a directed acyclic graph (G) that represents the CCT or call graph, and a Pandas [57] DataFrame (D) that stores the associated perfor_x0002_mance metrics. ", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Metric correlation view. In order to compare inclusive and exclusive metrics for a given call site, we produce a scatterplot that captures the correlation between the two. For ensembles, each \u201cdot\u201d in the scatter plot represents a callsite for a single run, making it possible to study such correlations across ensemble members, e.g., by comparing a target run to the ensemble, as shown in the figure. The scatter plot itself is also interactive, as hovering over a dot highlights call site by their names for the user to compare the runtime metrics.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}]}, {"author": "zsz", "index_original": 601, "paper_title": "Scalable Comparative Visualization of Ensembles of Call Graphs", "pub_year": 2023, "domain": "Optimizing the performance of large-scale parallel codes", "requirement": {"requirement_text": "A2. Analyze Performance Variability (T2) Across Runs(R2.3). Identifying the overall trend as well as outlier runsrequires analyzing the performance metrics across the ensem-ble. Furthermore, to provide a full context and details neces-sary for performance improvements, such analyses must bemade available for each module, library, function, etc.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Here, we are given an ensemble of multiprocess perfor_x0002_mance profiles to study weak scaling of an application across eight execution parameters: 1, 8, 27, 64, 125, 216, 343, and 512 processes. In particular, the application is LULESH [64], a hydrodynamics mini-application that uses both MPI and OpenMP to achieve parallelism. Recently, a similar case study was conducted by Bhatele et al. [52] to study run-to-run performance differences for identifying the most time-consuming regions of the code. We use a similar collection of profiles and showcase the advantages of visual analytics for such exploration using EnsembleCallFlow.", "data_code": {"quantitative": 1}}, "solution": [{"solution_text": "In CallFlow, the performance profiles are converted into GraphFrames using Hatchet [52], an open-source profile analysis tool. A GraphFrame (G) is the key data source for CallFlow and consists of two data structures: a directed acyclic graph (G) that represents the CCT or call graph, and a Pandas [57] DataFrame (D) that stores the associated perfor_x0002_mance metrics. ", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Although we could directly compute a density estimate (e.g., using KDE) instead of a histogram to generate a smoother distribution, such techniques introduce additional parameters (i.e., kernel width), which are typically harder to interpret and may miss features regardless due to an unsuit- able choice. Instead, we use histograms (with a customizable bin count) and use linear gradients to smooth the distribution. In this way, the resulting ensemble gradient shows the full distri- bution of the selected metric across runs. The default choice of color for the distribution is a single-hue white\u2014red colormap. The colors are mapped consistently across all nodes to allow evaluation of distributions not just within a single supernode, but also across supernodes.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "zsz", "index_original": 602, "paper_title": "Scalable Comparative Visualization of Ensembles of Call Graphs", "pub_year": 2023, "domain": "Optimizing the performance of large-scale parallel codes", "requirement": {"requirement_text": "A3. Analyze Performance Variability (T2) Across MPIRanks (R2.4). Summary statistics are not sufficient to ana-lyze performance distribution across resources because thedistribution is expected to be nontrivial. To aid the explora-tion, complete distributions are essential to visualize.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Here, we are given an ensemble of multiprocess perfor_x0002_mance profiles to study weak scaling of an application across eight execution parameters: 1, 8, 27, 64, 125, 216, 343, and 512 processes. In particular, the application is LULESH [64], a hydrodynamics mini-application that uses both MPI and OpenMP to achieve parallelism. Recently, a similar case study was conducted by Bhatele et al. [52] to study run-to-run performance differences for identifying the most time-consuming regions of the code. We use a similar collection of profiles and showcase the advantages of visual analytics for such exploration using EnsembleCallFlow.", "data_code": {"quantitative": 1}}, "solution": [{"solution_text": "In CallFlow, the performance profiles are converted into GraphFrames using Hatchet [52], an open-source profile analysis tool. A GraphFrame (G) is the key data source for CallFlow and consists of two data structures: a directed acyclic graph (G) that represents the CCT or call graph, and a Pandas [57] DataFrame (D) that stores the associated perfor_x0002_mance metrics. ", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Call site correspondence view (see Fig. 3c) lets us focus on process-level distribution, and also scan call site informa- tion based on data and graph properties. To explore varia- tions across multiple target metrics, we utilize a boxplot to study the different runtime ranges occupied by the observed runtime distribution for each call site (A3). Box- plots use quartiles of the distribution to indicate the spread of data, and can also reveal outliers. In this view, we enu- merate the call sites that are not present in the ensemble view, and highlight the median, the interquartile range (IQR = Q3Q1), as well as outliers (above and below 1.5 the IQR). Additional information is also provided as text labels (e.g., minimum, median, and maximum runtime).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "box", "axial_code": [], "componenet_code": ["boxplot"]}]}, {"author": "zsz", "index_original": 603, "paper_title": "Scalable Comparative Visualization of Ensembles of Call Graphs", "pub_year": 2023, "domain": "Optimizing the performance of large-scale parallel codes", "requirement": {"requirement_text": "A4. Compare a Selected Run With an Ensemble (R1, R3).For both our targets, it is important to compare a selected runwith the ensemble behavior. An important constraint to con-sider is that the constructed baseline must match the expert\u2019sunderstanding of the overall calling context, despite minor dif-ferences in the individual ensemble members.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Here, we are given an ensemble of multiprocess perfor_x0002_mance profiles to study weak scaling of an application across eight execution parameters: 1, 8, 27, 64, 125, 216, 343, and 512 processes. In particular, the application is LULESH [64], a hydrodynamics mini-application that uses both MPI and OpenMP to achieve parallelism. Recently, a similar case study was conducted by Bhatele et al. [52] to study run-to-run performance differences for identifying the most time-consuming regions of the code. We use a similar collection of profiles and showcase the advantages of visual analytics for such exploration using EnsembleCallFlow.", "data_code": {"quantitative": 1}}, "solution": [{"solution_text": "Target-ensemble comparison mode is triggered when the user selects a particular execution from the ensemble to study in detail from \u201cSelect Target run\u201d, which lists all ensemble members. This operation allows the user to compare a selected run\u2019s performance with the ensemble across all 6 views.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "zsz", "index_original": 604, "paper_title": "Scalable Comparative Visualization of Ensembles of Call Graphs", "pub_year": 2023, "domain": "Optimizing the performance of large-scale parallel codes", "requirement": {"requirement_text": "A5. Compare Call Graphs Across Levels of Detail (R2.2).Although comparisons across super graphs (semanticallyaggregated call graphs) is useful, when experts identifyproblem areas, they are then interested in looking atselected regions in more detail (T1). Therefore, it is impor-tant to manage \ufb01ne-level details and visualize them uponrequest.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Here, we are given an ensemble of multiprocess perfor_x0002_mance profiles to study weak scaling of an application across eight execution parameters: 1, 8, 27, 64, 125, 216, 343, and 512 processes. In particular, the application is LULESH [64], a hydrodynamics mini-application that uses both MPI and OpenMP to achieve parallelism. Recently, a similar case study was conducted by Bhatele et al. [52] to study run-to-run performance differences for identifying the most time-consuming regions of the code. We use a similar collection of profiles and showcase the advantages of visual analytics for such exploration using EnsembleCallFlow.", "data_code": {"quantitative": 1}}, "solution": [{"solution_text": "In CallFlow, the performance profiles are converted into GraphFrames using Hatchet [52], an open-source profile analysis tool. A GraphFrame (G) is the key data source for CallFlow and consists of two data structures: a directed acyclic graph (G) that represents the CCT or call graph, and a Pandas [57] DataFrame (D) that stores the associated perfor_x0002_mance metrics. ", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "the diff view provided by EnsembleCallFlow canreproduce such analysis through a more-effective visualmedium (see Figs. 6b and 6c). The result highlights not onlythe modules that are slower (with respect to the diff order)but also communicates the relative degree of performancedegradation easily (A5).", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "sankey+bar", "axial_code": [], "componenet_code": ["bar", "sankey"]}]}, {"author": "zsz", "index_original": 605, "paper_title": "SimuExplorer: Visual Exploration of Game Simulation in Table Tennis", "pub_year": 2023, "domain": "Sports", "requirement": {"requirement_text": "What are the impacts brought by return adjustments of different players? Players have individual styles of playing, and the impacts of returns differ by player. The experts hope to browse different players\u2019 impact distributions over return adjustments.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We collected data from 306 table tennis matches among 21 top players (10 males and 11 females) from 2005 to 2012. The data was collected by manually coding videos. We have tried using machine learning models to automatically extract structured data from video. However, low video quality means that the accuracy of automatically extracted stroke features, such as stroke technique, is relatively low (around 75%). Each match was collected as a CSV file, which contained hundreds of rows representing the hun_x0002_dreds of strokes in the match. Each row records different features of a stroke as follows. _x0001_Rally ID denotes the rally the stroke belongs to. Stroke ID is the sequence number of the stroke in the rally. Stroke player denotes the player who gave the stroke. Stroke technique denotes the technique the player uses to give the stroke. There are 14 techniques. Ball position denotes the drop point of the stroke. There are ten drop points.", "data_code": {"categorical": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "Lames [6] first proposed simulating a table tennis rally through a Markov chain model. The model views a rally as a sequence of possible stroke states, and simulates the impacts of strokes on players\u2019 scoring rates. Here we go through how Lames\u2019 model [6] has been applied in previous studies and explain how the model simulates a rally. Then we propose a modified model and explain how intermediate results of the model measure returns and impacts of return adjustments.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Click a player in the player view, and examine correspond- ing impacts in the adjustment view.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}, {"solution_text": "Bar charts in the player view represent the impact distribution over different returns for a chosen player.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "text+bar", "axial_code": [], "componenet_code": ["text", "bar"]}]}, {"author": "zsz", "index_original": 606, "paper_title": "SimuExplorer: Visual Exploration of Game Simulation in Table Tennis", "pub_year": 2023, "domain": "Sports", "requirement": {"requirement_text": "What are the impacts brought by return adjustments of different players? Players have individual styles of playing, and the impacts of returns differ by player. The experts hope to browse different players\u2019 impact distributions over return adjustments.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We collected data from 306 table tennis matches among 21 top players (10 males and 11 females) from 2005 to 2012. The data was collected by manually coding videos. We have tried using machine learning models to automatically extract structured data from video. However, low video quality means that the accuracy of automatically extracted stroke features, such as stroke technique, is relatively low (around 75%). Each match was collected as a CSV file, which contained hundreds of rows representing the hun_x0002_dreds of strokes in the match. Each row records different features of a stroke as follows. _x0001_Rally ID denotes the rally the stroke belongs to. Stroke ID is the sequence number of the stroke in the rally. Stroke player denotes the player who gave the stroke. Stroke technique denotes the technique the player uses to give the stroke. There are 14 techniques. Ball position denotes the drop point of the stroke. There are ten drop points.", "data_code": {"categorical": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "Lames [6] first proposed simulating a table tennis rally through a Markov chain model. The model views a rally as a sequence of possible stroke states, and simulates the impacts of strokes on players\u2019 scoring rates. Here we go through how Lames\u2019 model [6] has been applied in previous studies and explain how the model simulates a rally. Then we propose a modified model and explain how intermediate results of the model measure returns and impacts of return adjustments.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The player view (Fig. 4A) provides a browsable overview ofthe impacts of return adjustments for different players (T1).This view contains a table that lists all the top table tennisplayers. A bar chart (Fig. 4A-4) in each row in the tablepresents the impact distribution of return adjustments for aplayer. Because stroke technique is the most important attri-bute, in the left column, we group typical returns accordingto six stroke technique values based on the first stroke in thereturn, obtaining six groups. In the right column, we grouptypical returns according to five stroke technique valuesbased on the second stroke in the return, obtaining fivegroups. We encode the average impact of adjusting returnsin each group as a bar in the bar chart. The orange barsdenote return adjustments with positive impacts, while theblue bars denote return adjustments with negative impacts.The axes are scaled according to the largest impact.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "text+bar", "axial_code": [], "componenet_code": ["text", "bar"]}, {"solution_text": "Withthis view, users can quickly detect whether a player canchange their scoring rate by adjusting a group of returnsinvolving specific stroke technique values. Two switch but-tons (Figs. 4A-1 and 4A-2) allow switching between serveand other rallies and between male and female players.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 607, "paper_title": "SimuExplorer: Visual Exploration of Game Simulation in Table Tennis", "pub_year": 2023, "domain": "Sports", "requirement": {"requirement_text": "What are the impacts brought by different return adjust- ments of a player? Adjustments to different returns exert varying impacts for a player. The experts need to detect the impact patterns of different return adjustments.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We collected data from 306 table tennis matches among 21 top players (10 males and 11 females) from 2005 to 2012. The data was collected by manually coding videos. We have tried using machine learning models to automatically extract structured data from video. However, low video quality means that the accuracy of automatically extracted stroke features, such as stroke technique, is relatively low (around 75%). Each match was collected as a CSV file, which contained hundreds of rows representing the hun_x0002_dreds of strokes in the match. Each row records different features of a stroke as follows. _x0001_Rally ID denotes the rally the stroke belongs to. Stroke ID is the sequence number of the stroke in the rally. Stroke player denotes the player who gave the stroke. Stroke technique denotes the technique the player uses to give the stroke. There are 14 techniques. Ball position denotes the drop point of the stroke. There are ten drop points.", "data_code": {"categorical": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "Lames [6] first proposed simulating a table tennis rally through a Markov chain model. The model views a rally as a sequence of possible stroke states, and simulates the impacts of strokes on players\u2019 scoring rates. Here we go through how Lames\u2019 model [6] has been applied in previous studies and explain how the model simulates a rally. Then we propose a modified model and explain how intermediate results of the model measure returns and impacts of return adjustments.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The player view (Fig. 4A) provides a browsable overview ofthe impacts of return adjustments for different players (T1).This view contains a table that lists all the top table tennisplayers. A bar chart (Fig. 4A-4) in each row in the tablepresents the impact distribution of return adjustments for aplayer. Because stroke technique is the most important attri-bute, in the left column, we group typical returns accordingto six stroke technique values based on the first stroke in thereturn, obtaining six groups. In the right column, we grouptypical returns according to five stroke technique valuesbased on the second stroke in the return, obtaining fivegroups. We encode the average impact of adjusting returnsin each group as a bar in the bar chart. The orange barsdenote return adjustments with positive impacts, while theblue bars denote return adjustments with negative impacts.The axes are scaled according to the largest impact.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "text+bar", "axial_code": [], "componenet_code": ["text", "bar"]}, {"solution_text": "When a user is interested in the impact distribution of a particular player, she/he can click and select the player in the player view.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}, {"solution_text": "The adjustment view will then display the impacts of adjusting different returns for the player.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "matrix+text", "axial_code": [], "componenet_code": ["text", "matrix"]}]}, {"author": "zsz", "index_original": 608, "paper_title": "SimuExplorer: Visual Exploration of Game Simulation in Table Tennis", "pub_year": 2023, "domain": "Sports", "requirement": {"requirement_text": "What are the impacts brought by different return adjust- ments of a player? Adjustments to different returns exert varying impacts for a player. The experts need to detect the impact patterns of different return adjustments.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We collected data from 306 table tennis matches among 21 top players (10 males and 11 females) from 2005 to 2012. The data was collected by manually coding videos. We have tried using machine learning models to automatically extract structured data from video. However, low video quality means that the accuracy of automatically extracted stroke features, such as stroke technique, is relatively low (around 75%). Each match was collected as a CSV file, which contained hundreds of rows representing the hun_x0002_dreds of strokes in the match. Each row records different features of a stroke as follows. _x0001_Rally ID denotes the rally the stroke belongs to. Stroke ID is the sequence number of the stroke in the rally. Stroke player denotes the player who gave the stroke. Stroke technique denotes the technique the player uses to give the stroke. There are 14 techniques. Ball position denotes the drop point of the stroke. There are ten drop points.", "data_code": {"categorical": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "Lames [6] first proposed simulating a table tennis rally through a Markov chain model. The model views a rally as a sequence of possible stroke states, and simulates the impacts of strokes on players\u2019 scoring rates. Here we go through how Lames\u2019 model [6] has been applied in previous studies and explain how the model simulates a rally. Then we propose a modified model and explain how intermediate results of the model measure returns and impacts of return adjustments.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The player view (Fig. 4A) provides a browsable overview ofthe impacts of return adjustments for different players (T1).This view contains a table that lists all the top table tennisplayers. A bar chart (Fig. 4A-4) in each row in the tablepresents the impact distribution of return adjustments for aplayer. Because stroke technique is the most important attri-bute, in the left column, we group typical returns accordingto six stroke technique values based on the first stroke in thereturn, obtaining six groups. In the right column, we grouptypical returns according to five stroke technique valuesbased on the second stroke in the return, obtaining fivegroups. We encode the average impact of adjusting returnsin each group as a bar in the bar chart. The orange barsdenote return adjustments with positive impacts, while theblue bars denote return adjustments with negative impacts.The axes are scaled according to the largest impact.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "text+bar", "axial_code": [], "componenet_code": ["text", "bar"]}, {"solution_text": "After a user selects a player for analysis, the adjustmentview (Fig. 4B) provides an overview of the impacts ofadjusting the player\u2019s different returns (T2).", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}, {"solution_text": "This view contains a matrix similar to that in Fig. 2A.Each entry in the matrix (Fig. 4B-1) represents one return ofPlayer A. The column headers denote the combinations ofstroke technique and ball position values at the \ufb01rst strokeof the return (given by Player A), and row headers denotethose at the second stroke (given by opponents of Player A).The area of the rectangle in an entry encodes the impact ofadjusting the corresponding return. (To reiterate, adjustinga return means successfully in\ufb02uencing the opponents ofPlayer A to use a speci\ufb01c type of second stroke more fre-quently after Player A uses another speci\ufb01c type of stroke.)The matrix hides any rows and columns with small valuesin order to emphasize key return adjustments. To ease navi-gation of this large amount of returns, we group columnsand rows according to the stroke technique of the \ufb01rst andsecond stroke in the return, respectively. Initially, detailedinformation for each entry is not visible. Instead, groupblocks are displayed, where the lightness of a block encodesthe average impacts of returns within that block.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "matrix+text", "axial_code": [], "componenet_code": ["text", "matrix"]}, {"solution_text": "If a user isinterested in a block of returns, she/he can click the blockand examine the detailed impacts of each return within theblock (Fig. 4B-2). When a user hovers over an entry, the sizeof the impact will be displayed (Fig. 4B-3). Users can alsoclick an entry to select particular a return adjustment ofPlayer A, to explore how the impacts of this adjustment aregenerated and accumulated.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 609, "paper_title": "SimuExplorer: Visual Exploration of Game Simulation in Table Tennis", "pub_year": 2023, "domain": "Sports", "requirement": {"requirement_text": "How do the impacts brought by a return adjustment accu- mulate over tactics in a rally? A rally in table tennis contains a sequence of tactics. The experts hope to explore how the impacts of a return adjustment in sequential tactics accumulate in a rally.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "We collected data from 306 table tennis matches among 21 top players (10 males and 11 females) from 2005 to 2012. The data was collected by manually coding videos. We have tried using machine learning models to automatically extract structured data from video. However, low video quality means that the accuracy of automatically extracted stroke features, such as stroke technique, is relatively low (around 75%). Each match was collected as a CSV file, which contained hundreds of rows representing the hun_x0002_dreds of strokes in the match. Each row records different features of a stroke as follows. _x0001_Rally ID denotes the rally the stroke belongs to. Stroke ID is the sequence number of the stroke in the rally. Stroke player denotes the player who gave the stroke. Stroke technique denotes the technique the player uses to give the stroke. There are 14 techniques. Ball position denotes the drop point of the stroke. There are ten drop points.", "data_code": {"categorical": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "Lames [6] first proposed simulating a table tennis rally through a Markov chain model. The model views a rally as a sequence of possible stroke states, and simulates the impacts of strokes on players\u2019 scoring rates. Here we go through how Lames\u2019 model [6] has been applied in previous studies and explain how the model simulates a rally. Then we propose a modified model and explain how intermediate results of the model measure returns and impacts of return adjustments.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "This view contains a matrix similar to that in Fig. 2A.Each entry in the matrix (Fig. 4B-1) represents one return ofPlayer A. The column headers denote the combinations ofstroke technique and ball position values at the \ufb01rst strokeof the return (given by Player A), and row headers denotethose at the second stroke (given by opponents of Player A).The area of the rectangle in an entry encodes the impact ofadjusting the corresponding return. (To reiterate, adjustinga return means successfully in\ufb02uencing the opponents ofPlayer A to use a speci\ufb01c type of second stroke more fre-quently after Player A uses another speci\ufb01c type of stroke.)The matrix hides any rows and columns with small valuesin order to emphasize key return adjustments. To ease navi-gation of this large amount of returns, we group columnsand rows according to the stroke technique of the \ufb01rst andsecond stroke in the return, respectively. Initially, detailedinformation for each entry is not visible. Instead, groupblocks are displayed, where the lightness of a block encodesthe average impacts of returns within that block.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "matrix+text", "axial_code": [], "componenet_code": ["text", "matrix"]}, {"solution_text": "Click an impact in the adjustment view and examine its accumulation in the accumulation view. The user can further select the impact of an interesting return adjustment by clicking it in the adjustment view.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}, {"solution_text": "Click an impact in the adjustment view and examine its accumulation in the accumulation view. The user can further select the impact of an interesting return adjustment by clicking it in the adjustment view. The accumulation view provides visual tracking of the accumulation of impacts over tactics.", "solution_category": "visualization", "solution_axial": "annotation", "solution_compoent": "flow+bar+pie", "axial_code": [], "componenet_code": ["bar", "pie", "flow"]}]}, {"author": "zsz", "index_original": 610, "paper_title": "SimuExplorer: Visual Exploration of Game Simulation in Table Tennis", "pub_year": 2023, "domain": "Sports", "requirement": {"requirement_text": "How do the impacts brought by a return adjustment accu- mulate over tactics in a rally? A rally in table tennis contains a sequence of tactics. The experts hope to explore how the impacts of a return adjustment in sequential tactics accumulate in a rally.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "We collected data from 306 table tennis matches among 21 top players (10 males and 11 females) from 2005 to 2012. The data was collected by manually coding videos. We have tried using machine learning models to automatically extract structured data from video. However, low video quality means that the accuracy of automatically extracted stroke features, such as stroke technique, is relatively low (around 75%). Each match was collected as a CSV file, which contained hundreds of rows representing the hun_x0002_dreds of strokes in the match. Each row records different features of a stroke as follows. _x0001_Rally ID denotes the rally the stroke belongs to. Stroke ID is the sequence number of the stroke in the rally. Stroke player denotes the player who gave the stroke. Stroke technique denotes the technique the player uses to give the stroke. There are 14 techniques. Ball position denotes the drop point of the stroke. There are ten drop points.", "data_code": {"categorical": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "Lames [6] first proposed simulating a table tennis rally through a Markov chain model. The model views a rally as a sequence of possible stroke states, and simulates the impacts of strokes on players\u2019 scoring rates. Here we go through how Lames\u2019 model [6] has been applied in previous studies and explain how the model simulates a rally. Then we propose a modified model and explain how intermediate results of the model measure returns and impacts of return adjustments.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "This view contains a matrix similar to that in Fig. 2A.Each entry in the matrix (Fig. 4B-1) represents one return ofPlayer A. The column headers denote the combinations ofstroke technique and ball position values at the \ufb01rst strokeof the return (given by Player A), and row headers denotethose at the second stroke (given by opponents of Player A).The area of the rectangle in an entry encodes the impact ofadjusting the corresponding return. (To reiterate, adjustinga return means successfully in\ufb02uencing the opponents ofPlayer A to use a speci\ufb01c type of second stroke more fre-quently after Player A uses another speci\ufb01c type of stroke.)The matrix hides any rows and columns with small valuesin order to emphasize key return adjustments. To ease navi-gation of this large amount of returns, we group columnsand rows according to the stroke technique of the \ufb01rst andsecond stroke in the return, respectively. Initially, detailedinformation for each entry is not visible. Instead, groupblocks are displayed, where the lightness of a block encodesthe average impacts of returns within that block.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "matrix+text", "axial_code": [], "componenet_code": ["text", "matrix"]}, {"solution_text": "After a user selects a return adjustment of Player A, theaccumulation view (Fig. 4C) enables visual tracking of howthe impacts of this return adjustment accumulate in eachtactic (T3).", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}, {"solution_text": "This view contains a \ufb02ow chart that represents the varia-tion in Player A\u2019s scoring rate after he or she adjusts a returnduring his or her tactics (Fig. 3C). Flow charts are widelyused to visualize temporal variations in variables [50], [51],[52]. In the accumulation view, the \ufb02ow chart intuitivelyillustrates the process through which impacts accumulategradually over the course of different tactics and \ufb01nallyaggregate into the total impact. In the accumulation view,the width of the main \ufb02ow (Fig. 4C-4) encodes the accumu-lated changed scoring rate, and the width of branches(Fig. 4C-1) encodes the variation of the scoring rate at eachtactic. When users hover on each branch, the original scor-ing rate (above) and changed scoring rate (below) are dis-played in a detail view (Fig. 4C-3). For the changed scoringrate, a pie chart displays how much of this rate change wascaused by the adjustment at the current tactic versus howmuch was caused by adjustments at previous tactics.The bar chart (Fig. 4C-7) of each tactic shows six strokestates, arranged based on their impact on scoring probabili-ties when they are at that tactic\u2019s last stroke. The \ufb01rst threestates displayed are those that do the most to increase scor-ing probabilities. The \ufb01nal three displayed are those that dothe most to decrease those probabilities. The height of theorange bar encodes the increase in scoring probability,while the height of the blue bar encodes the decrease. Thishelps users to see how adjustments impact a tactic\u2019s laststroke and the corresponding scoring probabilities.", "solution_category": "visualization", "solution_axial": "annotation", "solution_compoent": "flow+bar+pie", "axial_code": [], "componenet_code": ["bar", "pie", "flow"]}]}, {"author": "zsz", "index_original": 611, "paper_title": "SimuExplorer: Visual Exploration of Game Simulation in Table Tennis", "pub_year": 2023, "domain": "Sports", "requirement": {"requirement_text": "How does a return adjustment influence the scoring chan- ces at a tactic? The experts hope to examine how the impacts are generated in a tactic and how many impacts in a tactic are due to previous tactics.", "requirement_code": {"identify_main_cause_aggregate": 1}}, "data": {"data_text": "We collected data from 306 table tennis matches among 21 top players (10 males and 11 females) from 2005 to 2012. The data was collected by manually coding videos. We have tried using machine learning models to automatically extract structured data from video. However, low video quality means that the accuracy of automatically extracted stroke features, such as stroke technique, is relatively low (around 75%). Each match was collected as a CSV file, which contained hundreds of rows representing the hun_x0002_dreds of strokes in the match. Each row records different features of a stroke as follows. _x0001_Rally ID denotes the rally the stroke belongs to. Stroke ID is the sequence number of the stroke in the rally. Stroke player denotes the player who gave the stroke. Stroke technique denotes the technique the player uses to give the stroke. There are 14 techniques. Ball position denotes the drop point of the stroke. There are ten drop points.", "data_code": {"categorical": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "Lames [6] first proposed simulating a table tennis rally through a Markov chain model. The model views a rally as a sequence of possible stroke states, and simulates the impacts of strokes on players\u2019 scoring rates. Here we go through how Lames\u2019 model [6] has been applied in previous studies and explain how the model simulates a rally. Then we propose a modified model and explain how intermediate results of the model measure returns and impacts of return adjustments.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "This view contains a \ufb02ow chart that represents the varia-tion in Player A\u2019s scoring rate after he or she adjusts a returnduring his or her tactics (Fig. 3C). Flow charts are widelyused to visualize temporal variations in variables [50], [51],[52]. In the accumulation view, the \ufb02ow chart intuitivelyillustrates the process through which impacts accumulategradually over the course of different tactics and \ufb01nallyaggregate into the total impact. In the accumulation view,the width of the main \ufb02ow (Fig. 4C-4) encodes the accumu-lated changed scoring rate, and the width of branches(Fig. 4C-1) encodes the variation of the scoring rate at eachtactic. When users hover on each branch, the original scor-ing rate (above) and changed scoring rate (below) are dis-played in a detail view (Fig. 4C-3). For the changed scoringrate, a pie chart displays how much of this rate change wascaused by the adjustment at the current tactic versus howmuch was caused by adjustments at previous tactics.The bar chart (Fig. 4C-7) of each tactic shows six strokestates, arranged based on their impact on scoring probabili-ties when they are at that tactic\u2019s last stroke. The \ufb01rst threestates displayed are those that do the most to increase scor-ing probabilities. The \ufb01nal three displayed are those that dothe most to decrease those probabilities. The height of theorange bar encodes the increase in scoring probability,while the height of the blue bar encodes the decrease. Thishelps users to see how adjustments impact a tactic\u2019s laststroke and the corresponding scoring probabilities.", "solution_category": "visualization", "solution_axial": "annotation", "solution_compoent": "flow+bar+pie", "axial_code": [], "componenet_code": ["bar", "pie", "flow"]}, {"solution_text": "Click a tactic in the accumulation view and examine how the impact is generated at this tactic in the impact view.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}, {"solution_text": "Click a tactic in the accumulation view and examine how the impact is generated at this tactic in the impact view. When the user clicks a tactic in the accumulation view, the impact view presents the generation process of the impact at that tactic.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "bar+matrix", "axial_code": [], "componenet_code": ["bar", "matrix"]}]}, {"author": "zsz", "index_original": 612, "paper_title": "SimuExplorer: Visual Exploration of Game Simulation in Table Tennis", "pub_year": 2023, "domain": "Sports", "requirement": {"requirement_text": "How does a return adjustment influence the scoring chan- ces at a tactic? The experts hope to examine how the impacts are generated in a tactic and how many impacts in a tactic are due to previous tactics.", "requirement_code": {"identify_main_cause_aggregate": 1}}, "data": {"data_text": "We collected data from 306 table tennis matches among 21 top players (10 males and 11 females) from 2005 to 2012. The data was collected by manually coding videos. We have tried using machine learning models to automatically extract structured data from video. However, low video quality means that the accuracy of automatically extracted stroke features, such as stroke technique, is relatively low (around 75%). Each match was collected as a CSV file, which contained hundreds of rows representing the hun_x0002_dreds of strokes in the match. Each row records different features of a stroke as follows. _x0001_Rally ID denotes the rally the stroke belongs to. Stroke ID is the sequence number of the stroke in the rally. Stroke player denotes the player who gave the stroke. Stroke technique denotes the technique the player uses to give the stroke. There are 14 techniques. Ball position denotes the drop point of the stroke. There are ten drop points.", "data_code": {"categorical": 1, "tables": 1, "textual": 1}}, "solution": [{"solution_text": "Lames [6] first proposed simulating a table tennis rally through a Markov chain model. The model views a rally as a sequence of possible stroke states, and simulates the impacts of strokes on players\u2019 scoring rates. Here we go through how Lames\u2019 model [6] has been applied in previous studies and explain how the model simulates a rally. Then we propose a modified model and explain how intermediate results of the model measure returns and impacts of return adjustments.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "This view contains a \ufb02ow chart that represents the varia-tion in Player A\u2019s scoring rate after he or she adjusts a returnduring his or her tactics (Fig. 3C). Flow charts are widelyused to visualize temporal variations in variables [50], [51],[52]. In the accumulation view, the \ufb02ow chart intuitivelyillustrates the process through which impacts accumulategradually over the course of different tactics and \ufb01nallyaggregate into the total impact. In the accumulation view,the width of the main \ufb02ow (Fig. 4C-4) encodes the accumu-lated changed scoring rate, and the width of branches(Fig. 4C-1) encodes the variation of the scoring rate at eachtactic. When users hover on each branch, the original scor-ing rate (above) and changed scoring rate (below) are dis-played in a detail view (Fig. 4C-3). For the changed scoringrate, a pie chart displays how much of this rate change wascaused by the adjustment at the current tactic versus howmuch was caused by adjustments at previous tactics.The bar chart (Fig. 4C-7) of each tactic shows six strokestates, arranged based on their impact on scoring probabili-ties when they are at that tactic\u2019s last stroke. The \ufb01rst threestates displayed are those that do the most to increase scor-ing probabilities. The \ufb01nal three displayed are those that dothe most to decrease those probabilities. The height of theorange bar encodes the increase in scoring probability,while the height of the blue bar encodes the decrease. Thishelps users to see how adjustments impact a tactic\u2019s laststroke and the corresponding scoring probabilities.", "solution_category": "visualization", "solution_axial": "annotation", "solution_compoent": "flow+bar+pie", "axial_code": [], "componenet_code": ["bar", "pie", "flow"]}, {"solution_text": "After a user selects a tactic, the impact view (Fig. 4D)presents how the impact is generated through the tworeturns within the tactic.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}, {"solution_text": "After a user selects a tactic, the impact view (Fig. 4D)presents how the impact is generated through the tworeturns within the tactic. The impact view must display theadjusted many-to-many probabilities of states in the \ufb01rstreturn, the changed probabilities of states in the secondreturn, and the scoring rate after the third stroke (T4).Justi\ufb01cation: There exist more than two hundred returnstates, with each state involving a speci\ufb01c \ufb01rst stroke andsecond stroke (Fig. 2). Displaying probabilities of returnstates is a many-to-many relation visualization problem.Flow or Matrix. As discussed in Section 3.3, existing visu-alization studies employ node-link diagrams, \ufb02ow charts,and matrix views [53] to display many-to-many relations.We initially employed \ufb02ow charts to present the changedprobabilities, thinking they would be intuitive to under-stand. However, these ended up visually cluttered due tothe many probabilities involved. We thus switched to a clut-ter-free matrix view to help users browse how probabilitieswere changed by the adjustments.Layout. Two matrices display changing probabilities overtwo returns. As discussed in Section 3.3, the layouts used byMatrixWave [49] and iTTVis [22] were not suitable for thisproject. Instead, we propose our current design, whichincludes clutter-free matrix views and straight leader lineswith interactions that help users understand and interprethow impacts compile within a tactic. The details of thedesign are introduced as follows.Description. The impact view uses a pair of matrices to rep-resent the changed probabilities of the states at the tworeturns of a tactic, thus explaining the varying scoring rateafter the third stroke of this tactic. The \ufb01rst matrix (Fig. 4D-2)presents how adjustments are made at the \ufb01rst return of thetactic. The second matrix (Fig. 4D-4) displays how the adjust-ment changes the probabilities of states at the second returnand further changes the scoring rate after the third stroke.The three columns of stroke bars (Figs. 4D-1, 4D-3, and 4D-5)from left to right represent the changed probabilities ofstroke states (different stroke technique and ball position val-ues) at the three strokes in a tactic. These probabilities in eachbar column are derived by aggregating the probabilities incorresponding columns in the prior matrix. The detailedencodings are as follows.Bars for Strokes. Three columns of bars (Figs. 4D-1, 4D-3,and 4 D-5) present the changed probabilities of stroke statesat the \ufb01rst, second, and third strokes. The height of each barencodes the changed probability of each stroke state. The barcolor indicates whether the probability increases (orange) ordecreases (blue). The probability of each stroke state isdivided into two parts. The \ufb01rst part is the changed scoringprobability after the stroke, i.e., the change in the probabilitythat the player gives a stroke with this state and scoresdirectly. This is encoded by bars on the left. (This part of the\ufb01rst stroke is not shown, because it is not considered in thecurrent tactic). The second part is the change in the probabil-ity that the player gives a stroke with this state and does notscore. This part is encoded by bars on the right. When usershover on each part of a bar, the corresponding rows and col-umns in the two matrices are highlighted (Fig. 4D). The origi-nal and changed probabilities are also displayed in the detailview (Fig. 5C). A pie chart displays how much of this proba-bility change was caused by the adjustment at the currenttactic versus how much was caused by adjustments at previ-ous tactics.Matrices for Returns. A pair of matrices presents thechanged probabilities of all states that make up the tworeturns within a tactic. The \ufb01rst matrix (Fig. 4D-2) presentsthe adjustments (Fig. 3B) to the \ufb01rst return in the tactic. Thesecond matrix (Fig. 4D-4) presents the impacts on the sec-ond return of the tactic (Fig. 3A). Similar to the layout ofreturn states in Fig. 2A, the changed probabilities of returnstates are arranged as a matrix whose column and rowheaders represent the stroke state at the \ufb01rst and second ofthe two strokes, respectively. The headers are linked bybars representing aggregate changed probabilities. As theimpacts pass through the matrix, the changed probabilitiesof stroke states branch into the changed probabilities ofreturn states, and then merge into the changed probabilitiesof the next stroke states. With this detailed display ofchanged probabilities of return states, experts can betterunderstand how state probabilities of a return are adjusted,how adjustments to previous returns change the state prob-abilities of following returns, and how these changed proba-bilities aggregate into changed scoring rates. When theserve tactic (i.e., the tactic comprising Strokes 1\u20133) is pre-sented in the impact view, the columns of the \ufb01rst matrixare altered because the \ufb01rst stroke in the serve tactic mustbe a serve stroke.In each matrix entry, the area of the circle encodes thechanged probability as it transforms from the columnheader to the row header. The circle color indicates whetherthe probability increases (orange) or decreases (blue). Aswitch button is placed at the bottom of the bars for thethird stroke. The right part of the button (corresponding tothe right part of the bars) is enabled by default. Circles inentries of the second matrix represent the changed probabil-ities from the column headers to the row headers. Whenusers switch to the left part of the button (which corre-sponds to the left part of the bars), the circles in the entriesof the second matrix are transformed into rectangles. Theareas of the rectangles represent the changed scoring proba-bilities at the third stroke due to the changed probabilitiesof the \ufb01rst and second. When users hover on an entry, thecorresponding row and column are highlighted, and adetail view (Fig. 5B) is displayed.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "bar+matrix", "axial_code": [], "componenet_code": ["bar", "matrix"]}, {"solution_text": "When users switch to the left part of the button (which corre-sponds to the left part of the bars), the circles in the entriesof the second matrix are transformed into rectangles. Theareas of the rectangles represent the changed scoring proba-bilities at the third stroke due to the changed probabilitiesof the \ufb01rst and second. When users hover on an entry, thecorresponding row and column are highlighted, and adetail view (Fig. 5B) is displayed.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 613, "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling", "pub_year": 2023, "domain": "urban planning", "requirement": {"requirement_text": "E1: Summarize Trip Set. Users analyze data with spe-cific conditions, such as trips that took place duringweekend or peak time, rather than the entire data.Thus, they apply filters to summarize the trip set[Summarize ! Trip Set].", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.", "data_code": {"tables": 1, "temporal": 1, "ordinal": 1, "sequential": 1}}, "solution": [{"solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes.", "solution_category": "data_manipulation", "solution_axial": "Excluding,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding"], "componenet_code": ["algorithmic_calculation", "excluding"]}, {"solution_text": "The OD-Trip view (Fig. 2B) supports the interactive modifica_x0002_tion of filtering conditions applied to the active trip set (Task E1). The filtering conditions are represented as badges in the view header (Fig. 2 (1)). The conditions can be divided into two types: by departure time and by attributes. The two time bar charts (i.e., two bar charts on the left of the OD-Trip view) summarize the num_x0002_ber of trips aggregated by departure time, such as time of day (AM peak (from 07:00 to 10:00), Mid-day (between AM and PM peak), PM peak (from 17:00 to 20:00), and Overnight (between PM and AM peak)), and day of the week. All these time spans were determined, reflecting domain experts\u2019 exploration practice identified during the domain situation analysis. The attributes panel on the right visualizes the active trip set\u2019s OD pairs and associated trips with their attributes. In this panel, a column represents either an OD or route attri_x0002_bute of OD pairs. A column header shows the distribution of the corresponding attribute as a matrix or a histogram. Below the column headers, each row (Fig. 4 (1)) represents an OD pair and its attribute values.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "bar+matrix", "axial_code": [], "componenet_code": ["bar", "matrix"]}, {"solution_text": "The OD-Trip view (Fig. 2B) supports the interactive modifica_x0002_tion of filtering conditions applied to the active trip set (Task E1).", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 614, "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling", "pub_year": 2023, "domain": "urban planning", "requirement": {"requirement_text": "E2: Explore Geographical Distribution of Trips. Usersexplore how riders\u2019 trips are geographically distrib-uted, especially areas, flows, or roads with heavy traf-fic [Explore ! Feature].", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.", "data_code": {"geometry": 1, "ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes.", "solution_category": "data_manipulation", "solution_axial": "Excluding,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding"], "componenet_code": ["algorithmic_calculation", "excluding"]}, {"solution_text": "To represent the other two targets (i.e., OD pair and road segment), we overlay two vis- ualizations on the map view: a flow map and a road heatmap. These allow users to explore the geo- graphical distribution of different targets; in the flow map, trips are aggregated and shown as flows between OD pairs, while in the road heatmap, the traffic on individ- ual roads is color-encoded.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "map+flow+heatmap+glyph", "axial_code": [], "componenet_code": ["heatmap", "map", "flow", "glyph"]}]}, {"author": "zsz", "index_original": 615, "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling", "pub_year": 2023, "domain": "urban planning", "requirement": {"requirement_text": "E2: Explore Geographical Distribution of Trips. Usersexplore how riders\u2019 trips are geographically distrib-uted, especially areas, flows, or roads with heavy traf-fic [Explore ! Feature].", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.", "data_code": {"geometry": 1, "ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes.", "solution_category": "data_manipulation", "solution_axial": "Excluding,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding"], "componenet_code": ["algorithmic_calculation", "excluding"]}, {"solution_text": "In designing the station view, we mainly considered theconsistency and interactivity of the map. The reason fordoing so is to allow users to identify the geographical distri-bution of data represented in the station view to performTask E2. For example, we make the color of the total trafficbars the same as that of the station symbol in the map view.The shape on the left of the total traffic bar represents a sta-tion type and is also the same as the map view. The OD barsof in-flow and out-flow share the same color and thicknessas the map view\u2019s edge.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "bar+matrix", "axial_code": [], "componenet_code": ["bar", "matrix"]}]}, {"author": "zsz", "index_original": 616, "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling", "pub_year": 2023, "domain": "urban planning", "requirement": {"requirement_text": "E2: Explore Geographical Distribution of Trips. Usersexplore how riders\u2019 trips are geographically distrib-uted, especially areas, flows, or roads with heavy traf-fic [Explore ! Feature].", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.", "data_code": {"geometry": 1, "ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes.", "solution_category": "data_manipulation", "solution_axial": "Excluding,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding"], "componenet_code": ["algorithmic_calculation", "excluding"]}, {"solution_text": "The route view shows the details of all routes taken between a particular OD pair, such as matched paths and route attributes. Unlike the aforementioned views, the route view allows users to take a detailed look at geographical dis- tribution or route attribute distribution within a single OD pair.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "map+path", "axial_code": [], "componenet_code": ["link", "map"]}, {"solution_text": "Unlike the aforementioned views, the route view allows users to take a detailed look at geographical dis- tribution or route attribute distribution within a single OD pair.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 617, "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling", "pub_year": 2023, "domain": "urban planning", "requirement": {"requirement_text": "E3: Identify Attribute Distribution of Chosen Routes.Users inspect distributions of chosen routes\u2019 attri-bute values in each OD pair [Identify ! Distribution].Thus, they can obtain an overview of riders\u2019 percep-tions of the route attributes and how biased the cho-sen attribute values are before modeling.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.", "data_code": {"geometry": 1, "ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes.", "solution_category": "data_manipulation", "solution_axial": "Excluding,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding"], "componenet_code": ["algorithmic_calculation", "excluding"]}, {"solution_text": "The route view shows the details of all routes taken between a particular OD pair, such as matched paths and route attributes. Unlike the aforementioned views, the route view allows users to take a detailed look at geographical dis- tribution or route attribute distribution within a single OD pair.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "map+path", "axial_code": [], "componenet_code": ["link", "map"]}, {"solution_text": "Unlike the aforementioned views, the route view allows users to take a detailed look at geographical dis- tribution or route attribute distribution within a single OD pair.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "zsz", "index_original": 618, "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling", "pub_year": 2023, "domain": "urban planning", "requirement": {"requirement_text": "E3: Identify Attribute Distribution of Chosen Routes.Users inspect distributions of chosen routes\u2019 attri-bute values in each OD pair [Identify ! Distribution].Thus, they can obtain an overview of riders\u2019 percep-tions of the route attributes and how biased the cho-sen attribute values are before modeling.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.", "data_code": {"geometry": 1, "ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes.", "solution_category": "data_manipulation", "solution_axial": "Excluding,AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding"], "componenet_code": ["algorithmic_calculation", "excluding"]}, {"solution_text": "Route Choice Behaviors. Before modeling, the experts attempted to hypothesize about route choice behavior by checking the distortion of the distribution of route attributes. In the OD bubble plot, the total mean nonpara- metric skew for Route Distance was about 0.23, indicating that route choices were biased toward routes with relatively short distances.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "bubble", "axial_code": [], "componenet_code": ["bubble"]}]}, {"author": "zsz", "index_original": 619, "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling", "pub_year": 2023, "domain": "urban planning", "requirement": {"requirement_text": "M1: Perform Modeling with Different Sets of Hyperpara-meters. Users perform choice set generation andmodel estimation with various sets of hyperpara-meters to \ufb01nd a meaningful model instance with ahigh goodness of \ufb01t [Derive ! Model Instance].", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.", "data_code": {"ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes; The general process of route choice modeling is twofold: choice set generation and model estimation. In this section, we will briefly describe the concept of each step and intro_x0002_duce the methods used in our study.", "solution_category": "data_manipulation", "solution_axial": "Excluding,AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding", "Modeling"], "componenet_code": ["algorithmic_calculation", "excluding", "modeling"]}, {"solution_text": "The configuration view allows users to specify hyperparameter configurations for the two procedures. The configuration view (Fig. 6B) allows users to produce hyperparameter configurations for choice set generation and model estimation. We chose a data-driven approach to gener_x0002_ate choice sets, where we cluster the observed routes (routes that are actually taken). Once the choice sets for all trips are generated, we fit a model that predicts the probability of routes being chosen from their characteristics (i.e., route attributes).", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "table+matrix+bar", "axial_code": [], "componenet_code": ["bar", "matrix", "table"]}, {"solution_text": "The configuration view allows users to specify hyperparameter configurations for the two procedures.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "zsz", "index_original": 620, "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling", "pub_year": 2023, "domain": "urban planning", "requirement": {"requirement_text": "M2: Obtain an Overview of Model Instances. Usersobtain an overview of many different model instancesto identify their common or different patterns [Sum-marize ! Model Instance].", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.", "data_code": {"ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes; The general process of route choice modeling is twofold: choice set generation and model estimation. In this section, we will briefly describe the concept of each step and intro_x0002_duce the methods used in our study.", "solution_category": "data_manipulation", "solution_axial": "Excluding,AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding", "Modeling"], "componenet_code": ["algorithmic_calculation", "excluding", "modeling"]}, {"solution_text": "The model view (Fig. 6C) supports an Overview+Detail approach for exploring the model instances. From the overview (Fig. 6C.1), users can grasp overall pat_x0002_terns of the model instances (Task M2).", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}]}, {"author": "zsz", "index_original": 621, "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling", "pub_year": 2023, "domain": "urban planning", "requirement": {"requirement_text": "M3: Compare Model Instances. Users compare statis-tics and estimates between model instances to choosea model instance for explaining route choice behav-iors [Compare ! Model Instance].", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.", "data_code": {"ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes; The general process of route choice modeling is twofold: choice set generation and model estimation. In this section, we will briefly describe the concept of each step and intro_x0002_duce the methods used in our study.", "solution_category": "data_manipulation", "solution_axial": "Excluding,AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding", "Modeling"], "componenet_code": ["algorithmic_calculation", "excluding", "modeling"]}, {"solution_text": "The model view (Fig. 6C) supports an Overview+Detail approach for exploring the model instances. From the detail (Fig. 6C.2), users can compare the instances with the help of the interactions, such as sorting, grouping, and hiding unnecessary results (Task M3). If there is an interesting model instance during the analysis, further investigation of the instance can be done in the reasoning interface.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "table+bar", "axial_code": [], "componenet_code": ["bar", "table"]}, {"solution_text": "The model view (Fig. 6C) supports an Overview+Detail approach for exploring the model instances. From the detail (Fig. 6C.2), users can compare the instances with the help of the interactions, such as sorting, grouping, and hiding unnecessary results (Task M3). If there is an interesting model instance during the analysis, further investigation of the instance can be done in the reasoning interface.", "solution_category": "interaction", "solution_axial": "Reconfigure,Participation/Collaboration,Filtering", "solution_compoent": "", "axial_code": ["Reconfigure", "Filtering", "Participation/Collaboration"], "componenet_code": ["reconfigure", "filtering", "participation_collaboration"]}]}, {"author": "zsz", "index_original": 622, "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling", "pub_year": 2023, "domain": "urban planning", "requirement": {"requirement_text": "M3: Compare Model Instances. Users compare statis-tics and estimates between model instances to choosea model instance for explaining route choice behav-iors [Compare ! Model Instance].", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.", "data_code": {"ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes; The general process of route choice modeling is twofold: choice set generation and model estimation. In this section, we will briefly describe the concept of each step and intro_x0002_duce the methods used in our study.", "solution_category": "data_manipulation", "solution_axial": "Excluding,AlgorithmicCalculation,Modeling", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Excluding", "Modeling"], "componenet_code": ["algorithmic_calculation", "excluding", "modeling"]}, {"solution_text": "For an effective comparison between the model instances(Task M3), the model instance table supports sorting orgrouping rows by each column or hiding rows that do notseem important. The columns representing numerical val-ues, such as the _x0016_r2 (rhoSB in the interface), can be used tosort rows. Other columns related to the set of hyperpara-meters, such as the k (LL), the set of distances (GG), or the setof model attributes (SS), can be used to group rows. Thecommon analysis scenario using grouping is to investigatethe effect of the set composition of model attributes (SS) onmodel instances; users can group by SS, as in Fig. 6C.2.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "table+bar", "axial_code": [], "componenet_code": ["bar", "table"]}]}, {"author": "zsz", "index_original": 623, "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling", "pub_year": 2023, "domain": "urban planning", "requirement": {"requirement_text": "R1: Discover Route Choices Contributing to an Estima-tion Result. Users discover trips and OD pairs thatfollow the model\u2019s estimated coef\ufb01cients well. Forexample, when a model instance has a negative coef-\ufb01cient for Route Distance, trips with a relatively shorttravel distance within their OD pair are deemed tocontribute to the estimation result. As users investi-gate such route choices, they aim to gain a deeperunderstanding of the model and better explain theroute choice behaviors [Summarize ! Trip Set].", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.", "data_code": {"geometry": 1, "ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "To this end, users can brush OD pairs having high ECS in the OD bubble plot and closely inspect their characteristics in the map view or the station view. By doing so, users can determine which trips or OD pairs mainly contributed to estimating the coefficients.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "zsz", "index_original": 624, "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling", "pub_year": 2023, "domain": "urban planning", "requirement": {"requirement_text": "R2: Re-estimate to Obtain Better Fitting Model Instances.An essential premise of RCM is that all individualroute choices have rationality. Therefore, if usersencounter route choices that seem irrational in rea-soning, they remove these trips or OD pairs and re-estimate the model. Their goal is to get a re\ufb01nedmodel instance that is well \ufb01tted to the data and betterre\ufb02ects riders\u2019 perceptions [Derive ! Model Instance].", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.", "data_code": {"geometry": 1, "ordinal": 1, "tables": 1}}, "solution": [{"solution_text": "The reasoning interface facilitates re-estimation of the active model instance by applying more filtering conditions. The general workflow of the re-estimation pro- cess is shown in figure.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "dxf", "index_original": 0, "paper_title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts", "pub_year": 2023, "domain": "Disease", "requirement": {"requirement_text": "DG 1: Guided Analysis based on Three Modalities. To facilitate an effective workflow, with computational analysis support (specifically ML), our system should help the user prioritize more salient (1) features, (2) regions, and (3) subjects to choose data subsets for detailed analysis. The regions and features should be standard and interpretable so that experts can easily grasp the physiological basis, assimilate existing literature, and make hypotheses. Due to a large feature space relative to the number of scans, we must strive to avoid overfitting, reduce ranking instability, and highlight the uncertainties. ", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "MRI parameters, such as gradient direction, b-value, and voxel resolution, have a crucial impact on the scalar meas_x0002_urements used for a clinical study. To prevent errors, the MRI images provided by PPMI are collected based on stan_x0002_dardized and strict acquisition protocols developed by the steering committee on 3T Siemens scanners. Each visit includes DTI and T1-weighted images. For each DTI image, a 2D echo-planar DTI sequence is acquired with the following parameters: TR \u00bc 900 ms, TE \u00bc 88 ms, image matrix \u00bc 116 _x0003_ 116 _x0003_ 72 and voxel resolution \u00bc 1:98 _x0003_ 1:98_x0003_ 2 _x0003_ mm3, 64 gradient volumes (b \u00bc 1; 000s=mm2), and one non-gradient volume (b \u00bc 0s=mm2). The acquisition parame_x0002_ters for T1-weighted images are as follows: TR \u00bc 2; 300 ms, TE \u00bc 2:98 ms, image matrix\u00bc 160 _x0003_ 240 _x0003_ 256, and voxel res_x0002_olution \u00bc 1 _x0003_ 1 _x0003_ 1 _x0003_ mm3.", "data_code": {"media": 1}}, "solution": [{"solution_text": "Here describe the details of data processing, which con_x0002_sists of three steps: fiber tracking, feature extraction, and cohort formulation. These are performed outside of the VA system. 1) Fiber Tracking. This step generates white matter fiber tracts from the RAW images (from DTI to white matter fiber tracts in Fig. 1). We first convert the RAW images from the Digital Image and Communications in Medicine (DICOM) format to the Neuroimaging Informatics Technology Initia_x0002_tive (NIFTI) format. Then, we perform MRI data denoising and preprocessing, including eddy-current induced distor_x0002_tion correction, motion correction, and susceptibility induced distortion correction, using \u201cdwidenoise\u201d and \u201cdwipreproc\u201d scripts in MRtrix3, which is a recommended data cleaning process that uses FSL\u2019s \u201ceddy\u201d [69],\u201ctoppup\u201d [70], and \u201capplytopup\u201d [71] tools. This can reduce artifacts in MRI images and address many additional effects of noise during brain fiber reconstruction, such as the bias of fiber orientation estimation and error tracking of bifurcated fibers. Then, we fix magnet inhomogeneity (e.g., intensity loss and blurring) and perform image correction (e.g., eddy current correction and head motion correction) using the standard \u201crecon-all\u201d script in FreeSurfer [74]. Afterward, we align the T1-weighted images to the DTI images (intra_x0002_subject registration) using FSL [72]. Intra-subject registra_x0002_tion reduces the distortion in the anatomical structure of fibers extracted from the region of interest (ROI) in a subject. We also perform inter-subject registration using FSL, which applies the standard template (MNI152) to each of the sub_x0002_ject\u2019s MRI images. After intra-registration and inter-registra_x0002_tion, we perform brain parcellation using FSL, which splits the brain into regions. The parcellation based on Free_x0002_Surfer\u2019s default atlas (the Desikan/Killiany cortical atlas), which consists of 42 cortical regions [73]. We then perform brain fiber tractography using a state-of-the-art framework [7], which can facilitate biologically plausible fiber recon_x0002_struction and provide anatomically reliable brain fiber tracts. Afterward, by referring to the brain parcellation information, we can categorize fiber tracts and obtain the corresponding features at the whole-brain level and brain region level.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "The diffusion measures we compute with FreeSurfer include: the raw T2 signal (S0), the eigen values (1;2, and3) representing diffusion in the directions of each of the three eigen vectors of the diffusion tensor, fractional anisotropy (FA), mode of anisotropy (MO), and mean diffu-sivity (MD). With MRtrix3, we also compute the other met-rics, including radial diffusivity (RD), relative anisotropy(RA), axial diffusivity (AD), and the Westin metrics (linear-ity (CL), planarity (CP), and sphericity (CS)). O\u2019Donnellet al. provide the definitions of these measures [77].We use MRtrix3 and FreeSurfer to bundle fibers based on which cortical regions are passed by each fiber. Here, we use FreeSurfer to apply the cortical structure parcellation,which assigns a neuro anatomical label to each corticalregion.Also, since PD has been reported to start from one region and then spread to others, we further divide the bundles into two categories: intra- and inter-connects by referring to the information of passed cortical regions. Intra-connects (or intra-parcel connections) represent connections that both start and end within the same cortical region while the inter-connects (or inter-parcel connections) represent the connec-tions that start and end different cortical regions. Each corti-cal region has both intra- and inter-connects. Note that this definition of inter-connects does not refer to fibers that con-nect the two hemispheres (e.g., commissural fibers).Also, since cortical asymmetry and hemispheric predom-inance have been discovered in neuro degenerative disease[78], the features \u2018Delta-LR\u2019 are extracted to represent asym-metry between the left and right hemispheres. The tract-based features include two categories: cortical region meas-ures and whole brain measures. The former includes the number of fibers, average fiber length, intra- and inter-fiber numbers, intra- and inter-fiber lengths, and \u2018Delta-LR\u2019 aver-age fiber length. The latter includes the number of associa-tion fibers, projection fibers, commissural fibers, and the fibers in each brain lobe. Tensor-based features are averagedover the different bundles of fibers.  Our choice of features is motivated by literature review and to favor interpretability, follow standard conventions,and support fiber-tract-based analysis.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "dxf", "index_original": 1, "paper_title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts", "pub_year": 2023, "domain": "Disease", "requirement": {"requirement_text": "DG 1: Guided Analysis based on Three Modalities. To facilitate an effective workflow, with computational analysis support (specifically ML), our system should help the user prioritize more salient (1) features, (2) regions, and (3) subjects to choose data subsets for detailed analysis. The regions and features should be standard and interpretable so that experts can easily grasp the physiological basis, assimilate existing literature, and make hypotheses. Due to a large feature space relative to the number of scans, we must strive to avoid overfitting, reduce ranking instability, and highlight the uncertainties. ", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "MRI parameters, such as gradient direction, b-value, and voxel resolution, have a crucial impact on the scalar meas_x0002_urements used for a clinical study. To prevent errors, the MRI images provided by PPMI are collected based on stan_x0002_dardized and strict acquisition protocols developed by the steering committee on 3T Siemens scanners. Each visit includes DTI and T1-weighted images. For each DTI image, a 2D echo-planar DTI sequence is acquired with the following parameters: TR \u00bc 900 ms, TE \u00bc 88 ms, image matrix \u00bc 116 _x0003_ 116 _x0003_ 72 and voxel resolution \u00bc 1:98 _x0003_ 1:98_x0003_ 2 _x0003_ mm3, 64 gradient volumes (b \u00bc 1; 000s=mm2), and one non-gradient volume (b \u00bc 0s=mm2). The acquisition parame_x0002_ters for T1-weighted images are as follows: TR \u00bc 2; 300 ms, TE \u00bc 2:98 ms, image matrix\u00bc 160 _x0003_ 240 _x0003_ 256, and voxel res_x0002_olution \u00bc 1 _x0003_ 1 _x0003_ 1 _x0003_ mm3.", "data_code": {"media": 1}}, "solution": [{"solution_text": "Here describe the details of data processing, which con_x0002_sists of three steps: fiber tracking, feature extraction, and cohort formulation. These are performed outside of the VA system. 1) Fiber Tracking. This step generates white matter fiber tracts from the RAW images (from DTI to white matter fiber tracts in Fig. 1). We first convert the RAW images from the Digital Image and Communications in Medicine (DICOM) format to the Neuroimaging Informatics Technology Initia_x0002_tive (NIFTI) format. Then, we perform MRI data denoising and preprocessing, including eddy-current induced distor_x0002_tion correction, motion correction, and susceptibility induced distortion correction, using \u201cdwidenoise\u201d and \u201cdwipreproc\u201d scripts in MRtrix3, which is a recommended data cleaning process that uses FSL\u2019s \u201ceddy\u201d [69],\u201ctoppup\u201d [70], and \u201capplytopup\u201d [71] tools. This can reduce artifacts in MRI images and address many additional effects of noise during brain fiber reconstruction, such as the bias of fiber orientation estimation and error tracking of bifurcated fibers. Then, we fix magnet inhomogeneity (e.g., intensity loss and blurring) and perform image correction (e.g., eddy current correction and head motion correction) using the standard \u201crecon-all\u201d script in FreeSurfer [74]. Afterward, we align the T1-weighted images to the DTI images (intra_x0002_subject registration) using FSL [72]. Intra-subject registra_x0002_tion reduces the distortion in the anatomical structure of fibers extracted from the region of interest (ROI) in a subject. We also perform inter-subject registration using FSL, which applies the standard template (MNI152) to each of the sub_x0002_ject\u2019s MRI images. After intra-registration and inter-registra_x0002_tion, we perform brain parcellation using FSL, which splits the brain into regions. The parcellation based on Free_x0002_Surfer\u2019s default atlas (the Desikan/Killiany cortical atlas), which consists of 42 cortical regions [73]. We then perform brain fiber tractography using a state-of-the-art framework [7], which can facilitate biologically plausible fiber recon_x0002_struction and provide anatomically reliable brain fiber tracts. Afterward, by referring to the brain parcellation information, we can categorize fiber tracts and obtain the corresponding features at the whole-brain level and brain region level.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Feature Extraction. We extract fiber features from the constructed brain fiber tracts (Fig. 1) and diffusion tensors, using MRtrix3 [28] and FreeSurfer [74]. The features fall into two categories: tract-based and tensor-based. The for_x0002_mer measures regional fiber structures (e.g., density and length) while the latter measures water diffusivity patterns based on a tensor model. Evidence suggests that both cate_x0002_gories are affected by neurodegenerative disease [75], [76]. The diffusion measures we compute with FreeSurfer include: the raw T2 signal (S0), the eigenvalues (_x0002_ 1; _x0002_2, and 3) representing diffusion in the directions of each of the three eigenvectors of the diffusion tensor, fractional anisotropy (FA), mode of anisotropy (MO), and mean diffu_x0002_sivity (MD). With MRtrix3, we also compute the other met_x0002_rics, including radial diffusivity (RD), relative anisotropy (RA), axial diffusivity (AD), and the Westin metrics (linear_x0002_ity (CL), planarity (CP), and sphericity (CS)). O\u2019Donnell et al. provide the definitions of these measures [77]. We use MRtrix3 and FreeSurfer to bundle fibers based on which cortical regions are passed by each fiber. Here, we use FreeSurfer to apply the cortical structure parcellation, which assigns a neuroanatomical label to each cortical region. Also, since PD has been reported to start from one region and then spread to others, we further divide the bundles into two categories: intra- and inter-connects by referring to the information of passed cortical regions. Intra-connects (or intra-parcel connections) represent connections that both start and end within the same cortical region while the inter_x0002_connects (or inter-parcel connections) represent the connec_x0002_tions that start and end different cortical regions. Each corti_x0002_cal region has both intra- and inter-connects. Note that this definition of inter-connects does not refer to fibers that con_x0002_nect the two hemispheres (e.g., commissural fibers). Also, since cortical asymmetry and hemispheric predom_x0002_inance have been discovered in neurodegenerative disease [78], the features \u2018Delta-LR\u2019 are extracted to represent asym_x0002_metry between the left and right hemispheres. The tract_x0002_based features include two categories: cortical region meas_x0002_ures and whole brain measures. The former includes the number of fibers, average fiber length, intra- and inter-fiber numbers, intra- and inter-fiber lengths, and \u2018Delta-LR\u2019 aver_x0002_age fiber length. The latter includes the number of associa_x0002_tion fibers, projection fibers, commissural fibers, and the fibers in each brain lobe. Tensor-based features are averaged over the different bundles of fibers. Our choice of features is motivated by literature review and to favor interpretability, follow standard conventions, and support fiber-tract-based analysis (DG1).", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "Cohort Formation. This step formulates cohort data (the table in Fig. 1) from all scans and their attributes, including the extracted features and the corresponding demographics(age and gender) as well as their annotations, including a label of their brain status (e.g., PD or HC) and visits of scan_x0002_ning MRI. Each subject has multiple scans if they have mul_x0002_tiple visits. The formulated cohort data is used in the ML learning pipeline described below. Note that we only use the label and extracted features to train the ML models, while the demographics and visit dates provide context when displaying the ML results.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "The goal of our ML pipeline is to guide the user to effec_x0002_tively explore the data by providing measures of saliency for each feature, region, and subject (DG1). The feature saliency indicates how strongly the corresponding aspect (e.g., fiber length) relates to, for example, the differences of scans with different labels (i.e., PD or HC). The ML pipeline, corresponding to \u201cSalience Guided Exploration Interface\u201d in Fig. 2, is described in detail in Fig. 3. The pipeline\u2019s input is the cohort data generated in Section 4.2 and the outputs are feature scores, region scores, and subject/scan class probabilities. The whole pipeline is executed inside CV iterations. In each CV iteration, we exe_x0002_cute feature ranking and binary classification to obtain the saliency measures. Then, we produce the averages and stan_x0002_dard deviations of the scores over all iterations as the final outputs. In the following, we describe the details. One of our objectives is subject-level exploration (DG1) using probabilistic predictions. However, both bootstrap_x0002_ping and repeated randomized CV cannot guarantee that each scan appears in a test set an equal number of times. Standard k-fold CV guarantees this but may suffer from sensitivity to variance (which is a particular problem in our domain). For these reasons, we use an extension of k-fold CV, which is performed t times with randomization, result_x0002_ing t _x0003_ k iterations in total. This allows equal testing of scans (t _x0003_ \u00f0k _x0004_ 1\u00de each)\u3002 This stage relies on a binary classification model that learns a function f\u00f0X\u00de \u00bc ^y, where X is a matrix of the cohort data (Fig. 1), with which rows and columns represent scans and attributes respectively, and ^y is a prediction as to the true class labels, y, that the scans belong to. In our case, we obtain probabilistic predictions as to whether the scan belongs to the disease (PD) or healthy (HC) group, which are used as saliency measures representing an estimation of how closely the scan exhibits patterns that are associated with the disease in the given features. The probabilities are then thresholded at 0.5 to obtain the binary prediction.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "This stage relies on a binary classification model that learns a function f\u00f0X\u00de \u00bc ^y, where X is a matrix of the cohort data(Fig. 1), with which rows and columns represent scans and attributes respectively, and ^y is a prediction as to the true class labels, y, that the scans belong to. In our case, we obtain probabilistic predictions as to whether the scan belongs to the disease (PD) or healthy (HC) group, which are used as saliency measures representing an estimation of how closely the scan exhibits patterns that are associated with the disease in the given features. The probabilities are then thresholded at 0.5 to obtain the binary prediction. Since the input X is a standard form that is compatible with many classification models, we can use many different models (e.g., SVM, decision trees, and neural networks). As a default model and the one used through this paper, we use a linear SVM. This model is popular due to its high performance with various data (including neurological data [64], [81]), robustness against overfitting, and ability to return class probabilities (rather than just predictions). With all of these qualities, a linear SVM is a good model for our domain and the objectives stated in DG1.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "dxf", "index_original": 2, "paper_title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts", "pub_year": 2023, "domain": "Disease", "requirement": {"requirement_text": "DG 2: Quality Visualization For anatomical understanding, VA of the fiber tracts and salient variables of Brain Fibers. For anatomical understanding, VA of the fiber tracts and salient variables in the physical space is required. High-quality graphics rendering can help understand the spatial relations of brain fibers. While the rendering should be effective at showing the structure, it should be also efficient enough to interactively render multiple large fiber sets.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "MRI parameters, such as gradient direction, b-value, and voxel resolution, have a crucial impact on the scalar meas_x0002_urements used for a clinical study. To prevent errors, the MRI images provided by PPMI are collected based on stan_x0002_dardized and strict acquisition protocols developed by the steering committee on 3T Siemens scanners. Each visit includes DTI and T1-weighted images. For each DTI image, a 2D echo-planar DTI sequence is acquired with the following parameters: TR \u00bc 900 ms, TE \u00bc 88 ms, image matrix \u00bc 116 _x0003_ 116 _x0003_ 72 and voxel resolution \u00bc 1:98 _x0003_ 1:98_x0003_ 2 _x0003_ mm3, 64 gradient volumes (b \u00bc 1; 000s=mm2), and one non-gradient volume (b \u00bc 0s=mm2). The acquisition parame_x0002_ters for T1-weighted images are as follows: TR \u00bc 2; 300 ms, TE \u00bc 2:98 ms, image matrix\u00bc 160 _x0003_ 240 _x0003_ 256, and voxel res_x0002_olution \u00bc 1 _x0003_ 1 _x0003_ 1 _x0003_ mm3.", "data_code": {"media": 1}}, "solution": [{"solution_text": "The brain fibers are rendered (Fig. 5) as path tubes with SSAO, which produces a high-quality visualization with an enhanced spatial perception [35], [36]. The path tubes are constructed on the fly through the GPU rendering pipeline in the geometry shader. This allows the path tubes to be constructed and rendered quickly with an interactively adjustable radius without additional memory overhead.  ", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The brain fibers are rendered (Fig. 5) as path tubes with SSAO, which produces a high-quality visualization with an enhanced spatial perception [35], [36]. The path tubes are constructed on the fly through the GPU rendering pipeline in the geometry shader. This allows the path tubes to be constructed and rendered quickly with an interactively adjustable radius without additional memory overhead. The scans/subjects, regions, and features selected from their respective exploration modules, automatically determine which fibers are rendered and which features are used for color mapping. For example, when one brain region is selected, this view only renders the fibers related to the selected region. In addition to direct color mapping without value scaling, we provide two scaling options: contrastive color mapping\u2014using the difference from the mean value of all the given measures in a specific brain region over the entire HC group\u2014to emphasize anomaly and logarithmic scaling to better reflect subtle value differences. However, it is important to understand that one cannot find a direct fiber-to-fiber correspondence between subjects. These design decisions reflect DG2 by providing a high-quality visualization of fiber-microstructure with interactive framerates for large sets of brain fibers.", "solution_category": "visualization", "solution_axial": "mirror", "solution_compoent": "Others", "axial_code": [], "componenet_code": ["others"]}, {"solution_text": "In addition to direct color mapping without value scaling, we provide two scaling options: contrastive color mapping\u2014using the difference from the mean value of all the given measures in a specific brain region over the entire HC group\u2014to emphasize anomaly and logarithmic scaling to better reflect subtle value differences. However, it is important to understand that one cannot find a direct fiber-to-fiber correspondence between subjects. These design decisions reflect DG2 by providing a high-quality visualization of fiber-microstructure with interactive framerates for large sets of brain fibers.", "solution_category": "interaction", "solution_axial": "Filtering;Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures", "Filtering"], "componenet_code": ["extraction_of_features", "filtering"]}]}, {"author": "dxf", "index_original": 3, "paper_title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts", "pub_year": 2023, "domain": "Disease", "requirement": {"requirement_text": "DG 3: Modalities for Comparison.To address the hypothesis generation, it is fundamental to perform the comparison of different subjects and/or groups with different modalities and time steps. Through the comparison, experts can relate diverse aspects to clinical outcomes (e.g., healthy versus PD). As we utilize ML for the comparison, the system should also depict the relationships between data and the prediction.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "MRI parameters, such as gradient direction, b-value, and voxel resolution, have a crucial impact on the scalar meas_x0002_urements used for a clinical study. To prevent errors, the MRI images provided by PPMI are collected based on stan_x0002_dardized and strict acquisition protocols developed by the steering committee on 3T Siemens scanners. Each visit includes DTI and T1-weighted images. For each DTI image, a 2D echo-planar DTI sequence is acquired with the following parameters: TR \u00bc 900 ms, TE \u00bc 88 ms, image matrix \u00bc 116 _x0003_ 116 _x0003_ 72 and voxel resolution \u00bc 1:98 _x0003_ 1:98_x0003_ 2 _x0003_ mm3, 64 gradient volumes (b \u00bc 1; 000s=mm2), and one non-gradient volume (b \u00bc 0s=mm2). The acquisition parame_x0002_ters for T1-weighted images are as follows: TR \u00bc 2; 300 ms, TE \u00bc 2:98 ms, image matrix\u00bc 160 _x0003_ 240 _x0003_ 256, and voxel res_x0002_olution \u00bc 1 _x0003_ 1 _x0003_ 1 _x0003_ mm3.", "data_code": {"media": 1}}, "solution": [{"solution_text": "Here describe the details of data processing, which con_x0002_sists of three steps: fiber tracking, feature extraction, and cohort formulation. These are performed outside of the VA system. 1) Fiber Tracking. This step generates white matter fiber tracts from the RAW images (from DTI to white matter fiber tracts in Fig. 1). We first convert the RAW images from the Digital Image and Communications in Medicine (DICOM) format to the Neuroimaging Informatics Technology Initia_x0002_tive (NIFTI) format. Then, we perform MRI data denoising and preprocessing, including eddy-current induced distor_x0002_tion correction, motion correction, and susceptibility induced distortion correction, using \u201cdwidenoise\u201d and \u201cdwipreproc\u201d scripts in MRtrix3, which is a recommended data cleaning process that uses FSL\u2019s \u201ceddy\u201d [69],\u201ctoppup\u201d [70], and \u201capplytopup\u201d [71] tools. This can reduce artifacts in MRI images and address many additional effects of noise during brain fiber reconstruction, such as the bias of fiber orientation estimation and error tracking of bifurcated fibers. Then, we fix magnet inhomogeneity (e.g., intensity loss and blurring) and perform image correction (e.g., eddy current correction and head motion correction) using the standard \u201crecon-all\u201d script in FreeSurfer [74]. Afterward, we align the T1-weighted images to the DTI images (intra_x0002_subject registration) using FSL [72]. Intra-subject registra_x0002_tion reduces the distortion in the anatomical structure of fibers extracted from the region of interest (ROI) in a subject. We also perform inter-subject registration using FSL, which applies the standard template (MNI152) to each of the sub_x0002_ject\u2019s MRI images. After intra-registration and inter-registra_x0002_tion, we perform brain parcellation using FSL, which splits the brain into regions. The parcellation based on Free_x0002_Surfer\u2019s default atlas (the Desikan/Killiany cortical atlas), which consists of 42 cortical regions [73]. We then perform brain fiber tractography using a state-of-the-art framework [7], which can facilitate biologically plausible fiber recon_x0002_struction and provide anatomically reliable brain fiber tracts. Afterward, by referring to the brain parcellation information, we can categorize fiber tracts and obtain the corresponding features at the whole-brain level and brain region level.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Feature Extraction. We extract fiber features from the constructed brain fiber tracts (Fig. 1) and diffusion tensors, using MRtrix3 [28] and FreeSurfer [74]. The features fall into two categories: tract-based and tensor-based. The for_x0002_mer measures regional fiber structures (e.g., density and length) while the latter measures water diffusivity patterns based on a tensor model. Evidence suggests that both cate_x0002_gories are affected by neurodegenerative disease [75], [76]. The diffusion measures we compute with FreeSurfer include: the raw T2 signal (S0), the eigenvalues (_x0002_ 1; _x0002_2, and 3) representing diffusion in the directions of each of the three eigenvectors of the diffusion tensor, fractional anisotropy (FA), mode of anisotropy (MO), and mean diffu_x0002_sivity (MD). With MRtrix3, we also compute the other met_x0002_rics, including radial diffusivity (RD), relative anisotropy (RA), axial diffusivity (AD), and the Westin metrics (linear_x0002_ity (CL), planarity (CP), and sphericity (CS)). O\u2019Donnell et al. provide the definitions of these measures [77]. We use MRtrix3 and FreeSurfer to bundle fibers based on which cortical regions are passed by each fiber. Here, we use FreeSurfer to apply the cortical structure parcellation, which assigns a neuroanatomical label to each cortical region. Also, since PD has been reported to start from one region and then spread to others, we further divide the bundles into two categories: intra- and inter-connects by referring to the information of passed cortical regions. Intra-connects (or intra-parcel connections) represent connections that both start and end within the same cortical region while the inter_x0002_connects (or inter-parcel connections) represent the connec_x0002_tions that start and end different cortical regions. Each corti_x0002_cal region has both intra- and inter-connects. Note that this definition of inter-connects does not refer to fibers that con_x0002_nect the two hemispheres (e.g., commissural fibers). Also, since cortical asymmetry and hemispheric predom_x0002_inance have been discovered in neurodegenerative disease [78], the features \u2018Delta-LR\u2019 are extracted to represent asym_x0002_metry between the left and right hemispheres. The tract_x0002_based features include two categories: cortical region meas_x0002_ures and whole brain measures. The former includes the number of fibers, average fiber length, intra- and inter-fiber numbers, intra- and inter-fiber lengths, and \u2018Delta-LR\u2019 aver_x0002_age fiber length. The latter includes the number of associa_x0002_tion fibers, projection fibers, commissural fibers, and the fibers in each brain lobe. Tensor-based features are averaged over the different bundles of fibers. Our choice of features is motivated by literature review and to favor interpretability, follow standard conventions, and support fiber-tract-based analysis (DG1).", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "Cohort Formation. This step formulates cohort data (the table in Fig. 1) from all scans and their attributes, including the extracted features and the corresponding demographics(age and gender) as well as their annotations, including a label of their brain status (e.g., PD or HC) and visits of scan_x0002_ning MRI. Each subject has multiple scans if they have mul_x0002_tiple visits. The formulated cohort data is used in the ML learning pipeline described below. Note that we only use the label and extracted features to train the ML models, while the demographics and visit dates provide context when displaying the ML results.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "The goal of our ML pipeline is to guide the user to effec_x0002_tively explore the data by providing measures of saliency for each feature, region, and subject (DG1). The feature saliency indicates how strongly the corresponding aspect (e.g., fiber length) relates to, for example, the differences of scans with different labels (i.e., PD or HC). The ML pipeline, corresponding to \u201cSalience Guided Exploration Interface\u201d in Fig. 2, is described in detail in Fig. 3. The pipeline\u2019s input is the cohort data generated in Section 4.2 and the outputs are feature scores, region scores, and subject/scan class probabilities. The whole pipeline is executed inside CV iterations. In each CV iteration, we exe_x0002_cute feature ranking and binary classification to obtain the saliency measures. Then, we produce the averages and stan_x0002_dard deviations of the scores over all iterations as the final outputs. In the following, we describe the details. One of our objectives is subject-level exploration (DG1) using probabilistic predictions. However, both bootstrap_x0002_ping and repeated randomized CV cannot guarantee that each scan appears in a test set an equal number of times. Standard k-fold CV guarantees this but may suffer from sensitivity to variance (which is a particular problem in our domain). For these reasons, we use an extension of k-fold CV, which is performed t times with randomization, result_x0002_ing t _x0003_ k iterations in total. This allows equal testing of scans (t _x0003_ \u00f0k _x0004_ 1\u00de each)\u3002 This stage relies on a binary classification model that learns a function f\u00f0X\u00de \u00bc ^y, where X is a matrix of the cohort data (Fig. 1), with which rows and columns represent scans and attributes respectively, and ^y is a prediction as to the true class labels, y, that the scans belong to. In our case, we obtain probabilistic predictions as to whether the scan belongs to the disease (PD) or healthy (HC) group, which are used as saliency measures representing an estimation of how closely the scan exhibits patterns that are associated with the disease in the given features. The probabilities are then thresholded at 0.5 to obtain the binary prediction.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The average predicted class probabilities and their standard deviations over all CV iterations are also shown in the table view and guide across-subject exploration. This can aid the comparison and hypothesis generation (DG3) in a number of ways, including: model failure (why some scans do not fit the model, possible confounding conditions), model success (an obvious case of neurodegenerative expression might be found), model ambiguity (subtle expression might be found through VA or different features might be needed to disambiguate these subjects). In addition, we want to explore the same subject\u2019s expression across time. Changes in prediction across time can guide clinical analysis and disease progression.", "solution_category": "visualization", "solution_axial": "mirror", "solution_compoent": "Table", "axial_code": [], "componenet_code": ["table"]}]}, {"author": "dxf", "index_original": 4, "paper_title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts", "pub_year": 2023, "domain": "Disease", "requirement": {"requirement_text": "DG 3: Modalities for Comparison.To address the hypothesis generation, it is fundamental to perform the comparison of different subjects and/or groups with different modalities and time steps. Through the comparison, experts can relate diverse aspects to clinical outcomes (e.g., healthy versus PD). As we utilize ML for the comparison, the system should also depict the relationships between data and the prediction.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "MRI parameters, such as gradient direction, b-value, and voxel resolution, have a crucial impact on the scalar meas_x0002_urements used for a clinical study. To prevent errors, the MRI images provided by PPMI are collected based on stan_x0002_dardized and strict acquisition protocols developed by the steering committee on 3T Siemens scanners. Each visit includes DTI and T1-weighted images. For each DTI image, a 2D echo-planar DTI sequence is acquired with the following parameters: TR \u00bc 900 ms, TE \u00bc 88 ms, image matrix \u00bc 116 _x0003_ 116 _x0003_ 72 and voxel resolution \u00bc 1:98 _x0003_ 1:98_x0003_ 2 _x0003_ mm3, 64 gradient volumes (b \u00bc 1; 000s=mm2), and one non-gradient volume (b \u00bc 0s=mm2). The acquisition parame_x0002_ters for T1-weighted images are as follows: TR \u00bc 2; 300 ms, TE \u00bc 2:98 ms, image matrix\u00bc 160 _x0003_ 240 _x0003_ 256, and voxel res_x0002_olution \u00bc 1 _x0003_ 1 _x0003_ 1 _x0003_ mm3.", "data_code": {"media": 1}}, "solution": [{"solution_text": "The brain fibers are rendered (Fig. 5) as path tubes with SSAO, which produces a high-quality visualization with an enhanced spatial perception [35], [36]. The path tubes are constructed on the fly through the GPU rendering pipeline in the geometry shader. This allows the path tubes to be constructed and rendered quickly with an interactively adjustable radius without additional memory overhead.  ", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The comparative visual analysis (DG3, Fig. 2 D and 2 E) is performed with fiber rendering and linked information visualization modules. These views show the information related to features, regions, and scans/subjects of interest that are selected from the exploration modules.The 3D fiber rendering module facilitates comparative analysis together with the linked information visualization module. To aid comparison, we provide two views (as shown in Fig. 2 E) and use the same colormap across all the selected scans from the other modules by referring to a global value range across the scans. Also, the contrastive color mapping helps supports the comparison of the individuals against the HC group, which may better emphasize important differences.", "solution_category": "visualization", "solution_axial": "mirror", "solution_compoent": "Others", "axial_code": [], "componenet_code": ["others"]}]}, {"author": "dxf", "index_original": 6, "paper_title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts", "pub_year": 2023, "domain": "Disease", "requirement": {"requirement_text": "DG 4: Easy Non-Linear Exploration. Due to a large number of features, fiber tracts, and subjects, diverse aspects could be explored. The analytical process may proceed and change according to emerged patterns or discovered knowledge during the analysis. Therefore, our system should provide an intuitive and fully interactive UI to serve the neuroscientists\u2019 changing analysis needs.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "MRI parameters, such as gradient direction, b-value, and voxel resolution, have a crucial impact on the scalar meas_x0002_urements used for a clinical study. To prevent errors, the MRI images provided by PPMI are collected based on stan_x0002_dardized and strict acquisition protocols developed by the steering committee on 3T Siemens scanners. Each visit includes DTI and T1-weighted images. For each DTI image, a 2D echo-planar DTI sequence is acquired with the following parameters: TR \u00bc 900 ms, TE \u00bc 88 ms, image matrix \u00bc 116 _x0003_ 116 _x0003_ 72 and voxel resolution \u00bc 1:98 _x0003_ 1:98_x0003_ 2 _x0003_ mm3, 64 gradient volumes (b \u00bc 1; 000s=mm2), and one non-gradient volume (b \u00bc 0s=mm2). The acquisition parame_x0002_ters for T1-weighted images are as follows: TR \u00bc 2; 300 ms, TE \u00bc 2:98 ms, image matrix\u00bc 160 _x0003_ 240 _x0003_ 256, and voxel res_x0002_olution \u00bc 1 _x0003_ 1 _x0003_ 1 _x0003_ mm3.", "data_code": {"media": 1}}, "solution": [{"solution_text": "Since each region has its own set of features and feature saliency measures, when a region is selected, all other related views are updated (e.g., the feature exploration module). One design consideration is whether to compute the saliencies for all regions at once or on demand. To make the VA process interactive and support easy non-linear exploration (DG4), we choose to do it all at once as this way can avoid waiting time when the user interactively explores different brain regions.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 7, "paper_title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches", "pub_year": 2022, "domain": "Feature selection", "requirement": {"requirement_text": "G1: Division of Data Space Into Slices Based on Predicted Probabilities, for Transparent Local Feature Contribution.Our goal is to assist in the search for distinctive features that might contribute more to instances that are harder or easier to classify. By splitting the data space into quadrants, we aim to assure that users\u2019 interference with the engineering of specific features does not cause problems in key parts of the data space. The tool should begin by training an initial model with the original features of each data set, which will be a starting point (i.e., \u201cstate zero\u201d) for users to compare future interactions and take further decisions.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "Features", "data_code": {"tables": 1, "quantitative": 1, "clusters_and_sets_and_lists": 1, "textual": 1}}, "solution": [{"solution_text": "In FeatureEnVi, data instances are sorted according to the predicted probability of belonging to the ground truth class, as shown in Fig. 1a. The initial step before the exploration of features is to pre-train the XGBoost [29] on the original pool of features, and then divide the data space into four groups automatically (i.e., Worst, Bad, Good, and Best). The vertical black line is the stable threshold anchored precisely at 50% predictive probability, which separates the correctly from the wrongly classified instances. The other two thresholds partition the prior subspace into their half areas for the default option. However, the user can alter the vertical gray lines as indicated in Fig. 5a.1\u2013a.4, with a degree of freedom set to =t20% from the defaults. The vertical positioning of the instances is purely used to avoid\u2014as much as possible\u2014overlapping/cluttering issues via jittering. The data space will always be divided into four parts conveying extra information to the user. If no instances belong to a slice of the data space, the system works normally, but there will be no values for the statistical measures (see Section 4.3). Overall, the user\u2019s goal is to move as many instances as possible from the left side (Worst and Bad subspaces) to the right side (Good and Best subspaces) while avoiding the opposite. Nevertheless, the primary purpose of this view is to provide better local and global explainabilities of the impact of features according to the user-defined slices.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "dxf", "index_original": 8, "paper_title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches", "pub_year": 2022, "domain": "Feature selection", "requirement": {"requirement_text": "G2: Deployment of Different Types of Feature Selection Techniques to Support Stepwise Selection. There are several different techniques for computing feature importance that produce diverse outcomes per feature. The tool should facilitate the visual comparison of alternative feature selection techniques for each feature (T2). Another key point is that users should have the ability to include and exclude features during the entire exploration phase.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Features", "data_code": {"tables": 1, "quantitative": 1, "clusters_and_sets_and_lists": 1, "textual": 1}}, "solution": [{"solution_text": "In FeatureEnVi, data instances are sorted according to the predicted probability of belonging to the ground truth class, as shown in Fig. 1a. The initial step before the exploration of features is to pre-train the XGBoost [29] on the original pool of features, and then divide the data space into four groups automatically (i.e., Worst, Bad, Good, and Best). The vertical black line is the stable threshold anchored precisely at 50% predictive probability, which separates the correctly from the wrongly classified instances. The other two thresholds partition the prior subspace into their half areas for the default option. However, the user can alter the vertical gray lines as indicated in Fig. 5a.1\u2013a.4, with a degree of freedom set to =t20% from the defaults. The vertical positioning of the instances is purely used to avoid\u2014as much as possible\u2014overlapping/cluttering issues via jittering. The data space will always be divided into four parts conveying extra information to the user. If no instances belong to a slice of the data space, the system works normally, but there will be no values for the statistical measures (see Section 4.3). Overall, the user\u2019s goal is to move as many instances as possible from the left side (Worst and Bad subspaces) to the right side (Good and Best subspaces) while avoiding the opposite. Nevertheless, the primary purpose of this view is to provide better local and global explainabilities of the impact of features according to the user-defined slices.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "It is a table heatmap view with five automatic feature selection techniques, their Average contribution, and an # Action # button to exclude any number of features. As we originally train our ML algorithm with all features, the yellow color (one of the standard colors used for highlighting [77]) in the last column symbolizes that all features are included in the current phase (if excluded, then B/W stripe patterns appear).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Matrix", "axial_code": [], "componenet_code": ["matrix"]}]}, {"author": "dxf", "index_original": 9, "paper_title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches", "pub_year": 2022, "domain": "Feature selection", "requirement": {"requirement_text": "G3: Application of Alternative Feature Transformations According to Feedback Received From Statistical Measures. In continuation of the preceding goal, the tool should provide sufficient visual guidance to users to choose between diverse feature transformations (T4). Statistical measures such as target correlation and mutual information shared between features, along with per class correlation, are necessary to evaluate the features\u2019 influences in the result. Also, the tool should use variance influence factor and in-between features\u2019 correlation for identifying colinearity issues. When checking how to modify features, users should be able to estimate the impact of such transformations.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "feature space detail", "data_code": {"tables": 1, "quantitative": 1, "clusters_and_sets_and_lists": 1, "textual": 1}}, "solution": [{"solution_text": "In FeatureEnVi, data instances are sorted according to the predicted probability of belonging to the ground truth class, as shown in Fig. 1a. The initial step before the exploration of features is to pre-train the XGBoost [29] on the original pool of features, and then divide the data space into four groups automatically (i.e., Worst, Bad, Good, and Best). The vertical black line is the stable threshold anchored precisely at 50% predictive probability, which separates the correctly from the wrongly classified instances. The other two thresholds partition the prior subspace into their half areas for the default option. However, the user can alter the vertical gray lines as indicated in Fig. 5a.1\u2013a.4, with a degree of freedom set to =t20% from the defaults. The vertical positioning of the instances is purely used to avoid\u2014as much as possible\u2014overlapping/cluttering issues via jittering. The data space will always be divided into four parts conveying extra information to the user. If no instances belong to a slice of the data space, the system works normally, but there will be no values for the statistical measures (see Section 4.3). Overall, the user\u2019s goal is to move as many instances as possible from the left side (Worst and Bad subspaces) to the right side (Good and Best subspaces) while avoiding the opposite. Nevertheless, the primary purpose of this view is to provide better local and global explainabilities of the impact of features according to the user-defined slices.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "the radial tree providing an overview of the features with statistical measures for the different groups of instances, as set by the user-defined data slices. The graph visualization in Fig. 3e uses glyphs to encode further details for every feature, i.e., node. The per target correlation and MI are the same as in the radial tree, but an additional horizontal bar chart encodes the per class correlation of each feature with three colors (olive for fine class, purple for superior class, and turquoise for inferior class). Moreover, there are four states that indicate if the VIF per feature was greater than 10, between 10 and 5, between 5 and 2.5, and finally, less than 2.5. They are represented by up to four circular segments in gray (see Fig. 3e) laid out clockwise within the node. We decided for these states as the prior research suggests they reflect concerns or problem_x0002_atic cases of colinearity [89], [90], [91]. Edges are displayed between feature nodes with the correlation above the cur_x0002_rent threshold (0.6 in our example), with edge widths encoding correlation values.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Circle;Bar", "axial_code": [], "componenet_code": ["bar", "circle"]}]}, {"author": "dxf", "index_original": 10, "paper_title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches", "pub_year": 2022, "domain": "Feature selection", "requirement": {"requirement_text": "G4: Generation of new Features and Comparison With the Original Features. With the same statistical evidence as defined in G3, users should get visually informed about Strongly correlated features that perform the same for each class. Next, the tool can use automatic feature selection techniques to compare the new features with the original ones, using the same methods as in G2. Finally, the tool should let users select the proper mathematical operation according to their prior experience and the visual feedback.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "feature space detail", "data_code": {"tables": 1, "quantitative": 1, "clusters_and_sets_and_lists": 1, "textual": 1}}, "solution": [{"solution_text": "In FeatureEnVi, data instances are sorted according to the predicted probability of belonging to the ground truth class, as shown in Fig. 1a. The initial step before the exploration of features is to pre-train the XGBoost [29] on the original pool of features, and then divide the data space into four groups automatically (i.e., Worst, Bad, Good, and Best). The vertical black line is the stable threshold anchored precisely at 50% predictive probability, which separates the correctly from the wrongly classified instances. The other two thresholds partition the prior subspace into their half areas for the default option. However, the user can alter the vertical gray lines as indicated in Fig. 5a.1\u2013a.4, with a degree of freedom set to =t20% from the defaults. The vertical positioning of the instances is purely used to avoid\u2014as much as possible\u2014overlapping/cluttering issues via jittering. The data space will always be divided into four parts conveying extra information to the user. If no instances belong to a slice of the data space, the system works normally, but there will be no values for the statistical measures (see Section 4.3). Overall, the user\u2019s goal is to move as many instances as possible from the left side (Worst and Bad subspaces) to the right side (Good and Best subspaces) while avoiding the opposite. Nevertheless, the primary purpose of this view is to provide better local and global explainabilities of the impact of features according to the user-defined slices.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "during the detailed examination phase, check the different transformations of the features with statistical measures and compare the combinations of two or three features that result in newlygenerated features. The graph visualization in Fig. 3e uses glyphs to encode further details for every feature, i.e., node. The per target correlation and MI are the same as in the radial tree, but an additional horizontal bar chart encodes the per class correlation of each feature with three colors (olive for fine class, purple for superior class, and turquoise for inferior class). Moreover, there are four states that indicate if the VIF per feature was greater than 10, between 10 and 5, between 5 and 2.5, and finally, less than 2.5. They are represented by up to four circular segments in gray (see Fig. 3e) laid out clockwise within the node. We decided for these states as the prior research suggests they reflect concerns or problem_x0002_atic cases of colinearity [89], [90], [91]. Edges are displayed between feature nodes with the correlation above the cur_x0002_rent threshold (0.6 in our example), with edge widths encoding correlation values.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Circle;Bar", "axial_code": [], "componenet_code": ["bar", "circle"]}]}, {"author": "dxf", "index_original": 11, "paper_title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches", "pub_year": 2022, "domain": "Feature selection", "requirement": {"requirement_text": "G5: Reassessment of the Instances\u2019 Predicted Probabilities and Performance, Computed With Appropriate Validation Metrics. In the end, users\u2019 interactions should be tracked in order to preserve a history of modifications in the features, and the performance should be monitored with validation metrics. At all stages, the tool should highlight the movement of the instances from one data slice to the other due to the new prediction probability values of every instance. The best case is for all instances to relocate from the left to the right-most side.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "feature space detail", "data_code": {"tables": 1, "quantitative": 1, "clusters_and_sets_and_lists": 1, "textual": 1}}, "solution": [{"solution_text": "In FeatureEnVi, data instances are sorted according to the predicted probability of belonging to the ground truth class, as shown in Fig. 1a. The initial step before the exploration of features is to pre-train the XGBoost [29] on the original pool of features, and then divide the data space into four groups automatically (i.e., Worst, Bad, Good, and Best). The vertical black line is the stable threshold anchored precisely at 50% predictive probability, which separates the correctly from the wrongly classified instances. The other two thresholds partition the prior subspace into their half areas for the default option. However, the user can alter the vertical gray lines as indicated in Fig. 5a.1\u2013a.4, with a degree of freedom set to =t20% from the defaults. The vertical positioning of the instances is purely used to avoid\u2014as much as possible\u2014overlapping/cluttering issues via jittering. The data space will always be divided into four parts conveying extra information to the user. If no instances belong to a slice of the data space, the system works normally, but there will be no values for the statistical measures (see Section 4.3). Overall, the user\u2019s goal is to move as many instances as possible from the left side (Worst and Bad subspaces) to the right side (Good and Best subspaces) while avoiding the opposite. Nevertheless, the primary purpose of this view is to provide better local and global explainabilities of the impact of features according to the user-defined slices.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Each action of the user is registered in the punchcard visualization (see Fig. 1e). The basic recorded steps are: (1) Include, (2) Exclude, (3) Transform, and (4) Generate for each feature. The size of the circle encodes the order of the main actions, with larger radii for recent steps. The brown color is used only if the overall performance increases. The calculation is according to three validation metrics after we subtract their standard deviations. The grouped bar chart presents the performance based on accuracy, weighted precision, and weighted recall and their standard deviations due to crossvalidation (error margins in black). Teal color encodes the current action\u2019s score, and brown the best result reached so far. The choice of colors was made deliberately because they complement each other, and the former denotes the current action since it is brighter than the latter. If the list of features is long, the user can scroll this view. Finally, there is a button to extract the best combination of features (i.e., the \u201cnew\u201d data set). The brown circles in the punchcard in Fig. 1e enable us to acknowledge that the feature generation boosted the overall performance of the classifier. High scores were reached in terms of accuracy, precision, and recall. All in all with FeatureEnVi, we improve the total combined score by using 6 well-engineered features instead of the original 11. On the contrary, Rojo et al. [33] reported a slight decrease in performance when selecting 6 features for this task as a regression problem.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Matrix;Circle", "axial_code": [], "componenet_code": ["circle", "matrix"]}]}, {"author": "dxf", "index_original": 12, "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures", "pub_year": 2023, "domain": "History", "requirement": {"requirement_text": "T1: Exploring a chronology from multiple aspects. Historians are interested in the ups and downs in the lifetime of each person. The life story of a historical figure generally comprises multiple aspects, such as political events and social activities.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "historical figures and events", "data_code": {"sequential": 1, "tables": 1, "textual": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "Evidence from interviews of historians suggest that a sim_x0002_plified overview is needed to help them understand the var_x0002_iation of a historical figure\u2019s life (T1). Thus, we set a scoring rule to evaluate the situation of a historical figure during a specific period. A higher score indicates that the corre_x0002_sponding historical figures are more likely to have a better reputation. To evaluate the score of an event e, five key factors are considered: K1. Importance of the involved figures. For example, criticisms from the emperor have more impacts than that from anybody else. K2. Figure\u2019s role in the event. For example, if the per_x0002_son received an award or was promoted, the effect on the historical figure is positive. In contrast, critics have a negative effect. K3. Event occurrence time. The influence of an event diminishes over time. K4. Frequency of the events\u2019 types. Sparse events such as major successes and weddings should be highlighted. K5. Interests of historians. Event types that historians are interested in are more important. Lexicon-based sentiment analysis [48] takes the sum of emotional scores of each word in the text to measure the  text\u2019s positive or negative sentiment. We refine this approach by computing the scores over time for each historical figure p in year t. The definition of a score can be expressed as follows: Grade\u00f0e; re\u00de is a user-labeled value that considers the tar_x0002_get figures\u2019 role re (e.g., victim or inflicter) in the event (K2). According to the needs of historians, a range of discrete val_x0002_ues of about [-10,10] provides sufficient precision for histori_x0002_ans to express their negative or positive cognition toward the event type (K5). Five historians are invited to grade 836 event types. We gather their grades and average them. In addition, I\u00f0e\u00de measures the importance of event e: I\u00f0e\u00de \u00bc e_x0004_ tf\u00f0e\u00de _x0003_ idf\u00f0e\u00de _x0003_ w\u00f0typee\u00de (2) To reduce the bias induced by repeating with a high fre_x0002_quency normal events (K4), the component e_x0004_ tf\u00f0e\u00de _x0003_ idf\u00f0e\u00de is used from the TF-IDF method in text mining [50]. For example, records of promotion can be easily gathered through historical official documents, which may be over_x0002_sampled compared to other event types. Last, w\u00f0typee\u00de is the weight of a specific event type, which can be controlled by historians (K5). For each year t, we apply the weighted-accumulated scores of events before year t within N years. Because his_x0002_torical figures have a short lifespan on average, their situa_x0002_tions may change relatively quickly. Historians typically consider the influence of events within five years, and ear_x0002_lier events may be ignored (K3). So we set N as 5, and use the exponential decay function to represent the attenuation value of influence before year t: f\u00f0Dt\u00de \u00bc e_x0004_ Dt ; Dt 2 f0; 1; ... ; N _x0004_ 1g (3) Thus, the averaged score in year t should be given as: score\u00f0t\u00de \u00bc X N_x0004_ 1 Dt\u00bc0 f\u00f0Dt\u00de _x0003_ score\u00f0t _x0004_ Dt\u00de (4) Based on the proposed model, we calculate the score for each historical figure and for each year, and visualize the variance of their life time with line charts, as shown in Fig. 3b. It provides an an overview of the chronology of his_x0002_torical figures.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "dxf", "index_original": 13, "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures", "pub_year": 2023, "domain": "History", "requirement": {"requirement_text": "T1: Exploring a chronology from multiple aspects. Historians are interested in the ups and downs in the lifetime of each person. The life story of a historical figure generally comprises multiple aspects, such as political events and social activities.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "historical figures and events", "data_code": {"sequential": 1, "tables": 1, "textual": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "In the general pipeline of uncertainty reasoning in the system, analysts need to select a historical figure and identify uncertain events from the corresponding chronology timeline to trigger the visualization (T1).The control panel (Fig. 3a) shows the selected figure\u2019s brief biography and provides a set of adjustment operations. The weight of each event type, wetypeeT (Section 4.2), can also be interactively defined in the control panel. The Life Mountain view (Fig. 3b) is the entrance for historical exploration and uncertainty reasoning, which provides an overview of Wang\u2019s life experience. Inspired by the met_x0002_aphor of typical Chinese landscape painting (Fig. 4a), a streamgraph-based visual design is proposed to intuitively depict the ups and downs in Wang\u2019s lifetime. A horizontal timeline (Fig. 4b) is adopted, where the important events in history are marked at their time of occurrence and visualized as short vertical lines. Hence, historians can associate Wang\u2019s life experience with the historic envi_x0002_ronment. Wang\u2019s life duration is also highlighted to help his_x0002_torians identify events that occurred before his birth or after his death. The height of the stream graph at time t encodes scoreetT (Section 4.2), indicating the evolution of Wang\u2019s life_x0002_time. Different streams show Wang\u2019s evolution from differ_x0002_ent perspectives according to the event categories (e.g., political, academic, military, etc.). These streams are stacked together to depict Wang\u2019s overall rise and fall. The events in which Wang is involved are encoded in dots and superim_x0002_posed on the stream graph as an analogy of the brushwork in a Chinese landscape painting. Along the timeline, an event is positive if the dot is above the center of the mountain, and is otherwise negative. The closer the dot is near the top or the bottom of the mountain, the more positive or negative the event is, respectively. The types of all presented events are vertically arranged in the top-right corner according to when they first appear, like an analogy of the inscription in a Chinese painting snapshot. The grayscale of each topic indicates its number of occurrences. A darker color indicates a larger number of events and vice versa. Events with uncertain occurrence times are also encoded as dots below the timeline. Each uncertain event is located at the recommended time extracted from the most similar event (Section 4.3). The size of the dot indicates the event\u2019s importance, while the transparency represents the probabil_x0002_ity of the recommended occurrence time. Historians can therefore select uncertain events with relatively higher cred_x0002_ibility to support further reasoning. This view also supports the comparison of multiple historical figures. When histori_x0002_ans choose two figures, for example, in the control panel, their lifelines are simultaneously shown in the Mountain view.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line;Area", "axial_code": [], "componenet_code": ["area", "line"]}]}, {"author": "dxf", "index_original": 15, "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures", "pub_year": 2023, "domain": "History", "requirement": {"requirement_text": "T1: Exploring a chronology from multiple aspects. Historians are interested in the ups and downs in the lifetime of each person. The life story of a historical figure generally comprises multiple aspects, such as political events and social activities.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "historical figures and events", "data_code": {"sequential": 1, "tables": 1, "textual": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "We design the map view to visualize uncertain spatio-temporal information (Fig. 3d). The example shows Wang\u2019s moving pattern by displaying the event locations on the map. For each location, we obtain a set of events that involved Wang and occurred at this location. Events with missing location information are recommended to occur at the location extracted from the most similar event (Section 4.3). A set of pie charts shown in Fig. 3d represent the propor_x0002_tion of certain and uncertain events in event sets at different locations. The size of the pie chart encodes the number of all events in this set. Events with certain time and location information are connected in chronological order to show Wang\u2019s moving pattern. Zooming and panning tools are also provided. Once historians select a pie chart, the detailed information on the events in this set will be listed in a tooltip. The listed events are sorted by probability, with certain events on the top followed by uncertain events.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Matrix", "axial_code": [], "componenet_code": ["matrix"]}, {"solution_text": "Zooming and panning tools are also provided. Once historians select a pie chart, the detailed information on the events in this set will be listed in a tooltip. The listed events are sorted by probability, with certain events on the top followed by uncertain events.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 17, "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures", "pub_year": 2023, "domain": "History", "requirement": {"requirement_text": "T2: Identifying and reducing the spatio-temporal uncertainty of events. The system should provide historians with visual analysis of spatio-temporal uncertainty, and enables them to perform cross-validations across multiple views using domain knowledge. Reducing uncertainty allows historians to obtain accurate information, so as to improve the analysis efficiency.", "requirement_code": {"discover_observation": 1, "collect_evidence": 1}}, "data": {"data_text": "historical figures and events", "data_code": {"sequential": 1, "tables": 1, "textual": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "dxf", "index_original": 18, "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures", "pub_year": 2023, "domain": "History", "requirement": {"requirement_text": "T2: Identifying and reducing the spatio-temporal uncertainty of events. The system should provide historians with visual analysis of spatio-temporal uncertainty, and enables them to perform cross-validations across multiple views using domain knowledge. Reducing uncertainty allows historians to obtain accurate information, so as to improve the analysis efficiency.", "requirement_code": {"discover_observation": 1, "collect_evidence": 1}}, "data": {"data_text": "historical figures and events", "data_code": {"sequential": 1, "tables": 1, "textual": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "We construct a heterogeneous network to store the data from CBDB and BSADB.For the sake of simplicity, we only store historical events and related records that support spatio-temporal information reasoning. Two types of records are treated as events, including status (e.g., friendship, adversary relationship, etc.) and actions (e.g., criticizing, writing articles, becoming friends,etc.). Two types of records are treated as events, including status (e.g., friendship, adversary relationship, etc.) and actions (e.g., criticizing, writing articles, becoming friends,etc.).", "solution_category": "data_manipulation", "solution_axial": "Modeling;Excluding", "solution_compoent": "", "axial_code": ["Modeling", "Excluding"], "componenet_code": ["modeling", "excluding"]}, {"solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Inference of the selected event is then enabled in the uncertainty reasoning view based on the entity recommendation. We thus design the proposed reasoning view (Fig. 3e). Once an uncertain event is selected in the chronology/map view, it will appear at the center of the reasoning view, with other similar events surrounded for uncertainty inference and cross-verification. As shown in Fig. 6, the reasoning view consists of two major components: reasoning content and reasoning rules. Reasoning content includes a central event (the selected uncertain event to be reasoned, CE) and other supplementary events (SE) that provide important reasoning cues. We use the top-50 recommended SEs that are most similar to the CE (Section 4.3) as default. This threshold can be adjusted in the control panel. The selection of SEs is based on the aforementioned hypothesis that events sharing the same entities might be related. Therefore, we further extract the entities ([description, time, location, person]) from SEs. Reasoning rules refer to the user-defined rules for filtering the displayed SEs and entities in the reasoning content(Fig. 6), in order to obtain more supportive evidence. Two rules are supported: 1) intersection that preserves SEs containing all selected entities and 2) union that preserves SEs containing at least one of the entities. Historians can interactively drag one or a set of entities from the reasoning content and add rules. Fig. 6 shows the visual design of the reasoning rules. Entities are organized according to the drag operation, and rules are represented by circular nodes. Pink nodes indi_x0002_cate the intersection operations, and green nodes indicate the union. In particular, historians can add new rules to previous rules, ultimately formulating a rule tree.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Word", "axial_code": [], "componenet_code": ["text"]}, {"solution_text": "Historians can interactively drag one or a set of entities from the reasoning content and add rules. Fig. 6 shows the visual design of the reasoning rules. Entities are organized according to the drag operation, and rules are represented by circular nodes. Pink nodes indi_x0002_cate the intersection operations, and green nodes indicate the union. In particular, historians can add new rules to previous rules, ultimately formulating a rule tree.", "solution_category": "interaction", "solution_axial": "Selecting;Filtering", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "dxf", "index_original": 19, "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures", "pub_year": 2023, "domain": "History", "requirement": {"requirement_text": "T2: Identifying and reducing the spatio-temporal uncertainty of events. The system should provide historians with visual analysis of spatio-temporal uncertainty, and enables them to perform cross-validations across multiple views using domain knowledge. Reducing uncertainty allows historians to obtain accurate information, so as to improve the analysis efficiency.", "requirement_code": {"discover_observation": 1, "collect_evidence": 1}}, "data": {"data_text": "historical figures and events", "data_code": {"sequential": 1, "tables": 1, "textual": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The Life Mountain view (Fig. 3b) is the entrance for historical exploration and uncertainty reasoning, which provides an overview of Wang\u2019s life experience. Inspired by the met_x0002_aphor of typical Chinese landscape painting (Fig. 4a), a streamgraph-based visual design is proposed to intuitively depict the ups and downs in Wang\u2019s lifetime. A horizontal timeline (Fig. 4b) is adopted, where the important events in history are marked at their time of occurrence and visualized as short vertical lines. Hence, historians can associate Wang\u2019s life experience with the historic environment. Wang\u2019s life duration is also highlighted to help historians identify events that occurred before his birth or after his death. The height of the stream graph at time t encodes scoreetT (Section 4.2), indicating the evolution of Wang\u2019s life_x0002_time. Different streams show Wang\u2019s evolution from differ_x0002_ent perspectives according to the event categories (e.g., political, academic, military, etc.). These streams are stacked together to depict Wang\u2019s overall rise and fall. The events in which Wang is involved are encoded in dots and superim_x0002_posed on the stream graph as an analogy of the brushwork in a Chinese landscape painting. Along the timeline, an event is positive if the dot is above the center of the mountain, and is otherwise negative. The closer the dot is near the top or the bottom of the mountain, the more positive or negative the event is, respectively. The types of all presented events are vertically arranged in the top-right corner according to when they first appear, like an analogy of the inscription in a Chinese painting snapshot. The grayscale of each topic indicates its number of occurrences. A darker color indicates a larger number of events and vice versa. Events with uncertain occurrence times are also encoded as dots below the timeline. Each uncertain event is located at the recommended time extracted from the most similar event (Section 4.3). The size of the dot indicates the event\u2019s importance, while the transparency represents the probabil_x0002_ity of the recommended occurrence time. Historians can therefore select uncertain events with relatively higher cred_x0002_ibility to support further reasoning. This view also supports the comparison of multiple historical figures. When histori_x0002_ans choose two figures, for example, in the control panel, their lifelines are simultaneously shown in the Mountain view.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line;Area", "axial_code": [], "componenet_code": ["area", "line"]}, {"solution_text": "Historians can therefore select uncertain events with relatively higher cred_x0002_ibility to support further reasoning. This view also supports the comparison of multiple historical figures. When histori_x0002_ans choose two figures, for example, in the control panel, their lifelines are simultaneously shown in the Mountain view.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 20, "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures", "pub_year": 2023, "domain": "History", "requirement": {"requirement_text": "T2: Identifying and reducing the spatio-temporal uncertainty of events. The system should provide historians with visual analysis of spatio-temporal uncertainty, and enables them to perform cross-validations across multiple views using domain knowledge. Reducing uncertainty allows historians to obtain accurate information, so as to improve the analysis efficiency.", "requirement_code": {"discover_observation": 1, "collect_evidence": 1}}, "data": {"data_text": "historical figures and events", "data_code": {"sequential": 1, "tables": 1, "textual": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Based on this result, we can identify the nearest k similar events of the event. Those events are more likely to have cor_x0002_relations, causal relationships, and sequential relationships with the target event. We further evaluate the hit ratio (the probability that the nearest k similar events contain the correct answer) of the proposed algorithm with the test data. We collect 1,000 completed events from CBDB, where some entities (e.g., location ortime of the events) are manually hidden to sim_x0002_ulate the uncertainty. The hidden parts are viewed as labels. We divide the data into training and testing sets and apply a\n10-fold cross-validation method on the testing sets. We run the algorithm to identify the k most similar events for each tar_x0002_getevent. In the k similarevents, if any similarevent has the hid_x0002_den entity of the target event (e.g., a given location), the target event is considered to be hit and a hit ratio is computed. The top-5 hit ratio of the 10-fold cross-validation is 48.1%, and the top-15 is 70.7%. Meanwhile, this process markedly reduces the search space for finding similar entities. With the visuali_x0002_zation of suggested candidates, historians can better investi_x0002_gate uncertainty with domain knowledge.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "We design the relation view to enable historians to study historical figures\u2019 group behaviors in more details (Fig. 3f). With the assistance of PageRank (Section 4.2), we extract the top-20 historical figures most related to Wang by default. The threshold is adjustable. The grid ei; jT in the relation view represents a set of events that involve person i and person j. Historians can switch among three modes to display different types of information of these grids. The first mode displays the size of the event set in grayscale, where the darker color indicates more events. The second mode shows the positiveness and negativeness of the event set. A yellow color indicates that the average grade of events is positive, and blue indicates negative. These grades are man_x0002_ually labeled by historians, as described in Section 4.2. The third mode uses a consistent color mapping, as in the Life Mountain view, to depict the leading category (e.g., politics or literature) of the event set. The leading category represents the one shared by the events that occupy the largest proportion in the set. In the relation view, the grids are sorted by the Girvan Newman algorithm [54] so that histori_x0002_cal figures with closer relations are gathered.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Matrix", "axial_code": [], "componenet_code": ["matrix"]}]}, {"author": "dxf", "index_original": 21, "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures", "pub_year": 2023, "domain": "History", "requirement": {"requirement_text": "T2: Identifying and reducing the spatio-temporal uncertainty of events. The system should provide historians with visual analysis of spatio-temporal uncertainty, and enables them to perform cross-validations across multiple views using domain knowledge. Reducing uncertainty allows historians to obtain accurate information, so as to improve the analysis efficiency.", "requirement_code": {"discover_observation": 1, "collect_evidence": 1}}, "data": {"data_text": "historical figures and events", "data_code": {"sequential": 1, "tables": 1, "textual": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Based on this result, we can identify the nearest k similar events of the event. Those events are more likely to have cor_x0002_relations, causal relationships, and sequential relationships with the target event. We further evaluate the hit ratio (the probability that the nearest k similar events contain the correct answer) of the proposed algorithm with the test data. We collect 1,000 completed events from CBDB, where some entities (e.g., location ortime of the events) are manually hidden to sim_x0002_ulate the uncertainty. The hidden parts are viewed as labels. We divide the data into training and testing sets and apply a\n10-fold cross-validation method on the testing sets. We run the algorithm to identify the k most similar events for each tar_x0002_getevent. In the k similarevents, if any similarevent has the hid_x0002_den entity of the target event (e.g., a given location), the target event is considered to be hit and a hit ratio is computed. The top-5 hit ratio of the 10-fold cross-validation is 48.1%, and the top-15 is 70.7%. Meanwhile, this process markedly reduces the search space for finding similar entities. With the visuali_x0002_zation of suggested candidates, historians can better investi_x0002_gate uncertainty with domain knowledge.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "We design the relation view to enable historians to study historical figures\u2019 group behaviors in more details (Fig. 3f). With the assistance of PageRank (Section 4.2), we extract the top-20 historical figures most related to Wang by default. The threshold is adjustable. The grid ei; jT in the relation view represents a set of events that involve person i and person j. Historians can switch among three modes to display different types of information of these grids. The first mode displays the size of the event set in grayscale, where the darker color indicates more events. The second mode shows the positiveness and negativeness of the event set. A yellow color indicates that the average grade of events is positive, and blue indicates negative. These grades are man_x0002_ually labeled by historians, as described in Section 4.2. The third mode uses a consistent color mapping, as in the Life Mountain view, to depict the leading category (e.g., politics or literature) of the event set. The leading category represents the one shared by the events that occupy the largest proportion in the set. In the relation view, the grids are sorted by the Girvan Newman algorithm [54] so that histori_x0002_cal figures with closer relations are gathered.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Matrix", "axial_code": [], "componenet_code": ["matrix"]}]}, {"author": "dxf", "index_original": 22, "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures", "pub_year": 2023, "domain": "History", "requirement": {"requirement_text": "T3: Reasoning the collective behavior of historical figures based on refined results. The final goal of information refinement is to discover group behaviors, such as inter- and intraparty relationships.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "historical figures and events", "data_code": {"sequential": 1, "tables": 1, "textual": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Inference of the selected event is then enabled in the uncertainty reasoning view based on the entity recommendation. We thus design the proposed reasoning view (Fig. 3e). Once an uncertain event is selected in the chronology/map view, it will appear at the center of the reasoning view, with other similar events surrounded for uncertainty inference and cross-verification. As shown in Fig. 6, the reasoning view consists of two major components: reasoning content and reasoning rules. Reasoning content includes a central event (the selected uncertain event to be reasoned, CE) and other supplementary events (SE) that provide important reasoning cues. We use the top-50 recommended SEs that are most similar to the CE (Section 4.3) as default. This threshold can be adjusted in the control panel. The selection of SEs is based on the aforementioned hypothesis that events sharing the same entities might be related. Therefore, we further extract the entities ([description, time, location, person]) from SEs. Reasoning rules refer to the user-defined rules for filtering the displayed SEs and entities in the reasoning content(Fig. 6), in order to obtain more supportive evidence. Two rules are supported: 1) intersection that preserves SEs containing all selected entities and 2) union that preserves SEs containing at least one of the entities. Historians can interactively drag one or a set of entities from the reasoning content and add rules. Fig. 6 shows the visual design of the reasoning rules. Entities are organized according to the drag operation, and rules are represented by circular nodes. Pink nodes indi_x0002_cate the intersection operations, and green nodes indicate the union. In particular, historians can add new rules to previous rules, ultimately formulating a rule tree.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Word", "axial_code": [], "componenet_code": ["text"]}, {"solution_text": "In particular, historians can add new rules to previous rules, ultimately formulating a rule tree.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "dxf", "index_original": 23, "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures", "pub_year": 2023, "domain": "History", "requirement": {"requirement_text": "T3: Reasoning the collective behavior of historical figures based on refined results. The final goal of information refinement is to discover group behaviors, such as inter- and intraparty relationships.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "historical figures and events", "data_code": {"sequential": 1, "tables": 1, "textual": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The Life Mountain view (Fig. 3b) is the entrance for historical exploration and uncertainty reasoning, which provides an overview of Wang\u2019s life experience. Inspired by the met_x0002_aphor of typical Chinese landscape painting (Fig. 4a), a streamgraph-based visual design is proposed to intuitively depict the ups and downs in Wang\u2019s lifetime. A horizontal timeline (Fig. 4b) is adopted, where the important events in history are marked at their time of occurrence and visualized as short vertical lines. Hence, historians can associate Wang\u2019s life experience with the historic environment. Wang\u2019s life duration is also highlighted to help historians identify events that occurred before his birth or after his death. The height of the stream graph at time t encodes scoreetT (Section 4.2), indicating the evolution of Wang\u2019s life_x0002_time. Different streams show Wang\u2019s evolution from differ_x0002_ent perspectives according to the event categories (e.g., political, academic, military, etc.). These streams are stacked together to depict Wang\u2019s overall rise and fall. The events in which Wang is involved are encoded in dots and superim_x0002_posed on the stream graph as an analogy of the brushwork in a Chinese landscape painting. Along the timeline, an event is positive if the dot is above the center of the mountain, and is otherwise negative. The closer the dot is near the top or the bottom of the mountain, the more positive or negative the event is, respectively. The types of all presented events are vertically arranged in the top-right corner according to when they first appear, like an analogy of the inscription in a Chinese painting snapshot. The grayscale of each topic indicates its number of occurrences. A darker color indicates a larger number of events and vice versa. Events with uncertain occurrence times are also encoded as dots below the timeline. Each uncertain event is located at the recommended time extracted from the most similar event (Section 4.3). The size of the dot indicates the event\u2019s importance, while the transparency represents the probabil_x0002_ity of the recommended occurrence time. Historians can therefore select uncertain events with relatively higher cred_x0002_ibility to support further reasoning. This view also supports the comparison of multiple historical figures. When histori_x0002_ans choose two figures, for example, in the control panel, their lifelines are simultaneously shown in the Mountain view.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line;Area", "axial_code": [], "componenet_code": ["area", "line"]}, {"solution_text": "Historians can therefore select uncertain events with relatively higher cred_x0002_ibility to support further reasoning. This view also supports the comparison of multiple historical figures. When histori_x0002_ans choose two figures, for example, in the control panel, their lifelines are simultaneously shown in the Mountain view.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 25, "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures", "pub_year": 2023, "domain": "History", "requirement": {"requirement_text": "T3: Reasoning the collective behavior of historical figures based on refined results. The final goal of information refinement is to discover group behaviors, such as inter- and intraparty relationships.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "historical figures and events", "data_code": {"sequential": 1, "tables": 1, "textual": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "We design the map view to visualize uncertain spatio-temporal information (Fig. 3d). The example shows Wang\u2019s moving pattern by displaying the event locations on the map. For each location, we obtain a set of events that involved Wang and occurred at this location. Events with missing location information are recommended to occur at the location extracted from the most similar event (Section 4.3). A set of pie charts shown in Fig. 3d represent the propor_x0002_tion of certain and uncertain events in event sets at different locations. The size of the pie chart encodes the number of all events in this set. Events with certain time and location information are connected in chronological order to show Wang\u2019s moving pattern. Zooming and panning tools are also provided. Once historians select a pie chart, the detailed information on the events in this set will be listed in a tooltip. The listed events are sorted by probability, with certain events on the top followed by uncertain events.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Matrix", "axial_code": [], "componenet_code": ["matrix"]}, {"solution_text": "Zooming and panning tools are also provided. Once historians select a pie chart, the detailed information on the events in this set will be listed in a tooltip. The listed events are sorted by probability, with certain events on the top followed by uncertain events.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 26, "paper_title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks", "pub_year": 2023, "domain": "Graph neural networks", "requirement": {"requirement_text": "R1: Provide an Overview of GNN Results.To gain an overview of the dataset and classification results, the system needs to summarize various types of information, such as degree distribution and ground truth label distribution. This information, covering various aspects of a GNN model, needs to be organized and presented in a clear manner. Meanwhile, the correlation among this information should be presented to help users develop initial hypotheses about any possible error patterns in GNN results, i.e., a set of wrong predictions that share similar characteristics.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "GNN prediction results", "data_code": {"tables": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Inspired by the fact that GNN prediction results are influ_x0002_enced by both graph structure and node features [14], we define two proxy models to analyze the influence of the graph structure and node features on GNN prediction results. Through expert interviews, experts are concerned about whether the graph structure or node features have a greater impact on GNN prediction, and then determine which components will have more impact. Hence, similar to the ablation study when evaluating GNN models [29], we define two proxy models such as GNN Without Using Fea_x0002_tures (GNNWUF) and Multi-Layer Perceptron (MLP). The two models are chosen, since the two proxy models have the same model architectures as the GNN but are trained using different input data. GNNWUF is trained only using the graph structure while MLP is trained only using the node features. When training GNNWUF, we use one hot encoding as the node feature for each node, meaning GNNWUF considers only the graph structures. When GNN considers only the features of the node itself, then it can degenerate into an MLP model. Hence, MLP is chosen as the other GNN proxy model that only considers the node features and is used to evaluate the influence of node struc_x0002_tures. We train both proxy models with the same settings as the training of GNN. To further help users understand the impact of the graph structure and node features, we also provide a number of metrics, including graph structure based metrics that take into account the graph structure but ignore the node fea_x0002_tures, and node feature based metrics that take the node fea_x0002_tures into account but ignore the graph structure. Those metrics are derived from expert interviews. Details are pre_x0002_sented in the following paragraphs.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "visualize node-level metrics using Parallel Sets. We propose to use Parallel Sets to investigate error patterns in GNN prediction results. Users can select what metrics are to be displayed in the Parallel Sets through Parallel Sets Settings Modal. In general, displaying fewer than five axes in Parallel Sets is a good practice to reduce visual clutter and make efficient use of functions in Parallel Sets. Due to the constraint that the Parallel Sets are used to display the categorical variables, we need to convert the continuous metrics to categorical variables by grouping a range of values into one category. Then we can also show them in the Parallel Sets View.The axis is partitioned into multiple segments representing different categories of the variable. The width of each segment represents the number of nodes falling into that category. We can directly see the distribution of the categories on the axis. Between two consecutive axes, multiple ribbons are shown to connect the two axes, each simultaneously representing the nodes that satisfy the conditions specified by the two axes.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Stripe", "axial_code": [], "componenet_code": ["stripe"]}]}, {"author": "dxf", "index_original": 27, "paper_title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks", "pub_year": 2023, "domain": "Graph neural networks", "requirement": {"requirement_text": "R2: Identify Error Patterns. After developing initial hypotheses about the error patterns, users need more detailed information to verify them. Specifically, user need to examine the characteristics shared by a set of wrong predictions and verify whether error patterns formed by these characteristics make sense in analyzing GNNs based on their domain knowledge. During the interview, experts agreed that they usually use several characteristics to group the wrong predictions and identify error patterns.The system should support users in examining these characteristics and identifying error patterns.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "GNN prediction results", "data_code": {"tables": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Inspired by the fact that GNN prediction results are influ_x0002_enced by both graph structure and node features [14], we define two proxy models to analyze the influence of the graph structure and node features on GNN prediction results. Through expert interviews, experts are concerned about whether the graph structure or node features have a greater impact on GNN prediction, and then determine which components will have more impact. Hence, similar to the ablation study when evaluating GNN models [29], we define two proxy models such as GNN Without Using Fea_x0002_tures (GNNWUF) and Multi-Layer Perceptron (MLP). The two models are chosen, since the two proxy models have the same model architectures as the GNN but are trained using different input data. GNNWUF is trained only using the graph structure while MLP is trained only using the node features. When training GNNWUF, we use one hot encoding as the node feature for each node, meaning GNNWUF considers only the graph structures. When GNN considers only the features of the node itself, then it can degenerate into an MLP model. Hence, MLP is chosen as the other GNN proxy model that only considers the node features and is used to evaluate the influence of node struc_x0002_tures. We train both proxy models with the same settings as the training of GNN. To further help users understand the impact of the graph structure and node features, we also provide a number of metrics, including graph structure based metrics that take into account the graph structure but ignore the node fea_x0002_tures, and node feature based metrics that take the node fea_x0002_tures into account but ignore the graph structure. Those metrics are derived from expert interviews. Details are pre_x0002_sented in the following paragraphs.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "visualize node-level metrics using Parallel Sets. We propose to use Parallel Sets to investigate error patterns in GNN prediction results. Users can select what metrics are to be displayed in the Parallel Sets through Parallel Sets Settings Modal. In general, displaying fewer than five axes in Parallel Sets is a good practice to reduce visual clutter and make efficient use of functions in Parallel Sets. Due to the constraint that the Parallel Sets are used to display the categorical variables, we need to convert the continuous metrics to categorical variables by grouping a range of values into one category. Then we can also show them in the Parallel Sets View.The axis is partitioned into multiple segments representing different categories of the variable. The width of each segment represents the number of nodes falling into that category. We can directly see the distribution of the categories on the axis. Between two consecutive axes, multiple ribbons are shown to connect the two axes, each simultaneously representing the nodes that satisfy the conditions specified by the two axes.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Stripe", "axial_code": [], "componenet_code": ["stripe"]}]}, {"author": "dxf", "index_original": 28, "paper_title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks", "pub_year": 2023, "domain": "Graph neural networks", "requirement": {"requirement_text": "R2: Identify Error Patterns. After developing initial hypotheses about the error patterns, users need more detailed information to verify them. Specifically, user need to examine the characteristics shared by a set of wrong predictions and verify whether error patterns formed by these characteristics make sense in analyzing GNNs based on their domain knowledge. During the interview, experts agreed that they usually use several characteristics to group the wrong predictions and identify error patterns.The system should support users in examining these characteristics\nand identifying error patterns.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "GNN prediction results", "data_code": {"tables": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Inspired by the fact that GNN prediction results are influ_x0002_enced by both graph structure and node features [14], we define two proxy models to analyze the influence of the graph structure and node features on GNN prediction results. Through expert interviews, experts are concerned about whether the graph structure or node features have a greater impact on GNN prediction, and then determine which components will have more impact. Hence, similar to the ablation study when evaluating GNN models [29], we define two proxy models such as GNN Without Using Fea_x0002_tures (GNNWUF) and Multi-Layer Perceptron (MLP). The two models are chosen, since the two proxy models have the same model architectures as the GNN but are trained using different input data. GNNWUF is trained only using the graph structure while MLP is trained only using the node features. When training GNNWUF, we use one hot encoding as the node feature for each node, meaning GNNWUF considers only the graph structures. When GNN considers only the features of the node itself, then it can degenerate into an MLP model. Hence, MLP is chosen as the other GNN proxy model that only considers the node features and is used to evaluate the influence of node struc_x0002_tures. We train both proxy models with the same settings as the training of GNN. To further help users understand the impact of the graph structure and node features, we also provide a number of metrics, including graph structure based metrics that take into account the graph structure but ignore the node fea_x0002_tures, and node feature based metrics that take the node fea_x0002_tures into account but ignore the graph structure. Those metrics are derived from expert interviews. Details are pre_x0002_sented in the following paragraphs.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "To project those node glyphs in 2D plane, users can choose to use the t-SNE [58] or UMAP [59] projection as the basic layout algorithm. Moreover, the force-directed collision-avoidance method is integrated into the basic layout algorithm to prevent the overlapping of node glyphs.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "We group a subset of node-level metrics, display them in glyphs, and further project them to the 2D plane. The Projection View allows users to investigate the similarity of nodes regarding different perspectives. It can be helpful for investigating whether the nodes with similar node metrics share similar error patterns. In the Projection View, we provide a set of linked projection planes of the nodes that use different features. Different from similar designs in EmbeddingVis [57], we design different node glyphs to display different combinations of node-level metrics. To project those node glyphs in 2D plane, users can choose to use the t-SNE [58] or UMAP [59] projection as the basic layout algorithm. Moreover, the force-directed collision-avoidance method is integrated into the basic layout algorithm to prevent the overlapping of node glyphs. When users lasso-select a set of nodes in a projection plane, the links between the same nodes in different planes will be shown to help users identify the nodes and other aspects of those nodes\u2019 properties, as shown in Fig. 4. After users hover on the node glyphs, the legend and detailed information of those node glyphs will be displayed. However, due to the limited screen space, it cannot display hundreds let alone thousands of node glyphs. Therefore, we apply a hierarchical clustering algorithm with complete linkage to cluster these nodes based on the corresponding distance function [60]. Two clusters will be merged into one cluster when the distance of two clusters is less than or equal to a threshold, which is empirically set to 0.5.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Circle;Bar", "axial_code": [], "componenet_code": ["bar", "circle"]}, {"solution_text": "We group a subset of node-level metrics, display them in glyphs, and further project them to the 2D plane. The Projection View allows users to investigate the similarity of nodes regarding different perspectives. It can be helpful for investigating whether the nodes with similar node metrics share similar error patterns. In the Projection View, we provide a set of linked projection planes of the nodes that use different features. Different from similar designs in EmbeddingVis [57], we design different node glyphs to display different combinations of node-level metrics. To project those node glyphs in 2D plane, users can choose to use the t-SNE [58] or UMAP [59] projection as the basic layout algorithm. Moreover, the force-directed collision-avoidance method is integrated into the basic layout algorithm to prevent the overlapping of node glyphs. When users lasso-select a set of nodes in a projection plane, the links between the same nodes in different planes will be shown to help users identify the nodes and other aspects of those nodes\u2019 properties, as shown in Fig. 4. After users hover on the node glyphs, the legend and detailed information of those node glyphs will be displayed. However, due to the limited screen space, it cannot display hundreds let alone thousands of node glyphs. Therefore, we apply a hierarchical clustering algorithm with complete linkage to cluster these nodes based on the corresponding distance function [60]. Two clusters will be merged into one cluster when the distance of two clusters is less than or equal to a threshold, which is empirically set to 0.5.", "solution_category": "interaction", "solution_axial": "History", "solution_compoent": "", "axial_code": ["History"], "componenet_code": ["history"]}]}, {"author": "dxf", "index_original": 29, "paper_title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks", "pub_year": 2023, "domain": "Graph neural networks", "requirement": {"requirement_text": "R2: Identify Error Patterns. After developing initial hypotheses about the error patterns, users need more detailed information to verify them. Specifically, user need to examine the characteristics shared by a set of wrong predictions and verify whether error patterns formed by these characteristics make sense in analyzing GNNs based on their domain knowledge. During the interview, experts agreed that they usually use several characteristics to group the wrong predictions and identify error patterns.The system should support users in examining these characteristics\nand identifying error patterns.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "GNN prediction results", "data_code": {"tables": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Inspired by the fact that GNN prediction results are influ_x0002_enced by both graph structure and node features [14], we define two proxy models to analyze the influence of the graph structure and node features on GNN prediction results. Through expert interviews, experts are concerned about whether the graph structure or node features have a greater impact on GNN prediction, and then determine which components will have more impact. Hence, similar to the ablation study when evaluating GNN models [29], we define two proxy models such as GNN Without Using Fea_x0002_tures (GNNWUF) and Multi-Layer Perceptron (MLP). The two models are chosen, since the two proxy models have the same model architectures as the GNN but are trained using different input data. GNNWUF is trained only using the graph structure while MLP is trained only using the node features. When training GNNWUF, we use one hot encoding as the node feature for each node, meaning GNNWUF considers only the graph structures. When GNN considers only the features of the node itself, then it can degenerate into an MLP model. Hence, MLP is chosen as the other GNN proxy model that only considers the node features and is used to evaluate the influence of node struc_x0002_tures. We train both proxy models with the same settings as the training of GNN. To further help users understand the impact of the graph structure and node features, we also provide a number of metrics, including graph structure based metrics that take into account the graph structure but ignore the node fea_x0002_tures, and node feature based metrics that take the node fea_x0002_tures into account but ignore the graph structure. Those metrics are derived from expert interviews. Details are pre_x0002_sented in the following paragraphs.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "We use the classic node-link diagram with the force directed collision-avoidance layout to visualize the graph dataset. Users can get a sense of the distribution of the selected nodes in the graph, and inspect the neighborhood of the nodes. To further facilitate the convenient exploration of the reasons for errors, we also design a node glyph to encode a group of node-level metrics. Users can hover a node in the Graph View, which will be further highlighted with the radius doubled. The Graph View allows users to quickly check any interesting neighboring nodes. Users can also switch to the \u201cSubgraph\u201d mode. It will display the one-hop and two-hop neighbors of selected nodes, enabling users to explore different hops of neighborhood nodes. Users can visualize the specific subgraphs on their own and can explore them by changing the \u201cSubgraph\u201d options. An overview of the graph is displayed in the bottom right-hand corner to support users navigating the graph. Users can click the specific position in the overview to navigate the displayed area of the graph. Users can choose to filter out the unfocused nodes to accelerate the rendering and reduce the visual clutter in the graph. To investigate the node features and most similar features of training nodes, users can click on the nodes of interest in the Graph View and further explore the node-level features in the Feature Matrix View.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Line;Circle;Pie", "axial_code": [], "componenet_code": ["line", "circle", "pie"]}, {"solution_text": "The Graph View allows users to quickly check any interesting neighboring nodes. Users can also switch to the \u201cSubgraph\u201d mode. It will display the one-hop and two-hop neighbors of selected nodes, enabling users to explore different hops of neighborhood nodes. Users can visualize the specific subgraphs on their own and can explore them by changing the \u201cSubgraph\u201d options. An overview of the graph is displayed in the bottom right-hand corner to support users navigating the graph. Users can click the specific position in the overview to navigate the displayed area of the graph. Users can choose to filter out the unfocused nodes to accelerate the rendering and reduce the visual clutter in the graph. To investigate the node features and most similar features of training nodes, users can click on the nodes of interest in the Graph View and further explore the node-level features in the Feature Matrix View.", "solution_category": "interaction", "solution_axial": "Selecting;Filtering;OverviewandExplore", "solution_compoent": "", "axial_code": ["Selecting", "Filtering", "OverviewandExplore"], "componenet_code": ["selecting", "filtering", "overview_and_explore"]}]}, {"author": "dxf", "index_original": 30, "paper_title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks", "pub_year": 2023, "domain": "Graph neural networks", "requirement": {"requirement_text": "R3: Analyze the Cause of Error Patterns. After identifying error patterns, finding the causes of these errors is important for users to understand, diagnose, and improve the GNNs. More detailed information is needed to understand the possible causes of error patterns. Specifically, users need to inspect the graph structures and node features to determine the causes of error patterns. According to the feedback from expert interviews, there are two main sources of wrong GNN predictions: noise in the training data and inaccurate feature aggregation in GNNs. To predict the label of a node, GNN aggregates the node\u2019s own feature with the features of the neighboring nodes at each layer. Noise in the training data, e.g., the same nodes but different labels, can confuse the GNN and lead to wrong predictions. Inaccurate feature aggregation at any layer will also influence the GNN prediction of the node.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "GNN prediction results", "data_code": {"tables": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Inspired by the fact that GNN prediction results are influ_x0002_enced by both graph structure and node features [14], we define two proxy models to analyze the influence of the graph structure and node features on GNN prediction results. Through expert interviews, experts are concerned about whether the graph structure or node features have a greater impact on GNN prediction, and then determine which components will have more impact. Hence, similar to the ablation study when evaluating GNN models [29], we define two proxy models such as GNN Without Using Fea_x0002_tures (GNNWUF) and Multi-Layer Perceptron (MLP). The two models are chosen, since the two proxy models have the same model architectures as the GNN but are trained using different input data. GNNWUF is trained only using the graph structure while MLP is trained only using the node features. When training GNNWUF, we use one hot encoding as the node feature for each node, meaning GNNWUF considers only the graph structures. When GNN considers only the features of the node itself, then it can degenerate into an MLP model. Hence, MLP is chosen as the other GNN proxy model that only considers the node features and is used to evaluate the influence of node struc_x0002_tures. We train both proxy models with the same settings as the training of GNN. To further help users understand the impact of the graph structure and node features, we also provide a number of metrics, including graph structure based metrics that take into account the graph structure but ignore the node fea_x0002_tures, and node feature based metrics that take the node fea_x0002_tures into account but ignore the graph structure. Those metrics are derived from expert interviews. Details are pre_x0002_sented in the following paragraphs.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "To project those node glyphs in 2D plane, users can choose to use the t-SNE [58] or UMAP [59] projection as the basic layout algorithm. Moreover, the force-directed collision-avoidance method is integrated into the basic layout algorithm to prevent the overlapping of node glyphs.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "We group a subset of node-level metrics, display them in glyphs, and further project them to the 2D plane. The Projection View allows users to investigate the similarity of nodes regarding different perspectives. It can be helpful for investigating whether the nodes with similar node metrics share similar error patterns. In the Projection View, we provide a set of linked projection planes of the nodes that use different features. Different from similar designs in EmbeddingVis [57], we design different node glyphs to display different combinations of node-level metrics. To project those node glyphs in 2D plane, users can choose to use the t-SNE [58] or UMAP [59] projection as the basic layout algorithm. Moreover, the force-directed collision-avoidance method is integrated into the basic layout algorithm to prevent the overlapping of node glyphs. When users lasso-select a set of nodes in a projection plane, the links between the same nodes in different planes will be shown to help users identify the nodes and other aspects of those nodes\u2019 properties, as shown in Fig. 4. After users hover on the node glyphs, the legend and detailed information of those node glyphs will be displayed. However, due to the limited screen space, it cannot display hundreds let alone thousands of node glyphs. Therefore, we apply a hierarchical clustering algorithm with complete linkage to cluster these nodes based on the corresponding distance function [60]. Two clusters will be merged into one cluster when the distance of two clusters is less than or equal to a threshold, which is empirically set to 0.5.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Circle;Bar", "axial_code": [], "componenet_code": ["bar", "circle"]}, {"solution_text": "We group a subset of node-level metrics, display them in glyphs, and further project them to the 2D plane. The Projection View allows users to investigate the similarity of nodes regarding different perspectives. It can be helpful for investigating whether the nodes with similar node metrics share similar error patterns. In the Projection View, we provide a set of linked projection planes of the nodes that use different features. Different from similar designs in EmbeddingVis [57], we design different node glyphs to display different combinations of node-level metrics. To project those node glyphs in 2D plane, users can choose to use the t-SNE [58] or UMAP [59] projection as the basic layout algorithm. Moreover, the force-directed collision-avoidance method is integrated into the basic layout algorithm to prevent the overlapping of node glyphs. When users lasso-select a set of nodes in a projection plane, the links between the same nodes in different planes will be shown to help users identify the nodes and other aspects of those nodes\u2019 properties, as shown in Fig. 4. After users hover on the node glyphs, the legend and detailed information of those node glyphs will be displayed. However, due to the limited screen space, it cannot display hundreds let alone thousands of node glyphs. Therefore, we apply a hierarchical clustering algorithm with complete linkage to cluster these nodes based on the corresponding distance function [60]. Two clusters will be merged into one cluster when the distance of two clusters is less than or equal to a threshold, which is empirically set to 0.5.", "solution_category": "interaction", "solution_axial": "History", "solution_compoent": "", "axial_code": ["History"], "componenet_code": ["history"]}]}, {"author": "dxf", "index_original": 31, "paper_title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks", "pub_year": 2023, "domain": "Graph neural networks", "requirement": {"requirement_text": "R3: Analyze the Cause of Error Patterns. After identifying error patterns, finding the causes of these errors is important for users to understand, diagnose, and improve the GNNs. More detailed information is needed to understand the possible causes of error patterns. Specifically, users need to inspect the graph structures and node features to determine the causes of error patterns. According to the feedback from expert interviews, there are two main sources of wrong GNN predictions: noise in the training data and inaccurate feature aggregation in GNNs. To predict the label of a node, GNN aggregates the node\u2019s own feature with the features of the neighboring nodes at each layer. Noise in the training data, e.g., the same nodes but different labels, can confuse the GNN and lead to wrong predictions. Inaccurate feature aggregation at any layer will also influence the GNN prediction of the node.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "GNN prediction results", "data_code": {"tables": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Inspired by the fact that GNN prediction results are influ_x0002_enced by both graph structure and node features [14], we define two proxy models to analyze the influence of the graph structure and node features on GNN prediction results. Through expert interviews, experts are concerned about whether the graph structure or node features have a greater impact on GNN prediction, and then determine which components will have more impact. Hence, similar to the ablation study when evaluating GNN models [29], we define two proxy models such as GNN Without Using Fea_x0002_tures (GNNWUF) and Multi-Layer Perceptron (MLP). The two models are chosen, since the two proxy models have the same model architectures as the GNN but are trained using different input data. GNNWUF is trained only using the graph structure while MLP is trained only using the node features. When training GNNWUF, we use one hot encoding as the node feature for each node, meaning GNNWUF considers only the graph structures. When GNN considers only the features of the node itself, then it can degenerate into an MLP model. Hence, MLP is chosen as the other GNN proxy model that only considers the node features and is used to evaluate the influence of node struc_x0002_tures. We train both proxy models with the same settings as the training of GNN. To further help users understand the impact of the graph structure and node features, we also provide a number of metrics, including graph structure based metrics that take into account the graph structure but ignore the node fea_x0002_tures, and node feature based metrics that take the node fea_x0002_tures into account but ignore the graph structure. Those metrics are derived from expert interviews. Details are pre_x0002_sented in the following paragraphs.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "We use the classic node-link diagram with the force directed collision-avoidance layout to visualize the graph dataset. Users can get a sense of the distribution of the selected nodes in the graph, and inspect the neighborhood of the nodes. To further facilitate the convenient exploration of the reasons for errors, we also design a node glyph to encode a group of node-level metrics. Users can hover a node in the Graph View, which will be further highlighted with the radius doubled. The Graph View allows users to quickly check any interesting neighboring nodes. Users can also switch to the \u201cSubgraph\u201d mode. It will display the one-hop and two-hop neighbors of selected nodes, enabling users to explore different hops of neighborhood nodes. Users can visualize the specific subgraphs on their own and can explore them by changing the \u201cSubgraph\u201d options. An overview of the graph is displayed in the bottom right-hand corner to support users navigating the graph. Users can click the specific position in the overview to navigate the displayed area of the graph. Users can choose to filter out the unfocused nodes to accelerate the rendering and reduce the visual clutter in the graph. To investigate the node features and most similar features of training nodes, users can click on the nodes of interest in the Graph View and further explore the node-level features in the Feature Matrix View.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Line;Circle;Pie", "axial_code": [], "componenet_code": ["line", "circle", "pie"]}, {"solution_text": "The Graph View allows users to quickly check any interesting neighboring nodes. Users can also switch to the \u201cSubgraph\u201d mode. It will display the one-hop and two-hop neighbors of selected nodes, enabling users to explore different hops of neighborhood nodes. Users can visualize the specific subgraphs on their own and can explore them by changing the \u201cSubgraph\u201d options. An overview of the graph is displayed in the bottom right-hand corner to support users navigating the graph. Users can click the specific position in the overview to navigate the displayed area of the graph. Users can choose to filter out the unfocused nodes to accelerate the rendering and reduce the visual clutter in the graph. To investigate the node features and most similar features of training nodes, users can click on the nodes of interest in the Graph View and further explore the node-level features in the Feature Matrix View.", "solution_category": "interaction", "solution_axial": "Selecting;Filtering;OverviewandExplore", "solution_compoent": "", "axial_code": ["Selecting", "Filtering", "OverviewandExplore"], "componenet_code": ["selecting", "filtering", "overview_and_explore"]}]}, {"author": "dxf", "index_original": 33, "paper_title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents", "pub_year": 2023, "domain": "Legal", "requirement": {"requirement_text": "T1: Identification of explicit and potential citations: The system should exhibit decisions that explicitly cite a specific binding precedent and identify/exhibit those that could potentially mention it (non-explicit citation).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In total, we have 58 binding precedents, idealized as a mechanism to create a consolidated understanding among the STF\u2019s justices. We collected our dataset from a partner_x0002_ship with the Supremo em N_x0001_umeros (\u201cSTF in Numbers\u201d, in free translation project [43] \u2014 a project that seeks to assess legal and computer knowledge to produce unprecedented data on the Supreme Court. This dataset contains more than 2,500,000 documents since 1988 and metadata such as the number of the BP being cited (if that is the case) and docu_x0002_ment type. Considering only decisions that explicitly cite at least one of the 58 BPs, there are 38,364 documents, totaling 41,031 citations (some documents may mention more than one BP). The number of citations per BP does not follow a uniform distribution, as we can see in Fig. 2.", "data_code": {"tables": 1, "temporal": 1}}, "solution": [{"solution_text": "Text Embedding. As mentioned above, we use a classifica_x0002_tion model to assess the probability of a document citing a BP. To assess different classifiers\u2019 performance, we rely on text embedding methods to generate a vector representation of the documents. We test different embeddings to evaluate their performance: TF-IDF [47], Doc2vec [44], Universal Sen_x0002_tence Encoder (USE) [10], and Longformer [48]. In TF-IDF\u2019s particular case, we have also applied a dimensionality reduction procedure to map the documents to a 50-dimen_x0002_sional space using Truncated Singular Value Decomposition (SVD) [49]. We call the final representation Truncated TF_x0002_IDF. The reason to consider Longformer, instead of a more popular language model, e.g., BERT [50], is that Longformer overcomes BERT\u2019s limitation of 512 tokens. Note that USE and Longformer do not need a pre-processed text (as described in the previous paragraph) because both have their tokenization mechanism.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Classification. Support Vector Machines (SVM) are com_x0002_plex enough for real-world classification problems and sim_x0002_ple enough to be analyzed mathematically [51]. Our study considers SVM with linear and Radial Basis Function (RBF) kernels. Moreover, given that Longformer [48] has a linear classification neural network layer plugged into it, we also fine-tuned it to assess its classification performance in our context. The described classifiers are used to search for potential citations in unlabeled data, assigning probabilities to each document to belong to each class. To get probabilities from an SVM, we need to make calibration: using labeled data, create a map from the classifier\u2019s output (SVM scores) to a probability estimate for each class, which sum up to 1. To create this map, we use the calibration method from Platt et al. [52] for SVM with linear kernel and the Wu et al.\u2019s [53] method for RBF kernel. Platt et al.\u2019s method, precisely, does not support the multiclass case, so we calibrate each class in a \u201cone-versus-rest\u201d approach and normalize the results in the end to sum up to 1. This map will receive the SVM scores from a data instance and output a probability esti_x0002_mate for each class.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "This section describes how we use the classifiers to identify potential citations and understand the models\u2019 decisions (see Fig. 3b). Citation Inference. The association between a text embed_x0002_ding technique and a classifier gives us what we call a model. A model receives the raw text and returns the classifier\u2019s probabilities, which we interpret as the document\u2019s proba_x0002_bility to cite each precedent. We consider a potential citation when the citation probability is greater or equal to a thresh_x0002_old tc 2 \u00bd0; 1_x0003_ (chosen by the user). Interpretability. A concern when dealing with machine learning models is their interpretability. For instance, if Truncated TF-IDF combined with SVM points out that a document potentially cites BP 10 because of high returned probability, how can we understand the reasons behind this decision? Generally speaking, how can we trust this model? Understanding why a model is taking a particular decision is of paramount importance [45], especially in sensitive sce_x0002_narios like a legal document analysis. Consider document X and its embedding vector x 2 Rd (e.g., TF-IDF vector). This document\u2019s probability of belong_x0002_ing to class C is given by fC\u00f0x\u00de, with fC : Rd ! \u00bd0; 1_x0003_ a mod_x0002_el\u2019s returned probability for class C. Our particular interest is to know the importance of each sentence from document X to the given probability, i.e., if each sentence has positive, negative, or neutral importance, and the magnitude of this importance. After preliminary tests have discarded the employment of the leave-one-out feature importance (LOO) [54] for interpretability due to its high sensitivity, we chose the Local Interpretable Model-agnostic Explanations (Lime) [45] method to tackle this issue. The intuition is that, by removing a spe_x0002_cific sentence and obtaining the variation of probability DfC, we can measure the importance of this sentence for the prediction fC\u00f0x\u00de. More formally, we randomly remove some sentences from document X, vectorize this new docu_x0002_ment to z, and add it to set Z. Doing this many times, we have a collection Z of vectors around x in high dimensional space. We approximate fC\u00f0z\u00de using a linear model g\u00f0z\u00de, minimizing the approximation error L\u00f0fC; g; z\u00de in z 2 Z, but also constraining the complexity This task can be interpreted as a weighted linear regres_x0002_V\u00f0g\u00de of g. sion with regularization. We end with a model g\u00f0z\u00de that is linear over the sentences (i.e., the presence or absence of a sentence), where the linear model\u2019s coefficients can be inter_x0002_preted as the importance score of each sentence to the final prediction By default, Lime works with words, and there is also the fC\u00f0x\u00de. possibility of interpreting entire paragraphs. A word divi_x0002_sion brings some advantages, such as detailing. Still, it does not immediately assess each sentence\u2019s importance to the decision (e.g., sentences similar to the BP text). A paragraph division is also interesting for visualization purposes (see, for example, Section 6.3), but it merges various sentences into one, hampering the precise identification of relevant parts. Therefore, our choice of working with sentences com_x0002_prises the \u201cbest of both worlds\u201d. In our experiments, Lime proved to be very robust and reliable when dealing with sentences.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "In this view, the x-axis represents the time a document was published (monthly resolution), and the y-axis represents the BP. The red pins ( ) refer to the publication date of the corresponding BPs. For each one, a vertical bar on a particular date indicates the existence of documents published on that date that cite such BP.The height of each bar reflects the number of documents published on that date, and its color is defined such that (i) blue bars (without borders) indicate that every document in that month cites the BP explicitly, (ii) orange bars (without borders) suggest that every document potentially (rather than explicitly) cites the BP, and (iii) blue bars with orange edges indicate the presence of both explicit and potential citations.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "dxf", "index_original": 34, "paper_title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents", "pub_year": 2023, "domain": "Legal", "requirement": {"requirement_text": "T1: Identification of explicit and potential citations: The system should exhibit decisions that explicitly cite a specific binding precedent and identify/exhibit those that could potentially mention it (non-explicit citation).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In total, we have 58 binding precedents, idealized as a mechanism to create a consolidated understanding among the STF\u2019s justices. We collected our dataset from a partner_x0002_ship with the Supremo em N_x0001_umeros (\u201cSTF in Numbers\u201d, in free translation project [43] \u2014 a project that seeks to assess legal and computer knowledge to produce unprecedented data on the Supreme Court. This dataset contains more than 2,500,000 documents since 1988 and metadata such as the number of the BP being cited (if that is the case) and docu_x0002_ment type. Considering only decisions that explicitly cite at least one of the 58 BPs, there are 38,364 documents, totaling 41,031 citations (some documents may mention more than one BP). The number of citations per BP does not follow a uniform distribution, as we can see in Fig. 2.", "data_code": {"textual": 1, "fields": 1, "tables": 1, "geometry": 1}}, "solution": [{"solution_text": "Text Embedding. As mentioned above, we use a classifica_x0002_tion model to assess the probability of a document citing a BP. To assess different classifiers\u2019 performance, we rely on text embedding methods to generate a vector representation of the documents. We test different embeddings to evaluate their performance: TF-IDF [47], Doc2vec [44], Universal Sen_x0002_tence Encoder (USE) [10], and Longformer [48]. In TF-IDF\u2019s particular case, we have also applied a dimensionality reduction procedure to map the documents to a 50-dimen_x0002_sional space using Truncated Singular Value Decomposition (SVD) [49]. We call the final representation Truncated TF_x0002_IDF. The reason to consider Longformer, instead of a more popular language model, e.g., BERT [50], is that Longformer overcomes BERT\u2019s limitation of 512 tokens. Note that USE and Longformer do not need a pre-processed text (as described in the previous paragraph) because both have their tokenization mechanism.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Classification. Support Vector Machines (SVM) are com_x0002_plex enough for real-world classification problems and sim_x0002_ple enough to be analyzed mathematically [51]. Our study considers SVM with linear and Radial Basis Function (RBF) kernels. Moreover, given that Longformer [48] has a linear classification neural network layer plugged into it, we also fine-tuned it to assess its classification performance in our context. The described classifiers are used to search for potential citations in unlabeled data, assigning probabilities to each document to belong to each class. To get probabilities from an SVM, we need to make calibration: using labeled data, create a map from the classifier\u2019s output (SVM scores) to a probability estimate for each class, which sum up to 1. To create this map, we use the calibration method from Platt et al. [52] for SVM with linear kernel and the Wu et al.\u2019s [53] method for RBF kernel. Platt et al.\u2019s method, precisely, does not support the multiclass case, so we calibrate each class in a \u201cone-versus-rest\u201d approach and normalize the results in the end to sum up to 1. This map will receive the SVM scores from a data instance and output a probability esti_x0002_mate for each class.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "This section describes how we use the classifiers to identify potential citations and understand the models\u2019 decisions (see Fig. 3b). Citation Inference. The association between a text embed_x0002_ding technique and a classifier gives us what we call a model. A model receives the raw text and returns the classifier\u2019s probabilities, which we interpret as the document\u2019s proba_x0002_bility to cite each precedent. We consider a potential citation when the citation probability is greater or equal to a thresh_x0002_old tc 2 \u00bd0; 1_x0003_ (chosen by the user). Interpretability. A concern when dealing with machine learning models is their interpretability. For instance, if Truncated TF-IDF combined with SVM points out that a document potentially cites BP 10 because of high returned probability, how can we understand the reasons behind this decision? Generally speaking, how can we trust this model? Understanding why a model is taking a particular decision is of paramount importance [45], especially in sensitive sce_x0002_narios like a legal document analysis. Consider document X and its embedding vector x 2 Rd (e.g., TF-IDF vector). This document\u2019s probability of belong_x0002_ing to class C is given by fC\u00f0x\u00de, with fC : Rd ! \u00bd0; 1_x0003_ a mod_x0002_el\u2019s returned probability for class C. Our particular interest is to know the importance of each sentence from document X to the given probability, i.e., if each sentence has positive, negative, or neutral importance, and the magnitude of this importance. After preliminary tests have discarded the employment of the leave-one-out feature importance (LOO) [54] for interpretability due to its high sensitivity, we chose the Local Interpretable Model-agnostic Explanations (Lime) [45] method to tackle this issue. The intuition is that, by removing a spe_x0002_cific sentence and obtaining the variation of probability DfC, we can measure the importance of this sentence for the prediction fC\u00f0x\u00de. More formally, we randomly remove some sentences from document X, vectorize this new docu_x0002_ment to z, and add it to set Z. Doing this many times, we have a collection Z of vectors around x in high dimensional space. We approximate fC\u00f0z\u00de using a linear model g\u00f0z\u00de, minimizing the approximation error L\u00f0fC; g; z\u00de in z 2 Z, but also constraining the complexity This task can be interpreted as a weighted linear regres_x0002_V\u00f0g\u00de of g. sion with regularization. We end with a model g\u00f0z\u00de that is linear over the sentences (i.e., the presence or absence of a sentence), where the linear model\u2019s coefficients can be inter_x0002_preted as the importance score of each sentence to the final prediction By default, Lime works with words, and there is also the fC\u00f0x\u00de. possibility of interpreting entire paragraphs. A word divi_x0002_sion brings some advantages, such as detailing. Still, it does not immediately assess each sentence\u2019s importance to the decision (e.g., sentences similar to the BP text). A paragraph division is also interesting for visualization purposes (see, for example, Section 6.3), but it merges various sentences into one, hampering the precise identification of relevant parts. Therefore, our choice of working with sentences com_x0002_prises the \u201cbest of both worlds\u201d. In our experiments, Lime proved to be very robust and reliable when dealing with sentences.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "the color of each stacked bar stands for explicit (blue) or potential (orange) citation. Once the user has found a set of documents (i.e., a bar) of interest in Global View, he/she can select such a set by clicking on the bar. The user is then redirected to the Paragraph Similarities View (Fig. 5b), which presents in the y-axis all documents from the selected bar, that is, documents that cite a particular BP on a given date. Each document is divided into paragraphs, indicated by the horizontal stack of bars. The bar\u2019s size means the size of the paragraph, and the color intensity reflects the similarity between the corresponding paragraph and the BP text; the darker the color, the greater the similarity, and the more common parts exist between the BP and the paragraph (T5). Similarly to Global View, the color of each stacked bar stands for explicit (blue) or potential (orange) citation (T1). The similarity is given by the angular dis_x0002_tance [10] between two Truncated-TF-IDF vectors, but other embeddings and similarities (e.g., the raw cosine similarity) could be adopted. To guide users further exploring the set of documents, we group them into clusters (T4 \u2013 details below). Inside each cluster, the documents are positioned in descending order of similarity between the document and the BP, which is defined as the maximum similarity between its paragraphs and the BP (T4). Showing documents in descending order of similarity is helpful because the user can promptly identify the top-k most similar, and therefore most interesting, documents. Further_x0002_more, to quickly assess the document distribution w.r.t. the similarity values, we also show an interactive bar chart above each color bar. Document Clustering. Since a binding precedent may cover decisions related to different subjects, the Brazilian Supreme Court website provides, for each BP, some clusters of decisions created according to their subjects. There are only a few clusters for each BP, each containing a few labeled documents (for instance, there are 8 clusters for BP 4, each containing two documents on average). We employed NLP text pre-processing and applied topic modeling strategies to cluster documents according to their relevant words to take advantage of this limited but useful ground truth. We have tested different and well-established topic extraction methods, including LDA [57], NMF [58], SNMF [59], and PSMF [60]. The NMF (Frobenius norm) method presented the best results, so we adopted it as the default method.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "Once the user has found a set of documents (i.e., a bar) of interest in Global View, he/she can select such a set by clicking on the bar. The user is then redirected to the Paragraph Similarities View (Fig. 5b), which presents in the y-axis all documents from the selected bar, that is, documents that cite a particular BP on a given date. Each document is divided into paragraphs, indicated by the horizontal stack of bars. ", "solution_category": "interaction", "solution_axial": "Selecting;Connect/Relate;Encode", "solution_compoent": "", "axial_code": ["Selecting", "Encode", "Connect/Relate"], "componenet_code": ["selecting", "encode", "connect_relate"]}]}, {"author": "dxf", "index_original": 36, "paper_title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents", "pub_year": 2023, "domain": "Legal", "requirement": {"requirement_text": "T2: Overview of decisions and binding precedents: The system should provide an overview of all the decisions and binding precedents, highlighting the chronological order they appear.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "In total, we have 58 binding precedents, idealized as a mechanism to create a consolidated understanding among the STF\u2019s justices. We collected our dataset from a partner_x0002_ship with the Supremo em N_x0001_umeros (\u201cSTF in Numbers\u201d, in free translation project [43] \u2014 a project that seeks to assess legal and computer knowledge to produce unprecedented data on the Supreme Court. This dataset contains more than 2,500,000 documents since 1988 and metadata such as the number of the BP being cited (if that is the case) and docu_x0002_ment type. Considering only decisions that explicitly cite at least one of the 58 BPs, there are 38,364 documents, totaling 41,031 citations (some documents may mention more than one BP). The number of citations per BP does not follow a uniform distribution, as we can see in Fig. 2.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "Text Embedding. As mentioned above, we use a classifica_x0002_tion model to assess the probability of a document citing a BP. To assess different classifiers\u2019 performance, we rely on text embedding methods to generate a vector representation of the documents. We test different embeddings to evaluate their performance: TF-IDF [47], Doc2vec [44], Universal Sen_x0002_tence Encoder (USE) [10], and Longformer [48]. In TF-IDF\u2019s particular case, we have also applied a dimensionality reduction procedure to map the documents to a 50-dimen_x0002_sional space using Truncated Singular Value Decomposition (SVD) [49]. We call the final representation Truncated TF_x0002_IDF. The reason to consider Longformer, instead of a more popular language model, e.g., BERT [50], is that Longformer overcomes BERT\u2019s limitation of 512 tokens. Note that USE and Longformer do not need a pre-processed text (as described in the previous paragraph) because both have their tokenization mechanism.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Classification. Support Vector Machines (SVM) are com_x0002_plex enough for real-world classification problems and sim_x0002_ple enough to be analyzed mathematically [51]. Our study considers SVM with linear and Radial Basis Function (RBF) kernels. Moreover, given that Longformer [48] has a linear classification neural network layer plugged into it, we also fine-tuned it to assess its classification performance in our context. The described classifiers are used to search for potential citations in unlabeled data, assigning probabilities to each document to belong to each class. To get probabilities from an SVM, we need to make calibration: using labeled data, create a map from the classifier\u2019s output (SVM scores) to a probability estimate for each class, which sum up to 1. To create this map, we use the calibration method from Platt et al. [52] for SVM with linear kernel and the Wu et al.\u2019s [53] method for RBF kernel. Platt et al.\u2019s method, precisely, does not support the multiclass case, so we calibrate each class in a \u201cone-versus-rest\u201d approach and normalize the results in the end to sum up to 1. This map will receive the SVM scores from a data instance and output a probability esti_x0002_mate for each class.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "This section describes how we use the classifiers to identify potential citations and understand the models\u2019 decisions (see Fig. 3b). Citation Inference. The association between a text embed_x0002_ding technique and a classifier gives us what we call a model. A model receives the raw text and returns the classifier\u2019s probabilities, which we interpret as the document\u2019s proba_x0002_bility to cite each precedent. We consider a potential citation when the citation probability is greater or equal to a thresh_x0002_old tc 2 \u00bd0; 1_x0003_ (chosen by the user). Interpretability. A concern when dealing with machine learning models is their interpretability. For instance, if Truncated TF-IDF combined with SVM points out that a document potentially cites BP 10 because of high returned probability, how can we understand the reasons behind this decision? Generally speaking, how can we trust this model? Understanding why a model is taking a particular decision is of paramount importance [45], especially in sensitive sce_x0002_narios like a legal document analysis. Consider document X and its embedding vector x 2 Rd (e.g., TF-IDF vector). This document\u2019s probability of belong_x0002_ing to class C is given by fC\u00f0x\u00de, with fC : Rd ! \u00bd0; 1_x0003_ a mod_x0002_el\u2019s returned probability for class C. Our particular interest is to know the importance of each sentence from document X to the given probability, i.e., if each sentence has positive, negative, or neutral importance, and the magnitude of this importance. After preliminary tests have discarded the employment of the leave-one-out feature importance (LOO) [54] for interpretability due to its high sensitivity, we chose the Local Interpretable Model-agnostic Explanations (Lime) [45] method to tackle this issue. The intuition is that, by removing a spe_x0002_cific sentence and obtaining the variation of probability DfC, we can measure the importance of this sentence for the prediction fC\u00f0x\u00de. More formally, we randomly remove some sentences from document X, vectorize this new docu_x0002_ment to z, and add it to set Z. Doing this many times, we have a collection Z of vectors around x in high dimensional space. We approximate fC\u00f0z\u00de using a linear model g\u00f0z\u00de, minimizing the approximation error L\u00f0fC; g; z\u00de in z 2 Z, but also constraining the complexity This task can be interpreted as a weighted linear regres_x0002_V\u00f0g\u00de of g. sion with regularization. We end with a model g\u00f0z\u00de that is linear over the sentences (i.e., the presence or absence of a sentence), where the linear model\u2019s coefficients can be inter_x0002_preted as the importance score of each sentence to the final prediction By default, Lime works with words, and there is also the fC\u00f0x\u00de. possibility of interpreting entire paragraphs. A word divi_x0002_sion brings some advantages, such as detailing. Still, it does not immediately assess each sentence\u2019s importance to the decision (e.g., sentences similar to the BP text). A paragraph division is also interesting for visualization purposes (see, for example, Section 6.3), but it merges various sentences into one, hampering the precise identification of relevant parts. Therefore, our choice of working with sentences com_x0002_prises the \u201cbest of both worlds\u201d. In our experiments, Lime proved to be very robust and reliable when dealing with sentences.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "This view is responsible for showing an overview of the dataset under a temporal perspective. In this view, the x-axis represents the time a document was published (monthly resolution), and the y-axis represents the BP. The red pins ( ) refer to the publication date of the corresponding BPs. For each one, a vertical bar on a particular date indicates the existence of documents published on that date that cite such BP.The height of each bar reflects the number of documents published on that date, and its color is defined such that (i) blue bars (without borders) indicate that every document in that month cites the BP explicitly, (ii) orange bars (without borders) suggest that every document potentially (rather than explicitly) cites the BP, and (iii) blue bars with orange edges indicate the presence of both explicit and potential citations.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "dxf", "index_original": 38, "paper_title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents", "pub_year": 2023, "domain": "Legal", "requirement": {"requirement_text": "T4: Grouping and ordering decisions: The system should identify and group similar decisions that cite the same binding precedent on a given date (Q2.2) and order them in the layout according to the similarity between binding precedent and parts of a decision.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "In total, we have 58 binding precedents, idealized as a mechanism to create a consolidated understanding among the STF\u2019s justices. We collected our dataset from a partner_x0002_ship with the Supremo em N_x0001_umeros (\u201cSTF in Numbers\u201d, in free translation project [43] \u2014 a project that seeks to assess legal and computer knowledge to produce unprecedented data on the Supreme Court. This dataset contains more than 2,500,000 documents since 1988 and metadata such as the number of the BP being cited (if that is the case) and docu_x0002_ment type. Considering only decisions that explicitly cite at least one of the 58 BPs, there are 38,364 documents, totaling 41,031 citations (some documents may mention more than one BP). The number of citations per BP does not follow a uniform distribution, as we can see in Fig. 2.", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Text Embedding. As mentioned above, we use a classifica_x0002_tion model to assess the probability of a document citing a BP. To assess different classifiers\u2019 performance, we rely on text embedding methods to generate a vector representation of the documents. We test different embeddings to evaluate their performance: TF-IDF [47], Doc2vec [44], Universal Sen_x0002_tence Encoder (USE) [10], and Longformer [48]. In TF-IDF\u2019s particular case, we have also applied a dimensionality reduction procedure to map the documents to a 50-dimen_x0002_sional space using Truncated Singular Value Decomposition (SVD) [49]. We call the final representation Truncated TF_x0002_IDF. The reason to consider Longformer, instead of a more popular language model, e.g., BERT [50], is that Longformer overcomes BERT\u2019s limitation of 512 tokens. Note that USE and Longformer do not need a pre-processed text (as described in the previous paragraph) because both have their tokenization mechanism.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Classification. Support Vector Machines (SVM) are com_x0002_plex enough for real-world classification problems and sim_x0002_ple enough to be analyzed mathematically [51]. Our study considers SVM with linear and Radial Basis Function (RBF) kernels. Moreover, given that Longformer [48] has a linear classification neural network layer plugged into it, we also fine-tuned it to assess its classification performance in our context. The described classifiers are used to search for potential citations in unlabeled data, assigning probabilities to each document to belong to each class. To get probabilities from an SVM, we need to make calibration: using labeled data, create a map from the classifier\u2019s output (SVM scores) to a probability estimate for each class, which sum up to 1. To create this map, we use the calibration method from Platt et al. [52] for SVM with linear kernel and the Wu et al.\u2019s [53] method for RBF kernel. Platt et al.\u2019s method, precisely, does not support the multiclass case, so we calibrate each class in a \u201cone-versus-rest\u201d approach and normalize the results in the end to sum up to 1. This map will receive the SVM scores from a data instance and output a probability esti_x0002_mate for each class.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "This section describes how we use the classifiers to identify potential citations and understand the models\u2019 decisions (see Fig. 3b). Citation Inference. The association between a text embed_x0002_ding technique and a classifier gives us what we call a model. A model receives the raw text and returns the classifier\u2019s probabilities, which we interpret as the document\u2019s proba_x0002_bility to cite each precedent. We consider a potential citation when the citation probability is greater or equal to a thresh_x0002_old tc 2 \u00bd0; 1_x0003_ (chosen by the user). Interpretability. A concern when dealing with machine learning models is their interpretability. For instance, if Truncated TF-IDF combined with SVM points out that a document potentially cites BP 10 because of high returned probability, how can we understand the reasons behind this decision? Generally speaking, how can we trust this model? Understanding why a model is taking a particular decision is of paramount importance [45], especially in sensitive sce_x0002_narios like a legal document analysis. Consider document X and its embedding vector x 2 Rd (e.g., TF-IDF vector). This document\u2019s probability of belong_x0002_ing to class C is given by fC\u00f0x\u00de, with fC : Rd ! \u00bd0; 1_x0003_ a mod_x0002_el\u2019s returned probability for class C. Our particular interest is to know the importance of each sentence from document X to the given probability, i.e., if each sentence has positive, negative, or neutral importance, and the magnitude of this importance. After preliminary tests have discarded the employment of the leave-one-out feature importance (LOO) [54] for interpretability due to its high sensitivity, we chose the Local Interpretable Model-agnostic Explanations (Lime) [45] method to tackle this issue. The intuition is that, by removing a spe_x0002_cific sentence and obtaining the variation of probability DfC, we can measure the importance of this sentence for the prediction fC\u00f0x\u00de. More formally, we randomly remove some sentences from document X, vectorize this new docu_x0002_ment to z, and add it to set Z. Doing this many times, we have a collection Z of vectors around x in high dimensional space. We approximate fC\u00f0z\u00de using a linear model g\u00f0z\u00de, minimizing the approximation error L\u00f0fC; g; z\u00de in z 2 Z, but also constraining the complexity This task can be interpreted as a weighted linear regres_x0002_V\u00f0g\u00de of g. sion with regularization. We end with a model g\u00f0z\u00de that is linear over the sentences (i.e., the presence or absence of a sentence), where the linear model\u2019s coefficients can be inter_x0002_preted as the importance score of each sentence to the final prediction By default, Lime works with words, and there is also the fC\u00f0x\u00de. possibility of interpreting entire paragraphs. A word divi_x0002_sion brings some advantages, such as detailing. Still, it does not immediately assess each sentence\u2019s importance to the decision (e.g., sentences similar to the BP text). A paragraph division is also interesting for visualization purposes (see, for example, Section 6.3), but it merges various sentences into one, hampering the precise identification of relevant parts. Therefore, our choice of working with sentences com_x0002_prises the \u201cbest of both worlds\u201d. In our experiments, Lime proved to be very robust and reliable when dealing with sentences.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "we group them into clusters (T4 \u2013 details below). Inside each cluster, the documents are positioned in descending order of similarity between the document and the BP, which is defined as the maximum similarity between its paragraphs and the BP. the color of each stacked bar stands for explicit (blue) or potential (orange) citation. Once the user has found a set of documents (i.e., a bar) of interest in Global View, he/she can select such a set by clicking on the bar. The user is then redirected to the Paragraph Similarities View (Fig. 5b), which presents in the y-axis all documents from the selected bar, that is, documents that cite a particular BP on a given date. Each document is divided into paragraphs, indicated by the horizontal stack of bars. The bar\u2019s size means the size of the paragraph, and the color intensity reflects the similarity between the corresponding paragraph and the BP text; the darker the color, the greater the similarity, and the more common parts exist between the BP and the paragraph (T5). Similarly to Global View, the color of each stacked bar stands for explicit (blue) or potential (orange) citation (T1). The similarity is given by the angular dis_x0002_tance [10] between two Truncated-TF-IDF vectors, but other embeddings and similarities (e.g., the raw cosine similarity) could be adopted. To guide users further exploring the set of documents, we group them into clusters (T4 \u2013 details below). Inside each cluster, the documents are positioned in descending order of similarity between the document and the BP, which is defined as the maximum similarity between its paragraphs and the BP (T4). Showing documents in descending order of similarity is helpful because the user can promptly identify the top-k most similar, and therefore most interesting, documents. Further_x0002_more, to quickly assess the document distribution w.r.t. the similarity values, we also show an interactive bar chart above each color bar. Document Clustering. Since a binding precedent may cover decisions related to different subjects, the Brazilian Supreme Court website provides, for each BP, some clusters of decisions created according to their subjects. There are only a few clusters for each BP, each containing a few labeled documents (for instance, there are 8 clusters for BP 4, each containing two documents on average). We employed NLP text pre-processing and applied topic modeling strategies to cluster documents according to their relevant words to take advantage of this limited but useful ground truth. We have tested different and well-established topic extraction methods, including LDA [57], NMF [58], SNMF [59], and PSMF [60]. The NMF (Frobenius norm) method presented the best results, so we adopted it as the default method.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "Once the user has found a set of documents (i.e., a bar) of interest in Global View, he/she can select such a set by clicking on the bar. The user is then redirected to the Paragraph Similarities View (Fig. 5b), which presents in the y-axis all documents from the selected bar, that is, documents that cite a particular BP on a given date. Each document is divided into paragraphs, indicated by the horizontal stack of bars. ", "solution_category": "interaction", "solution_axial": "Selecting;Connect/Relate;Encode", "solution_compoent": "", "axial_code": ["Selecting", "Encode", "Connect/Relate"], "componenet_code": ["selecting", "encode", "connect_relate"]}]}, {"author": "dxf", "index_original": 39, "paper_title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents", "pub_year": 2023, "domain": "Legal", "requirement": {"requirement_text": "T5: Highlight in decisions\u2019 relevant parts: The system should quickly identify the most similar paragraphs/sentences to the binding precedent. It should also highlight those parts that (either explicitly or potentially) cite that precedent.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "In total, we have 58 binding precedents, idealized as a mechanism to create a consolidated understanding among the STF\u2019s justices. We collected our dataset from a partner_x0002_ship with the Supremo em N_x0001_umeros (\u201cSTF in Numbers\u201d, in free translation project [43] \u2014 a project that seeks to assess legal and computer knowledge to produce unprecedented data on the Supreme Court. This dataset contains more than 2,500,000 documents since 1988 and metadata such as the number of the BP being cited (if that is the case) and docu_x0002_ment type. Considering only decisions that explicitly cite at least one of the 58 BPs, there are 38,364 documents, totaling 41,031 citations (some documents may mention more than one BP). The number of citations per BP does not follow a uniform distribution, as we can see in Fig. 2.", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Text Embedding. As mentioned above, we use a classifica_x0002_tion model to assess the probability of a document citing a BP. To assess different classifiers\u2019 performance, we rely on text embedding methods to generate a vector representation of the documents. We test different embeddings to evaluate their performance: TF-IDF [47], Doc2vec [44], Universal Sen_x0002_tence Encoder (USE) [10], and Longformer [48]. In TF-IDF\u2019s particular case, we have also applied a dimensionality reduction procedure to map the documents to a 50-dimen_x0002_sional space using Truncated Singular Value Decomposition (SVD) [49]. We call the final representation Truncated TF_x0002_IDF. The reason to consider Longformer, instead of a more popular language model, e.g., BERT [50], is that Longformer overcomes BERT\u2019s limitation of 512 tokens. Note that USE and Longformer do not need a pre-processed text (as described in the previous paragraph) because both have their tokenization mechanism.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Classification. Support Vector Machines (SVM) are com_x0002_plex enough for real-world classification problems and sim_x0002_ple enough to be analyzed mathematically [51]. Our study considers SVM with linear and Radial Basis Function (RBF) kernels. Moreover, given that Longformer [48] has a linear classification neural network layer plugged into it, we also fine-tuned it to assess its classification performance in our context. The described classifiers are used to search for potential citations in unlabeled data, assigning probabilities to each document to belong to each class. To get probabilities from an SVM, we need to make calibration: using labeled data, create a map from the classifier\u2019s output (SVM scores) to a probability estimate for each class, which sum up to 1. To create this map, we use the calibration method from Platt et al. [52] for SVM with linear kernel and the Wu et al.\u2019s [53] method for RBF kernel. Platt et al.\u2019s method, precisely, does not support the multiclass case, so we calibrate each class in a \u201cone-versus-rest\u201d approach and normalize the results in the end to sum up to 1. This map will receive the SVM scores from a data instance and output a probability esti_x0002_mate for each class.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "This section describes how we use the classifiers to identify potential citations and understand the models\u2019 decisions (see Fig. 3b). Citation Inference. The association between a text embed_x0002_ding technique and a classifier gives us what we call a model. A model receives the raw text and returns the classifier\u2019s probabilities, which we interpret as the document\u2019s proba_x0002_bility to cite each precedent. We consider a potential citation when the citation probability is greater or equal to a thresh_x0002_old tc 2 \u00bd0; 1_x0003_ (chosen by the user). Interpretability. A concern when dealing with machine learning models is their interpretability. For instance, if Truncated TF-IDF combined with SVM points out that a document potentially cites BP 10 because of high returned probability, how can we understand the reasons behind this decision? Generally speaking, how can we trust this model? Understanding why a model is taking a particular decision is of paramount importance [45], especially in sensitive sce_x0002_narios like a legal document analysis. Consider document X and its embedding vector x 2 Rd (e.g., TF-IDF vector). This document\u2019s probability of belong_x0002_ing to class C is given by fC\u00f0x\u00de, with fC : Rd ! \u00bd0; 1_x0003_ a mod_x0002_el\u2019s returned probability for class C. Our particular interest is to know the importance of each sentence from document X to the given probability, i.e., if each sentence has positive, negative, or neutral importance, and the magnitude of this importance. After preliminary tests have discarded the employment of the leave-one-out feature importance (LOO) [54] for interpretability due to its high sensitivity, we chose the Local Interpretable Model-agnostic Explanations (Lime) [45] method to tackle this issue. The intuition is that, by removing a spe_x0002_cific sentence and obtaining the variation of probability DfC, we can measure the importance of this sentence for the prediction fC\u00f0x\u00de. More formally, we randomly remove some sentences from document X, vectorize this new docu_x0002_ment to z, and add it to set Z. Doing this many times, we have a collection Z of vectors around x in high dimensional space. We approximate fC\u00f0z\u00de using a linear model g\u00f0z\u00de, minimizing the approximation error L\u00f0fC; g; z\u00de in z 2 Z, but also constraining the complexity This task can be interpreted as a weighted linear regres_x0002_V\u00f0g\u00de of g. sion with regularization. We end with a model g\u00f0z\u00de that is linear over the sentences (i.e., the presence or absence of a sentence), where the linear model\u2019s coefficients can be inter_x0002_preted as the importance score of each sentence to the final prediction By default, Lime works with words, and there is also the fC\u00f0x\u00de. possibility of interpreting entire paragraphs. A word divi_x0002_sion brings some advantages, such as detailing. Still, it does not immediately assess each sentence\u2019s importance to the decision (e.g., sentences similar to the BP text). A paragraph division is also interesting for visualization purposes (see, for example, Section 6.3), but it merges various sentences into one, hampering the precise identification of relevant parts. Therefore, our choice of working with sentences com_x0002_prises the \u201cbest of both worlds\u201d. In our experiments, Lime proved to be very robust and reliable when dealing with sentences.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Each document is divided into paragraphs, indicated by the horizontal stack of bars. The bar\u2019s size means the size of the paragraph, and the color intensity reflects the similarity between the corresponding paragraph and the BP text; the darker the color, the greater the similarity, and the more common parts exist between the BP and the paragraph. the color of each stacked bar stands for explicit (blue) or potential (orange) citation. Once the user has found a set of documents (i.e., a bar) of interest in Global View, he/she can select such a set by clicking on the bar. The user is then redirected to the Paragraph Similarities View (Fig. 5b), which presents in the y-axis all documents from the selected bar, that is, documents that cite a particular BP on a given date. Each document is divided into paragraphs, indicated by the horizontal stack of bars. The bar\u2019s size means the size of the paragraph, and the color intensity reflects the similarity between the corresponding paragraph and the BP text; the darker the color, the greater the similarity, and the more common parts exist between the BP and the paragraph (T5). Similarly to Global View, the color of each stacked bar stands for explicit (blue) or potential (orange) citation (T1). The similarity is given by the angular dis_x0002_tance [10] between two Truncated-TF-IDF vectors, but other embeddings and similarities (e.g., the raw cosine similarity) could be adopted. To guide users further exploring the set of documents, we group them into clusters (T4 \u2013 details below). Inside each clus_ter, the documents are positioned in descending order of similarity between the document and the BP, which is defined as the maximum similarity between its paragraphs and the BP (T4). Showing documents in descending order of similarity is helpful because the user can promptly identify the top-k most similar, and therefore most interesting, documents. Further_x0002_more, to quickly assess the document distribution w.r.t. the similarity values, we also show an interactive bar chart above each color bar. Document Clustering. Since a binding precedent may cover decisions related to different subjects, the Brazilian Supreme Court website provides, for each BP, some clusters of decisions created according to their subjects. There are only a few clusters for each BP, each containing a few labeled documents (for instance, there are 8 clusters for BP 4, each containing two documents on average). We employed NLP text pre-processing and applied topic modeling strategies to cluster documents according to their relevant words to take advantage of this limited but useful ground truth. We have tested different and well-established topic extraction methods, including LDA [57], NMF [58], SNMF [59], and PSMF [60]. The NMF (Frobenius norm) method presented the best results, so we adopted it as the default method.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "Once the user has found a set of documents (i.e., a bar) of interest in Global View, he/she can select such a set by clicking on the bar. The user is then redirected to the Paragraph Similarities View (Fig. 5b), which presents in the y-axis all documents from the selected bar, that is, documents that cite a particular BP on a given date. Each document is divided into paragraphs, indicated by the horizontal stack of bars. ", "solution_category": "interaction", "solution_axial": "Selecting;Connect/Relate;Encode", "solution_compoent": "", "axial_code": ["Selecting", "Encode", "Connect/Relate"], "componenet_code": ["selecting", "encode", "connect_relate"]}]}, {"author": "dxf", "index_original": 40, "paper_title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents", "pub_year": 2023, "domain": "Legal", "requirement": {"requirement_text": "T5: Highlight in decisions\u2019 relevant parts: The system should quickly identify the most similar paragraphs/sentences to the binding precedent. It should also highlight those parts that (either explicitly or potentially) cite that precedent.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "In total, we have 58 binding precedents, idealized as a mechanism to create a consolidated understanding among the STF\u2019s justices. We collected our dataset from a partner_x0002_ship with the Supremo em N_x0001_umeros (\u201cSTF in Numbers\u201d, in free translation project [43] \u2014 a project that seeks to assess legal and computer knowledge to produce unprecedented data on the Supreme Court. This dataset contains more than 2,500,000 documents since 1988 and metadata such as the number of the BP being cited (if that is the case) and docu_x0002_ment type. Considering only decisions that explicitly cite at least one of the 58 BPs, there are 38,364 documents, totaling 41,031 citations (some documents may mention more than one BP). The number of citations per BP does not follow a uniform distribution, as we can see in Fig. 2.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "When a paragraph is hovered over, its content and the BP text are shown as illustrated in Fig. 6, with the common parts highlighted or not depending on the user\u2019s needs.Document Reader also incorporates Lime when the opened document refers to a potential citation. Comparison between a selected paragraph and the binding precedent. By hovering over the paragraphs of a selected document in Doc_x0002_ument Reader, users can compare their contents with the binding precedent being cited. Optionally, common parts are highlighted to help in the analysis (yellow).", "solution_category": "interaction", "solution_axial": "Selecting;Abstract/Elaborate;Connect/Relate", "solution_compoent": "", "axial_code": ["Selecting", "Abstract/Elaborate", "Connect/Relate"], "componenet_code": ["selecting", "abstract_elaborate", "connect_relate"]}]}, {"author": "dxf", "index_original": 42, "paper_title": "SD2: Slicing and Dicing Scholarly Data for Interactive Evaluation of Academic Performance", "pub_year": 2023, "domain": "Scholarly", "requirement": {"requirement_text": "R0: Hierarchical Exploration With Visual Scent Cues. The tool should provide information on multiple levels of detail, allowing users to drill down to the desired publication and citation information gradually. At each level, additional information should be provided as visual cues to guide further navigation, as inspired by scented widgets. The specific tasks to be performed at each level (coarsest, intermediate and finest) and the visual scents to be presented will be explained in further detail in the later requirements.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Our data come from Microsoft Academic Graph (MAG) and Google Scholar (GS). MAG is a knowledge graph updated weekly. The version we use contains more than 164 million papers and one billion citation relationships. Each paper has its ID, title, keywords, abstract, authors and their institutions, venue name, topics, and fields of study. Each citation relationship is a pair of the reference paper ID and the citing paper ID. However, although MAG contains rich information, it provides limited power to distinguish researchers of the same name. In contrast, GS also maintains comprehensive citation records but suffers less from author name ambiguity as many authors retain their own paper lists. However, GS does not make its data open, which makes it impractical for large-scale analysis. In our experi_x0002_ment, we mostly rely on MAG for the publication and cita_x0002_tion information. We convert all data in MAG into a SQL database for efficient retrieval. We only use GS to mitigate name ambiguity. Author Name Disambiguation. We rely on GS to identify the list of papers authored by a specific researcher. Although GS is not entirely error-free, it is by far one of the most accurate data sources for this purpose. For each researcher to be examined, we crawl his/her paper list from GS. Then, we query the SQL database to obtain the information of each paper (including the papers citing this work and their infor_x0002_mation) from MAG. This avoids frequent queries to GS, which is prohibited by Google, but still connects the authors to their papers with reasonable accuracy.", "data_code": {"tables": 1, "textual": 1, "temporal": 1}}, "solution": [{"solution_text": "the interface of SD2 consists of three tightly-coupled views, each of which corresponds to one step in the interaction model (R0). Namely, the three views are: (a) the scholar view (\u201csearch\u201d), (b) the publication view (\u201cshow context\u201d), and (c) the hierarchical histogram view (\u201cexpand on demand\u201d). Typically, users will start by adding scholars of interest in the scholar view (R1), as shown in Fig. 1a. They may further specify set operations to combine the papers of the existing scholars (R2) so that the desired relationships can be studied. Then, in the publication view, users can see how the number of papers changes over time for each paper set, as shown in Fig. 1b. Finally, users can add the paper sets of interest to the hierarchical histogram view for detail exploration, as shown in Fig. 1c. They can customize the hierarchical histogram by specifying attributes at each level to reveal the information of their interest (R3). Users can add two paper sets for detailed comparison as well (R4).", "solution_category": "interaction", "solution_axial": "Selecting;Reconfigure;Filtering;OverviewandExplore;Extractionoffeatures", "solution_compoent": "", "axial_code": ["Selecting", "Reconfigure", "OverviewandExplore", "Extractionoffeatures", "Filtering"], "componenet_code": ["selecting", "reconfigure", "overview_and_explore", "extraction_of_features", "filtering"]}]}, {"author": "dxf", "index_original": 43, "paper_title": "SD2: Slicing and Dicing Scholarly Data for Interactive Evaluation of Academic Performance", "pub_year": 2023, "domain": "Scholarly", "requirement": {"requirement_text": "R1: Specifying Researchers of Interest and Generating Overview. At the coarsest level, users should be able to specify researchers for investigation, and the tool should generate an overview of their papers and collaboration record. The overview should provide the high-level information of a researcher, e.g., who are the most frequent collaborators of the researcher, and how many papers does the researcher publish with each collaborator, respectively? The overview will assist users in selecting additional researchers or specify how the information of existing researchers should be combined for further investigation.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Our data come from Microsoft Academic Graph (MAG) and Google Scholar (GS). MAG is a knowledge graph updated weekly. The version we use contains more than 164 million papers and one billion citation relationships. Each paper has its ID, title, keywords, abstract, authors and their institutions, venue name, topics, and fields of study. Each citation relationship is a pair of the reference paper ID and the citing paper ID. However, although MAG contains rich information, it provides limited power to distinguish researchers of the same name. In contrast, GS also maintains comprehensive citation records but suffers less from author name ambiguity as many authors retain their own paper lists. However, GS does not make its data open, which makes it impractical for large-scale analysis. In our experi_x0002_ment, we mostly rely on MAG for the publication and cita_x0002_tion information. We convert all data in MAG into a SQL database for efficient retrieval. We only use GS to mitigate name ambiguity. Author Name Disambiguation. We rely on GS to identify the list of papers authored by a specific researcher. Although GS is not entirely error-free, it is by far one of the most accurate data sources for this purpose. For each researcher to be examined, we crawl his/her paper list from GS. Then, we query the SQL database to obtain the information of each paper (including the papers citing this work and their infor_x0002_mation) from MAG. This avoids frequent queries to GS, which is prohibited by Google, but still connects the authors to their papers with reasonable accuracy.", "data_code": {"tables": 1, "textual": 1, "temporal": 1}}, "solution": [{"solution_text": "The scholar view is used to select scholars as paper sets and specify the set operations to \u201ccombine\u201d these paper sets. The design of this view is inspired by UpSet [17]. The upper panel of this view shows the scholars under exploration (left) and the co-authors of the scholar of focus (right), as shown in Fig. 1a. The ordered list of co-authors is obtained from the author\u2019s Google Scholar profile. The bar charts show the number of papers authored by each scholar. For each of the co-authors, we further display an orange bar to indicate the number of co-authored papers with the scholar of focus. For the example given in Fig. 1a, the orange bars indicate that more than half of Pascal Vincent\u2019s papers are co-authored with Yoshua Bengio, and Yoshua Bengio also collaborates with Aaron Courville frequently. Users can click on any scholar under exploration to switch the focus, so that his/her co-authors will be displayed on the right. Users can click on any co-author to add them to the selected list as well. To add a paper set for further examination, users can com_x0002_bine the scholars using set operations provided in the lower panel, as shown in Fig. 1a. This panel supports four opera_x0002_tors for each scholar: namely, \u201cnot\u201d, \u201cignore\u201d \u201cand\u201d, and \u201cor\u201d. The design is similar to UpSet but with an additional \u201cor\u201d operator to allow the union of scholars, which is essential for studying the combined research output. The default operator for a scholar is \u201cignore\u201d, meaning that the scholar is irrelevant to the paper set being generated. Users can specify any other operator to get a scholar involved. A textual description of the resulting paper set will be updated whenever an operator is changed. ", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "Users can click on any scholar under exploration to switch the focus, so that his/her co-authors will be displayed on the right. Users can click on any co-author to add them to the selected list as well. To add a paper set for further examination, users can com_x0002_bine the scholars using set operations provided in the lower panel, as shown in Fig. 1a. This panel supports four opera_x0002_tors for each scholar: namely, \u201cnot\u201d, \u201cignore\u201d \u201cand\u201d, and \u201cor\u201d. The design is similar to UpSet but with an additional \u201cor\u201d operator to allow the union of scholars, which is essential for studying the combined research output. The default operator for a scholar is \u201cignore\u201d, meaning that the scholar is irrelevant to the paper set being generated. Users can specify any other operator to get a scholar involved. A textual description of the resulting paper set will be updated whenever an operator is changed. ", "solution_category": "interaction", "solution_axial": "Selecting;Filtering", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "dxf", "index_original": 44, "paper_title": "SD2: Slicing and Dicing Scholarly Data for Interactive Evaluation of Academic Performance", "pub_year": 2023, "domain": "Scholarly", "requirement": {"requirement_text": "R2: Combining of Record From Multiple Researchers and Revealing Relationship. At the intermediate level, users should be able to combine the record of existing researchers to reveal the desired relationships. For example, consider two researchers, A and B. Users should be able to generate the record with both A and B to examine their collaboration, the record with A but without B to evaluate the independence of A, and the record with either A or B to investigate their combined impact. Similar rules should be allowed to combine more researchers. At this level, the tool should provide temporal patterns of the combined record (e.g., the paper numbers over the years) for users to verify that the combination is indeed meaningful.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Our data come from Microsoft Academic Graph (MAG) and Google Scholar (GS). MAG is a knowledge graph updated weekly. The version we use contains more than 164 million papers and one billion citation relationships. Each paper has its ID, title, keywords, abstract, authors and their institutions, venue name, topics, and fields of study. Each citation relationship is a pair of the reference paper ID and the citing paper ID. However, although MAG contains rich information, it provides limited power to distinguish researchers of the same name. In contrast, GS also maintains comprehensive citation records but suffers less from author name ambiguity as many authors retain their own paper lists. However, GS does not make its data open, which makes it impractical for large-scale analysis. In our experi_x0002_ment, we mostly rely on MAG for the publication and cita_x0002_tion information. We convert all data in MAG into a SQL database for efficient retrieval. We only use GS to mitigate name ambiguity. Author Name Disambiguation. We rely on GS to identify the list of papers authored by a specific researcher. Although GS is not entirely error-free, it is by far one of the most accurate data sources for this purpose. For each researcher to be examined, we crawl his/her paper list from GS. Then, we query the SQL database to obtain the information of each paper (including the papers citing this work and their infor_x0002_mation) from MAG. This avoids frequent queries to GS, which is prohibited by Google, but still connects the authors to their papers with reasonable accuracy.", "data_code": {"sequential": 1, "textual": 1, "tables": 1, "temporal": 1}}, "solution": [{"solution_text": "The scholar view is used to select scholars as paper sets and specify the set operations to \u201ccombine\u201d these paper sets. The design of this view is inspired by UpSet [17]. The upper panel of this view shows the scholars under exploration (left) and the co-authors of the scholar of focus (right), as shown in Fig. 1a. The ordered list of co-authors is obtained from the author\u2019s Google Scholar profile. The bar charts show the number of papers authored by each scholar. For each of the co-authors, we further display an orange bar to indicate the number of co-authored papers with the scholar of focus. For the example given in Fig. 1a, the orange bars indicate that more than half of Pascal Vincent\u2019s papers are co-authored with Yoshua Bengio, and Yoshua Bengio also collaborates with Aaron Courville frequently. Users can click on any scholar under exploration to switch the focus, so that his/her co-authors will be displayed on the right. Users can click on any co-author to add them to the selected list as well. To add a paper set for further examination, users can com_x0002_bine the scholars using set operations provided in the lower panel, as shown in Fig. 1a. This panel supports four opera_x0002_tors for each scholar: namely, \u201cnot\u201d, \u201cignore\u201d \u201cand\u201d, and \u201cor\u201d. The design is similar to UpSet but with an additional \u201cor\u201d operator to allow the union of scholars, which is essential for studying the combined research output. The default operator for a scholar is \u201cignore\u201d, meaning that the scholar is irrelevant to the paper set being generated. Users can specify any other operator to get a scholar involved. A textual description of the resulting paper set will be updated whenever an operator is changed. ", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "Users can click on any scholar under exploration to switch the focus, so that his/her co-authors will be displayed on the right. Users can click on any co-author to add them to the selected list as well. To add a paper set for further examination, users can com_x0002_bine the scholars using set operations provided in the lower panel, as shown in Fig. 1a. This panel supports four opera_x0002_tors for each scholar: namely, \u201cnot\u201d, \u201cignore\u201d \u201cand\u201d, and \u201cor\u201d. The design is similar to UpSet but with an additional \u201cor\u201d operator to allow the union of scholars, which is essential for studying the combined research output. The default operator for a scholar is \u201cignore\u201d, meaning that the scholar is irrelevant to the paper set being generated. Users can specify any other operator to get a scholar involved. A textual description of the resulting paper set will be updated whenever an operator is changed. ", "solution_category": "interaction", "solution_axial": "Selecting;Filtering", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "dxf", "index_original": 45, "paper_title": "SD2: Slicing and Dicing Scholarly Data for Interactive Evaluation of Academic Performance", "pub_year": 2023, "domain": "Scholarly", "requirement": {"requirement_text": "R3: Partitioning the Information in Multiple Ways and Showing the Details. At the finest level, users should be able to slice and dice the information so that different aspects of the publication and citation record can be revealed and studied. For example, when a collection of papers is first partitioned by the topics and then by the publication years, we may find out how the research interest of the corresponding researchers changes over time. When a collection of citations is first partitioned by the topics of citing papers and then by their citation years, we may find out how the corresponding researchers\u2019influence on different topics evolves. When a collection of citations is first partitioned by the venue ranks of reference and then by the ranks of citing papers, we may verify whether the papers published at better venues receive more citations from top venues, etc. Leveraging the rich information associated with the papers, we should be able to answer questions from diverse aspects.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Our data come from Microsoft Academic Graph (MAG) and Google Scholar (GS). MAG is a knowledge graph updated weekly. The version we use contains more than 164 million papers and one billion citation relationships. Each paper has its ID, title, keywords, abstract, authors and their institutions, venue name, topics, and fields of study. Each citation relationship is a pair of the reference paper ID and the citing paper ID. However, although MAG contains rich information, it provides limited power to distinguish researchers of the same name. In contrast, GS also maintains comprehensive citation records but suffers less from author name ambiguity as many authors retain their own paper lists. However, GS does not make its data open, which makes it impractical for large-scale analysis. In our experi_x0002_ment, we mostly rely on MAG for the publication and cita_x0002_tion information. We convert all data in MAG into a SQL database for efficient retrieval. We only use GS to mitigate name ambiguity. Author Name Disambiguation. We rely on GS to identify the list of papers authored by a specific researcher. Although GS is not entirely error-free, it is by far one of the most accurate data sources for this purpose. For each researcher to be examined, we crawl his/her paper list from GS. Then, we query the SQL database to obtain the information of each paper (including the papers citing this work and their infor_x0002_mation) from MAG. This avoids frequent queries to GS, which is prohibited by Google, but still connects the authors to their papers with reasonable accuracy.", "data_code": {"tables": 1, "textual": 1, "temporal": 1}}, "solution": [{"solution_text": "Venue Classification. For publication/citation venues, we follow the list of computer science conferences and journals recommended by China Computer Federation (CCF) [4]. The CCF recommendation list classifies 571 major venues into ten categories: system and architecture, networks, secu_x0002_rity, database and mining, software engineering, theory, computer graphics, artificial intelligence, human-computer interaction, and interdisciplinary. For each category, the venues are further divided into three ranks from A to C with rank A venues being the most prestigious ones. During exploration, we use the categories and ranks as additional attributes to group the venues.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The hierarchical histogram view allows users to break down a set of papers or citations into a hierarchy of bars to answer specific questions. Without loss of generality, we describe our approach using paper sets. The interface of the hierarchical histogram view is shown in Fig. 1c. Up to two paper sets can be added in this view as diverging bar charts for efficient comparison: one will be mapped to the upper histogram and the other to the lower histogram. Unlike the tradi_x0002_tional bar charts, which list all bars of a histogram at a single level, the hierarchical histogram features a multilevel design: several levels of horizontal bars at the intermediate lev_x0002_els of the hierarchy where the width of each bar indicates the corresponding number of leaf nodes, followed by one level of vertical bars at the finest (i.e., leaf) level of the hierarchy where the height of each bar can indicate the number of papers, the total number of citations received by the papers, or the h-index of the papers.", "solution_category": "visualization", "solution_axial": "mirror", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "Bar Grouping. We provide the bar grouping feature, which allows users to manually group the bars to further reduce visual complexity. Fig. 1c illustrates such an example. Users can brush the histogram of the \u201cP. Year\u201d attribute to create periods of years. In the resulting hierarchical histogram, each period, instead of each year, forms a bar. Once a bar group is formed, users can also remove it (acting as a filter). For example, users can update the \u201cP. CCF Rank\u201d attribute to form bar groups and remove the one containing papers that are not in CCF rank A, leaving only CCF rank A papers to be shown and explored in the hierarchical histogram. To remove a group, users can either rename the group as \u201cignore\u201d or simply click the button with a minus sign at the upper left corner of the group. When both the attribute lock and alignment are disabled, the two mini-maps can be used independently to scale and scroll the corresponding histograms. When the attribute lock is enabled, the bars in both histograms carry similar mean_x0002_ing. Therefore, we link the scaling operation of both histo_x0002_grams to enforce the same width for all the bars at the top level. When the alignment is enabled, we further link the scrolling of the two histograms, meaning that scrolling one histogram will move the other histogram simultaneously", "solution_category": "interaction", "solution_axial": "Filtering;Participation/Collaboration", "solution_compoent": "", "axial_code": ["Filtering", "Participation/Collaboration"], "componenet_code": ["filtering", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 46, "paper_title": "SD2: Slicing and Dicing Scholarly Data for Interactive Evaluation of Academic Performance", "pub_year": 2023, "domain": "Scholarly", "requirement": {"requirement_text": "R4: Aligning Partitioned Data for Comparison. The comparison between two collections of papers should be performed at the finest level using the partitioned information. Individual comparison is supported when each collection is produced from a single researcher, and group comparison is enabled when each collection corresponds to a group of researchers. The visual representation should allow the information to be aligned so that the corresponding parts can be easily compared. Both automatic and manual alignment should be provided. Automatic alignment should coordinate the same items in the two collections, while manual alignment should allow users to customize the alignment to match items carrying similar meanings. For example, to address the difference between different career stages, users should be able to dynamically align the time axes of two researchers.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Our data come from Microsoft Academic Graph (MAG) and Google Scholar (GS). MAG is a knowledge graph updated weekly. The version we use contains more than 164 million papers and one billion citation relationships. Each paper has its ID, title, keywords, abstract, authors and their institutions, venue name, topics, and fields of study. Each citation relationship is a pair of the reference paper ID and the citing paper ID. However, although MAG contains rich information, it provides limited power to distinguish researchers of the same name. In contrast, GS also maintains comprehensive citation records but suffers less from author name ambiguity as many authors retain their own paper lists. However, GS does not make its data open, which makes it impractical for large-scale analysis. In our experi_x0002_ment, we mostly rely on MAG for the publication and cita_x0002_tion information. We convert all data in MAG into a SQL database for efficient retrieval. We only use GS to mitigate name ambiguity. Author Name Disambiguation. We rely on GS to identify the list of papers authored by a specific researcher. Although GS is not entirely error-free, it is by far one of the most accurate data sources for this purpose. For each researcher to be examined, we crawl his/her paper list from GS. Then, we query the SQL database to obtain the information of each paper (including the papers citing this work and their infor_x0002_mation) from MAG. This avoids frequent queries to GS, which is prohibited by Google, but still connects the authors to their papers with reasonable accuracy.", "data_code": {"tables": 1, "textual": 1, "temporal": 1}}, "solution": [{"solution_text": "Venue Classification. For publication/citation venues, we follow the list of computer science conferences and journals recommended by China Computer Federation (CCF) [4]. The CCF recommendation list classifies 571 major venues into ten categories: system and architecture, networks, secu_x0002_rity, database and mining, software engineering, theory, computer graphics, artificial intelligence, human-computer interaction, and interdisciplinary. For each category, the venues are further divided into three ranks from A to C with rank A venues being the most prestigious ones. During exploration, we use the categories and ranks as additional attributes to group the venues.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The hierarchical histogram view allows users to break down a set of papers or citations into a hierarchy of bars to answer specific questions. Without loss of generality, we describe our approach using paper sets. The interface of the hierarchical histogram view is shown in Fig. 1c. Up to two paper sets can be added in this view as diverging bar charts for efficient comparison: one will be mapped to the upper histogram and the other to the lower histogram. Unlike the tradi_x0002_tional bar charts, which list all bars of a histogram at a single level, the hierarchical histogram features a multilevel design: several levels of horizontal bars at the intermediate lev_x0002_els of the hierarchy where the width of each bar indicates the corresponding number of leaf nodes, followed by one level of vertical bars at the finest (i.e., leaf) level of the hierarchy where the height of each bar can indicate the number of papers, the total number of citations received by the papers, or the h-index of the papers.", "solution_category": "visualization", "solution_axial": "mirror", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "Bar Grouping. We provide the bar grouping feature, which allows users to manually group the bars to further reduce visual complexity. Fig. 1c illustrates such an example. Users can brush the histogram of the \u201cP. Year\u201d attribute to create periods of years. In the resulting hierarchical histogram, each period, instead of each year, forms a bar. Once a bar group is formed, users can also remove it (acting as a filter). For example, users can update the \u201cP. CCF Rank\u201d attribute to form bar groups and remove the one containing papers that are not in CCF rank A, leaving only CCF rank A papers to be shown and explored in the hierarchical histogram. To remove a group, users can either rename the group as \u201cignore\u201d or simply click the button with a minus sign at the upper left corner of the group. When both the attribute lock and alignment are disabled, the two mini-maps can be used independently to scale and scroll the corresponding histograms. When the attribute lock is enabled, the bars in both histograms carry similar mean_x0002_ing. Therefore, we link the scaling operation of both histo_x0002_grams to enforce the same width for all the bars at the top level. When the alignment is enabled, we further link the scrolling of the two histograms, meaning that scrolling one histogram will move the other histogram simultaneously", "solution_category": "interaction", "solution_axial": "Filtering;Participation/Collaboration", "solution_compoent": "", "axial_code": ["Filtering", "Participation/Collaboration"], "componenet_code": ["filtering", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 49, "paper_title": "EpiMob: Interactive Visual Analytics of Citywide Human Mobility Restrictions for Epidemic Control", "pub_year": 2023, "domain": "Human mobility", "requirement": {"requirement_text": "E1: A Set of Spatial Views Assists Policy Exploration. To solve P2, a special brainstorming session was held to collect reference information on developing regional mobility con_x0002_trol policies, which are summarized as follows: (i) infection hotspots. An infection hotspot means that many people are infected in the area relative to other areas. The ability to efficiently locate these areas is beneficial for epidemic control. To map the citywide infection hotspots, the user must first acquire the infection locations. However, determining the accurate locations of real infected individuals during an outbreak is difficult. Thus, running the simulation to trace and collect infection locations becomes a common strategy. The experts\u2019 current solution for infection hotspot visualiza_x0002_tion included the heatmap and scatter map, which are sim_x0002_ple plots without informative interactive design and lacking in-depth exploration. The experts requested us to design a hotspot view that integrates the interaction function and allows for in-depth analysis. (ii) workplaces visited with high_x0002_frequency. Many workplaces have emerged as clusters of infection during COVID-19. Identifying workplaces with high-frequency visitation is important in deciding where to enforce remote working. The approach of the experts in rep_x0002_resenting workplace distribution was limited to the choro_x0002_pleth map at the municipality level. For finer granularity, data-driven workplace estimation is necessary while they do not have related background. Therefore, they wanted us to provide a view displaying the spatial density distribution of people\u2019s workplaces. (iii) screening point exploration. The government often set up screening points in certain areas during COVID-19. In practice, the experts identified these areas by plotting a scatter plot of the POI distributions. Then the locations where certain types of POIs are denser were selected. Specifically, these POIs are either at high risk of exposure, such as entertainment places (e.g., bars, kar_x0002_aokes) and restaurants, or have high human traffic such as stations, shopping malls, and public spaces (e.g., parks, zoos, and attractions). The experts suggested integrating this reference information into the system.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In this work, the data in Japan over a three-year period (from August 1, 2010, to July 31, 2013) was employed. It con_x0002_tains approximately 30 billion GPS records from about 1.6 million mobile phone users, covering around 1% of the real_x0002_world population. Furthermore, the Greater Tokyo Area (including Tokyo Metropolis and the prefectures of Kana_x0002_gawa, Chiba, and Saitama) was selected as the target area for research. The user is picked to the experimental data if 80% of its trajectory points are located in the target area. As a result, 145,507 user trajectories were obtained.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "To satisfy requirement S2, a two-stage simulation mecha_x0002_nism was devised: Stage 1-Restricted Mobility Generation. Given a mobility restriction policy or a combination of several policies F, citywide human mobility forcibly changes owing to the given F. In this study, a mobility replacement model denoted as F MOB was utilized to generate the restricted human mobility G0 w.r.t F as follows: G0 \u00bc F MOB\u00f0G ; F\u00de (6) Stage 2-Epidemic Simulation with Restricted Mobility. Given the restricted citywide human mobility G0 w.r.t F and dis_x0002_ease transmission parameters Q0 , simulation of the epidemic for the restriction policy settings E0 sim can be implemented as follows: E0 sim \u00bc F EPI \u00f0G0 ; Q0 \u00de (7) This reflects the extensibility of the simulation mechanism. Developers can design new restriction strategies at Stage 1 and use other epidemic models at Stage 2.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "An iterative simulation process called \u201ccontagion ; move_x0002_ment\u201d is proposed (Fig. 2). In the contagion stage, it is assumed that each person has a certain probability of being infected by others inside the same grid at the same time slot. In the movement stage, the user movements update the user lists of grids and introduce the disease to other grids. By iterating the process over the target time range, each user\u2019s state change is traced and the infection events are col_x0002_lected as simulation results. During the simulation, once a user\u2019s health state changes from susceptible to exposed, the corresponding infection time and location are recorded as an infection event. To start the iterative simulation process, I0 initially infected individuals was selected by random sampling, and hence, their user state was updated to \u201cinfected.\u201d By default, I0 was set to 10 in this study. Unlike the classical SEIR model, the proposed model exhib_x0002_its randomness. Given \u00f0G; Q\u00de, the simulation result Esim is unstable with respect to the total number of infection events and their occurrence times and locations. Therefore, multiple simulations are necessary. The number of repetitions m was set to 100 in this study. In addition, parallel computing was applied for the repetition simulations to ensure computational efficiency.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "When a user is affected by a mobility restriction policy, the mobility behavior is influenced accordingly. For example, a user frequents a shopping mall every weekend. The mall is locked on a particular day, and the user\u2019s future trajectory will not cover it. The affected trajectory is replaced by an unaffected trajectory, which is referred to as the trajectory replacement strategy. Here, the replacement methodology is elaborated for each restriction policy. Telecommuting. To implement telecommuting, first mobile phone users\u2019 homes and workplaces are extracted from the raw trajectory. After that, the administrative districts of the workplaces are identified to support the policy at the admin_x0002_istrative district level. Given a group of telecommuting dis_x0002_tricts and a corresponding time range, users whose workplaces are in the target districts are initially determined. Then, each affected user\u2019s status in the time range is deter_x0002_mined on a day-by-day basis, that is, whether they go to work. For the days on which a user goes to work, the trajec_x0002_tory of that day is replaced with the home address (i.e., make a stay at home and work remotely). For the days on which the user does not go to work, no changes are made. Regional Lockdown. Given a group of lockdown regions and time ranges, the regions are first mapped to a set of mesh grids covering them. Then, the affected users are divided into two categories. For users who stayed in the restricted area at the beginning of the lockdown period, their location remains unchanged throughout this period. For users who visited the restricted area during the lock_x0002_down, the affected trajectories are replaced with the unaf_x0002_fected trajectories. Specifically, a historical trajectory database is built for the users. For the affected day, the user\u2019s one-day trajectory that did not cross the restricted area is randomly selected from the historical database as the replacement. Screening. In our system, users are allowed to set temper_x0002_ature screening points at the grid level. Contrary to the other two mobility restriction policies, the selected grids are\nscreened during the epidemic simulation stage. Specifically, given a set of screening points and time ranges, all people in the grid are detected at each timestamp. If a person is healthy or in an incubation/latent period, it is assumed that the probability of an abnormal temperature is 0. If a person is infected, the probability of abnormal temperature is 87.9%, which is set according to the latest research on COVID-19[44]. Once an infected person is detected, he/she is quarantined (i.e., subsequent trajectories are discontin_x0002_ued) to prevent infecting others. Without loss of generality, EpiMob can also support other screening types (e.g., the nucleic acid test to detect latent patients).", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "In this section, three interactive views of the restriction settings are introduced for E1 and E2. The view of each setting includes a spatial view displayed on a map (E1) and a set_x0002_ting panel (E2). The switches located in the upper-right corner of each panel (Figs. 1 A1, A2, and A3) set the visibility of corresponding spatial views, thus users can obtain sufficient and effective prior knowledge. Regional Lockdown View: To help users formulate appropriate lockdown policies, we devised the regional lockdown view, which consists of a map view showing the regional infection severity (Fig. 4) and a setting panel (Fig. 1 A1) allowing the corresponding parameters to be specified. Here, a definition of regional nfection severity is provided\u2014Given a region R and a simulation result Esim, the infection severity of R indicates the accumulated number of infection events that occurred. The regional infection severity was defined as a reference indicator to determine the hotspots. To visualize the regional infection severity, specifying the simulation result Esim is required. A select box is placed\u2014\u201cBind with\u201d in the setting panel (Fig. 1 A1), where all existing simulation results are listed to allow users to choose. When using the regional lockdown view for the first time, it is recommended to perform a no policy simulation and then bind it to explore the infection hotspots under no intervention. Telecommuting View: To help determine remote work policies, a telecommuting view was designed, including a spatial view to identify workplaces that are frequented more regularly, along with a setting panel to finish the configuration of the concrete policy. Fig. 5 C shows the spatial view that displays all users\u2019workplaces with a heatmap. The darker the color (i.e., red in this case), the more people work there. Because workplaces tend to have a high degree of overlap, for example, many people working in a single building, a heatmap was selected to depict the distribution of workplaces. Compared to scatter plots (Fig. 5 A&B), the heatmap depicts the spatial accumulation of points more effectively. Moreover, when the mouse hovers over the heatmap, the name of the administrative dis_x0002_trict of the area is displayed. With this information, users can obtain the names of the regions to implement telecommuting. Fig. 1 A3 shows the panel for setting the telecommuting view. Users can set a series of regions to execute telecommuting here. This panel enables the \u201creduction rate\u201d of regions to be specified, i.e., the percentage of people working from home in the target region, to control the intensity of execution. For example, \u201cRegion Toshima Reduction 70%\u201d means that 70% of people working in Toshima are working from home. Moreover, according to regional conditions, the strength of the policy execution can also be quantified by set_x0002_ting the start date and duration.  Screening View: A superimposable scatter plot is integrated to display POI information (Fig. 6). Users can select one or several types of POIs simultaneously to explore potential screening points. To achieve E2-iii, the user is allowed to set up while explor_x0002_ing. When the user finds a target region, they can draw selection areas directly on the map or drag markers to the locations of interest, as shown in the screening control panel(Fig. 1 A2). After successfully adding a screening point, a mark is generated on the selected grid, indicating that screening will be performed. The time range for policy implementation can also be set. In the subsequent simula_x0002_tion, all people passing the screening points during the implementation of the policy are screened.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Map;Circle", "axial_code": [], "componenet_code": ["map", "circle"]}, {"solution_text": "A select box is placed\u2014\u201cBind with\u201d in the setting panel (Fig. 1 A1), where all existing simulation results are listed to allow users to choose. Moreover, when the mouse hovers over the heatmap, the name of the administrative dis_x0002_trict of the area is displayed. Users can select one or several types of POIs simultaneously to explore potential screening points. To achieve E2-iii, the user is allowed to set up while explor_x0002_ing. When the user finds a target region, they can draw selection areas directly on the map or drag markers to the locations of interest, as shown in the screening control panel(Fig. 1 A2). After successfully adding a screening point, a mark is generated on the selected grid, indicating that screening will be performed. The time range for policy implementation can also be set. In the subsequent simula_x0002_tion, all people passing the screening points during the implementation of the policy are screened.", "solution_category": "interaction", "solution_axial": "Selecting;Filtering;Participation/Collaboration", "solution_compoent": "", "axial_code": ["Selecting", "Filtering", "Participation/Collaboration"], "componenet_code": ["selecting", "filtering", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 53, "paper_title": "EpiMob: Interactive Visual Analytics of Citywide Human Mobility Restrictions for Epidemic Control", "pub_year": 2023, "domain": "Human mobility", "requirement": {"requirement_text": "L1: Display of Single Control Policy Result. Basic requirements: plotting the infection curve is a basic operation to present the results. Meanwhile, because contagion simulations generally have randomness, confidence intervals should be considered. (ii) Distinguishability: There may be sets of simulations with only slight setting differences (e.g., two lockdown policies intersect to a large extent at the target regions). (iii) In-depth analysis of results: \u201cWhat are the secondary effects of implementing the policy? e.g., the new infection hotspots.\u201d The infection hotspot view can be equipped to directly perceive the transition of hotspots. \u201cWhat roles do the regions perform in the spread? Are there any striking patterns of transmission?\u201c, a network view is required to observe and explore the spatial propagation feature.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "In this work, the data in Japan over a three-year period (from August 1, 2010, to July 31, 2013) was employed. It con_x0002_tains approximately 30 billion GPS records from about 1.6 million mobile phone users, covering around 1% of the real_x0002_world population. Furthermore, the Greater Tokyo Area (including Tokyo Metropolis and the prefectures of Kana_x0002_gawa, Chiba, and Saitama) was selected as the target area for research. The user is picked to the experimental data if 80% of its trajectory points are located in the target area. As a result, 145,507 user trajectories were obtained.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "To satisfy requirement S2, a two-stage simulation mecha_x0002_nism was devised: Stage 1-Restricted Mobility Generation. Given a mobility restriction policy or a combination of several policies F, citywide human mobility forcibly changes owing to the given F. In this study, a mobility replacement model denoted as F MOB was utilized to generate the restricted human mobility G0 w.r.t F as follows: G0 \u00bc F MOB\u00f0G ; F\u00de (6) Stage 2-Epidemic Simulation with Restricted Mobility. Given the restricted citywide human mobility G0 w.r.t F and dis_x0002_ease transmission parameters Q0 , simulation of the epidemic for the restriction policy settings E0 sim can be implemented as follows: E0 sim \u00bc F EPI \u00f0G0 ; Q0 \u00de (7) This reflects the extensibility of the simulation mechanism. Developers can design new restriction strategies at Stage 1 and use other epidemic models at Stage 2.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "An iterative simulation process called \u201ccontagion ; move_x0002_ment\u201d is proposed (Fig. 2). In the contagion stage, it is assumed that each person has a certain probability of being infected by others inside the same grid at the same time slot. In the movement stage, the user movements update the user lists of grids and introduce the disease to other grids. By iterating the process over the target time range, each user\u2019s state change is traced and the infection events are col_x0002_lected as simulation results. During the simulation, once a user\u2019s health state changes from susceptible to exposed, the corresponding infection time and location are recorded as an infection event. To start the iterative simulation process, I0 initially infected individuals was selected by random sampling, and hence, their user state was updated to \u201cinfected.\u201d By default, I0 was set to 10 in this study. Unlike the classical SEIR model, the proposed model exhib_x0002_its randomness. Given \u00f0G; Q\u00de, the simulation result Esim is unstable with respect to the total number of infection events and their occurrence times and locations. Therefore, multiple simulations are necessary. The number of repetitions m was set to 100 in this study. In addition, parallel computing was applied for the repetition simulations to ensure computational efficiency.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "This view was designed to assist users to intuitively observe the simulation results and conduct a comparative analysis(L1, L2), including two subviews: a single policy result view(L1) and a comparative analysis view (L2). Furthermore, a single policy result view consists of an Infection Curve (L1-i) and a Spatial Propagation Feature View (L1-iii). Spatial Propagation Feature View. This view (Fig. 7 B) was designed to analyze the policies\u2019 secondary effects from the perspective of spatial propagation, and to understand the roles and patterns of different regions, including a map component (B1) to display the spatial transmission network, a filter component (B2) to explore the significant cross-region propagation patterns, and a tracing panel (B3) to localize the local infection patterns of interest. To this end, the \u201cinitial infection location, secondary propagation location\u201c pairs of infected individuals were captured and treated as the subject of analysis. It is easy to find that the data conforms to origin and destination (OD) data characteristics. A spatial line chart (Fig. 7 C1) was first employed to visualize all OD pairs to display the spatial transmission network, but the clutter problem is severe. The transparency of the line (C2) was further reduced, but the problem remained. One possible solution is OD aggregation visualization, but the clutter still exists according to [46]. The final solution is presented in Fig. 7 B, which inherits from a third party library[47] designed for flow visualization. The bidirection of the edges encodes the direction of spatial diffusion. Both the width and color of the edges map the intensity of the transition. This library solves the clutter by stacking and highlighting the edges according to their transition intensity. It also supports scaling and clustering when the zoom level changes (B4). To trace the local infection patterns of interest, all infections at the nodes were counted and categorized by infection type: input propagation\u2013the infection source comes from outside the area, and internal/local propagation\u2013the infection source comes from within the area. The local infection pattern is encoded as the respec_x0002_tive proportion of the two infection types in the total infec_x0002_tion (blue and red bar) to facilitate exploration and comparison. Finally, the infection distribution bars are listed in the order of node infection count (B3). When the mouse hovers on the bar of interest, its in-out flow, and spatial coverage are highlighted on the map. In this case, the area with the highest input propagation ratio in the top 50 infection count nodes was highlighted and it was determined that the infection mainly originated in Higashi-Kanagawa (B5). To explore the significant cross-region propagation patterns, a filter panel is provided to identify the primary risk sources and output destinations of nodes (B2). The user is allowed to fil_x0002_ter based on the proportion of flow to the total input in the target node and on the proportion of flow to the total output in the source node. The flows of nodes that meet the criteria are highlighted (in yellow in this study).", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line;Map", "axial_code": [], "componenet_code": ["line", "map"]}, {"solution_text": "It also supports scaling and clustering when the zoom level changes (B4). To trace the local infection patterns of interest, all infections at the nodes were counted and categorized by infection type: input propagation\u2013the infection source comes from outside the area, and internal/local propagation\u2013the infection source comes from within the area. The local infection pattern is encoded as the respec_x0002_tive proportion of the two infection types in the total infec_x0002_tion (blue and red bar) to facilitate exploration and comparison. Finally, the infection distribution bars are listed in the order of node infection count (B3). When the mouse hovers on the bar of interest, its in-out flow, and spatial coverage are highlighted on the map. In this case, the area with the highest input propagation ratio in the top 50 infection count nodes was highlighted and it was determined that the infection mainly originated in Higashi-Kanagawa (B5). To explore the significant cross-region propagation patterns, a filter panel is provided to identify the primary risk sources and output destinations of nodes (B2). The user is allowed to fil_x0002_ter based on the proportion of flow to the total input in the target node and on the proportion of flow to the total output in the source node. The flows of nodes that meet the criteria are highlighted (in yellow in this study).", "solution_category": "interaction", "solution_axial": "Selecting;Encode;Filtering;OverviewandExplore", "solution_compoent": "", "axial_code": ["Selecting", "Encode", "Filtering", "OverviewandExplore"], "componenet_code": ["selecting", "encode", "filtering", "overview_and_explore"]}]}, {"author": "dxf", "index_original": 54, "paper_title": "EpiMob: Interactive Visual Analytics of Citywide Human Mobility Restrictions for Epidemic Control", "pub_year": 2023, "domain": "Human mobility", "requirement": {"requirement_text": "L2: Comparative Analysis of Different Control Policies.To find the best policy, it is necessary to compare the effects of different control policies based on performance criteria. An interactive logic as convenient as \u201cselect, compare, and display\u201dis required.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "In this work, the data in Japan over a three-year period (from August 1, 2010, to July 31, 2013) was employed. It con_x0002_tains approximately 30 billion GPS records from about 1.6 million mobile phone users, covering around 1% of the real_x0002_world population. Furthermore, the Greater Tokyo Area (including Tokyo Metropolis and the prefectures of Kana_x0002_gawa, Chiba, and Saitama) was selected as the target area for research. The user is picked to the experimental data if 80% of its trajectory points are located in the target area. As a result, 145,507 user trajectories were obtained.", "data_code": {"tables": 1, "textual": 1}}, "solution": [{"solution_text": "To satisfy requirement S2, a two-stage simulation mecha_x0002_nism was devised: Stage 1-Restricted Mobility Generation. Given a mobility restriction policy or a combination of several policies F, citywide human mobility forcibly changes owing to the given F. In this study, a mobility replacement model denoted as F MOB was utilized to generate the restricted human mobility G0 w.r.t F as follows: G0 \u00bc F MOB\u00f0G ; F\u00de (6) Stage 2-Epidemic Simulation with Restricted Mobility. Given the restricted citywide human mobility G0 w.r.t F and dis_x0002_ease transmission parameters Q0 , simulation of the epidemic for the restriction policy settings E0 sim can be implemented as follows: E0 sim \u00bc F EPI \u00f0G0 ; Q0 \u00de (7) This reflects the extensibility of the simulation mechanism. Developers can design new restriction strategies at Stage 1 and use other epidemic models at Stage 2.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "An iterative simulation process called \u201ccontagion ; move_x0002_ment\u201d is proposed (Fig. 2). In the contagion stage, it is assumed that each person has a certain probability of being infected by others inside the same grid at the same time slot. In the movement stage, the user movements update the user lists of grids and introduce the disease to other grids. By iterating the process over the target time range, each user\u2019s state change is traced and the infection events are col_x0002_lected as simulation results. During the simulation, once a user\u2019s health state changes from susceptible to exposed, the corresponding infection time and location are recorded as an infection event. To start the iterative simulation process, I0 initially infected individuals was selected by random sampling, and hence, their user state was updated to \u201cinfected.\u201d By default, I0 was set to 10 in this study. Unlike the classical SEIR model, the proposed model exhib_x0002_its randomness. Given \u00f0G; Q\u00de, the simulation result Esim is unstable with respect to the total number of infection events and their occurrence times and locations. Therefore, multiple simulations are necessary. The number of repetitions m was set to 100 in this study. In addition, parallel computing was applied for the repetition simulations to ensure computational efficiency.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "This view was designed to assist users to intuitively observe the simulation results and conduct a comparative analysis(L1, L2), including two subviews: a single policy result view(L1) and a comparative analysis view (L2). Furthermore, a single policy result view consists of an Infection Curve (L1-i) and a Spatial Propagation Feature View (L1-iii). Spatial Propagation Feature View. This view (Fig. 7 B) was designed to analyze the policies\u2019 secondary effects from the perspective of spatial propagation, and to understand the roles and patterns of different regions, including a map component (B1) to display the spatial transmission network, a filter component (B2) to explore the significant cross-region propagation patterns, and a tracing panel (B3) to localize the local infection patterns of interest. To this end, the \u201cinitial infection location, secondary propagation location\u201c pairs of infected individuals were captured and treated as the subject of analysis. It is easy to find that the data conforms to origin and destination (OD) data characteristics. A spatial line chart (Fig. 7 C1) was first employed to visualize all OD pairs to display the spatial transmission network, but the clutter problem is severe. The transparency of the line (C2) was further reduced, but the problem remained. One possible solution is OD aggregation visualization, but the clutter still exists according to [46]. The final solution is presented in Fig. 7 B, which inherits from a third party library[47] designed for flow visualization. The bidirection of the edges encodes the direction of spatial diffusion. Both the width and color of the edges map the intensity of the transition. This library solves the clutter by stacking and highlighting the edges according to their transition intensity. It also supports scaling and clustering when the zoom level changes (B4). To trace the local infection patterns of interest, all infections at the nodes were counted and categorized by infection type: input propagation\u2013the infection source comes from outside the area, and internal/local propagation\u2013the infection source comes from within the area. The local infection pattern is encoded as the respec_x0002_tive proportion of the two infection types in the total infec_x0002_tion (blue and red bar) to facilitate exploration and comparison. Finally, the infection distribution bars are listed in the order of node infection count (B3). When the mouse hovers on the bar of interest, its in-out flow, and spatial coverage are highlighted on the map. In this case, the area with the highest input propagation ratio in the top 50 infection count nodes was highlighted and it was determined that the infection mainly originated in Higashi-Kanagawa (B5). To explore the significant cross-region propagation patterns, a filter panel is provided to identify the primary risk sources and output destinations of nodes (B2). The user is allowed to fil_x0002_ter based on the proportion of flow to the total input in the target node and on the proportion of flow to the total output in the source node. The flows of nodes that meet the criteria are highlighted (in yellow in this study).", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line;Map", "axial_code": [], "componenet_code": ["line", "map"]}, {"solution_text": "It also supports scaling and clustering when the zoom level changes (B4). To trace the local infection patterns of interest, all infections at the nodes were counted and categorized by infection type: input propagation\u2013the infection source comes from outside the area, and internal/local propagation\u2013the infection source comes from within the area. The local infection pattern is encoded as the respec_x0002_tive proportion of the two infection types in the total infec_x0002_tion (blue and red bar) to facilitate exploration and comparison. Finally, the infection distribution bars are listed in the order of node infection count (B3). When the mouse hovers on the bar of interest, its in-out flow, and spatial coverage are highlighted on the map. In this case, the area with the highest input propagation ratio in the top 50 infection count nodes was highlighted and it was determined that the infection mainly originated in Higashi-Kanagawa (B5). To explore the significant cross-region propagation patterns, a filter panel is provided to identify the primary risk sources and output destinations of nodes (B2). The user is allowed to fil_x0002_ter based on the proportion of flow to the total input in the target node and on the proportion of flow to the total output in the source node. The flows of nodes that meet the criteria are highlighted (in yellow in this study).", "solution_category": "interaction", "solution_axial": "Selecting;Encode;Filtering;OverviewandExplore", "solution_compoent": "", "axial_code": ["Selecting", "Encode", "Filtering", "OverviewandExplore"], "componenet_code": ["selecting", "encode", "filtering", "overview_and_explore"]}]}, {"author": "dxf", "index_original": 55, "paper_title": "GestureLens: Visual Analysis of Gestures in Presentation Videos", "pub_year": 2023, "domain": "presentation", "requirement": {"requirement_text": "T1: Obtain the spatial summary of gestures. Based on the interviews, a gesture summary can help coaches quickly explore gestures and identify examples. It is useful to obtain a summary of the spatial distribution of the hand movements, so that users can know where speakers tend to put their hands and what kind of gestures a speaker may employ. Such a spatial summary can also indicate speakers\u2019gesture styles.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In this paper, we focus on those videos recording a speak_x0002_er\u2019s presentation, where the speaker stands in front of the camera. Given a presentation video, the video can be mod_x0002_eled as a series of images: V \u00bc fI1; I2; ...; Ii; ...; IN g, where Ii indicates i-th frame and N indicates the frame number in\nthe video.", "data_code": {"geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Body Keypoint Detection. To obtain the detailed gestures of a speaker, we need to detect body keypoints first. Here we adopt OpenPose [48], a widely used real-time multi-person keypoint detection library for body and hand estimation. For each frame Ii, we can detect the corresponding body keypoints, Ji \u00bc \u00bdPi0; Pi 1; ...; Pi k; ...; Pi 24_x0003_ , where Pi k \u00bc \u00bdxk; yk; ck_x0003_ indicates the coordinates, xk is x-axis value, yk is y-axis value and ck is the confidence probability. Since we mainly focus on the upper body hand gestures, we extract 9 body key_x0002_points of the upper body, which are highlighted in red as shown in Fig. 2.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Gesture Space Mapping. After detecting the body key_x0002_points, we can further obtain gesture coordinates. However, there are two major issues in analyzing gestures, i.e., coordi_x0002_nate normalization and spatial description of gestures. Coordi_x0002_nate normalization is important because it is hard to measure the gesture differences between different frames or presentation videos without a unified space. Thus, we normalize the gesture coordinates based on Keypoint 0 (the red circle annotated \u201c0\u201d in Fig. 2). To be specific, we regard Keypoint 0 as the coordinate origin. The coordi_x0002_nates of other keypoints are calculated based on Keypoint 0. Further, we normalize coordinates to \u00bd_x0004_1 _x0005_ 1_x0003_ range by using the height of the person. For the spatial description of gestures, we employ the gesture space defined in McNeill\u2019s gesture space theory [25], as shown in the blue dashed rectangles of Fig. 2. The three-level dashed rectan_x0002_gles represent the spatial regions of the center-center, center, and periphery from the inner to the outside. Specifically, the center-center region is the area directly in front of the chest; the center region is the area surrounding the center_x0002_center region, which stretches from the shoulders down to the waist and covers both sides of the body; the periphery region stretches from ears to the knees, surrounding the center-center region.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Gesture Segmentation. A speaker often uses various ges_x0002_tures for different speech content. We segment gestures according to word phrases, since we are interested in the relationships between gestures and speech content.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Gesture Distance Calculation. To find similar gestures, it is necessary to define gesture distance between different con_x0002_tinuous gestures. Since gestures may contain different frame lengths, we first define a distance function for measuring similarities between two static frames; then we utilize dynamic time warping (DTW) [49], a distance measure algorithm for time-series data with variable lengths. Given two keypoint coordinates in frames F and G, and each key_x0002_point vector is represented as Pi k \u00bc \u00bdxk; yk; ck_x0003_ , where k indi_x0002_cates k-th keypoint in F and G, xk is x-axis value, yk is y_x0002_axis value and ck is the confidence probability, the distance between gesture in two frames can be defined as [50]: D\u00f0F; G\u00de \u00bc 1 P8 k\u00bc0 Fck _x0006_ X 8 k\u00bc0 Fck _x0006_ kFxyk _x0004_ Gxyk k where Fck is the confidence probability for k-th keypoint, Fxyk and Gxyk is the coordinates of k-th keypoint of F and G, respectively.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "Gesture Type Calculation. After collecting different word phrase-based gestures, we classify gestures into three differ_x0002_ent categories, i.e., closed gestures, open gestures, and others[10]. Closed gestures refer to gestures where hands are put closely or overlapped with the torso; Open gestures refer to gesture where two hands are far away from each other and wrist points go outermost. For those gestures where hands are fall in the torso region, we named them others. More types can be calculated based on users\u2019 requirements", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Speech Content Processing. For a presentation video, the transcript can be obtained by adopting automatic transcrip_x0002_tion techniques.1 Further, to support word phrase analysis, we adopt an NLP library named textacy2 to extract semantic phrases, such as noun phrases (NP), verb phrases (VP), prep_x0002_ositional phrases (PP), and subject-verb object phrases (SVP).", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Data Alignment of Gesture Data and Transcripts. Following the aforementioned steps for body keypoints extraction, we extract keypoints frame by frame from a presentation video. Then, we can obtain the timestamp for keypoints of a frame. As for speech transcripts, the transcripts with timestamps can be obtained by adopting automatic transcription techniques (e.g., Amazon Transcribe). Therefore, the keypoints and speech transcripts are naturally aligned based on timestamps.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "To support intuitive and effective gesture-based and content-based explorations, we design an exploration view (Fig. 3B), which shows both spatial and temporal patterns of gestures, as well as corresponding content. Specifically, the exploration view contains a heatmap in the gesture space to reveal the spatial summary of gestures (T1). As shown in Fig. 3B1, to reveal spatial patterns of gestures used in presentation vid_x0002_eos, we layout a heatmap, a widely used technique for describing spatial patterns, over the gesture space. Three blue dashed rectangles are used to divide the gesture space into three areas, i.e., the center-center region, center region, and periphery region. A human stick-figure skeleton is also shown in the gesture space, which can provide some context for the gesture space and heatmap. The heatmap in the ges_x0002_ture space can clearly reveal where a speaker tends to put his/her hands during a presentation. Our coaches like this design and confirm that it is easy to understand. Further, they think the heatmap can reveal speakers\u2019 presentation styles in terms of spatial patterns.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Heatmap", "axial_code": [], "componenet_code": ["heatmap"]}]}, {"author": "dxf", "index_original": 56, "paper_title": "GestureLens: Visual Analysis of Gestures in Presentation Videos", "pub_year": 2023, "domain": "presentation", "requirement": {"requirement_text": "T2: Show the temporal evolution of gestures. Since speakers can change gestures over time, it is necessary to explore the temporal evolution of gestures regarding\nthe presentation content. A temporal summary of gestures makes users aware of how speakers move hands along time and gain deep insights into the temporal patterns of gesture usage. Besides, users can know how often to use certain gestures.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In this paper, we focus on those videos recording a speak_x0002_er\u2019s presentation, where the speaker stands in front of the camera. Given a presentation video, the video can be mod_x0002_eled as a series of images: V \u00bc fI1; I2; ...; Ii; ...; IN g, where Ii indicates i-th frame and N indicates the frame number in\nthe video.", "data_code": {"geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Body Keypoint Detection. To obtain the detailed gestures of a speaker, we need to detect body keypoints first. Here we adopt OpenPose [48], a widely used real-time multi-person keypoint detection library for body and hand estimation. For each frame Ii, we can detect the corresponding body keypoints, Ji \u00bc \u00bdPi0; Pi 1; ...; Pi k; ...; Pi 24_x0003_ , where Pi k \u00bc \u00bdxk; yk; ck_x0003_ indicates the coordinates, xk is x-axis value, yk is y-axis value and ck is the confidence probability. Since we mainly focus on the upper body hand gestures, we extract 9 body key_x0002_points of the upper body, which are highlighted in red as shown in Fig. 2.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Gesture Space Mapping. After detecting the body key_x0002_points, we can further obtain gesture coordinates. However, there are two major issues in analyzing gestures, i.e., coordi_x0002_nate normalization and spatial description of gestures. Coordi_x0002_nate normalization is important because it is hard to measure the gesture differences between different frames or presentation videos without a unified space. Thus, we normalize the gesture coordinates based on Keypoint 0 (the red circle annotated \u201c0\u201d in Fig. 2). To be specific, we regard Keypoint 0 as the coordinate origin. The coordi_x0002_nates of other keypoints are calculated based on Keypoint 0. Further, we normalize coordinates to \u00bd_x0004_1 _x0005_ 1_x0003_ range by using the height of the person. For the spatial description of gestures, we employ the gesture space defined in McNeill\u2019s gesture space theory [25], as shown in the blue dashed rectangles of Fig. 2. The three-level dashed rectan_x0002_gles represent the spatial regions of the center-center, center, and periphery from the inner to the outside. Specifically, the center-center region is the area directly in front of the chest; the center region is the area surrounding the center_x0002_center region, which stretches from the shoulders down to the waist and covers both sides of the body; the periphery region stretches from ears to the knees, surrounding the center-center region.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "As shown in Fig. 3B2 and B3, two mutually perpendicular timelines are used to describe hand movements. These two timelines are aligned with the gesture space in the heatmap part (Fig. 3B1). The horizontal timeline (Fig. 3B2) describes the vertical position of two hands. The purple line indicates the right hand, while the orange line indicates the left hand. The horizontal dashed line is aligned with the vertical cen_x0002_ter of the gesture space, so we can observe the vertical posi_x0002_tion of two hands. At the bottom of this timeline, there is a click area for users to seek to the corresponding parts of the presentation videos. A vertical black line indicates the cur_x0002_rent time frame. Similarly, the vertical timeline (Fig. 3B3) is used to describe the horizontal position of two hands. The same visual encoding is applied in this timeline. From the vertical timeline, users can observe how users move their hands horizontally. For example, whether users use open hand gestures or closed hand gestures can be observed from this timeline. Besides, two timelines are linked together, i.e., when brushing one timeline, the corresponding place in the other timeline will be highlighted.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}, {"solution_text": "Besides, two timelines are linked together, i.e., when brushing one timeline, the corresponding place in the other timeline will be highlighted.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "dxf", "index_original": 57, "paper_title": "GestureLens: Visual Analysis of Gestures in Presentation Videos", "pub_year": 2023, "domain": "presentation", "requirement": {"requirement_text": "T3: Explore the correlation between gestures and speech content. According to our interviews with the coaches, it is helpful for them to explore the correlation between\ngestures and speech content, e.g., whether the speakers use gestures that are harmonious with speech content. It is interesting to know what gestures are used to deliver certain content (content-based exploration). Furthermore, it is useful to know what content different gestures tend to convey (gesture-based exploration).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In this paper, we focus on those videos recording a speak_x0002_er\u2019s presentation, where the speaker stands in front of the camera. Given a presentation video, the video can be mod_x0002_eled as a series of images: V \u00bc fI1; I2; ...; Ii; ...; IN g, where Ii indicates i-th frame and N indicates the frame number in\nthe video.", "data_code": {"media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Body Keypoint Detection. To obtain the detailed gestures of a speaker, we need to detect body keypoints first. Here we adopt OpenPose [48], a widely used real-time multi-person keypoint detection library for body and hand estimation. For each frame Ii, we can detect the corresponding body keypoints, Ji \u00bc \u00bdPi0; Pi 1; ...; Pi k; ...; Pi 24_x0003_ , where Pi k \u00bc \u00bdxk; yk; ck_x0003_ indicates the coordinates, xk is x-axis value, yk is y-axis value and ck is the confidence probability. Since we mainly focus on the upper body hand gestures, we extract 9 body key_x0002_points of the upper body, which are highlighted in red as shown in Fig. 2.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Gesture Space Mapping. After detecting the body key_x0002_points, we can further obtain gesture coordinates. However, there are two major issues in analyzing gestures, i.e., coordi_x0002_nate normalization and spatial description of gestures. Coordi_x0002_nate normalization is important because it is hard to measure the gesture differences between different frames or presentation videos without a unified space. Thus, we normalize the gesture coordinates based on Keypoint 0 (the red circle annotated \u201c0\u201d in Fig. 2). To be specific, we regard Keypoint 0 as the coordinate origin. The coordi_x0002_nates of other keypoints are calculated based on Keypoint 0. Further, we normalize coordinates to \u00bd_x0004_1 _x0005_ 1_x0003_ range by using the height of the person. For the spatial description of gestures, we employ the gesture space defined in McNeill\u2019s gesture space theory [25], as shown in the blue dashed rectangles of Fig. 2. The three-level dashed rectan_x0002_gles represent the spatial regions of the center-center, center, and periphery from the inner to the outside. Specifically, the center-center region is the area directly in front of the chest; the center region is the area surrounding the center_x0002_center region, which stretches from the shoulders down to the waist and covers both sides of the body; the periphery region stretches from ears to the knees, surrounding the center-center region.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Gesture Segmentation. A speaker often uses various ges_x0002_tures for different speech content. We segment gestures according to word phrases, since we are interested in the relationships between gestures and speech content.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Gesture Distance Calculation. To find similar gestures, it is necessary to define gesture distance between different con_x0002_tinuous gestures. Since gestures may contain different frame lengths, we first define a distance function for measuring similarities between two static frames; then we utilize dynamic time warping (DTW) [49], a distance measure algorithm for time-series data with variable lengths. Given two keypoint coordinates in frames F and G, and each key_x0002_point vector is represented as Pi k \u00bc \u00bdxk; yk; ck_x0003_ , where k indi_x0002_cates k-th keypoint in F and G, xk is x-axis value, yk is y_x0002_axis value and ck is the confidence probability, the distance between gesture in two frames can be defined as [50]: D\u00f0F; G\u00de \u00bc 1 P8 k\u00bc0 Fck _x0006_ X 8 k\u00bc0 Fck _x0006_ kFxyk _x0004_ Gxyk k where Fck is the confidence probability for k-th keypoint, Fxyk and Gxyk is the coordinates of k-th keypoint of F and G, respectively.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "Gesture Type Calculation. After collecting different word phrase-based gestures, we classify gestures into three differ_x0002_ent categories, i.e., closed gestures, open gestures, and others[10]. Closed gestures refer to gestures where hands are put closely or overlapped with the torso; Open gestures refer to gesture where two hands are far away from each other and wrist points go outermost. For those gestures where hands are fall in the torso region, we named them others. More types can be calculated based on users\u2019 requirements", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Speech Content Processing. For a presentation video, the transcript can be obtained by adopting automatic transcrip_x0002_tion techniques.1 Further, to support word phrase analysis, we adopt an NLP library named textacy2 to extract semantic phrases, such as noun phrases (NP), verb phrases (VP), prep_x0002_ositional phrases (PP), and subject-verb object phrases (SVP).", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Data Alignment of Gesture Data and Transcripts. Following the aforementioned steps for body keypoints extraction, we extract keypoints frame by frame from a presentation video. Then, we can obtain the timestamp for keypoints of a frame. As for speech transcripts, the transcripts with timestamps can be obtained by adopting automatic transcription techniques (e.g., Amazon Transcribe). Therefore, the keypoints and speech transcripts are naturally aligned based on timestamps.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Description: As shown in Fig. 3A, word phrases and gesture glyphs are projected into the middle 2D plane and the bottom 2D plane respectively by using the t-SNE algorithm. In this way, word phrases with similar meaning or similar gesture glyphs are close to each other in the corresponding plane. ", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "Users are allowed to filter text phrases with a certain time range or occurrence number. Fur_x0002_ther, by clicking a legend type, the corresponding part can be shown or hidden. For example, as shown in Fig. 3A1, legend \u201cSVO (subject-verb-object)\u201d and \u201cothers\u201d are not filled with colors, which indicates the corresponding types are filtered. Some other interactions are provided to facilitate explora_x0002_tion. For example, when users click a word phrase or gesture glyph of interest, users can refer to the context of the word in the video, as well as the transcript area of the exploration view (Fig. 3B4). Further, bookmark interactions are provided to allow users to save the gestures of their interest, enabling future explorations.", "solution_category": "interaction", "solution_axial": "Filtering;Selecting", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "dxf", "index_original": 58, "paper_title": "GestureLens: Visual Analysis of Gestures in Presentation Videos", "pub_year": 2023, "domain": "presentation", "requirement": {"requirement_text": "T3: Explore the correlation between gestures and speech content. According to our interviews with the coaches, it is helpful for them to explore the correlation between\ngestures and speech content, e.g., whether the speakers use gestures that are harmonious with speech content. It is interesting to know what gestures are used to deliver certain content (content-based exploration). Furthermore, it is useful to know what content different gestures tend to convey (gesture-based exploration).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In this paper, we focus on those videos recording a speak_x0002_er\u2019s presentation, where the speaker stands in front of the camera. Given a presentation video, the video can be mod_x0002_eled as a series of images: V \u00bc fI1; I2; ...; Ii; ...; IN g, where Ii indicates i-th frame and N indicates the frame number in\nthe video.", "data_code": {"geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Body Keypoint Detection. To obtain the detailed gestures of a speaker, we need to detect body keypoints first. Here we adopt OpenPose [48], a widely used real-time multi-person keypoint detection library for body and hand estimation. For each frame Ii, we can detect the corresponding body keypoints, Ji \u00bc \u00bdPi0; Pi 1; ...; Pi k; ...; Pi 24_x0003_ , where Pi k \u00bc \u00bdxk; yk; ck_x0003_ indicates the coordinates, xk is x-axis value, yk is y-axis value and ck is the confidence probability. Since we mainly focus on the upper body hand gestures, we extract 9 body key_x0002_points of the upper body, which are highlighted in red as shown in Fig. 2.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Gesture Space Mapping. After detecting the body key_x0002_points, we can further obtain gesture coordinates. However, there are two major issues in analyzing gestures, i.e., coordi_x0002_nate normalization and spatial description of gestures. Coordi_x0002_nate normalization is important because it is hard to measure the gesture differences between different frames or presentation videos without a unified space. Thus, we normalize the gesture coordinates based on Keypoint 0 (the red circle annotated \u201c0\u201d in Fig. 2). To be specific, we regard Keypoint 0 as the coordinate origin. The coordi_x0002_nates of other keypoints are calculated based on Keypoint 0. Further, we normalize coordinates to \u00bd_x0004_1 _x0005_ 1_x0003_ range by using the height of the person. For the spatial description of gestures, we employ the gesture space defined in McNeill\u2019s gesture space theory [25], as shown in the blue dashed rectangles of Fig. 2. The three-level dashed rectan_x0002_gles represent the spatial regions of the center-center, center, and periphery from the inner to the outside. Specifically, the center-center region is the area directly in front of the chest; the center region is the area surrounding the center_x0002_center region, which stretches from the shoulders down to the waist and covers both sides of the body; the periphery region stretches from ears to the knees, surrounding the center-center region.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Gesture Segmentation. A speaker often uses various ges_x0002_tures for different speech content. We segment gestures according to word phrases, since we are interested in the relationships between gestures and speech content.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Gesture Distance Calculation. To find similar gestures, it is necessary to define gesture distance between different con_x0002_tinuous gestures. Since gestures may contain different frame lengths, we first define a distance function for measuring similarities between two static frames; then we utilize dynamic time warping (DTW) [49], a distance measure algorithm for time-series data with variable lengths. Given two keypoint coordinates in frames F and G, and each key_x0002_point vector is represented as Pi k \u00bc \u00bdxk; yk; ck_x0003_ , where k indi_x0002_cates k-th keypoint in F and G, xk is x-axis value, yk is y_x0002_axis value and ck is the confidence probability, the distance between gesture in two frames can be defined as [50]: D\u00f0F; G\u00de \u00bc 1 P8 k\u00bc0 Fck _x0006_ X 8 k\u00bc0 Fck _x0006_ kFxyk _x0004_ Gxyk k where Fck is the confidence probability for k-th keypoint, Fxyk and Gxyk is the coordinates of k-th keypoint of F and G, respectively.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "Gesture Type Calculation. After collecting different word phrase-based gestures, we classify gestures into three differ_x0002_ent categories, i.e., closed gestures, open gestures, and others[10]. Closed gestures refer to gestures where hands are put closely or overlapped with the torso; Open gestures refer to gesture where two hands are far away from each other and wrist points go outermost. For those gestures where hands are fall in the torso region, we named them others. More types can be calculated based on users\u2019 requirements", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Speech Content Processing. For a presentation video, the transcript can be obtained by adopting automatic transcrip_x0002_tion techniques.1 Further, to support word phrase analysis, we adopt an NLP library named textacy2 to extract semantic phrases, such as noun phrases (NP), verb phrases (VP), prep_x0002_ositional phrases (PP), and subject-verb object phrases (SVP).", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Data Alignment of Gesture Data and Transcripts. Following the aforementioned steps for body keypoints extraction, we extract keypoints frame by frame from a presentation video. Then, we can obtain the timestamp for keypoints of a frame. As for speech transcripts, the transcripts with timestamps can be obtained by adopting automatic transcription techniques (e.g., Amazon Transcribe). Therefore, the keypoints and speech transcripts are naturally aligned based on timestamps.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "it annotates gesture glyphs on each word in the transcript, and highlights the dramatic changes in gestures (Fig. 3B4), which allows users to explore both gestures and speech content (T3). a transcript area to show the corresponding speech content and gestures (T3). As shown in Fig. 3B4, the speech content is explicitly shown here. To bet_x0002_ter reveal gestures used for speech content, we overlay human stick figures on top of the corresponding words. To be specific, gestures of the frames within a word are drawn on top of the word, which shows aggregate information of gestures for that word. We further define spatial variation to describe the variation of gestures within a word, and tempo_x0002_ral change to describe the change of gestures between two words. We first calculate the average gesture skeleton of each word, then we calculate the gesture skeleton variation within a word as spatial variation, while we calculate the gesture skeleton change between two words as temporal change. We normalize both spatial variation and temporal change value to \u00bd0 _x0005_ 1_x0003_ . End users can customize the thresh_x0002_olds. As shown in Fig. 3B4, the change threshold and varia_x0002_tion threshold are set to 0.5 and 0.4, respectively. As shown in the transcript area, high spatial variations are encoded with red strokes, and large temporal changes are encoded with green triangles.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Word;Others", "axial_code": [], "componenet_code": ["others", "text"]}]}, {"author": "dxf", "index_original": 59, "paper_title": "GestureLens: Visual Analysis of Gestures in Presentation Videos", "pub_year": 2023, "domain": "presentation", "requirement": {"requirement_text": "T4: Find similar gestures used in presentations. Research shows that speakers may unconsciously use similar or repetitive gestures of their own styles [46]. Our coaches expressed that it is helpful to explore similar or repetitive gestures, which can make speakers better aware of their own gestures. Further, coaches can compare different similar gestures with different speech content, whereby understanding why speakers use such gestures and providing suggestions for improvements. For example, coaches could detect repetitive non-meaningful gestures made by speakers.", "requirement_code": {"discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "In this paper, we focus on those videos recording a speak_x0002_er\u2019s presentation, where the speaker stands in front of the camera. Given a presentation video, the video can be mod_x0002_eled as a series of images: V \u00bc fI1; I2; ...; Ii; ...; IN g, where Ii indicates i-th frame and N indicates the frame number in\nthe video.", "data_code": {"media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Body Keypoint Detection. To obtain the detailed gestures of a speaker, we need to detect body keypoints first. Here we adopt OpenPose [48], a widely used real-time multi-person keypoint detection library for body and hand estimation. For each frame Ii, we can detect the corresponding body keypoints, Ji \u00bc \u00bdPi0; Pi 1; ...; Pi k; ...; Pi 24_x0003_ , where Pi k \u00bc \u00bdxk; yk; ck_x0003_ indicates the coordinates, xk is x-axis value, yk is y-axis value and ck is the confidence probability. Since we mainly focus on the upper body hand gestures, we extract 9 body key_x0002_points of the upper body, which are highlighted in red as shown in Fig. 2.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Gesture Space Mapping. After detecting the body key_x0002_points, we can further obtain gesture coordinates. However, there are two major issues in analyzing gestures, i.e., coordi_x0002_nate normalization and spatial description of gestures. Coordi_x0002_nate normalization is important because it is hard to measure the gesture differences between different frames or presentation videos without a unified space. Thus, we normalize the gesture coordinates based on Keypoint 0 (the red circle annotated \u201c0\u201d in Fig. 2). To be specific, we regard Keypoint 0 as the coordinate origin. The coordi_x0002_nates of other keypoints are calculated based on Keypoint 0. Further, we normalize coordinates to \u00bd_x0004_1 _x0005_ 1_x0003_ range by using the height of the person. For the spatial description of gestures, we employ the gesture space defined in McNeill\u2019s gesture space theory [25], as shown in the blue dashed rectangles of Fig. 2. The three-level dashed rectan_x0002_gles represent the spatial regions of the center-center, center, and periphery from the inner to the outside. Specifically, the center-center region is the area directly in front of the chest; the center region is the area surrounding the center_x0002_center region, which stretches from the shoulders down to the waist and covers both sides of the body; the periphery region stretches from ears to the knees, surrounding the center-center region.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Gesture Segmentation. A speaker often uses various ges_x0002_tures for different speech content. We segment gestures according to word phrases, since we are interested in the relationships between gestures and speech content.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Gesture Distance Calculation. To find similar gestures, it is necessary to define gesture distance between different con_x0002_tinuous gestures. Since gestures may contain different frame lengths, we first define a distance function for measuring similarities between two static frames; then we utilize dynamic time warping (DTW) [49], a distance measure algorithm for time-series data with variable lengths. Given two keypoint coordinates in frames F and G, and each key_x0002_point vector is represented as Pi k \u00bc \u00bdxk; yk; ck_x0003_ , where k indi_x0002_cates k-th keypoint in F and G, xk is x-axis value, yk is y_x0002_axis value and ck is the confidence probability, the distance between gesture in two frames can be defined as [50]: D\u00f0F; G\u00de \u00bc 1 P8 k\u00bc0 Fck _x0006_ X 8 k\u00bc0 Fck _x0006_ kFxyk _x0004_ Gxyk k where Fck is the confidence probability for k-th keypoint, Fxyk and Gxyk is the coordinates of k-th keypoint of F and G, respectively.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "Gesture Type Calculation. After collecting different word phrase-based gestures, we classify gestures into three differ_x0002_ent categories, i.e., closed gestures, open gestures, and others[10]. Closed gestures refer to gestures where hands are put closely or overlapped with the torso; Open gestures refer to gesture where two hands are far away from each other and wrist points go outermost. For those gestures where hands are fall in the torso region, we named them others. More types can be calculated based on users\u2019 requirements", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Speech Content Processing. For a presentation video, the transcript can be obtained by adopting automatic transcrip_x0002_tion techniques.1 Further, to support word phrase analysis, we adopt an NLP library named textacy2 to extract semantic phrases, such as noun phrases (NP), verb phrases (VP), prep_x0002_ositional phrases (PP), and subject-verb object phrases (SVP).", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Data Alignment of Gesture Data and Transcripts. Following the aforementioned steps for body keypoints extraction, we extract keypoints frame by frame from a presentation video. Then, we can obtain the timestamp for keypoints of a frame. As for speech transcripts, the transcripts with timestamps can be obtained by adopting automatic transcription techniques (e.g., Amazon Transcribe). Therefore, the keypoints and speech transcripts are naturally aligned based on timestamps.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Description: As shown in Fig. 3A, word phrases and gesture glyphs are projected into the middle 2D plane and the bottom 2D plane respectively by using the t-SNE algorithm. In this way, word phrases with similar meaning or similar gesture glyphs are close to each other in the corresponding plane. ", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "Users are allowed to filter text phrases with a certain time range or occurrence number. Fur_x0002_ther, by clicking a legend type, the corresponding part can be shown or hidden. For example, as shown in Fig. 3A1, legend \u201cSVO (subject-verb-object)\u201d and \u201cothers\u201d are not filled with colors, which indicates the corresponding types are filtered. Some other interactions are provided to facilitate explora_x0002_tion. For example, when users click a word phrase or gesture glyph of interest, users can refer to the context of the word in the video, as well as the transcript area of the exploration view (Fig. 3B4). Further, bookmark interactions are provided to allow users to save the gestures of their interest, enabling future explorations.", "solution_category": "interaction", "solution_axial": "Filtering;Selecting", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "dxf", "index_original": 60, "paper_title": "GestureLens: Visual Analysis of Gestures in Presentation Videos", "pub_year": 2023, "domain": "presentation", "requirement": {"requirement_text": "T5: Enable interactive exploration of the presentation videos. Coaches also need to check the original presentation video and the corresponding transcripts to confirm their findings of a speaker's gesture usage. Thus, it is necessary to provide coaches with interactive exploration of the original presentation videos.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "In this paper, we focus on those videos recording a speak_x0002_er\u2019s presentation, where the speaker stands in front of the camera. Given a presentation video, the video can be mod_x0002_eled as a series of images: V \u00bc fI1; I2; ...; Ii; ...; IN g, where Ii indicates i-th frame and N indicates the frame number in\nthe video.", "data_code": {"media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "For video analysis, referring to the original video can sometimes provide a better explanation. Therefore, we embed the original video for exploration in the video view (Fig. 3C). After selecting a video of interest, the video is presented in this view. Based on the coaches\u2019 feedback, the screenshot interaction is added to record some interesting moments. The corresponding information (i.e., time and word) is placed under screenshots, which can facilitate gesture usage exploration. The video view is linked to other views. Users can refer to the video for detailed analysis.", "solution_category": "interaction", "solution_axial": "Selecting;History;Connect/Relate", "solution_compoent": "", "axial_code": ["Selecting", "Connect/Relate", "History"], "componenet_code": ["selecting", "connect_relate", "history"]}]}, {"author": "dxf", "index_original": 61, "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence", "pub_year": 2023, "domain": "semantic analysis", "requirement": {"requirement_text": "G1: Overview of the sequence distribution. The visualization should project music feature vectors into a 2D plane space as a distribution overview of music sequences. In this projection space, users can examine distribution patterns of music sequences and obtain guidance for further analysis.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Music", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "media": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "Inspired by the idea proposed by Galex [26], which extracts article features based on the article summarization data by using Doc2Vec to generate the feature vectors for articles, we apply Doc2Vec to compute the music vector representations by using music text representation. At first, we use text representations of the music sequences to train the Doc2Vec model. Then, we compute the corresponding feature vector based on the text representation of each music sequence. As a result, each music MIDI file is extracted as a musical semantic sequence, and each musical semantic sequence equips a corresponding 128-dimension feature vector by constructing the text representation and employing the Doc2Vec model.", "solution_category": "data_manipulation", "solution_axial": "Modeling;Rectification", "solution_compoent": "", "axial_code": ["Modeling", "Rectification"], "componenet_code": ["modeling", "rectification"]}, {"solution_text": "To visualize the distribution pattern among music more intuitively, we project the result of Doc2Vec to a 2D plane. We have considered three alternative dimension reduction methods includes principal component analysis (PCA) [12], multidimensional scaling (MDS) [24], and and t-distributed\nstochastic neighbor embedding (t-SNE) [33]. Although these dimension reduction methods are based on different under_x0002_lying algorithms, we have examined multiple parameter groups to select a proper method to produce the distribu_x0002_tion results. The test data includes 6,632 musical sequence vectors, and the size of each vector is 1 _x0003_ 128. These vectors are equipped with 4 genre attributes which are represented by 4 color points including yellow (classical), green (folk), red (rock), and purple (jazz). As shown in Fig. 2b, the result of t-SNE has more intui_x0002_tive distribution patterns than the other two dimension reduction methods (MDS and PCA). Consequently, we select t-SNE to project musical sequences into a distribution view with one theoretical consideration. Compared with MDS and PCA, t-SNE maximally preserves local neighbor_x0002_hoods of each data point (high dimensional vector), which ensures each musical sequence to have its similar neighbors based on the feature vectors in the distribution view [49]. In the distribution view, we use points to represent the musical sequences. To assist users in discovering distribu_x0002_tion patterns intuitively, we map two music features (i.e., music genres and instruments) to the points. In addition, to help users recognize the distribution patterns in the distri_x0002_bution view, we equip the distribution view with two visual components: a density contour component and a pathfinding component. The density contour is helpful for users to reveal the overall density distribution of the numerous points. How_x0002_ever, the traditional density contour only considers the geo_x0002_metric position of points and ignores the similarity. Thus, inspired by the marching square algorithm [20] in the den_x0002_sity contour, we propose an extended algorithm to modify the generation of density contours based on the similarity ofmusical sequence vectors. We apply the similarity to modify the basic matrix in the original marching square algorithm. As shown in Fig. 2c, the left contour is the raw result. In the marching square algorithm matrix, each number repre_x0002_sents the number of musical sequences in a grid. Our algo_x0002_rithm calculates and integrates the musical sequence similarities to modify the density value of the original matrix. When the grid similarity is larger than the threshold, these grids tend to be similar so that the number in the two grids would be added with a virtual value to change the state of the matrix. After the steps mentioned above, the modified density contour is generated to ensure that similar sequences have the same contour line. From the density contour shown in the right part of Fig. 2c, the points indicat_x0002_ing the same musical genre (jazz) are circled by the con_x0002_tours, which may indicate the apparent similarity of thes musical sequences. Including the density contour, we integrate a semantic change path component into the distribution view based on our proposed pathfinding algorithm to help users analyze the semantic change from one pattern to another pattern. The pathfinding algorithm, whose perplexity is O(n2), inte_x0002_grates the distances of music sequence projection coordi_x0002_nates and feature vectors. This heuristic algorithm follows four steps (S1 to S4) to find a path from the source music sequence to the target one in the distribution view. Four steps are introduced as follows. S1 Divide music sequences into corresponding grids based on their projection coordinates. S2 Obtain music sequences in the neighbor search grids of the source music sequence. If the neighbor search grids have no available sequences, enlarge the scope of neigh_x0002_bor search grids to search. S3 Calculate and determine the smallest euclidean distance of feature vectors between the source music sequence and each music sequence obtained in step S2. S4 Put the music sequence (found in step S3) into the path and make it a new source music sequence. Repeat step S1 to step S4 until the target music sequence is in the neighbor search grids. To ensure each new source music sequence to be approaching the target music sequence, we set the princi_x0002_ples of neighbor search grids [44]. Comparing the coordi_x0002_nates of the source music sequence and the target one, as shown in Fig. 3, we summarize eight neighbor search grids schemes. The eight schemes are \u201cupper left, low right, upper right, low left, horizontally left, horizontally right, vertically upper, and vertically low\u201d based on position sta_x0002_tus from the source music sequence to the target one. Users can sample the path points (sequences) if the path has too many music sequences to analyze the semantic change. In addition, the scheme of grids can be customized in different scenes. In this work, we offer two grids schemes including 128 _x0003_ 128 (which is selected in case 2) and 256 _x0003_ 256.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "To visualize the distribution pattern among music more intuitively, we project the result of Doc2Vec to a 2D plane. We have considered three alternative dimension reduction methods includes principal component analysis (PCA) [12], multidimensional scaling (MDS) [24], and and t-distributed\nstochastic neighbor embedding (t-SNE) [33]. Although these dimension reduction methods are based on different under_x0002_lying algorithms, we have examined multiple parameter groups to select a proper method to produce the distribu_x0002_tion results. The test data includes 6,632 musical sequence vectors, and the size of each vector is 1 _x0003_ 128. These vectors are equipped with 4 genre attributes which are represented by 4 color points including yellow (classical), green (folk), red (rock), and purple (jazz). As shown in Fig. 2b, the result of t-SNE has more intui_x0002_tive distribution patterns than the other two dimension reduction methods (MDS and PCA). Consequently, we select t-SNE to project musical sequences into a distribution view with one theoretical consideration. Compared with MDS and PCA, t-SNE maximally preserves local neighbor_x0002_hoods of each data point (high dimensional vector), which ensures each musical sequence to have its similar neighbors based on the feature vectors in the distribution view [49]. In the distribution view, we use points to represent the musical sequences. To assist users in discovering distribu_x0002_tion patterns intuitively, we map two music features (i.e., music genres and instruments) to the points. In addition, to help users recognize the distribution patterns in the distri_x0002_bution view, we equip the distribution view with two visual components: a density contour component and a pathfinding component. The density contour is helpful for users to reveal the overall density distribution of the numerous points. How_x0002_ever, the traditional density contour only considers the geo_x0002_metric position of points and ignores the similarity. Thus, inspired by the marching square algorithm [20] in the den_x0002_sity contour, we propose an extended algorithm to modify the generation of density contours based on the similarity ofmusical sequence vectors. We apply the similarity to modify the basic matrix in the original marching square algorithm. As shown in Fig. 2c, the left contour is the raw result. In the marching square algorithm matrix, each number repre_x0002_sents the number of musical sequences in a grid. Our algo_x0002_rithm calculates and integrates the musical sequence similarities to modify the density value of the original matrix. When the grid similarity is larger than the threshold, these grids tend to be similar so that the number in the two grids would be added with a virtual value to change the state of the matrix. After the steps mentioned above, the modified density contour is generated to ensure that similar sequences have the same contour line. From the density contour shown in the right part of Fig. 2c, the points indicat_x0002_ing the same musical genre (jazz) are circled by the con_x0002_tours, which may indicate the apparent similarity of thes musical sequences. Including the density contour, we integrate a semantic change path component into the distribution view based on our proposed pathfinding algorithm to help users analyze the semantic change from one pattern to another pattern. The pathfinding algorithm, whose perplexity is O(n2), inte_x0002_grates the distances of music sequence projection coordi_x0002_nates and feature vectors. This heuristic algorithm follows four steps (S1 to S4) to find a path from the source music sequence to the target one in the distribution view. Four steps are introduced as follows. S1 Divide music sequences into corresponding grids based on their projection coordinates. S2 Obtain music sequences in the neighbor search grids of the source music sequence. If the neighbor search grids have no available sequences, enlarge the scope of neigh_x0002_bor search grids to search. S3 Calculate and determine the smallest euclidean distance of feature vectors between the source music sequence and each music sequence obtained in step S2. S4 Put the music sequence (found in step S3) into the path and make it a new source music sequence. Repeat step S1 to step S4 until the target music sequence is in the neighbor search grids. To ensure each new source music sequence to be approaching the target music sequence, we set the princi_x0002_ples of neighbor search grids [44]. Comparing the coordi_x0002_nates of the source music sequence and the target one, as shown in Fig. 3, we summarize eight neighbor search grids schemes. The eight schemes are \u201cupper left, low right, upper right, low left, horizontally left, horizontally right, vertically upper, and vertically low\u201d based on position sta_x0002_tus from the source music sequence to the target one. Users can sample the path points (sequences) if the path has too many music sequences to analyze the semantic change. In addition, the scheme of grids can be customized in different scenes. In this work, we offer two grids schemes including 128 _x0003_ 128 (which is selected in case 2) and 256 _x0003_ 256.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Scatter;ContourLine", "axial_code": [], "componenet_code": ["contour", "scatter"]}]}, {"author": "dxf", "index_original": 62, "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence", "pub_year": 2023, "domain": "semantic analysis", "requirement": {"requirement_text": "G1: Overview of the sequence distribution. The visualization should project music feature vectors into a 2D plane space as a distribution overview of music sequences. In this projection space, users can examine distribution patterns of music sequences and obtain guidance for further analysis.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Music", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "media": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "To present the connection between genres with instruments and assist users in accessing the distribution patterns in the distribution view, we design a genre instrument tree on the left of the system interface (G1 & G2). In the genre instrument tree, as shown in Fig. 6a, each genre is displayed as a circle in the first column of this tree plot. For example, the green circle represents the folk genre while the yellow circle represents the classical genre. Each circle in the second column indicates the instrument information. The fill color of the circles indicates the instrument name. All instruments are collected and divided into differ_x0002_ent categories of instruments (G2). Instruments in the same category are encoded as a series of same color class circles with different color saturation. Higher color saturation indicates that the proportion of the instrument is larger than other instruments in the same category. For example, in Fig. 6a, two circles filled by two kinds of red colors repre_x0002_sent two string instruments (i.e., violin and viola). The higher color saturation indicates that the violin has larger proportion than the viola in music sequences.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Tree", "axial_code": [], "componenet_code": ["tree"]}]}, {"author": "dxf", "index_original": 63, "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence", "pub_year": 2023, "domain": "semantic analysis", "requirement": {"requirement_text": "G2: Representation of the semantic information. The visualization should present the semantic details and encode hierarchical information such as genre, instrument, and note to support effective exploration and comparison of semantic details.", "requirement_code": {"compare_entities": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "Music", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "media": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "Inspired by the idea proposed by Galex [26], which extracts article features based on the article summarization data by using Doc2Vec to generate the feature vectors for articles, we apply Doc2Vec to compute the music vector representations by using music text representation. At first, we use text representations of the music sequences to train the Doc2Vec model. Then, we compute the corresponding feature vector based on the text representation of each music sequence. As a result, each music MIDI file is extracted as a musical semantic sequence, and each musical semantic sequence equips a corresponding 128-dimension feature vector by constructing the text representation and employing the Doc2Vec model.", "solution_category": "data_manipulation", "solution_axial": "Modeling;Rectification", "solution_compoent": "", "axial_code": ["Modeling", "Rectification"], "componenet_code": ["modeling", "rectification"]}, {"solution_text": "To visualize the distribution pattern among music more intuitively, we project the result of Doc2Vec to a 2D plane. We have considered three alternative dimension reduction methods includes principal component analysis (PCA) [12], multidimensional scaling (MDS) [24], and and t-distributed\nstochastic neighbor embedding (t-SNE) [33]. Although these dimension reduction methods are based on different under_x0002_lying algorithms, we have examined multiple parameter groups to select a proper method to produce the distribu_x0002_tion results. The test data includes 6,632 musical sequence vectors, and the size of each vector is 1 _x0003_ 128. These vectors are equipped with 4 genre attributes which are represented by 4 color points including yellow (classical), green (folk), red (rock), and purple (jazz). As shown in Fig. 2b, the result of t-SNE has more intui_x0002_tive distribution patterns than the other two dimension reduction methods (MDS and PCA). Consequently, we select t-SNE to project musical sequences into a distribution view with one theoretical consideration. Compared with MDS and PCA, t-SNE maximally preserves local neighbor_x0002_hoods of each data point (high dimensional vector), which ensures each musical sequence to have its similar neighbors based on the feature vectors in the distribution view [49]. In the distribution view, we use points to represent the musical sequences. To assist users in discovering distribu_x0002_tion patterns intuitively, we map two music features (i.e., music genres and instruments) to the points. In addition, to help users recognize the distribution patterns in the distri_x0002_bution view, we equip the distribution view with two visual components: a density contour component and a pathfinding component. The density contour is helpful for users to reveal the overall density distribution of the numerous points. How_x0002_ever, the traditional density contour only considers the geo_x0002_metric position of points and ignores the similarity. Thus, inspired by the marching square algorithm [20] in the den_x0002_sity contour, we propose an extended algorithm to modify the generation of density contours based on the similarity ofmusical sequence vectors. We apply the similarity to modify the basic matrix in the original marching square algorithm. As shown in Fig. 2c, the left contour is the raw result. In the marching square algorithm matrix, each number repre_x0002_sents the number of musical sequences in a grid. Our algo_x0002_rithm calculates and integrates the musical sequence similarities to modify the density value of the original matrix. When the grid similarity is larger than the threshold, these grids tend to be similar so that the number in the two grids would be added with a virtual value to change the state of the matrix. After the steps mentioned above, the modified density contour is generated to ensure that similar sequences have the same contour line. From the density contour shown in the right part of Fig. 2c, the points indicat_x0002_ing the same musical genre (jazz) are circled by the con_x0002_tours, which may indicate the apparent similarity of thes musical sequences. Including the density contour, we integrate a semantic change path component into the distribution view based on our proposed pathfinding algorithm to help users analyze the semantic change from one pattern to another pattern. The pathfinding algorithm, whose perplexity is O(n2), inte_x0002_grates the distances of music sequence projection coordi_x0002_nates and feature vectors. This heuristic algorithm follows four steps (S1 to S4) to find a path from the source music sequence to the target one in the distribution view. Four steps are introduced as follows. S1 Divide music sequences into corresponding grids based on their projection coordinates. S2 Obtain music sequences in the neighbor search grids of the source music sequence. If the neighbor search grids have no available sequences, enlarge the scope of neigh_x0002_bor search grids to search. S3 Calculate and determine the smallest euclidean distance of feature vectors between the source music sequence and each music sequence obtained in step S2. S4 Put the music sequence (found in step S3) into the path and make it a new source music sequence. Repeat step S1 to step S4 until the target music sequence is in the neighbor search grids. To ensure each new source music sequence to be approaching the target music sequence, we set the princi_x0002_ples of neighbor search grids [44]. Comparing the coordi_x0002_nates of the source music sequence and the target one, as shown in Fig. 3, we summarize eight neighbor search grids schemes. The eight schemes are \u201cupper left, low right, upper right, low left, horizontally left, horizontally right, vertically upper, and vertically low\u201d based on position sta_x0002_tus from the source music sequence to the target one. Users can sample the path points (sequences) if the path has too many music sequences to analyze the semantic change. In addition, the scheme of grids can be customized in different scenes. In this work, we offer two grids schemes including 128 _x0003_ 128 (which is selected in case 2) and 256 _x0003_ 256.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "To visualize the distribution pattern among music more intuitively, we project the result of Doc2Vec to a 2D plane. We have considered three alternative dimension reduction methods includes principal component analysis (PCA) [12], multidimensional scaling (MDS) [24], and and t-distributed\nstochastic neighbor embedding (t-SNE) [33]. Although these dimension reduction methods are based on different under_x0002_lying algorithms, we have examined multiple parameter groups to select a proper method to produce the distribu_x0002_tion results. The test data includes 6,632 musical sequence vectors, and the size of each vector is 1 _x0003_ 128. These vectors are equipped with 4 genre attributes which are represented by 4 color points including yellow (classical), green (folk), red (rock), and purple (jazz). As shown in Fig. 2b, the result of t-SNE has more intui_x0002_tive distribution patterns than the other two dimension reduction methods (MDS and PCA). Consequently, we select t-SNE to project musical sequences into a distribution view with one theoretical consideration. Compared with MDS and PCA, t-SNE maximally preserves local neighbor_x0002_hoods of each data point (high dimensional vector), which ensures each musical sequence to have its similar neighbors based on the feature vectors in the distribution view [49]. In the distribution view, we use points to represent the musical sequences. To assist users in discovering distribu_x0002_tion patterns intuitively, we map two music features (i.e., music genres and instruments) to the points. In addition, to help users recognize the distribution patterns in the distri_x0002_bution view, we equip the distribution view with two visual components: a density contour component and a pathfinding component. The density contour is helpful for users to reveal the overall density distribution of the numerous points. How_x0002_ever, the traditional density contour only considers the geo_x0002_metric position of points and ignores the similarity. Thus, inspired by the marching square algorithm [20] in the den_x0002_sity contour, we propose an extended algorithm to modify the generation of density contours based on the similarity ofmusical sequence vectors. We apply the similarity to modify the basic matrix in the original marching square algorithm. As shown in Fig. 2c, the left contour is the raw result. In the marching square algorithm matrix, each number repre_x0002_sents the number of musical sequences in a grid. Our algo_x0002_rithm calculates and integrates the musical sequence similarities to modify the density value of the original matrix. When the grid similarity is larger than the threshold, these grids tend to be similar so that the number in the two grids would be added with a virtual value to change the state of the matrix. After the steps mentioned above, the modified density contour is generated to ensure that similar sequences have the same contour line. From the density contour shown in the right part of Fig. 2c, the points indicat_x0002_ing the same musical genre (jazz) are circled by the con_x0002_tours, which may indicate the apparent similarity of thes musical sequences. Including the density contour, we integrate a semantic change path component into the distribution view based on our proposed pathfinding algorithm to help users analyze the semantic change from one pattern to another pattern. The pathfinding algorithm, whose perplexity is O(n2), inte_x0002_grates the distances of music sequence projection coordi_x0002_nates and feature vectors. This heuristic algorithm follows four steps (S1 to S4) to find a path from the source music sequence to the target one in the distribution view. Four steps are introduced as follows. S1 Divide music sequences into corresponding grids based on their projection coordinates. S2 Obtain music sequences in the neighbor search grids of the source music sequence. If the neighbor search grids have no available sequences, enlarge the scope of neigh_x0002_bor search grids to search. S3 Calculate and determine the smallest euclidean distance of feature vectors between the source music sequence and each music sequence obtained in step S2. S4 Put the music sequence (found in step S3) into the path and make it a new source music sequence. Repeat step S1 to step S4 until the target music sequence is in the neighbor search grids. To ensure each new source music sequence to be approaching the target music sequence, we set the princi_x0002_ples of neighbor search grids [44]. Comparing the coordi_x0002_nates of the source music sequence and the target one, as shown in Fig. 3, we summarize eight neighbor search grids schemes. The eight schemes are \u201cupper left, low right, upper right, low left, horizontally left, horizontally right, vertically upper, and vertically low\u201d based on position sta_x0002_tus from the source music sequence to the target one. Users can sample the path points (sequences) if the path has too many music sequences to analyze the semantic change. In addition, the scheme of grids can be customized in different scenes. In this work, we offer two grids schemes including 128 _x0003_ 128 (which is selected in case 2) and 256 _x0003_ 256.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Scatter;ContourLine", "axial_code": [], "componenet_code": ["contour", "scatter"]}]}, {"author": "dxf", "index_original": 64, "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence", "pub_year": 2023, "domain": "semantic analysis", "requirement": {"requirement_text": "G2: Representation of the semantic information. The visualization should present the semantic details and encode hierarchical information such as genre, instrument, and note to support effective exploration and comparison of semantic details.", "requirement_code": {"compare_entities": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "Music", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "media": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "After the visualization experiments of semantic details (Section 4.2), we utilize the Notes Aggregation scheme to present the semantic details of music sequences (G2 & G3). As shown in Fig. 5g, a row represents a music sequence respectively, which contains four parts to display the semantic information. The first part is the music sequence ID, which helps users explore the musical sequences in other visualization views. The second part is a genre\u2019s text description to provide users with the genre information. The third part is a pie chart component to display the proportion of instruments in the music sequence. The last part presents the semantic details of the music sequence. Each small rectangle represents the semantic information, which has three attributes including the width, height, and fill color. The width indicates the duration, the height represents the average pitch, and the color depicts the type of instruments. As shown in Fig. 5g, the music sequence could be aggregated based on the Notes Aggregation scheme (Sec_x0002_tion 4.2) with a collation width of 13, which is automatically calculated by the Formula (1), (2), and (3).", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Bar;Table", "axial_code": [], "componenet_code": ["bar", "table"]}]}, {"author": "dxf", "index_original": 65, "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence", "pub_year": 2023, "domain": "semantic analysis", "requirement": {"requirement_text": "G2: Representation of the semantic information. The visualization should present the semantic details and encode hierarchical information such as genre, instrument, and note to support effective exploration and comparison of semantic details.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Music", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "media": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "To present the connection between genres with instruments and assist users in accessing the distribution patterns in the distribution view, we design a genre instrument tree on the left of the system interface (G1 & G2). In the genre instrument tree, as shown in Fig. 6a, each genre is displayed as a circle in the first column of this tree plot. For example, the green circle represents the folk genre while the yellow circle represents the classical genre. Each circle in the second column indicates the instrument information. The fill color of the circles indicates the instrument name. All instruments are collected and divided into differ_x0002_ent categories of instruments (G2). Instruments in the same category are encoded as a series of same color class circles with different color saturation. Higher color saturation indicates that the proportion of the instrument is larger than other instruments in the same category. For example, in Fig. 6a, two circles filled by two kinds of red colors repre_x0002_sent two string instruments (i.e., violin and viola). The higher color saturation indicates that the violin has larger proportion than the viola in music sequences.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Tree", "axial_code": [], "componenet_code": ["tree"]}]}, {"author": "dxf", "index_original": 66, "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence", "pub_year": 2023, "domain": "semantic analysis", "requirement": {"requirement_text": "G2: Representation of the semantic information. The visualization should present the semantic details and encode hierarchical information such as genre, instrument, and note to support effective exploration and comparison of semantic details.", "requirement_code": {"compare_entities": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "Music", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "media": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "To help users identify and analyze the similarity to the music genre of each music sequence(G2), we design a radar plot (Fig. 6b). The radar plot has six axes/dimensions including musicID, genre, folk, classical, jazz, and rock. Each line in the radar plot represents a music sequence, and the color of the line is encoded as the genre information. The position in the musicID axis demonstrates the ID of the music sequence. The position in the genre axis indicates the genre information of the music sequence. The positions in the folk, classical, jazz, and rock axes present the similarity to music genres. The higher position depicts the larger similarities to the corresponding music genres. To access the genre similarity of each music sequence, we calculate music genre vectors. For an example of the folk genre vector, we collected all the feature vectors of folk music sequences. Then we average the collected feature vectors of folk music sequences as the folk genre vector. In the end, we calculate the euclidean distance from the music sequence vector to the folk genre vector as the similarity between the music sequence and the folk genre.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "To help users identify and analyze the similarity to the music genre of each music sequence(G2), we design a radar plot (Fig. 6b). The radar plot has six axes/dimensions including musicID, genre, folk, classical, jazz, and rock. Each line in the radar plot represents a music sequence, and the color of the line is encoded as the genre information. The position in the musicID axis demonstrates the ID of the music sequence. The position in the genre axis indicates the genre information of the music sequence. The positions in the folk, classical, jazz, and rock axes present the similarity to music genres. The higher position depicts the larger similarities to the corresponding music genres. To access the genre similarity of each music sequence, we calculate music genre vectors. For an example of the folk genre vector, we collected all the feature vectors of folk music sequences. Then we average the collected feature vectors of folk music sequences as the folk genre vector. In the end, we calculate the euclidean distance from the music sequence vector to the folk genre vector as the similarity between the music sequence and the folk genre.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Radar", "axial_code": [], "componenet_code": ["radar"]}]}, {"author": "dxf", "index_original": 67, "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence", "pub_year": 2023, "domain": "semantic analysis", "requirement": {"requirement_text": "G3: Comparison of multi-variate sequences. The visualization should support users in comparing the semantic information of multivariate sequences, which is helpful for\nthe exploration of hidden semantic change, genre, instruments patterns, etc. In patterns analysis, users should be allowed to explore hierarchical information efficiently and validate whether the potential uncertainty issue is avoided or not.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Music", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "media": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "After the visualization experiments of semantic details (Section 4.2), we utilize the Notes Aggregation scheme to present the semantic details of music sequences (G2 & G3). As shown in Fig. 5g, a row represents a music sequence respectively, which contains four parts to display the semantic information. The first part is the music sequence ID, which helps users explore the musical sequences in other visualization views. The second part is a genre\u2019s text description to provide users with the genre information. The third part is a pie chart component to display the proportion of instruments in the music sequence. The last part presents the semantic details of the music sequence. Each small rectangle represents the semantic information, which has three attributes including the width, height, and fill color. The width indicates the duration, the height represents the average pitch, and the color depicts the type of instruments. As shown in Fig. 5g, the music sequence could be aggregated based on the Notes Aggregation scheme (Sec_x0002_tion 4.2) with a collation width of 13, which is automatically calculated by the Formula (1), (2), and (3).", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Bar;Table", "axial_code": [], "componenet_code": ["bar", "table"]}]}, {"author": "dxf", "index_original": 68, "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence", "pub_year": 2023, "domain": "semantic analysis", "requirement": {"requirement_text": "G3: Comparison of multi-variate sequences. The visualization should support users in comparing the semantic information of multivariate sequences, which is helpful for\nthe exploration of hidden semantic change, genre, instruments patterns, etc. In patterns analysis, users should be allowed to explore hierarchical information efficiently and validate whether the potential uncertainty issue is avoided or not.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Music", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "media": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "To help users access the semantic distribution patterns of music sequences, we design a parallel coordinate plot to display the semantic details simultaneously in multiple dimensions(G2 & G3). As shown in Fig. 5f, each line is encoded as a note in the parallel coordinate plot. It has seven dimensions including musicID, genre, instrument, note class, pitch, start time, and type. The dimensions are divided into three levels the sequence level, instrument level, and note level. The sequence level contains musicID and genre dimensions. The MusicID indicates the music sequence ID of the note, while the genre indicates the music sequence genre. The Instrument level has one dimension named instrument, which presents the instrument name of the note. The color of the line reveals the instrument name. The last note level includes note class, pitch, start time, and type dimensions. The note class dimension indicates that the note is a note or a chord\u2019s note. The pitch dimension presents the pitch value of each note. The start time dimension displays the timestamp of the note. The last type dimension introduces note\u2019s type information such as half, quarter, eighth, 16th, and whole.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 69, "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence", "pub_year": 2023, "domain": "semantic analysis", "requirement": {"requirement_text": "G3: Comparison of multi-variate sequences. The visualization should support users in comparing the semantic information of multivariate sequences, which is helpful for\nthe exploration of hidden semantic change, genre, instruments patterns, etc. In patterns analysis, users should be allowed to explore hierarchical information efficiently and validate whether the potential uncertainty issue is avoided or not.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Music", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "media": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "The line width of the \u201clink\u201d indicates the similar_x0002_ity value, which is the reciprocal of euclidean distance based on the feature vectors. Thicker line width indicates that the similarity between two music sequences is larger.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "To help users compare semantic features and explore the semantic change trends, we design a node-link graph to present statistics of music sequences (G3 & G4).This graph displays the basic information of music sequences and the similarity information between neighbor music sequences based on the feature vectors. In the sequence node-link graph, as shown in Fig. 6c, each music sequence is represented by a \u201cnode\u201d, which is a combination of one circle and three donuts. In the \u201cnode\u201d, the fill color of the circle represents the genre of the music sequence. As shown in Fig. 6c, the combined donuts are donut 1, donut 2, and donut 3 from inner to outer. The donut 1 displays the element number of the music sequence. The donut 2 reveals the proportions of notes and chords in the music sequence. In the donut 2, the light color part displays the ratio of notes in the music sequence, while another deep color part corresponds the ratio of chords. The donut 3 presents the ratios of instruments in the music sequence. In the donut 3, each part indicates statistic information of one instrument with four visual encodings including fill color, proportion, inner radius, and outer radius. The fill color is encoded as the instrument name. The proportion shows the ratio of the instrument in the music sequence. The inner radius displays the lowest pitch value performed by the instrument in the music sequence, while the outer radius displays the highest pitch value. As shown in Fig. 6c, the similarity between two music sequences is encoded as the \u201clink\u201d, which is a gray line in this graph. The line width of the \u201clink\u201d indicates the similar_x0002_ity value, which is the reciprocal of euclidean distance based on the feature vectors. Thicker line width indicates that the similarity between two music sequences is larger.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Pie", "axial_code": [], "componenet_code": ["pie"]}]}, {"author": "dxf", "index_original": 70, "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence", "pub_year": 2023, "domain": "semantic analysis", "requirement": {"requirement_text": "G4: Exploration of semantic changes. The visualization should allow users to mine the semantic changes, which is helpful for the exploration and analysis of distribution patterns in the projection space. ", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Music", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "media": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "The line width of the \u201clink\u201d indicates the similar_x0002_ity value, which is the reciprocal of euclidean distance based on the feature vectors. Thicker line width indicates that the similarity between two music sequences is larger.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "To help users compare semantic features and explore the semantic change trends, we design a node-link graph to present statistics of music sequences (G3 & G4).This graph displays the basic information of music sequences and the similarity information between neighbor music sequences based on the feature vectors. In the sequence node-link graph, as shown in Fig. 6c, each music sequence is represented by a \u201cnode\u201d, which is a combination of one circle and three donuts. In the \u201cnode\u201d, the fill color of the circle represents the genre of the music sequence. As shown in Fig. 6c, the combined donuts are donut 1, donut 2, and donut 3 from inner to outer. The donut 1 displays the element number of the music sequence. The donut 2 reveals the proportions of notes and chords in the music sequence. In the donut 2, the light color part displays the ratio of notes in the music sequence, while another deep color part corresponds the ratio of chords. The donut 3 presents the ratios of instruments in the music sequence. In the donut 3, each part indicates statistic information of one instrument with four visual encodings including fill color, proportion, inner radius, and outer radius. The fill color is encoded as the instrument name. The proportion shows the ratio of the instrument in the music sequence. The inner radius displays the lowest pitch value performed by the instrument in the music sequence, while the outer radius displays the highest pitch value. As shown in Fig. 6c, the similarity between two music sequences is encoded as the \u201clink\u201d, which is a gray line in this graph. The line width of the \u201clink\u201d indicates the similar_x0002_ity value, which is the reciprocal of euclidean distance based on the feature vectors. Thicker line width indicates that the similarity between two music sequences is larger.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Pie", "axial_code": [], "componenet_code": ["pie"]}]}, {"author": "dxf", "index_original": 71, "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence", "pub_year": 2023, "domain": "semantic analysis", "requirement": {"requirement_text": "G4: Exploration of semantic changes. The visualization should allow users to mine the semantic changes, which is helpful for the exploration and analysis of distribution patterns in the projection space. ", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Music", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "media": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "Inspired by the idea proposed by Galex [26], which extracts article features based on the article summarization data by using Doc2Vec to generate the feature vectors for articles, we apply Doc2Vec to compute the music vector representations by using music text representation. At first, we use text representations of the music sequences to train the Doc2Vec model. Then, we compute the corresponding feature vector based on the text representation of each music sequence. As a result, each music MIDI file is extracted as a musical semantic sequence, and each musical semantic sequence equips a corresponding 128-dimension feature vector by constructing the text representation and employing the Doc2Vec model.", "solution_category": "data_manipulation", "solution_axial": "Modeling;Rectification", "solution_compoent": "", "axial_code": ["Modeling", "Rectification"], "componenet_code": ["modeling", "rectification"]}, {"solution_text": "To visualize the distribution pattern among music more intuitively, we project the result of Doc2Vec to a 2D plane. We have considered three alternative dimension reduction methods includes principal component analysis (PCA) [12], multidimensional scaling (MDS) [24], and and t-distributed\nstochastic neighbor embedding (t-SNE) [33]. Although these dimension reduction methods are based on different under_x0002_lying algorithms, we have examined multiple parameter groups to select a proper method to produce the distribu_x0002_tion results. The test data includes 6,632 musical sequence vectors, and the size of each vector is 1 _x0003_ 128. These vectors are equipped with 4 genre attributes which are represented by 4 color points including yellow (classical), green (folk), red (rock), and purple (jazz). As shown in Fig. 2b, the result of t-SNE has more intui_x0002_tive distribution patterns than the other two dimension reduction methods (MDS and PCA). Consequently, we select t-SNE to project musical sequences into a distribution view with one theoretical consideration. Compared with MDS and PCA, t-SNE maximally preserves local neighbor_x0002_hoods of each data point (high dimensional vector), which ensures each musical sequence to have its similar neighbors based on the feature vectors in the distribution view [49]. In the distribution view, we use points to represent the musical sequences. To assist users in discovering distribu_x0002_tion patterns intuitively, we map two music features (i.e., music genres and instruments) to the points. In addition, to help users recognize the distribution patterns in the distri_x0002_bution view, we equip the distribution view with two visual components: a density contour component and a pathfinding component. The density contour is helpful for users to reveal the overall density distribution of the numerous points. How_x0002_ever, the traditional density contour only considers the geo_x0002_metric position of points and ignores the similarity. Thus, inspired by the marching square algorithm [20] in the den_x0002_sity contour, we propose an extended algorithm to modify the generation of density contours based on the similarity ofmusical sequence vectors. We apply the similarity to modify the basic matrix in the original marching square algorithm. As shown in Fig. 2c, the left contour is the raw result. In the marching square algorithm matrix, each number repre_x0002_sents the number of musical sequences in a grid. Our algo_x0002_rithm calculates and integrates the musical sequence similarities to modify the density value of the original matrix. When the grid similarity is larger than the threshold, these grids tend to be similar so that the number in the two grids would be added with a virtual value to change the state of the matrix. After the steps mentioned above, the modified density contour is generated to ensure that similar sequences have the same contour line. From the density contour shown in the right part of Fig. 2c, the points indicat_x0002_ing the same musical genre (jazz) are circled by the con_x0002_tours, which may indicate the apparent similarity of thes musical sequences. Including the density contour, we integrate a semantic change path component into the distribution view based on our proposed pathfinding algorithm to help users analyze the semantic change from one pattern to another pattern. The pathfinding algorithm, whose perplexity is O(n2), inte_x0002_grates the distances of music sequence projection coordi_x0002_nates and feature vectors. This heuristic algorithm follows four steps (S1 to S4) to find a path from the source music sequence to the target one in the distribution view. Four steps are introduced as follows. S1 Divide music sequences into corresponding grids based on their projection coordinates. S2 Obtain music sequences in the neighbor search grids of the source music sequence. If the neighbor search grids have no available sequences, enlarge the scope of neigh_x0002_bor search grids to search. S3 Calculate and determine the smallest euclidean distance of feature vectors between the source music sequence and each music sequence obtained in step S2. S4 Put the music sequence (found in step S3) into the path and make it a new source music sequence. Repeat step S1 to step S4 until the target music sequence is in the neighbor search grids. To ensure each new source music sequence to be approaching the target music sequence, we set the princi_x0002_ples of neighbor search grids [44]. Comparing the coordi_x0002_nates of the source music sequence and the target one, as shown in Fig. 3, we summarize eight neighbor search grids schemes. The eight schemes are \u201cupper left, low right, upper right, low left, horizontally left, horizontally right, vertically upper, and vertically low\u201d based on position sta_x0002_tus from the source music sequence to the target one. Users can sample the path points (sequences) if the path has too many music sequences to analyze the semantic change. In addition, the scheme of grids can be customized in different scenes. In this work, we offer two grids schemes including 128 _x0003_ 128 (which is selected in case 2) and 256 _x0003_ 256.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "To visualize the distribution pattern among music more intuitively, we project the result of Doc2Vec to a 2D plane. We have considered three alternative dimension reduction methods includes principal component analysis (PCA) [12], multidimensional scaling (MDS) [24], and and t-distributed\nstochastic neighbor embedding (t-SNE) [33]. Although these dimension reduction methods are based on different under_x0002_lying algorithms, we have examined multiple parameter groups to select a proper method to produce the distribu_x0002_tion results. The test data includes 6,632 musical sequence vectors, and the size of each vector is 1 _x0003_ 128. These vectors are equipped with 4 genre attributes which are represented by 4 color points including yellow (classical), green (folk), red (rock), and purple (jazz). As shown in Fig. 2b, the result of t-SNE has more intui_x0002_tive distribution patterns than the other two dimension reduction methods (MDS and PCA). Consequently, we select t-SNE to project musical sequences into a distribution view with one theoretical consideration. Compared with MDS and PCA, t-SNE maximally preserves local neighbor_x0002_hoods of each data point (high dimensional vector), which ensures each musical sequence to have its similar neighbors based on the feature vectors in the distribution view [49]. In the distribution view, we use points to represent the musical sequences. To assist users in discovering distribu_x0002_tion patterns intuitively, we map two music features (i.e., music genres and instruments) to the points. In addition, to help users recognize the distribution patterns in the distri_x0002_bution view, we equip the distribution view with two visual components: a density contour component and a pathfinding component. The density contour is helpful for users to reveal the overall density distribution of the numerous points. How_x0002_ever, the traditional density contour only considers the geo_x0002_metric position of points and ignores the similarity. Thus, inspired by the marching square algorithm [20] in the den_x0002_sity contour, we propose an extended algorithm to modify the generation of density contours based on the similarity ofmusical sequence vectors. We apply the similarity to modify the basic matrix in the original marching square algorithm. As shown in Fig. 2c, the left contour is the raw result. In the marching square algorithm matrix, each number repre_x0002_sents the number of musical sequences in a grid. Our algo_x0002_rithm calculates and integrates the musical sequence similarities to modify the density value of the original matrix. When the grid similarity is larger than the threshold, these grids tend to be similar so that the number in the two grids would be added with a virtual value to change the state of the matrix. After the steps mentioned above, the modified density contour is generated to ensure that similar sequences have the same contour line. From the density contour shown in the right part of Fig. 2c, the points indicat_x0002_ing the same musical genre (jazz) are circled by the con_x0002_tours, which may indicate the apparent similarity of thes musical sequences. Including the density contour, we integrate a semantic change path component into the distribution view based on our proposed pathfinding algorithm to help users analyze the semantic change from one pattern to another pattern. The pathfinding algorithm, whose perplexity is O(n2), inte_x0002_grates the distances of music sequence projection coordi_x0002_nates and feature vectors. This heuristic algorithm follows four steps (S1 to S4) to find a path from the source music sequence to the target one in the distribution view. Four steps are introduced as follows. S1 Divide music sequences into corresponding grids based on their projection coordinates. S2 Obtain music sequences in the neighbor search grids of the source music sequence. If the neighbor search grids have no available sequences, enlarge the scope of neigh_x0002_bor search grids to search. S3 Calculate and determine the smallest euclidean distance of feature vectors between the source music sequence and each music sequence obtained in step S2. S4 Put the music sequence (found in step S3) into the path and make it a new source music sequence. Repeat step S1 to step S4 until the target music sequence is in the neighbor search grids. To ensure each new source music sequence to be approaching the target music sequence, we set the princi_x0002_ples of neighbor search grids [44]. Comparing the coordi_x0002_nates of the source music sequence and the target one, as shown in Fig. 3, we summarize eight neighbor search grids schemes. The eight schemes are \u201cupper left, low right, upper right, low left, horizontally left, horizontally right, vertically upper, and vertically low\u201d based on position sta_x0002_tus from the source music sequence to the target one. Users can sample the path points (sequences) if the path has too many music sequences to analyze the semantic change. In addition, the scheme of grids can be customized in different scenes. In this work, we offer two grids schemes including 128 _x0003_ 128 (which is selected in case 2) and 256 _x0003_ 256.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Scatter;ContourLine", "axial_code": [], "componenet_code": ["contour", "scatter"]}]}, {"author": "dxf", "index_original": 75, "paper_title": "Diagnosing Ensemble Few-Shot Classifiers", "pub_year": 2023, "domain": "Few-shot learning", "requirement": {"requirement_text": "R2: Improving the Quality of the Shots. The representativeness of the shots is essential for few-shot classification. As there are only a few labeled samples, mislabeled or confusing shots, such as the overlapped ones between two categories, decrease the model performance greatly. Removing such lowquality shots and adding necessary new ones improve the coverage of the shots and overall performance. When diagnosing an ensemble few-shot classifier, the experts need to understand the coverage of each shot and find the samples that are not well covered by the shots. In addition, the experts required a tool that can automatically recommend low-quality shots to be removed and candidate samples to be added to the shot set, so that they can only examine a small subset and then quickly decide which ones to remove/add.", "requirement_code": {"identify_main_cause_aggregate": 1}}, "data": {"data_text": "Images", "data_code": {"tables": 1, "media": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Base learner selection aims to find a small subset of diverse and cooperative base learners to better predict the input samples (fitness). Here, U refers to the set of base learners fukgK k\u00bc1, and V is the set of samples fxigN i\u00bc1. As sparse subset selection encourages diversity among the selected learners, we then extend it by considering fitness and cooperation. Accordingly, Eq. (2) is rewritten as: where the first term is the representation cost, the second term is the sparsity term that prefers the learners with higher fitness, and the third term is the cooperation term. a1 and a2 control the trade-off among the three terms. Follow_x0002_ing Elhamifar et al. [37], a1 \u00bc a2 \u00bc 0:5amax, amax is the maximum distance between learners. In the first term, to calculate the representation cost, we need to define the distance between a base learner and a sample. A straightforward way is based on the prediction accuracy. However, we cannot evaluate the accuracy with_x0002_out ground-truth labels. Instead, we use the prediction con-fidence to measure the distance because samples with high prediction confidence tend to be classified correctly [39]. The prediction confidence of learner uk on xi is defined as the difference between the largest and the second-largest probabilities in the predicted label distribution yi, which is denoted as mki 2 \u00bd0; 1_x0007_ . The distance between the learner uk and the sample xi is then defined by dki \u00bc 1 _x0008_ mki because we prefer the base learners with larger confidence mki. In the second term, to encourage the selection of base learners with higher fitness, we emphasize the ones that bet_x0002_ter predict the given shots. A widely used measure, likeli_x0002_hood, is employed to estimate the fitness value. Accordingly, we add _x0002_ k for each learner uk, which is defined as its negative log-likelihood on the shots. In the third term, to encourage the cooperation between two learners, uk and ul, we penalize the difference between their predictions. Let yki and yli be the label distribution of sample xi predicted by uk and ul, respectively. Following the previous work of Dvornik et al. [1], the prediction difference is measured by the symmetric KL-divergence between their predictions: mkl \u00bc PN i\u00bc1\u00f0KL\u00f0ykijjyli\u00de \u00fe KL\u00f0ylijjyki\u00de\u00de=\u00f02N\u00de. mkl is 0 if the two learners make the same predictions.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Next, to achieve better class separation [44], we employ t-SNE to project the samples onto 2D space and utilize a scatterplot to visualize the projections.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "Users can also examine the coverage of the shots in the scatterplot and replace the low-quality shots with the high-quality ones (R2). sample view (Fig. 9b) to present the shots and unlabeled samples in context (R2). The sample view (Fig. 9b) enables users to examine the shots in the context of samples and tune their selection. For each sample, we first concatenate the features extracted by the base learners. Next, to achieve better class separation [44], we employ t-SNE to project the samples onto 2D space and utilize a scatterplot to visualize the projections. In the scatterplot, stars and circles are used to represent shots and unlabeled samples, respectively. Samples are colored according to their classes, and those with a confidence less than 0.2 are colored gray. For each shot, we utilize a clutter-aware label-layout algorithm [45] to place the image content close to the shot and reduce the overlap with other scatter points. When users select the samples of interest, the image content and label distributions are displayed at the bottom of the view (Fig. 9b). The label distributions are represented by colored bars, where the color encodes the class, and the length encodes the prediction probability. Users can click the checkbox on the right side to add it as a shot or remove it from the shot set.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "When users select the samples of interest, the image content and label distributions are displayed at the bottom of the view (Fig. 9b). The label distributions are represented by colored bars, where the color encodes the class, and the length encodes the prediction probability. Users can click the checkbox on the right side to add it as a shot or remove it from the shot set.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "dxf", "index_original": 77, "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving", "pub_year": 2023, "domain": "spatial-temporal visual analytics", "requirement": {"requirement_text": "R1.1: Explore the timestamps of a time series the failure cases are mainly distributed at, and the states of the autonomous driving vehicle when failure cases happen (When).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Fig. 1-b2 shows the velocity, acceleration, and wheel steering data from the GPS monitoring of the vehicle, which provides a way to observe the states of the autonomous driving vehicle (R1.1). We integrated the line chart and two dashboards in AVS (the Autonomous Visualization System) [2] into Fig. 1-b2 to share the timeline in Temporal View and better align the analysis between the vehicle states and the model detection results in the following chart. The vertical line shows the values of line charts at the current time. The horizontal axis can be brushed. The corresponding data of the brushed time period will be highlighted in all views (R2.1).", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 78, "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving", "pub_year": 2023, "domain": "spatial-temporal visual analytics", "requirement": {"requirement_text": "R1.1: Explore the timestamps of a time series the failure cases are mainly distributed at, and the states of the autonomous driving vehicle when failure cases happen (When).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.", "data_code": {"tables": 1, "geometry": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Fig. 1-b3 visualizes the detection sequences of objects from the tracklets, as well as the detection results from the model. This chart shows the correlation between object-level features and detection results in temporal distribution and further combines the analysis tasks of When and How (R1.1, R1.3).", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "dxf", "index_original": 79, "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving", "pub_year": 2023, "domain": "spatial-temporal visual analytics", "requirement": {"requirement_text": "R1.1: Explore the timestamps of a time series the failure cases are mainly distributed at, and the states of the autonomous driving vehicle when failure cases happen (When).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.", "data_code": {"tables": 1, "geometry": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Trajectory(Fig. 1-c2) combines the relative position in the bird\u2019s eye view from tracklets and detection results, and arranges them in the order of detection. Developers can combine such information in answering the questions of When and Where (R1.1, R1.2). The basic trajectory visualization method such as trajectory clustering by Lee et al. [31] usually visualizes the sequences that connect the absolute positions of the object. We combined the detection results and detection field with the object trajectory to have a global sense of the distribution of detection results in space. Considering that a global view in Scene is provided, the ego-centric view centered on the autonomous vehicle is also important, so we adopted such an ego-centric visualization form and relative positions. The rectangle at the bottom represents the camera of the autonomous driving vehicle, which is fixed as a reference. The arc in the view marks the position of 20 meters away from the camera, as a reference line for the relative distance perception. Each point indicates one object in one detection, and each line represents the trajectory of an object, and the direction of each arrow is the movement direction of the last two detections\u2019 positions of one object, which always combines trajectory lines to indicate the whole segment overall direction of relative movement. The dashed line indicates discon_x0002_tinuous detection time, i.e., the object has disappeared between the beginning and end detection of the dashed line. As shown in Fig. 7, three different trajectory patterns could be visualized. Developers can zoom in and brush later (R1.2). The brushed objects and corresponding paths will be reserved and redrawn. The update of other views will be triggered accordingly as well (R1.2, R2.1).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}, {"solution_text": "Developers can zoom in and brush later (R1.2). The brushed objects and corresponding paths will be reserved and redrawn. The update of other views will be triggered accordingly as well (R1.2, R2.1).", "solution_category": "interaction", "solution_axial": "Encode;Filtering;OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Encode", "Filtering"], "componenet_code": ["overview_and_explore", "encode", "filtering"]}]}, {"author": "dxf", "index_original": 80, "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving", "pub_year": 2023, "domain": "spatial-temporal visual analytics", "requirement": {"requirement_text": "R1.2: Explore the surroundings, positions of failure cases, especially the relative positions to the camera (Where).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.", "data_code": {"tables": 1, "geometry": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Trajectory(Fig. 1-c2) combines the relative position in the bird\u2019s eye view from tracklets and detection results, and arranges them in the order of detection. Developers can combine such information in answering the questions of When and Where (R1.1, R1.2). The basic trajectory visualization method such as trajectory clustering by Lee et al. [31] usually visualizes the sequences that connect the absolute positions of the object. We combined the detection results and detection field with the object trajectory to have a global sense of the distribution of detection results in space. Considering that a global view in Scene is provided, the ego-centric view centered on the autonomous vehicle is also important, so we adopted such an ego-centric visualization form and relative positions. The rectangle at the bottom represents the camera of the autonomous driving vehicle, which is fixed as a reference. The arc in the view marks the position of 20 meters away from the camera, as a reference line for the relative distance perception. Each point indicates one object in one detection, and each line represents the trajectory of an object, and the direction of each arrow is the movement direction of the last two detections\u2019 positions of one object, which always combines trajectory lines to indicate the whole segment overall direc_x0002_tion of relative movement. The dashed line indicates discon_x0002_tinuous detection time, i.e., the object has disappeared between the beginning and end detection of the dashed line. As shown in Fig. 7, three different trajectory patterns could be visualized. Developers can zoom in and brush later (R1.2). The brushed objects and corresponding paths will be reserved and redrawn. The update of other views will be triggered accordingly as well (R1.2, R2.1).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}, {"solution_text": "Developers can zoom in and brush later (R1.2). The brushed objects and corresponding paths will be reserved and redrawn. The update of other views will be triggered accordingly as well (R1.2, R2.1).", "solution_category": "interaction", "solution_axial": "Encode;Filtering;OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Encode", "Filtering"], "componenet_code": ["overview_and_explore", "encode", "filtering"]}]}, {"author": "dxf", "index_original": 84, "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving", "pub_year": 2023, "domain": "spatial-temporal visual analytics", "requirement": {"requirement_text": "R1.3: Observe whether the feature maps of failure cases exhibit abnormalities compared to normals(How).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.", "data_code": {"tables": 1, "geometry": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The processed features and detection results are combined in Fig. 1-a2. It visualizes the processed features of each detection and corresponds to IoU on a twodimensional coordinate system, providing an overview to observe the relative distance and distribution of the objectlevel features, and further answers the question of How the model fails (R1.3).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Scatter", "axial_code": [], "componenet_code": ["scatter"]}]}, {"author": "dxf", "index_original": 85, "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving", "pub_year": 2023, "domain": "spatial-temporal visual analytics", "requirement": {"requirement_text": "R1.3: Observe whether the feature maps of failure cases exhibit abnormalities compared to normals(How).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.", "data_code": {"tables": 1, "geometry": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Fig. 1-b3 visualizes the detection sequences of objects from the tracklets, as well as the detection results from the model. This chart shows the correlation between object-level features and detection results in temporal distribution and further combines the analysis tasks of When and How (R1.1, R1.3).", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "dxf", "index_original": 86, "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving", "pub_year": 2023, "domain": "spatial-temporal visual analytics", "requirement": {"requirement_text": "R2.1: In a whole sequence of images, for objects poorly detected over a period of interest, observe their surroundings and relative positions to the autonomous driving vehicle, their detection results, abnormalities of the feature maps, and the relationship between the detection results, the spatial location and the features (When ! Where and How).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.", "data_code": {"tables": 1, "geometry": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "This view can be replayed over time so that model developers can review how the feature maps, detection results, and surroundings change over time (R2.1).", "solution_category": "interaction", "solution_axial": "History", "solution_compoent": "", "axial_code": ["History"], "componenet_code": ["history"]}]}, {"author": "dxf", "index_original": 87, "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving", "pub_year": 2023, "domain": "spatial-temporal visual analytics", "requirement": {"requirement_text": "R2.1: In a whole sequence of images, for objects poorly detected over a period of interest, observe their surroundings and relative positions to the autonomous driving vehicle, their detection results, abnormalities of the feature maps, and the relationship between the detection results, the spatial location and the features (When ! Where and How).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.", "data_code": {"tables": 1, "geometry": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Fig. 1-b1 is a timeline that provides the horizontal axis for the following charts. It can be selected, dragged and played, and the detected objects in current time will be visualized in blue in all the views (R2.1).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}, {"solution_text": "Fig. 1-b1 is a timeline that provides the horizontal axis for the following charts. It can be selected, dragged and played, and the detected objects in current time will be visualized in blue in all the views (R2.1).", "solution_category": "interaction", "solution_axial": "Selecting;Filtering", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "dxf", "index_original": 88, "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving", "pub_year": 2023, "domain": "spatial-temporal visual analytics", "requirement": {"requirement_text": "R2.1: In a whole sequence of images, for objects poorly detected over a period of interest, observe their surroundings and relative positions to the autonomous driving vehicle, their detection results, abnormalities of the feature maps, and the relationship between the detection results, the spatial location and the features (When ! Where and How).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.", "data_code": {"tables": 1, "geometry": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Fig. 1-b2 shows the velocity, acceleration, and wheel steering data from the GPS monitoring of the vehicle, which provides a way to observe the states of the autonomous driving vehicle (R1.1). We inte_x0002_grated the line chart and two dashboards in AVS (the Autonomous Visualization System) [2] into Fig. 1-b2 to share the timeline in Temporal View and better align the anal_x0002_ysis between the vehicle states and the model detection results in the following chart. The vertical line shows the values of line charts at the current time. The horizontal axis can be brushed. The corresponding data of the brushed time period will be highlighted in all views (R2.1).", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}, {"solution_text": "Fig. 1-b2 shows the velocity, acceleration, and wheel steering data from the GPS monitoring of the vehicle, which provides a way to observe the states of the autonomous driving vehicle (R1.1). We inte_x0002_grated the line chart and two dashboards in AVS (the Autonomous Visualization System) [2] into Fig. 1-b2 to share the timeline in Temporal View and better align the anal_x0002_ysis between the vehicle states and the model detection results in the following chart. The vertical line shows the values of line charts at the current time. The horizontal axis can be brushed. The corresponding data of the brushed time period will be highlighted in all views (R2.1).", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "dxf", "index_original": 89, "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving", "pub_year": 2023, "domain": "spatial-temporal visual analytics", "requirement": {"requirement_text": "R2.2: Similarly, for objects detected in a distance from the autonomous driving vehicle, observe the timestamps they are distributed at, whether the feature maps of failure cases exhibit abnormalities compared to the normals, the detection results of the objects, and the relationship between the spatial distribution and the other three: temporal distribution, feature distribution and detection results (Where ! When and How).", "requirement_code": {"discover_observation": 1, "explain_differences": 1}}, "data": {"data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.", "data_code": {"tables": 1, "geometry": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Brushing the objects closer to the camera in Fig. 8-a3, we observe that their distribution is more concentrated in Fig. 8-c3 and this is consistent with the previous analysis that cyclists and pedestrians mostly appear closer to the camera in Fig. 8-d3 (R2.2).", "solution_category": "interaction", "solution_axial": "History", "solution_compoent": "", "axial_code": ["History"], "componenet_code": ["history"]}]}, {"author": "dxf", "index_original": 90, "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving", "pub_year": 2023, "domain": "spatial-temporal visual analytics", "requirement": {"requirement_text": "R2.3: For objects with abnormal feature maps, show their temporal information, their spatial information such as relative positions to the autonomous driving vehicle and surroundings, as well as their detection results. Moreover, the relationship between them (How ! When and Where).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.", "data_code": {"tables": 1, "geometry": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "This chart supports zooming in or out and brushing a polygon area of projection points. The selected objects will be highlighted in this chart and the update of other views will be triggered accordingly (R2.3).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}, {"solution_text": "This chart supports zooming in or out and brushing a polygon area of projection points. The selected objects will be highlighted in this chart and the update of other views will be triggered accordingly (R2.3).", "solution_category": "interaction", "solution_axial": "Selecting;Filtering", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "dxf", "index_original": 91, "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving", "pub_year": 2023, "domain": "spatial-temporal visual analytics", "requirement": {"requirement_text": "R2.3: For objects with abnormal feature maps, show their temporal information, their spatial information such as relative positions to the autonomous driving vehicle and surroundings, as well as their detection results. Moreover, the relationship between them (How ! When and Where).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.", "data_code": {"tables": 1, "geometry": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The control in Fig. 1-d2 takes the form of parallel coordinates to show the high-dimensional labeled data, which is plotted with IoU and locations of labeled objects, including detailed relative orientation (alpha and rotation-y) and position (x, y, z in the camera coordinate system: rightward, upward, and forward distances). Ignored objects are not drawn on the first axis, i.e., IoU, since they are not included in the AP calculation. Each axis can be brushed, and the intersection will be taken to filter the data (R1, R2.3).", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}, {"solution_text": "The control in Fig. 1-d2 takes the form of parallel coordinates to show the high-dimensional labeled data, which is plotted with IoU and locations of labeled objects, including detailed relative orientation (alpha and rotation-y) and position (x, y, z in the camera coordinate system: rightward, upward, and forward distances). Ignored objects are not drawn on the first axis, i.e., IoU, since they are not included in the AP calculation. Each axis can be brushed, and the intersection will be taken to filter the data (R1, R2.3).", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "dxf", "index_original": 92, "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving", "pub_year": 2023, "domain": "spatial-temporal visual analytics", "requirement": {"requirement_text": "R2.3: For objects with abnormal feature maps, show their temporal information, their spatial information such as relative positions to the autonomous driving vehicle and surroundings, as well as their detection results. Moreover, the relationship between them (How ! When and Where).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.", "data_code": {"tables": 1, "geometry": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Each density map has a \u201cScale Up\u201d function for easy viewing when hovering as shown in Fig. 1-b3. Developers can click the detection sequence of one object or the final density map of interest to select an object (R2.3), which would also display the density maps of the whole sequence of this object at the bottom.", "solution_category": "interaction", "solution_axial": "Selecting;Abstract/Elaborate", "solution_compoent": "", "axial_code": ["Selecting", "Abstract/Elaborate"], "componenet_code": ["selecting", "abstract_elaborate"]}]}, {"author": "dxf", "index_original": 93, "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving", "pub_year": 2023, "domain": "spatial-temporal visual analytics", "requirement": {"requirement_text": "R3.1: In a sequence of images, for objects detected in a spatial-temporal range of interest, observe whether the feature maps of failure cases exhibit abnormalities compared to the normals (When and Where !How).", "requirement_code": {"discover_observation": 1, "explain_differences": 1}}, "data": {"data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.", "data_code": {"tables": 1, "geometry": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We chose other easily distinguishable colors referring to ColorBrewer [23] to encode interaction-related results. As shown in Fig. 1-e, the objects highlighted with orange, blue, and yellow strokes are linked to the selected object, current time and intersection of them in all other views (R3.1).", "solution_category": "interaction", "solution_axial": "Selecting;Abstract/Elaborate;Connect/Relate", "solution_compoent": "", "axial_code": ["Selecting", "Abstract/Elaborate", "Connect/Relate"], "componenet_code": ["selecting", "abstract_elaborate", "connect_relate"]}]}, {"author": "dxf", "index_original": 94, "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving", "pub_year": 2023, "domain": "spatial-temporal visual analytics", "requirement": {"requirement_text": "R3.1: In a sequence of images, for objects detected in a spatial-temporal range of interest, observe whether the feature maps of failure cases exhibit abnormalities compared to the normals (When and Where !How).", "requirement_code": {"discover_observation": 1, "explain_differences": 1}}, "data": {"data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.", "data_code": {"tables": 1, "geometry": 1, "media": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Two of the three views are used as an overview together to explore the other one and the control panel. For example, combining the spatial-temporal information and detection results from the Temporal View, parallel coordinate diagram and Spatial View, developers can select the objects by the iterative brush of their interest and playing the timeline. Then, they are capable of observing the selected objects in other views. This interaction helps to explore how objects in some spatial-temporal regions are detected, their feature distribution, detection results, and the relationship between results and features (R3.1).", "solution_category": "interaction", "solution_axial": "Connect/Related;Selelcting", "solution_compoent": "", "axial_code": ["Selelcting", "Connect/Related"], "componenet_code": ["selecting", "connect_relate"]}]}, {"author": "dxf", "index_original": 96, "paper_title": "DOMINO: Visual Causal Reasoning With Time-Dependent Phenomena", "pub_year": 2023, "domain": "Causality analysis", "requirement": {"requirement_text": "T1: Generating causal propositions and hypotheses is often the first step in causality analysis. Most current works on temporal causality achieve this either by manually grouping relevant values and then assigning them semantic meanings, or by conducting an exhaustive search after evenly partitioning the time series data into a large amount of sections each considered an event. Both of these approaches are limited in efficiency and flexibility. Since in logic-based causality a causal relation is defined over a time lagged conditional distribution, analysts should be given direct access to such information by allowing them to generate causal propositions and hypotheses with visual support. Also, since an effect can have multiple causes, an overview of the values and boolean labels of each time series in a synchronized fashion will help the understanding of the compound relations.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "the Air Quality dataset", "data_code": {"tables": 1}}, "solution": [{"solution_text": "The testing of an event c being a cause of the effect e is based on the assumption that a true cause always increases the probability of the effect (we can view a preventative, which lowers the probability of e, as raising the probability of :e). Thus, we say c is a potential cause [25] of e if, taking into con_x0002_sideration the relative time delay window, it satisfies P\u00f0e\u00de < p and P\u00f0ejc\u00de _x0005_ p; (2) where P\u00f0ejc\u00de is calculated in the same fashion as Eq. (1). Additionally, if the effect e is defined on a continuous variable ve and we are looking for events that are potentially shifting the distribution of ve (as opposed to a value of ve falling into a specific range), the expected value of ve can be used instead for better sensitivity to change [23]. As such, c is considered a potential cause of e when E\u00bdve_x0004_ \u00bc E\u00bdvejc_x0004_ : (3) Here, the \u00bc sign can be replaced by either > or < to stipu_x0002_late only positive or negative causes. And the conditional expected value can be calculated as E\u00bdvejc_x0004_ \u00bc XyyQ\u00f0\u00bdve \u00bc y_x0004_ ^ c\u00deQ\u00f0c\u00de ; (4) where y are values in ve\u2019s domain and Q\u00f0x\u00de denotes the number of time points where x holds. To illustrate, Fig. 1 shows short sequences of a continu_x0002_ous variable ve and a causal event c. Averaging all values of ve, we have E\u00bdve_x0004_ =1.5. Then, when considering a time shift of exactly 1 unit, we have E\u00bdvejc_x0004_\u00bc\u00f00:9 \u00fe 3 \u00fe 2:3 \u00fe 1:3\u00de=4 \u00bc 1:875 (these are the values of ve exactly 1 unit after c is T). Since E\u00bdvejc_x0004_ > E\u00bde_x0004_ , according to Eq. (3), c increases the expected value of ve and thus is a potential cause of it. However, if we try to find the positive cause by instead bounding ve to a specific range, or to a specific value such as the mean of ve the event e would be defined as [ve > E\u00bdve_x0004_ ]. Then we would have P\u00f0e\u00de \u00bc 0:5 (e occurs 4 times out of 8 time points) and P\u00f0ejc\u00de \u00bc 0:5 (2 out of 4), where c would not be considered a potential cause because it is not raising the probability of e. This shows the reduced sensitivity to change that comes with trying to be more specific. We can generalize this framework to a set of causes X of an effect e. We measure the influence of X towards e by cal_x0002_culating the probability change of e as P\u00f0ejX\u00de _x0007_ P\u00f0e\u00de or the change of expected value of ve as E\u00bdvejX_x0004_ _x0007_ E\u00bdve_x0004_ , depend_x0002_ing on the definition of e. Note that while the conditional probability is bounded within \u00bd0; 1_x0004_ , the expected value could be any amount, and either positive or negative. As mentioned, a causal relation is only potential; it may not be direct or it may be spurious, even if Eqs. (2) or (3) holds. This can be due to two possible situations: (1) c and e are actually independent but are commonly caused by another event x (the confounder) with c being caused earlier than e (Fig. 2a), or (2) c causes e indirectly via x (mediation, Fig. 2b). In either condition, we may observe that Eqs. (2) or (3) holds and erroneously mark c as directly causing e. One way to eliminate such error is to compare the distribution of e when c and x both occur, i.e., P\u00f0ejc ^ x\u00de, to that when only x is present, i.e., P\u00f0ej:c ^ x\u00de. Then the two will be found equal (or almost equal) if c is a spurious cause of e. Note that this requires the time window \u00bdr; s_x0004_ to be sufficiently wide such that both x and c could have caused e [24]. This idea can be generalized to multivariate time series. When considering multiple time series in a dataset, for a given effect, we usually can recognize a number of potential causes. To identify the real causes that can better explain the effect, Eells [8] proposed the average significance of a potential cause c, among all potential causes X toward the effect e, calculated as \"avg\u00f0c; e\u00de \u00bc X x2X=c P\u00f0ejc ^ x\u00de _x0007_ P\u00f0ej:c ^ x\u00de jX=cj : (5) Here X=c is the set of potential causes excluding c and jX=cj is the number of events in it. We need at least two potential causes to make the computation meaningful and all calcula_x0002_tions are associated with a preset time window. Then, by setting a certain threshold \", c is called an \"-significant cause of e if j\"avg\u00f0c; e\u00dej _x0005_ \". Further, if e stands for the increase or decrease of a continuous variable ve over the time window, the conditional probability in Eq. (5) can be replaced by the conditional expected value such that \"avg\u00f0c; e\u00de \u00bc X x2X= E\u00bdvejc ^ x_x0004_ _x0007_ E\u00bdvej:c ^ x_x0004_ jX=cj : (6) Although the \" threshold is decisive in testing if a cause is significant, its value can be difficult to determine auto_x0002_matically in practice. In presence of a large number of (say, thousands of) potential causes where significant causes are rare, all such \"avg values usually follow a Gaussian distribu_x0002_tion [23]. As a result, the problem can be solved by testing the significance of a null hypothesis where p values rejecting the null hypothesis deviate from the mean [9]. However, we find that this theoretical method cannot really be applied in most of our applications since we rarely encounter such a large number of time-series and causal events, especially when we just wish to explore the impact of some specific causes on the target. In such cases, the \" threshold can only be assigned empirically and interactively by the analyst. This requirement for user assistance, together with other analytical tasks that will be discussed later, motivated the visual analytics system that is at the heart of our work. Since a potential cause elevates the probability (Eq. (2)) oralters the expected value (Eq. (3)) of the effect, the processof searching for a cause c is the same as deciding an appro\\x02priate numerical constraint on the cause variable vc, onwhich c is made, so that Eqs. (2) or (3) can be satisfied. Thisis relatively easy and straightforward when vc has discretevalues, where we can simply scan through vc\u2019s domain andmake c take all the values satisfying the condition.The search becomes more complex when vc\u2019s domain iscontinuous. One solution could be to discretize vc and thenapply the same scanning process, but determining a gooddiscretization strategy is difficult. Our approach is toinstead only look at vc at time points t, noted as vc\u00f0t\u00de, wheree holds after the specified time delay (vc\u00f0t\u00de\u02c6 \\x05 r;\\x03 se). Record\\x02ing all such vc\u00f0t\u00de as Tc, we then discretize vc adaptively byclustering values in Tc. The idea is to consider values that vcfrequently takes and lead to e as possibly causing e.The clustering process follows Algorithm 3.3, whichtakes the same approach as the incremental clustering forhigh-dimensional data [41] but is applied in 1-D. The original algorithm has shown great performance in learning 'informative value segments. The adapted version here iteratively scans values in Tc until all clusters converge or themaximum number of iterations is reached. In each iteration,a value is assigned to a cluster center if the distance betweenthem is smaller than some threshold u. A new cluster isadded when a point is too far away from all clusters. Thethreshold u controls the size of the clusters, which decideshow vc will be discretized later. At last, we transform vc byconsidering the value range each cluster covers as a level,and test if it fulfills Eqs. (2) or (3). If multiple levels arereturned, we try to merge them if they overlap and take theone that best elevates e as the most possible cause. 'The algorithm can be easily parallelized [41] if we modify the incremental process such that it searches clusters inbatches instead of inspecting them one by one, enablingscalability. Also, the trade-off of taking different u valuesis that a larger u tends to produce a looser constraint (alarger value range of vc) in c, often resulting in a smallerP\u00f0ejc\u00de or an E\u00bdvejccloser to E\u00bdve\u2013 a smaller u does theopposite. This is similar to the problem of under-/over-fitting. In our experiments, we found when u equals 0.15 ofvc\u2019s value range 5 iterations were usually sufficient toreach a plausible result \u2013 one that would be close to manual adjustment.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The testing of an event c being a cause of the effect e is based on the assumption that a true cause always increases the probability of the effect (we can view a preventative, which lowers the probability of e, as raising the probability of :e). Thus, we say c is a potential cause [25] of e if, taking into con_x0002_sideration the relative time delay window, it satisfies P\u00f0e\u00de < p and P\u00f0ejc\u00de _x0005_ p; (2) where P\u00f0ejc\u00de is calculated in the same fashion as Eq. (1). Additionally, if the effect e is defined on a continuous variable ve and we are looking for events that are potentially shifting the distribution of ve (as opposed to a value of ve falling into a specific range), the expected value of ve can be used instead for better sensitivity to change [23]. As such, c is considered a potential cause of e when E\u00bdve_x0004_ \u00bc E\u00bdvejc_x0004_ : (3) Here, the \u00bc sign can be replaced by either > or < to stipu_x0002_late only positive or negative causes. And the conditional expected value can be calculated as E\u00bdvejc_x0004_ \u00bc XyyQ\u00f0\u00bdve \u00bc y_x0004_ ^ c\u00deQ\u00f0c\u00de ; (4) where y are values in ve\u2019s domain and Q\u00f0x\u00de denotes the number of time points where x holds. To illustrate, Fig. 1 shows short sequences of a continu_x0002_ous variable ve and a causal event c. Averaging all values of ve, we have E\u00bdve_x0004_ =1.5. Then, when considering a time shift of exactly 1 unit, we have E\u00bdvejc_x0004_\u00bc\u00f00:9 \u00fe 3 \u00fe 2:3 \u00fe 1:3\u00de=4 \u00bc 1:875 (these are the values of ve exactly 1 unit after c is T). Since E\u00bdvejc_x0004_ > E\u00bde_x0004_ , according to Eq. (3), c increases the expected value of ve and thus is a potential cause of it. However, if we try to find the positive cause by instead bounding ve to a specific range, or to a specific value such as the mean of ve the event e would be defined as [ve > E\u00bdve_x0004_ ]. Then we would have P\u00f0e\u00de \u00bc 0:5 (e occurs 4 times out of 8 time points) and P\u00f0ejc\u00de \u00bc 0:5 (2 out of 4), where c would not be considered a potential cause because it is not raising the probability of e. This shows the reduced sensitivity to change that comes with trying to be more specific. We can generalize this framework to a set of causes X of an effect e. We measure the influence of X towards e by cal_x0002_culating the probability change of e as P\u00f0ejX\u00de _x0007_ P\u00f0e\u00de or the change of expected value of ve as E\u00bdvejX_x0004_ _x0007_ E\u00bdve_x0004_ , depend_x0002_ing on the definition of e. Note that while the conditional probability is bounded within \u00bd0; 1_x0004_ , the expected value could be any amount, and either positive or negative. As mentioned, a causal relation is only potential; it may not be direct or it may be spurious, even if Eqs. (2) or (3) holds. This can be due to two possible situations: (1) c and e are actually independent but are commonly caused by another event x (the confounder) with c being caused earlier than e (Fig. 2a), or (2) c causes e indirectly via x (mediation, Fig. 2b). In either condition, we may observe that Eqs. (2) or (3) holds and erroneously mark c as directly causing e. One way to eliminate such error is to compare the distribution of e when c and x both occur, i.e., P\u00f0ejc ^ x\u00de, to that when only x is present, i.e., P\u00f0ej:c ^ x\u00de. Then the two will be found equal (or almost equal) if c is a spurious cause of e. Note that this requires the time window \u00bdr; s_x0004_ to be sufficiently wide such that both x and c could have caused e [24]. This idea can be generalized to multivariate time series. When considering multiple time series in a dataset, for a given effect, we usually can recognize a number of potential causes. To identify the real causes that can better explain the effect, Eells [8] proposed the average significance of a potential cause c, among all potential causes X toward the effect e, calculated as \"avg\u00f0c; e\u00de \u00bc X x2X=c P\u00f0ejc ^ x\u00de _x0007_ P\u00f0ej:c ^ x\u00de jX=cj : (5) Here X=c is the set of potential causes excluding c and jX=cj is the number of events in it. We need at least two potential causes to make the computation meaningful and all calcula_x0002_tions are associated with a preset time window. Then, by setting a certain threshold \", c is called an \"-significant cause of e if j\"avg\u00f0c; e\u00dej _x0005_ \". Further, if e stands for the increase or decrease of a continuous variable ve over the time window, the conditional probability in Eq. (5) can be replaced by the conditional expected value such that \"avg\u00f0c; e\u00de \u00bc X x2X= E\u00bdvejc ^ x_x0004_ _x0007_ E\u00bdvej:c ^ x_x0004_ jX=cj : (6) Although the \" threshold is decisive in testing if a cause is significant, its value can be difficult to determine auto_x0002_matically in practice. In presence of a large number of (say, thousands of) potential causes where significant causes are rare, all such \"avg values usually follow a Gaussian distribu_x0002_tion [23]. As a result, the problem can be solved by testing the significance of a null hypothesis where p values rejecting the null hypothesis deviate from the mean [9]. However, we find that this theoretical method cannot really be applied in most of our applications since we rarely encounter such a large number of time-series and causal events, especially when we just wish to explore the impact of some specific causes on the target. In such cases, the \" threshold can only be assigned empirically and interactively by the analyst. This requirement for user assistance, together with other analytical tasks that will be discussed later, motivated the visual analytics system that is at the heart of our work. Since a potential cause elevates the probability (Eq. (2)) oralters the expected value (Eq. (3)) of the effect, the processof searching for a cause c is the same as deciding an appro\\x02priate numerical constraint on the cause variable vc, onwhich c is made, so that Eqs. (2) or (3) can be satisfied. Thisis relatively easy and straightforward when vc has discretevalues, where we can simply scan through vc\u2019s domain andmake c take all the values satisfying the condition.The search becomes more complex when vc\u2019s domain iscontinuous. One solution could be to discretize vc and thenapply the same scanning process, but determining a gooddiscretization strategy is difficult. Our approach is toinstead only look at vc at time points t, noted as vc\u00f0t\u00de, wheree holds after the specified time delay (vc\u00f0t\u00de\u02c6 \\x05 r;\\x03 se). Record\\x02ing all such vc\u00f0t\u00de as Tc, we then discretize vc adaptively byclustering values in Tc. The idea is to consider values that vcfrequently takes and lead to e as possibly causing e.The clustering process follows Algorithm 3.3, whichtakes the same approach as the incremental clustering forhigh-dimensional data [41] but is applied in 1-D. The original algorithm has shown great performance in learning 'informative value segments. The adapted version here iteratively scans values in Tc until all clusters converge or themaximum number of iterations is reached. In each iteration,a value is assigned to a cluster center if the distance betweenthem is smaller than some threshold u. A new cluster isadded when a point is too far away from all clusters. Thethreshold u controls the size of the clusters, which decideshow vc will be discretized later. At last, we transform vc byconsidering the value range each cluster covers as a level,and test if it fulfills Eqs. (2) or (3). If multiple levels arereturned, we try to merge them if they overlap and take theone that best elevates e as the most possible cause. 'The algorithm can be easily parallelized [41] if we modify the incremental process such that it searches clusters inbatches instead of inspecting them one by one, enablingscalability. Also, the trade-off of taking different u valuesis that a larger u tends to produce a looser constraint (alarger value range of vc) in c, often resulting in a smallerP\u00f0ejc\u00de or an E\u00bdvejccloser to E\u00bdve\u2013 a smaller u does theopposite. This is similar to the problem of under-/over-fitting. In our experiments, we found when u equals 0.15 ofvc\u2019s value range 5 iterations were usually sufficient toreach a plausible result \u2013 one that would be close to manual adjustment.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Our design of the conditional distribution view was motivated by the definition of a potential cause mentioned in Section 3. With the conditional distributions displayed here, analysts can directly observe a time-lagged phenomenon and hence make causal hypotheses (T1,T4). Upon selecting and loading a tabular data file of a multif_x0002_variate time series (top menu) the analyst selects one of the variables as the effect variable e and specifies the effect of interest, such as Value Increase, Value Decrease, or Value Range(second menu row). Upon selecting e the system displays a histogram of its values ve over the entire time series duration(blue bars). This histogram can be brushed when the selected effect is Value Range (the Brushed box, top right, must be checked). Next, the analyst selects a cause variable in the menu on the bottom left, and a frequency histogram of the variable\u2019s levels (if discrete) or values (if continuous) will be shown above the menu. The analyst then indicates the potential cause c by selecting the corresponding level bar(s) and specifying a time delay via the slider on the right. Following, the system displays a histogram of e when c\u2019s event conditions are met (green bars). This histogram is necessarily lower in magnitude since the conditions are only met for some ve. The vertical bars for the histograms indicate their respective means, E?veand E?vejc_x0004_ . The wider apart these means the more pronounced c\u2019s potential effect.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "Our design of the conditional distribution view was motivated by the definition of a potential cause mentioned in Section 3. With the conditional distributions displayed here, analysts can directly observe a time-lagged phenomenon and hence make causal hypotheses (T1,T4). Upon selecting and loading a tabular data file of a multif_x0002_variate time series (top menu) the analyst selects one of the variables as the effect variable e and specifies the effect of interest, such as Value Increase, Value Decrease, or Value Range(second menu row). Upon selecting e the system displays a histogram of its values ve over the entire time series duration(blue bars). This histogram can be brushed when the selected effect is Value Range (the Brushed box, top right, must be checked). Next, the analyst selects a cause variable in the menu on the bottom left, and a frequency histogram of the variable\u2019s levels (if discrete) or values (if continuous) will be shown above the menu. The analyst then indicates the potential cause c by selecting the corresponding level bar(s) and specifying a time delay via the slider on the right. Following, the system displays a histogram of e when c\u2019s event conditions are met (green bars). This histogram is necessarily lower in magnitude since the conditions are only met for some ve. The vertical bars for the histograms indicate their respective means, E?veand E?vejc_x0004_ . The wider apart these means the more pronounced c\u2019s potential effect.", "solution_category": "interaction", "solution_axial": "Selecting;Filtering;Abstract/Elaborate", "solution_compoent": "", "axial_code": ["Selecting", "Abstract/Elaborate", "Filtering"], "componenet_code": ["selecting", "abstract_elaborate", "filtering"]}]}, {"author": "dxf", "index_original": 97, "paper_title": "DOMINO: Visual Causal Reasoning With Time-Dependent Phenomena", "pub_year": 2023, "domain": "Causality analysis", "requirement": {"requirement_text": "T1: Generating causal propositions and hypotheses is often the first step in causality analysis. Most current works on temporal causality achieve this either by manually grouping relevant values and then assigning them semantic meanings, or by conducting an exhaustive search after evenly partitioning the time series data into a large amount of sections each considered an event. Both of these approaches are limited in efficiency and flexibility. Since in logic-based causality a causal relation is defined over a time lagged conditional distribution, analysts should be given direct access to such information by allowing them to generate causal propositions and hypotheses with visual support. Also, since an effect can have multiple causes, an overview of the values and boolean labels of each time series in a synchronized fashion will help the understanding of the compound relations.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "the Air Quality dataset", "data_code": {"sequential": 1, "tables": 1, "temporal": 1}}, "solution": [{"solution_text": "The testing of an event c being a cause of the effect e is based on the assumption that a true cause always increases the probability of the effect (we can view a preventative, which lowers the probability of e, as raising the probability of :e). Thus, we say c is a potential cause [25] of e if, taking into con_x0002_sideration the relative time delay window, it satisfies P\u00f0e\u00de < p and P\u00f0ejc\u00de _x0005_ p; (2) where P\u00f0ejc\u00de is calculated in the same fashion as Eq. (1). Additionally, if the effect e is defined on a continuous variable ve and we are looking for events that are potentially shifting the distribution of ve (as opposed to a value of ve falling into a specific range), the expected value of ve can be used instead for better sensitivity to change [23]. As such, c is considered a potential cause of e when E\u00bdve_x0004_ \u00bc E\u00bdvejc_x0004_ : (3) Here, the \u00bc sign can be replaced by either > or < to stipu_x0002_late only positive or negative causes. And the conditional expected value can be calculated as E\u00bdvejc_x0004_ \u00bc XyyQ\u00f0\u00bdve \u00bc y_x0004_ ^ c\u00deQ\u00f0c\u00de ; (4) where y are values in ve\u2019s domain and Q\u00f0x\u00de denotes the number of time points where x holds. To illustrate, Fig. 1 shows short sequences of a continu_x0002_ous variable ve and a causal event c. Averaging all values of ve, we have E\u00bdve_x0004_ =1.5. Then, when considering a time shift of exactly 1 unit, we have E\u00bdvejc_x0004_\u00bc\u00f00:9 \u00fe 3 \u00fe 2:3 \u00fe 1:3\u00de=4 \u00bc 1:875 (these are the values of ve exactly 1 unit after c is T). Since E\u00bdvejc_x0004_ > E\u00bde_x0004_ , according to Eq. (3), c increases the expected value of ve and thus is a potential cause of it. However, if we try to find the positive cause by instead bounding ve to a specific range, or to a specific value such as the mean of ve the event e would be defined as [ve > E\u00bdve_x0004_ ]. Then we would have P\u00f0e\u00de \u00bc 0:5 (e occurs 4 times out of 8 time points) and P\u00f0ejc\u00de \u00bc 0:5 (2 out of 4), where c would not be considered a potential cause because it is not raising the probability of e. This shows the reduced sensitivity to change that comes with trying to be more specific. We can generalize this framework to a set of causes X of an effect e. We measure the influence of X towards e by cal_x0002_culating the probability change of e as P\u00f0ejX\u00de _x0007_ P\u00f0e\u00de or the change of expected value of ve as E\u00bdvejX_x0004_ _x0007_ E\u00bdve_x0004_ , depend_x0002_ing on the definition of e. Note that while the conditional probability is bounded within \u00bd0; 1_x0004_ , the expected value could be any amount, and either positive or negative. As mentioned, a causal relation is only potential; it may not be direct or it may be spurious, even if Eqs. (2) or (3) holds. This can be due to two possible situations: (1) c and e are actually independent but are commonly caused by another event x (the confounder) with c being caused earlier than e (Fig. 2a), or (2) c causes e indirectly via x (mediation, Fig. 2b). In either condition, we may observe that Eqs. (2) or (3) holds and erroneously mark c as directly causing e. One way to eliminate such error is to compare the distribution of e when c and x both occur, i.e., P\u00f0ejc ^ x\u00de, to that when only x is present, i.e., P\u00f0ej:c ^ x\u00de. Then the two will be found equal (or almost equal) if c is a spurious cause of e. Note that this requires the time window \u00bdr; s_x0004_ to be sufficiently wide such that both x and c could have caused e [24]. This idea can be generalized to multivariate time series. When considering multiple time series in a dataset, for a given effect, we usually can recognize a number of potential causes. To identify the real causes that can better explain the effect, Eells [8] proposed the average significance of a potential cause c, among all potential causes X toward the effect e, calculated as \"avg\u00f0c; e\u00de \u00bc X x2X=c P\u00f0ejc ^ x\u00de _x0007_ P\u00f0ej:c ^ x\u00de jX=cj : (5) Here X=c is the set of potential causes excluding c and jX=cj is the number of events in it. We need at least two potential causes to make the computation meaningful and all calcula_x0002_tions are associated with a preset time window. Then, by setting a certain threshold \", c is called an \"-significant cause of e if j\"avg\u00f0c; e\u00dej _x0005_ \". Further, if e stands for the increase or decrease of a continuous variable ve over the time window, the conditional probability in Eq. (5) can be replaced by the conditional expected value such that \"avg\u00f0c; e\u00de \u00bc X x2X= E\u00bdvejc ^ x_x0004_ _x0007_ E\u00bdvej:c ^ x_x0004_ jX=cj : (6) Although the \" threshold is decisive in testing if a cause is significant, its value can be difficult to determine auto_x0002_matically in practice. In presence of a large number of (say, thousands of) potential causes where significant causes are rare, all such \"avg values usually follow a Gaussian distribu_x0002_tion [23]. As a result, the problem can be solved by testing the significance of a null hypothesis where p values rejecting the null hypothesis deviate from the mean [9]. However, we find that this theoretical method cannot really be applied in most of our applications since we rarely encounter such a large number of time-series and causal events, especially when we just wish to explore the impact of some specific causes on the target. In such cases, the \" threshold can only be assigned empirically and interactively by the analyst. This requirement for user assistance, together with other analytical tasks that will be discussed later, motivated the visual analytics system that is at the heart of our work. Since a potential cause elevates the probability (Eq. (2)) oralters the expected value (Eq. (3)) of the effect, the processof searching for a cause c is the same as deciding an appro\\x02priate numerical constraint on the cause variable vc, onwhich c is made, so that Eqs. (2) or (3) can be satisfied. Thisis relatively easy and straightforward when vc has discretevalues, where we can simply scan through vc\u2019s domain andmake c take all the values satisfying the condition.The search becomes more complex when vc\u2019s domain iscontinuous. One solution could be to discretize vc and thenapply the same scanning process, but determining a gooddiscretization strategy is difficult. Our approach is toinstead only look at vc at time points t, noted as vc\u00f0t\u00de, wheree holds after the specified time delay (vc\u00f0t\u00de\u02c6 \\x05 r;\\x03 se). Record\\x02ing all such vc\u00f0t\u00de as Tc, we then discretize vc adaptively byclustering values in Tc. The idea is to consider values that vcfrequently takes and lead to e as possibly causing e.The clustering process follows Algorithm 3.3, whichtakes the same approach as the incremental clustering forhigh-dimensional data [41] but is applied in 1-D. The original algorithm has shown great performance in learning 'informative value segments. The adapted version here iteratively scans values in Tc until all clusters converge or themaximum number of iterations is reached. In each iteration,a value is assigned to a cluster center if the distance betweenthem is smaller than some threshold u. A new cluster isadded when a point is too far away from all clusters. Thethreshold u controls the size of the clusters, which decideshow vc will be discretized later. At last, we transform vc byconsidering the value range each cluster covers as a level,and test if it fulfills Eqs. (2) or (3). If multiple levels arereturned, we try to merge them if they overlap and take theone that best elevates e as the most possible cause. 'The algorithm can be easily parallelized [41] if we modify the incremental process such that it searches clusters inbatches instead of inspecting them one by one, enablingscalability. Also, the trade-off of taking different u valuesis that a larger u tends to produce a looser constraint (alarger value range of vc) in c, often resulting in a smallerP\u00f0ejc\u00de or an E\u00bdvejccloser to E\u00bdve\u2013 a smaller u does theopposite. This is similar to the problem of under-/over-fitting. In our experiments, we found when u equals 0.15 ofvc\u2019s value range 5 iterations were usually sufficient toreach a plausible result \u2013 one that would be close to manual adjustment.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The time sequence view presents an enhanced rendering for examining the time sequences directly (T1). The sequences are stacked, with the specified effect on the top. Each sequence can be rendered in two modes \u2013 label mode and value mode, which can be switched by clicking the check box on the left (unchecked is \u2019labels\u2019 and checked is \u2019values\u2019). The former visualizes the Boolean labels of an event at each time as a strip of colored bars (green for true, red for false). The value mode displays an area chart if the variable is continuous or a strip of bars colored by the level the discrete variable takes at each time with the legend on the right. The same colors are also used in the box chart in the causal inference panel. In both modes, missing values are left blank and long sequences are scrollable.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Bar;Area", "axial_code": [], "componenet_code": ["area", "bar"]}, {"solution_text": "The time sequence view presents an enhanced rendering for examining the time sequences directly (T1). The sequences are stacked, with the specified effect on the top. Each sequence can be rendered in two modes \u2013 label mode and value mode, which can be switched by clicking the check box on the left (unchecked is \u2019labels\u2019 and checked is \u2019values\u2019). The former visualizes the Boolean labels of an event at each time as a strip of colored bars (green for true, red for false). The value mode displays an area chart if the variable is continuous or a strip of bars colored by the level the discrete variable takes at each time with the legend on the right. The same colors are also used in the box chart in the causal inference panel. In both modes, missing values are left blank and long sequences are scrollable.", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 98, "paper_title": "DOMINO: Visual Causal Reasoning With Time-Dependent Phenomena", "pub_year": 2023, "domain": "Causality analysis", "requirement": {"requirement_text": "T2: Identifying significant causes under a specified time delay is the most common task when investigating causality within time series. Examples are found in temporal causal analysis of the stock market, biomedical data, social activities, and terrorist activities. While the significance threshold determining the truthfulness of causes may often need to be decided empirically, a visual system should effectively externalize the identified causes and their levels of significance under the specified time delay, supporting the analyst\u2019s decision-making process.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "the Air Quality dataset", "data_code": {"sequential": 1, "tables": 1, "temporal": 1}}, "solution": [{"solution_text": "The testing of an event c being a cause of the effect e is based on the assumption that a true cause always increases the probability of the effect (we can view a preventative, which lowers the probability of e, as raising the probability of :e). Thus, we say c is a potential cause [25] of e if, taking into con_x0002_sideration the relative time delay window, it satisfies P\u00f0e\u00de < p and P\u00f0ejc\u00de _x0005_ p; (2) where P\u00f0ejc\u00de is calculated in the same fashion as Eq. (1). Additionally, if the effect e is defined on a continuous variable ve and we are looking for events that are potentially shifting the distribution of ve (as opposed to a value of ve falling into a specific range), the expected value of ve can be used instead for better sensitivity to change [23]. As such, c is considered a potential cause of e when E\u00bdve_x0004_ \u00bc E\u00bdvejc_x0004_ : (3) Here, the \u00bc sign can be replaced by either > or < to stipu_x0002_late only positive or negative causes. And the conditional expected value can be calculated as E\u00bdvejc_x0004_ \u00bc XyyQ\u00f0\u00bdve \u00bc y_x0004_ ^ c\u00deQ\u00f0c\u00de ; (4) where y are values in ve\u2019s domain and Q\u00f0x\u00de denotes the number of time points where x holds. To illustrate, Fig. 1 shows short sequences of a continu_x0002_ous variable ve and a causal event c. Averaging all values of ve, we have E\u00bdve_x0004_ =1.5. Then, when considering a time shift of exactly 1 unit, we have E\u00bdvejc_x0004_\u00bc\u00f00:9 \u00fe 3 \u00fe 2:3 \u00fe 1:3\u00de=4 \u00bc 1:875 (these are the values of ve exactly 1 unit after c is T). Since E\u00bdvejc_x0004_ > E\u00bde_x0004_ , according to Eq. (3), c increases the expected value of ve and thus is a potential cause of it. However, if we try to find the positive cause by instead bounding ve to a specific range, or to a specific value such as the mean of ve the event e would be defined as [ve > E\u00bdve_x0004_ ]. Then we would have P\u00f0e\u00de \u00bc 0:5 (e occurs 4 times out of 8 time points) and P\u00f0ejc\u00de \u00bc 0:5 (2 out of 4), where c would not be considered a potential cause because it is not raising the probability of e. This shows the reduced sensitivity to change that comes with trying to be more specific. We can generalize this framework to a set of causes X of an effect e. We measure the influence of X towards e by cal_x0002_culating the probability change of e as P\u00f0ejX\u00de _x0007_ P\u00f0e\u00de or the change of expected value of ve as E\u00bdvejX_x0004_ _x0007_ E\u00bdve_x0004_ , depend_x0002_ing on the definition of e. Note that while the conditional probability is bounded within \u00bd0; 1_x0004_ , the expected value could be any amount, and either positive or negative. As mentioned, a causal relation is only potential; it may not be direct or it may be spurious, even if Eqs. (2) or (3) holds. This can be due to two possible situations: (1) c and e are actually independent but are commonly caused by another event x (the confounder) with c being caused earlier than e (Fig. 2a), or (2) c causes e indirectly via x (mediation, Fig. 2b). In either condition, we may observe that Eqs. (2) or (3) holds and erroneously mark c as directly causing e. One way to eliminate such error is to compare the distribution of e when c and x both occur, i.e., P\u00f0ejc ^ x\u00de, to that when only x is present, i.e., P\u00f0ej:c ^ x\u00de. Then the two will be found equal (or almost equal) if c is a spurious cause of e. Note that this requires the time window \u00bdr; s_x0004_ to be sufficiently wide such that both x and c could have caused e [24]. This idea can be generalized to multivariate time series. When considering multiple time series in a dataset, for a given effect, we usually can recognize a number of potential causes. To identify the real causes that can better explain the effect, Eells [8] proposed the average significance of a potential cause c, among all potential causes X toward the effect e, calculated as \"avg\u00f0c; e\u00de \u00bc X x2X=c P\u00f0ejc ^ x\u00de _x0007_ P\u00f0ej:c ^ x\u00de jX=cj : (5) Here X=c is the set of potential causes excluding c and jX=cj is the number of events in it. We need at least two potential causes to make the computation meaningful and all calcula_x0002_tions are associated with a preset time window. Then, by setting a certain threshold \", c is called an \"-significant cause of e if j\"avg\u00f0c; e\u00dej _x0005_ \". Further, if e stands for the increase or decrease of a continuous variable ve over the time window, the conditional probability in Eq. (5) can be replaced by the conditional expected value such that \"avg\u00f0c; e\u00de \u00bc X x2X= E\u00bdvejc ^ x_x0004_ _x0007_ E\u00bdvej:c ^ x_x0004_ jX=cj : (6) Although the \" threshold is decisive in testing if a cause is significant, its value can be difficult to determine auto_x0002_matically in practice. In presence of a large number of (say, thousands of) potential causes where significant causes are rare, all such \"avg values usually follow a Gaussian distribu_x0002_tion [23]. As a result, the problem can be solved by testing the significance of a null hypothesis where p values rejecting the null hypothesis deviate from the mean [9]. However, we find that this theoretical method cannot really be applied in most of our applications since we rarely encounter such a large number of time-series and causal events, especially when we just wish to explore the impact of some specific causes on the target. In such cases, the \" threshold can only be assigned empirically and interactively by the analyst. This requirement for user assistance, together with other analytical tasks that will be discussed later, motivated the visual analytics system that is at the heart of our work. Since a potential cause elevates the probability (Eq. (2)) oralters the expected value (Eq. (3)) of the effect, the processof searching for a cause c is the same as deciding an appro\\x02priate numerical constraint on the cause variable vc, onwhich c is made, so that Eqs. (2) or (3) can be satisfied. Thisis relatively easy and straightforward when vc has discretevalues, where we can simply scan through vc\u2019s domain andmake c take all the values satisfying the condition.The search becomes more complex when vc\u2019s domain iscontinuous. One solution could be to discretize vc and thenapply the same scanning process, but determining a gooddiscretization strategy is difficult. Our approach is toinstead only look at vc at time points t, noted as vc\u00f0t\u00de, wheree holds after the specified time delay (vc\u00f0t\u00de\u02c6 \\x05 r;\\x03 se). Record\\x02ing all such vc\u00f0t\u00de as Tc, we then discretize vc adaptively byclustering values in Tc. The idea is to consider values that vcfrequently takes and lead to e as possibly causing e.The clustering process follows Algorithm 3.3, whichtakes the same approach as the incremental clustering forhigh-dimensional data [41] but is applied in 1-D. The original algorithm has shown great performance in learning 'informative value segments. The adapted version here iteratively scans values in Tc until all clusters converge or themaximum number of iterations is reached. In each iteration,a value is assigned to a cluster center if the distance betweenthem is smaller than some threshold u. A new cluster isadded when a point is too far away from all clusters. Thethreshold u controls the size of the clusters, which decideshow vc will be discretized later. At last, we transform vc byconsidering the value range each cluster covers as a level,and test if it fulfills Eqs. (2) or (3). If multiple levels arereturned, we try to merge them if they overlap and take theone that best elevates e as the most possible cause. 'The algorithm can be easily parallelized [41] if we modify the incremental process such that it searches clusters inbatches instead of inspecting them one by one, enablingscalability. Also, the trade-off of taking different u valuesis that a larger u tends to produce a looser constraint (alarger value range of vc) in c, often resulting in a smallerP\u00f0ejc\u00de or an E\u00bdvejccloser to E\u00bdve\u2013 a smaller u does theopposite. This is similar to the problem of under-/over-fitting. In our experiments, we found when u equals 0.15 ofvc\u2019s value range 5 iterations were usually sufficient toreach a plausible result \u2013 one that would be close to manual adjustment.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Each time the Add button in panel A is pressed, the cause c specified there is added to the set of potential causes X. DOMINO then automatically tests its significance with regards to e and time delay (T2,T4), and positions it as a box in the box chart (bottom left). The boxes are identical in size and ordered by significance; the value is signified by the two small handles attached at the left/right box center. Users can move a vertical slider up/down to adjust the \"-threshold. If there are too many boxes, a horizontal scrollbar will appear for scalability.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Box", "axial_code": [], "componenet_code": ["boxplot"]}, {"solution_text": "Each time the Add button in panel A is pressed, the cause c specified there is added to the set of potential causes X. DOMINO then automatically tests its significance with regards to e and time delay (T2,T4), and positions it as a box in the box chart (bottom left). The boxes are identical in size and ordered by significance; the value is signified by the two small handles attached at the left/right box center. Users can move a vertical slider up/down to adjust the \"-threshold. If there are too many boxes, a horizontal scrollbar will appear for scalability.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "dxf", "index_original": 100, "paper_title": "DOMINO: Visual Causal Reasoning With Time-Dependent Phenomena", "pub_year": 2023, "domain": "Causality analysis", "requirement": {"requirement_text": "T4: Interactive analysis. As mentioned, causality analysis is often associated with a number of meta parameters to be determined by analysts empirically, e.g., the numerical constraints in the causal propositions and the threshold in the significance tests. Determining these parameters is an essential task in causality analysis and often requires interaction, as illustrated in other projects. To support interactive analysis, an effective system should provide visual feedback along with each of the user\u2019s parameter updates. Users should also be able to save the discoveries in an overview for later re-examination.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "the Air Quality dataset", "data_code": {"tables": 1}}, "solution": [{"solution_text": "The testing of an event c being a cause of the effect e is based on the assumption that a true cause always increases the probability of the effect (we can view a preventative, which lowers the probability of e, as raising the probability of :e). Thus, we say c is a potential cause [25] of e if, taking into con_x0002_sideration the relative time delay window, it satisfies P\u00f0e\u00de < p and P\u00f0ejc\u00de _x0005_ p; (2) where P\u00f0ejc\u00de is calculated in the same fashion as Eq. (1). Additionally, if the effect e is defined on a continuous variable ve and we are looking for events that are potentially shifting the distribution of ve (as opposed to a value of ve falling into a specific range), the expected value of ve can be used instead for better sensitivity to change [23]. As such, c is considered a potential cause of e when E\u00bdve_x0004_ \u00bc E\u00bdvejc_x0004_ : (3) Here, the \u00bc sign can be replaced by either > or < to stipu_x0002_late only positive or negative causes. And the conditional expected value can be calculated as E\u00bdvejc_x0004_ \u00bc XyyQ\u00f0\u00bdve \u00bc y_x0004_ ^ c\u00deQ\u00f0c\u00de ; (4) where y are values in ve\u2019s domain and Q\u00f0x\u00de denotes the number of time points where x holds. To illustrate, Fig. 1 shows short sequences of a continu_x0002_ous variable ve and a causal event c. Averaging all values of ve, we have E\u00bdve_x0004_ =1.5. Then, when considering a time shift of exactly 1 unit, we have E\u00bdvejc_x0004_\u00bc\u00f00:9 \u00fe 3 \u00fe 2:3 \u00fe 1:3\u00de=4 \u00bc 1:875 (these are the values of ve exactly 1 unit after c is T). Since E\u00bdvejc_x0004_ > E\u00bde_x0004_ , according to Eq. (3), c increases the expected value of ve and thus is a potential cause of it. However, if we try to find the positive cause by instead bounding ve to a specific range, or to a specific value such as the mean of ve the event e would be defined as [ve > E\u00bdve_x0004_ ]. Then we would have P\u00f0e\u00de \u00bc 0:5 (e occurs 4 times out of 8 time points) and P\u00f0ejc\u00de \u00bc 0:5 (2 out of 4), where c would not be considered a potential cause because it is not raising the probability of e. This shows the reduced sensitivity to change that comes with trying to be more specific. We can generalize this framework to a set of causes X of an effect e. We measure the influence of X towards e by cal_x0002_culating the probability change of e as P\u00f0ejX\u00de _x0007_ P\u00f0e\u00de or the change of expected value of ve as E\u00bdvejX_x0004_ _x0007_ E\u00bdve_x0004_ , depend_x0002_ing on the definition of e. Note that while the conditional probability is bounded within \u00bd0; 1_x0004_ , the expected value could be any amount, and either positive or negative. As mentioned, a causal relation is only potential; it may not be direct or it may be spurious, even if Eqs. (2) or (3) holds. This can be due to two possible situations: (1) c and e are actually independent but are commonly caused by another event x (the confounder) with c being caused earlier than e (Fig. 2a), or (2) c causes e indirectly via x (mediation, Fig. 2b). In either condition, we may observe that Eqs. (2) or (3) holds and erroneously mark c as directly causing e. One way to eliminate such error is to compare the distribution of e when c and x both occur, i.e., P\u00f0ejc ^ x\u00de, to that when only x is present, i.e., P\u00f0ej:c ^ x\u00de. Then the two will be found equal (or almost equal) if c is a spurious cause of e. Note that this requires the time window \u00bdr; s_x0004_ to be sufficiently wide such that both x and c could have caused e [24]. This idea can be generalized to multivariate time series. When considering multiple time series in a dataset, for a given effect, we usually can recognize a number of potential causes. To identify the real causes that can better explain the effect, Eells [8] proposed the average significance of a potential cause c, among all potential causes X toward the effect e, calculated as \"avg\u00f0c; e\u00de \u00bc X x2X=c P\u00f0ejc ^ x\u00de _x0007_ P\u00f0ej:c ^ x\u00de jX=cj : (5) Here X=c is the set of potential causes excluding c and jX=cj is the number of events in it. We need at least two potential causes to make the computation meaningful and all calcula_x0002_tions are associated with a preset time window. Then, by setting a certain threshold \", c is called an \"-significant cause of e if j\"avg\u00f0c; e\u00dej _x0005_ \". Further, if e stands for the increase or decrease of a continuous variable ve over the time window, the conditional probability in Eq. (5) can be replaced by the conditional expected value such that \"avg\u00f0c; e\u00de \u00bc X x2X= E\u00bdvejc ^ x_x0004_ _x0007_ E\u00bdvej:c ^ x_x0004_ jX=cj : (6) Although the \" threshold is decisive in testing if a cause is significant, its value can be difficult to determine auto_x0002_matically in practice. In presence of a large number of (say, thousands of) potential causes where significant causes are rare, all such \"avg values usually follow a Gaussian distribu_x0002_tion [23]. As a result, the problem can be solved by testing the significance of a null hypothesis where p values rejecting the null hypothesis deviate from the mean [9]. However, we find that this theoretical method cannot really be applied in most of our applications since we rarely encounter such a large number of time-series and causal events, especially when we just wish to explore the impact of some specific causes on the target. In such cases, the \" threshold can only be assigned empirically and interactively by the analyst. This requirement for user assistance, together with other analytical tasks that will be discussed later, motivated the visual analytics system that is at the heart of our work. Since a potential cause elevates the probability (Eq. (2)) oralters the expected value (Eq. (3)) of the effect, the processof searching for a cause c is the same as deciding an appro\\x02priate numerical constraint on the cause variable vc, onwhich c is made, so that Eqs. (2) or (3) can be satisfied. Thisis relatively easy and straightforward when vc has discretevalues, where we can simply scan through vc\u2019s domain andmake c take all the values satisfying the condition.The search becomes more complex when vc\u2019s domain iscontinuous. One solution could be to discretize vc and thenapply the same scanning process, but determining a gooddiscretization strategy is difficult. Our approach is toinstead only look at vc at time points t, noted as vc\u00f0t\u00de, wheree holds after the specified time delay (vc\u00f0t\u00de\u02c6 \\x05 r;\\x03 se). Record\\x02ing all such vc\u00f0t\u00de as Tc, we then discretize vc adaptively byclustering values in Tc. The idea is to consider values that vcfrequently takes and lead to e as possibly causing e.The clustering process follows Algorithm 3.3, whichtakes the same approach as the incremental clustering forhigh-dimensional data [41] but is applied in 1-D. The original algorithm has shown great performance in learning 'informative value segments. The adapted version here iteratively scans values in Tc until all clusters converge or themaximum number of iterations is reached. In each iteration,a value is assigned to a cluster center if the distance betweenthem is smaller than some threshold u. A new cluster isadded when a point is too far away from all clusters. Thethreshold u controls the size of the clusters, which decideshow vc will be discretized later. At last, we transform vc byconsidering the value range each cluster covers as a level,and test if it fulfills Eqs. (2) or (3). If multiple levels arereturned, we try to merge them if they overlap and take theone that best elevates e as the most possible cause. 'The algorithm can be easily parallelized [41] if we modify the incremental process such that it searches clusters inbatches instead of inspecting them one by one, enablingscalability. Also, the trade-off of taking different u valuesis that a larger u tends to produce a looser constraint (alarger value range of vc) in c, often resulting in a smallerP\u00f0ejc\u00de or an E\u00bdvejccloser to E\u00bdve\u2013 a smaller u does theopposite. This is similar to the problem of under-/over-fitting. In our experiments, we found when u equals 0.15 ofvc\u2019s value range 5 iterations were usually sufficient toreach a plausible result \u2013 one that would be close to manual adjustment.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Our design of the conditional distribution view was motivated by the definition of a potential cause mentioned in Section 3. With the conditional distributions displayed here, analysts can directly observe a time-lagged phenomenon and hence make causal hypotheses (T1,T4). Upon selecting and loading a tabular data file of a multi_x0002_variate time series (top menu) the analyst selects one of the variables as the effect variable e and specifies the effect of interest, such as Value Increase, Value Decrease, or Value Range(second menu row). Upon selecting e the system displays a histogram of its values ve over the entire time series duration(blue bars). This histogram can be brushed when the selected effect is Value Range (the Brushed box, top right, must be checked). Next, the analyst selects a cause variable in the menu on the bottom left, and a frequency histogram of the variable\u2019s levels (if discrete) or values (if continuous) will be shown above the menu. The analyst then indicates the potential cause c by selecting the corresponding level bar(s) and specifying a time delay via the slider on the right. Following, the system displays a histogram of e when c\u2019s event conditions are met (green bars). This histogram is necessarily lower in magnitude since the conditions are only met for some ve. The vertical bars for the histograms indicate their respective means, E?veand E?vejc_x0004_ . The wider apart these means the more pronounced c\u2019s potential effect.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "Our design of the conditional distribution view was motivated by the definition of a potential cause mentioned in Section 3. With the conditional distributions displayed here, analysts can directly observe a time-lagged phenomenon and hence make causal hypotheses (T1,T4). Upon selecting and loading a tabular data file of a multi_x0002_variate time series (top menu) the analyst selects one of the variables as the effect variable e and specifies the effect of interest, such as Value Increase, Value Decrease, or Value Range(second menu row). Upon selecting e the system displays a histogram of its values ve over the entire time series duration(blue bars). This histogram can be brushed when the selected effect is Value Range (the Brushed box, top right, must be checked). Next, the analyst selects a cause variable in the menu on the bottom left, and a frequency histogram of the variable\u2019s levels (if discrete) or values (if continuous) will be shown above the menu. The analyst then indicates the potential cause c by selecting the corresponding level bar(s) and specifying a time delay via the slider on the right. Following, the system displays a histogram of e when c\u2019s event conditions are met (green bars). This histogram is necessarily lower in magnitude since the conditions are only met for some ve. The vertical bars for the histograms indicate their respective means, E?veand E?vejc_x0004_ . The wider apart these means the more pronounced c\u2019s potential effect.", "solution_category": "interaction", "solution_axial": "Selecting;Filtering;Abstract/Elaborate", "solution_compoent": "", "axial_code": ["Selecting", "Abstract/Elaborate", "Filtering"], "componenet_code": ["selecting", "abstract_elaborate", "filtering"]}]}, {"author": "dxf", "index_original": 101, "paper_title": "DOMINO: Visual Causal Reasoning With Time-Dependent Phenomena", "pub_year": 2023, "domain": "Causality analysis", "requirement": {"requirement_text": "T4: Interactive analysis. As mentioned, causality analysis is often associated with a number of meta parameters to be determined by analysts empirically, e.g., the numerical constraints in the causal propositions and the threshold in the significance tests. Determining these parameters is an essential task in causality analysis and often requires interaction, as illustrated in other projects. To support interactive analysis, an effective system should provide visual feedback along with each of the user\u2019s parameter updates. Users should also be able to save the discoveries in an overview for later re-examination.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "the Air Quality dataset", "data_code": {"sequential": 1, "tables": 1, "temporal": 1}}, "solution": [{"solution_text": "The testing of an event c being a cause of the effect e is based on the assumption that a true cause always increases the probability of the effect (we can view a preventative, which lowers the probability of e, as raising the probability of :e). Thus, we say c is a potential cause [25] of e if, taking into con_x0002_sideration the relative time delay window, it satisfies P\u00f0e\u00de < p and P\u00f0ejc\u00de _x0005_ p; (2) where P\u00f0ejc\u00de is calculated in the same fashion as Eq. (1). Additionally, if the effect e is defined on a continuous variable ve and we are looking for events that are potentially shifting the distribution of ve (as opposed to a value of ve falling into a specific range), the expected value of ve can be used instead for better sensitivity to change [23]. As such, c is considered a potential cause of e when E\u00bdve_x0004_ \u00bc E\u00bdvejc_x0004_ : (3) Here, the \u00bc sign can be replaced by either > or < to stipu_x0002_late only positive or negative causes. And the conditional expected value can be calculated as E\u00bdvejc_x0004_ \u00bc XyyQ\u00f0\u00bdve \u00bc y_x0004_ ^ c\u00deQ\u00f0c\u00de ; (4) where y are values in ve\u2019s domain and Q\u00f0x\u00de denotes the number of time points where x holds. To illustrate, Fig. 1 shows short sequences of a continu_x0002_ous variable ve and a causal event c. Averaging all values of ve, we have E\u00bdve_x0004_ =1.5. Then, when considering a time shift of exactly 1 unit, we have E\u00bdvejc_x0004_\u00bc\u00f00:9 \u00fe 3 \u00fe 2:3 \u00fe 1:3\u00de=4 \u00bc 1:875 (these are the values of ve exactly 1 unit after c is T). Since E\u00bdvejc_x0004_ > E\u00bde_x0004_ , according to Eq. (3), c increases the expected value of ve and thus is a potential cause of it. However, if we try to find the positive cause by instead bounding ve to a specific range, or to a specific value such as the mean of ve the event e would be defined as [ve > E\u00bdve_x0004_ ]. Then we would have P\u00f0e\u00de \u00bc 0:5 (e occurs 4 times out of 8 time points) and P\u00f0ejc\u00de \u00bc 0:5 (2 out of 4), where c would not be considered a potential cause because it is not raising the probability of e. This shows the reduced sensitivity to change that comes with trying to be more specific. We can generalize this framework to a set of causes X of an effect e. We measure the influence of X towards e by cal_x0002_culating the probability change of e as P\u00f0ejX\u00de _x0007_ P\u00f0e\u00de or the change of expected value of ve as E\u00bdvejX_x0004_ _x0007_ E\u00bdve_x0004_ , depend_x0002_ing on the definition of e. Note that while the conditional probability is bounded within \u00bd0; 1_x0004_ , the expected value could be any amount, and either positive or negative. As mentioned, a causal relation is only potential; it may not be direct or it may be spurious, even if Eqs. (2) or (3) holds. This can be due to two possible situations: (1) c and e are actually independent but are commonly caused by another event x (the confounder) with c being caused earlier than e (Fig. 2a), or (2) c causes e indirectly via x (mediation, Fig. 2b). In either condition, we may observe that Eqs. (2) or (3) holds and erroneously mark c as directly causing e. One way to eliminate such error is to compare the distribution of e when c and x both occur, i.e., P\u00f0ejc ^ x\u00de, to that when only x is present, i.e., P\u00f0ej:c ^ x\u00de. Then the two will be found equal (or almost equal) if c is a spurious cause of e. Note that this requires the time window \u00bdr; s_x0004_ to be sufficiently wide such that both x and c could have caused e [24]. This idea can be generalized to multivariate time series. When considering multiple time series in a dataset, for a given effect, we usually can recognize a number of potential causes. To identify the real causes that can better explain the effect, Eells [8] proposed the average significance of a potential cause c, among all potential causes X toward the effect e, calculated as \"avg\u00f0c; e\u00de \u00bc X x2X=c P\u00f0ejc ^ x\u00de _x0007_ P\u00f0ej:c ^ x\u00de jX=cj : (5) Here X=c is the set of potential causes excluding c and jX=cj is the number of events in it. We need at least two potential causes to make the computation meaningful and all calcula_x0002_tions are associated with a preset time window. Then, by setting a certain threshold \", c is called an \"-significant cause of e if j\"avg\u00f0c; e\u00dej _x0005_ \". Further, if e stands for the increase or decrease of a continuous variable ve over the time window, the conditional probability in Eq. (5) can be replaced by the conditional expected value such that \"avg\u00f0c; e\u00de \u00bc X x2X= E\u00bdvejc ^ x_x0004_ _x0007_ E\u00bdvej:c ^ x_x0004_ jX=cj : (6) Although the \" threshold is decisive in testing if a cause is significant, its value can be difficult to determine auto_x0002_matically in practice. In presence of a large number of (say, thousands of) potential causes where significant causes are rare, all such \"avg values usually follow a Gaussian distribu_x0002_tion [23]. As a result, the problem can be solved by testing the significance of a null hypothesis where p values rejecting the null hypothesis deviate from the mean [9]. However, we find that this theoretical method cannot really be applied in most of our applications since we rarely encounter such a large number of time-series and causal events, especially when we just wish to explore the impact of some specific causes on the target. In such cases, the \" threshold can only be assigned empirically and interactively by the analyst. This requirement for user assistance, together with other analytical tasks that will be discussed later, motivated the visual analytics system that is at the heart of our work. Since a potential cause elevates the probability (Eq. (2)) oralters the expected value (Eq. (3)) of the effect, the processof searching for a cause c is the same as deciding an appro\\x02priate numerical constraint on the cause variable vc, onwhich c is made, so that Eqs. (2) or (3) can be satisfied. Thisis relatively easy and straightforward when vc has discretevalues, where we can simply scan through vc\u2019s domain andmake c take all the values satisfying the condition.The search becomes more complex when vc\u2019s domain iscontinuous. One solution could be to discretize vc and thenapply the same scanning process, but determining a gooddiscretization strategy is difficult. Our approach is toinstead only look at vc at time points t, noted as vc\u00f0t\u00de, wheree holds after the specified time delay (vc\u00f0t\u00de\u02c6 \\x05 r;\\x03 se). Record\\x02ing all such vc\u00f0t\u00de as Tc, we then discretize vc adaptively byclustering values in Tc. The idea is to consider values that vcfrequently takes and lead to e as possibly causing e.The clustering process follows Algorithm 3.3, whichtakes the same approach as the incremental clustering forhigh-dimensional data [41] but is applied in 1-D. The original algorithm has shown great performance in learning 'informative value segments. The adapted version here iteratively scans values in Tc until all clusters converge or themaximum number of iterations is reached. In each iteration,a value is assigned to a cluster center if the distance betweenthem is smaller than some threshold u. A new cluster isadded when a point is too far away from all clusters. Thethreshold u controls the size of the clusters, which decideshow vc will be discretized later. At last, we transform vc byconsidering the value range each cluster covers as a level,and test if it fulfills Eqs. (2) or (3). If multiple levels arereturned, we try to merge them if they overlap and take theone that best elevates e as the most possible cause. 'The algorithm can be easily parallelized [41] if we modify the incremental process such that it searches clusters inbatches instead of inspecting them one by one, enablingscalability. Also, the trade-off of taking different u valuesis that a larger u tends to produce a looser constraint (alarger value range of vc) in c, often resulting in a smallerP\u00f0ejc\u00de or an E\u00bdvejccloser to E\u00bdve\u2013 a smaller u does theopposite. This is similar to the problem of under-/over-fitting. In our experiments, we found when u equals 0.15 ofvc\u2019s value range 5 iterations were usually sufficient toreach a plausible result \u2013 one that would be close to manual adjustment.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Each time the Add button in panel A is pressed, the cause c specified there is added to the set of potential causes X. DOMINO then automatically tests its significance with regards to e and time delay (T2,T4), and positions it as a box in the box chart (bottom left). The boxes are identical in size and ordered by significance; the value is signified by the two small handles attached at the left/right box center. Users can move a vertical slider up/down to adjust the \"-threshold. If there are too many boxes, a horizontal scrollbar will appear for scalability.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Box", "axial_code": [], "componenet_code": ["boxplot"]}, {"solution_text": "Each time the Add button in panel A is pressed, the cause c specified there is added to the set of potential causes X. DOMINO then automatically tests its significance with regards to e and time delay (T2,T4), and positions it as a box in the box chart (bottom left). The boxes are identical in size and ordered by significance; the value is signified by the two small handles attached at the left/right box center. Users can move a vertical slider up/down to adjust the \"-threshold. If there are too many boxes, a horizontal scrollbar will appear for scalability.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "dxf", "index_original": 102, "paper_title": "DOMINO: Visual Causal Reasoning With Time-Dependent Phenomena", "pub_year": 2023, "domain": "Causality analysis", "requirement": {"requirement_text": "T4: Interactive analysis. As mentioned, causality analysis is often associated with a number of meta parameters to be determined by analysts empirically, e.g., the numerical constraints in the causal propositions and the threshold in the significance tests. Determining these parameters is an essential task in causality analysis and often requires interaction, as illustrated in other projects. To support interactive analysis, an effective system should provide visual feedback along with each of the user\u2019s parameter updates. Users should also be able to save the discoveries in an overview for later re-examination.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "the Air Quality dataset", "data_code": {"tables": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "The testing of an event c being a cause of the effect e is based on the assumption that a true cause always increases the probability of the effect (we can view a preventative, which lowers the probability of e, as raising the probability of :e). Thus, we say c is a potential cause [25] of e if, taking into con_x0002_sideration the relative time delay window, it satisfies P\u00f0e\u00de < p and P\u00f0ejc\u00de _x0005_ p; (2) where P\u00f0ejc\u00de is calculated in the same fashion as Eq. (1). Additionally, if the effect e is defined on a continuous variable ve and we are looking for events that are potentially shifting the distribution of ve (as opposed to a value of ve falling into a specific range), the expected value of ve can be used instead for better sensitivity to change [23]. As such, c is considered a potential cause of e when E\u00bdve_x0004_ \u00bc E\u00bdvejc_x0004_ : (3) Here, the \u00bc sign can be replaced by either > or < to stipu_x0002_late only positive or negative causes. And the conditional expected value can be calculated as E\u00bdvejc_x0004_ \u00bc XyyQ\u00f0\u00bdve \u00bc y_x0004_ ^ c\u00deQ\u00f0c\u00de ; (4) where y are values in ve\u2019s domain and Q\u00f0x\u00de denotes the number of time points where x holds. To illustrate, Fig. 1 shows short sequences of a continu_x0002_ous variable ve and a causal event c. Averaging all values of ve, we have E\u00bdve_x0004_ =1.5. Then, when considering a time shift of exactly 1 unit, we have E\u00bdvejc_x0004_\u00bc\u00f00:9 \u00fe 3 \u00fe 2:3 \u00fe 1:3\u00de=4 \u00bc 1:875 (these are the values of ve exactly 1 unit after c is T). Since E\u00bdvejc_x0004_ > E\u00bde_x0004_ , according to Eq. (3), c increases the expected value of ve and thus is a potential cause of it. However, if we try to find the positive cause by instead bounding ve to a specific range, or to a specific value such as the mean of ve the event e would be defined as [ve > E\u00bdve_x0004_ ]. Then we would have P\u00f0e\u00de \u00bc 0:5 (e occurs 4 times out of 8 time points) and P\u00f0ejc\u00de \u00bc 0:5 (2 out of 4), where c would not be considered a potential cause because it is not raising the probability of e. This shows the reduced sensitivity to change that comes with trying to be more specific. We can generalize this framework to a set of causes X of an effect e. We measure the influence of X towards e by cal_x0002_culating the probability change of e as P\u00f0ejX\u00de _x0007_ P\u00f0e\u00de or the change of expected value of ve as E\u00bdvejX_x0004_ _x0007_ E\u00bdve_x0004_ , depend_x0002_ing on the definition of e. Note that while the conditional probability is bounded within \u00bd0; 1_x0004_ , the expected value could be any amount, and either positive or negative. As mentioned, a causal relation is only potential; it may not be direct or it may be spurious, even if Eqs. (2) or (3) holds. This can be due to two possible situations: (1) c and e are actually independent but are commonly caused by another event x (the confounder) with c being caused earlier than e (Fig. 2a), or (2) c causes e indirectly via x (mediation, Fig. 2b). In either condition, we may observe that Eqs. (2) or (3) holds and erroneously mark c as directly causing e. One way to eliminate such error is to compare the distribution of e when c and x both occur, i.e., P\u00f0ejc ^ x\u00de, to that when only x is present, i.e., P\u00f0ej:c ^ x\u00de. Then the two will be found equal (or almost equal) if c is a spurious cause of e. Note that this requires the time window \u00bdr; s_x0004_ to be sufficiently wide such that both x and c could have caused e [24]. This idea can be generalized to multivariate time series. When considering multiple time series in a dataset, for a given effect, we usually can recognize a number of potential causes. To identify the real causes that can better explain the effect, Eells [8] proposed the average significance of a potential cause c, among all potential causes X toward the effect e, calculated as \"avg\u00f0c; e\u00de \u00bc X x2X=c P\u00f0ejc ^ x\u00de _x0007_ P\u00f0ej:c ^ x\u00de jX=cj : (5) Here X=c is the set of potential causes excluding c and jX=cj is the number of events in it. We need at least two potential causes to make the computation meaningful and all calcula_x0002_tions are associated with a preset time window. Then, by setting a certain threshold \", c is called an \"-significant cause of e if j\"avg\u00f0c; e\u00dej _x0005_ \". Further, if e stands for the increase or decrease of a continuous variable ve over the time window, the conditional probability in Eq. (5) can be replaced by the conditional expected value such that \"avg\u00f0c; e\u00de \u00bc X x2X= E\u00bdvejc ^ x_x0004_ _x0007_ E\u00bdvej:c ^ x_x0004_ jX=cj : (6) Although the \" threshold is decisive in testing if a cause is significant, its value can be difficult to determine auto_x0002_matically in practice. In presence of a large number of (say, thousands of) potential causes where significant causes are rare, all such \"avg values usually follow a Gaussian distribu_x0002_tion [23]. As a result, the problem can be solved by testing the significance of a null hypothesis where p values rejecting the null hypothesis deviate from the mean [9]. However, we find that this theoretical method cannot really be applied in most of our applications since we rarely encounter such a large number of time-series and causal events, especially when we just wish to explore the impact of some specific causes on the target. In such cases, the \" threshold can only be assigned empirically and interactively by the analyst. This requirement for user assistance, together with other analytical tasks that will be discussed later, motivated the visual analytics system that is at the heart of our work. Since a potential cause elevates the probability (Eq. (2)) oralters the expected value (Eq. (3)) of the effect, the processof searching for a cause c is the same as deciding an appro\\x02priate numerical constraint on the cause variable vc, onwhich c is made, so that Eqs. (2) or (3) can be satisfied. Thisis relatively easy and straightforward when vc has discretevalues, where we can simply scan through vc\u2019s domain andmake c take all the values satisfying the condition.The search becomes more complex when vc\u2019s domain iscontinuous. One solution could be to discretize vc and thenapply the same scanning process, but determining a gooddiscretization strategy is difficult. Our approach is toinstead only look at vc at time points t, noted as vc\u00f0t\u00de, wheree holds after the specified time delay (vc\u00f0t\u00de\u02c6 \\x05 r;\\x03 se). Record\\x02ing all such vc\u00f0t\u00de as Tc, we then discretize vc adaptively byclustering values in Tc. The idea is to consider values that vcfrequently takes and lead to e as possibly causing e.The clustering process follows Algorithm 3.3, whichtakes the same approach as the incremental clustering forhigh-dimensional data [41] but is applied in 1-D. The original algorithm has shown great performance in learning 'informative value segments. The adapted version here iteratively scans values in Tc until all clusters converge or themaximum number of iterations is reached. In each iteration,a value is assigned to a cluster center if the distance betweenthem is smaller than some threshold u. A new cluster isadded when a point is too far away from all clusters. Thethreshold u controls the size of the clusters, which decideshow vc will be discretized later. At last, we transform vc byconsidering the value range each cluster covers as a level,and test if it fulfills Eqs. (2) or (3). If multiple levels arereturned, we try to merge them if they overlap and take theone that best elevates e as the most possible cause. 'The algorithm can be easily parallelized [41] if we modify the incremental process such that it searches clusters inbatches instead of inspecting them one by one, enablingscalability. Also, the trade-off of taking different u valuesis that a larger u tends to produce a looser constraint (alarger value range of vc) in c, often resulting in a smallerP\u00f0ejc\u00de or an E\u00bdvejccloser to E\u00bdve\u2013 a smaller u does theopposite. This is similar to the problem of under-/over-fitting. In our experiments, we found when u equals 0.15 ofvc\u2019s value range 5 iterations were usually sufficient toreach a plausible result \u2013 one that would be close to manual adjustment.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "A user can click on the variable name of a sequence to revisit and adjust the event\u2019s value constraint in the conditional distribution view (T4). An event can be removed with the delete button on the right of the sequence. Two indicator lines will be rendered and move along with the mouse pointer. The longer line shows the value or label, depending on the visualization mode, of each cause at the time point hovered on. The other line shows the value of the effect ahead, with the time shift set in the causal inference panel area chart.", "solution_category": "interaction", "solution_axial": "Selecting;Filtering;Participation/Collaboration;Reconfig", "solution_compoent": "", "axial_code": ["Selecting", "Filtering", "Reconfig", "Participation/Collaboration"], "componenet_code": ["selecting", "filtering", "reconfigure", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 103, "paper_title": "A Visual Analytics System for Improving Attention-based Traffic Forecasting Models", "pub_year": 2023, "domain": "Traffic Visualization", "requirement": {"requirement_text": "R1: a VA system should highlight the roads with low accuracy and provide information on when a model has low accuracy", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "In this work, we use the traffic data of two different road networks\u2013theurban and highway road networks to explore the model\u2019s inferenceprocess for speed prediction. For the urban road network, we use ded\\x02icated short range communication (DSRC) data [28] generated fromUlsan, South Korea (range: 9/1/2017\u223c12/28/2017), where more than1.1 million people live with more than 540,000 registered vehicles as of2017. A total of 116 DSRC sensors are used for data collection, whichare installed every 5.7km and cover a total of 68 main roads. For thehighway road network, we use the METR-LA data [24], which werecollected from 207 loop detectors (range: 3/1/2012\u223c6/27/2012) on thehighways of Los Angeles. Note that the highway network data we useare the standard benchmark data for traffic forecasting tasks [40, 77, 82].After discussing with domain experts and reviewing training results, wereplace the missing data and explicit errors with historical data. We alsouse 5-minute aggregated data to mitigate possible effects of outliers, asperformed in many previous studies (e.g., [40]).", "data_code": {"tables": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "We describe how we utilize ST-GRAT to demonstrate our VA approach.We choose ST-GRAT because 1) it has demonstrated state-of-the-artperformance and because 2) we can produce attention matrices onspatio-temporal dependency (e.g., Eq. 1, Fig. 3D) [51]. We also consider that the domain experts have expressed that attention models areextensively used at work for not only speed prediction but also othertasks, such as travel time [16] and taxi demand prediction [74]. ST-GRAT (Fig. 3) is a variation of the transformer [65] that usesthe encoder-decoder architecture with self-attention (i.e., temporal attention). Additionally, ST-GRAT utilizes graph attention as spatialattention before temporal attention with a sentinel vector. The sentinel vector acts as weights for skip connection within the same road.ST-GRAT utilizes 12-length sequential historical speed with encodedfeatures for each road and predicts 12 sequential speed predictions.There are three types of layers in the encoder and decoder: embedding, spatial attention, and temporal attention layers (Fig. 3B). To allowthe model to extract the spatio-temporal dependencies, we provide aroad network, speed, and observed time as the input features for theembedding layer (Fig. 3B). The road network graph is directed graphG(V,E,A), where a road is represented as a node (i.e., V) and theconnection between roads is shown as a link (i.e., E). Note that roadnetwork G is directed graph, which allow model to distinguish the roaddirections. Note that we provide the order of a given sequence usingthe position embedding method [65].The model captures spatial dependencies among neighbor roads inthe spatial attention layer (Fig. 3B) by using a graph attention network [66, 82], a well-known graph modeling method. In short, thespatial attention layer integrates information among neighboring roadsby directed graph attention. This directed spatial attention improvesdependency modeling and helps developers interpret ST-GRAT.The temporal attention layer (Fig. 3B) models the temporal dependency and trends of given sequences. For modeling, temporal attentionperforms multi-head attention to compute temporal correlations. Theattention type is decided by attention axes; spatial attention aggregatesfeatures among the spatial axis (i.e., neighboring roads), while temporal attention aggregates input features within the temporal axis (i.e.,different time steps of the same roads (Fig. 3C).To sum up, there are two important attention layers\u2013spatial andtemporal attention\u2013in ST-GRAT. Vaswani et al. [65] describe an attention function as mapping a query and a set of key-value pairs to anoutput, where the query, key, values, and output are all vectors. Fromthis perspective, from each key-value pair, we can derive an attentionmatrix from each attention layer, called SA and TA, and create a spatio temporal matrix (ST matrix). Specifically, for each attention head,given X before spatial attention and H after temporal attention, theirrelationship can be written as follows:H = (TA\u2299SA)X, (1)where TA \u2299 SA, named spatio-temporal attention, is the attention thatusers mainly explore for understanding model behaviors.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "In this work, we incorporate two automated methods\u2013warping (DTW) with spectral clustering [6, 57, 73] anddynamic timeGrangercausality tests [14], to allow users to determine which referencesare appropriate for predictions and whether a model employs properreferences for the predictions (R2-3) [38]. We use daily trends of roads as input for DTW computation, whichshould be explored since they are encoded as temporal dependenciesamong roads, which a model learns during training [74]. However,it is difficult to analyze road trends for two reasons. First, there aretoo many trends (i.e., roads) in a traffic dataset. Second, there aredifferent time gaps between the trends of roads due to different levelsof dependencies, which are presented with either lagging or precedingtrends [39]. For example, when congestion occurs on a road, the roadslinked to the congested road also become congested, but there are noconcrete patterns when the neighboring roads are congested. As such,we incorporate dynamic time warping (DTW) [6] in this work, whichcalculates the similarity in two time series over time gaps after findingthe best matching alignment that minimizes the distance, as Le Guenand Thome do for their time-series forecasting model design [35]. Forefficiency, we use FastDTW [57], whose complexity is O(N). Then we perform spectral clustering [47] with the computed DTWscores to further allow cluster-based analysis, which has shown itseffectiveness in analyzing traffic data [5, 73]. For example, users caneasily confirm whether referred roads of a target road have similar dailytrends as the target road using clusters. If the target and referred roadsare in the same cluster, it is highly possible that the model learns thedependencies of the target and referred roads during training [39]. Notethat we use the ELBOW method [1] with visual inspection to choosethe number of clusters.As Wu et al. [71] show in their study, catching preceding patterns intime-series enables effective analysis of deep learning models. Therefore, to help users better explore the preceding patterns in the trafficdata, and to complement DTW that distorts the time during its computation, we incorporate the Granger causality test [14] in this work,a well-known temporal dependency investigation method. We first describe the definition of Granger causality based on two principles: (1) the cause happens prior to its effect and (2) has unique informationabout the future values of its effect. Given these two principles aboutcausality, Granger causality proposes testing the following hypothesisfor the identification of a causal effect.GrangerCausality = P[Y(t ;1) \u2208 A|I (t)] = P[Y(t ;1) \u2208 |I\u2212X (t)]Here, P is probability, A is an arbitrary non-empty set, and I (t) andI\u2212X (t) denote the information available as of time t in the entireuniverse, and in the modified universe where X is excluded, respectively.If the above hypothesis is accepted, we say X Granger-causes Y.", "solution_category": "data_manipulation", "solution_axial": "Modeling;Explainability", "solution_compoent": "", "axial_code": ["Modeling", "Explainability"], "componenet_code": ["modeling", "explainability"]}, {"solution_text": "Users can filter out the roads using Mean Absolute Error (A1: : scaled: min\u2013max values) and spatio-temporal attention (A2: scaled: 0\u20131) values. When roads\u2019 MAEs are higher than the threshold value, the center circle of the roads are colored in black (R1). If a road\u2019s MAE value is below the value at Q1 (i.e., top 25% in accuracy), the road is included in the low error group. If the MAE of a road is above the value at Q3 (i.e., bottom 25% in accuracy), it belongs to the high error group. When users hover over the filters, a tooltip pops up to show the MAE values at the Q1 and Q3 boundaries (Fig. 1, A1). The attention filter is applied to AttnArrows (e.g., Fig. 4) on the map, when a road is selected for investigation. There are two legends to represent clusters (Fig. 1 D1) and ratio values (D2).", "solution_category": "interaction", "solution_axial": "Selecting;Participation/Collaboration", "solution_compoent": "", "axial_code": ["Selecting", "Participation/Collaboration"], "componenet_code": ["selecting", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 104, "paper_title": "A Visual Analytics System for Improving Attention-based Traffic Forecasting Models", "pub_year": 2023, "domain": "Traffic Visualization", "requirement": {"requirement_text": "R2: a system should provide a method for effectively exploring encoded dependencies", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "In this work, we use the traffic data of two different road networks\u2013theurban and highway road networks to explore the model\u2019s inferenceprocess for speed prediction. For the urban road network, we use ded\\x02icated short range communication (DSRC) data [28] generated fromUlsan, South Korea (range: 9/1/2017\u223c12/28/2017), where more than1.1 million people live with more than 540,000 registered vehicles as of2017. A total of 116 DSRC sensors are used for data collection, whichare installed every 5.7km and cover a total of 68 main roads. For thehighway road network, we use the METR-LA data [24], which werecollected from 207 loop detectors (range: 3/1/2012\u223c6/27/2012) on thehighways of Los Angeles. Note that the highway network data we useare the standard benchmark data for traffic forecasting tasks [40, 77, 82].After discussing with domain experts and reviewing training results, wereplace the missing data and explicit errors with historical data. We alsouse 5-minute aggregated data to mitigate possible effects of outliers, asperformed in many previous studies (e.g., [40]).", "data_code": {"tables": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "To help users investigate roads\u2019 temporal dependencies (R2), comparing road speeds, we provide a line chart view (Fig. 1C), where the xaxis and y-axis represent time (5-minute intervals) and road\u2019s speed, respectively. Users can add raw and predicted speeds by selecting roads from other views, such as table, map, and attention views, and remove the lines by toggling road labels at the top.", "solution_category": "interaction", "solution_axial": "Selecting;Participation/Collaboration", "solution_compoent": "", "axial_code": ["Selecting", "Participation/Collaboration"], "componenet_code": ["selecting", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 105, "paper_title": "A Visual Analytics System for Improving Attention-based Traffic Forecasting Models", "pub_year": 2023, "domain": "Traffic Visualization", "requirement": {"requirement_text": "R2-1: Example information for supporting spatio-temporal dependency exploration includes (R2-1) historical traffic patterns of roads, speed distribution of data, standard deviation, daily speed trends (Q3, Q4), ", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "In this work, we use the traffic data of two different road networks\u2013theurban and highway road networks to explore the model\u2019s inferenceprocess for speed prediction. For the urban road network, we use ded\\x02icated short range communication (DSRC) data [28] generated fromUlsan, South Korea (range: 9/1/2017\u223c12/28/2017), where more than1.1 million people live with more than 540,000 registered vehicles as of2017. A total of 116 DSRC sensors are used for data collection, whichare installed every 5.7km and cover a total of 68 main roads. For thehighway road network, we use the METR-LA data [24], which werecollected from 207 loop detectors (range: 3/1/2012\u223c6/27/2012) on thehighways of Los Angeles. Note that the highway network data we useare the standard benchmark data for traffic forecasting tasks [40, 77, 82].After discussing with domain experts and reviewing training results, wereplace the missing data and explicit errors with historical data. We alsouse 5-minute aggregated data to mitigate possible effects of outliers, asperformed in many previous studies (e.g., [40]).", "data_code": {"tables": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "We describe how we utilize ST-GRAT to demonstrate our VA approach.We choose ST-GRAT because 1) it has demonstrated state-of-the-artperformance and because 2) we can produce attention matrices onspatio-temporal dependency (e.g., Eq. 1, Fig. 3D) [51]. We also consider that the domain experts have expressed that attention models areextensively used at work for not only speed prediction but also othertasks, such as travel time [16] and taxi demand prediction [74]. ST-GRAT (Fig. 3) is a variation of the transformer [65] that usesthe encoder-decoder architecture with self-attention (i.e., temporal attention). Additionally, ST-GRAT utilizes graph attention as spatialattention before temporal attention with a sentinel vector. The sentinel vector acts as weights for skip connection within the same road.ST-GRAT utilizes 12-length sequential historical speed with encodedfeatures for each road and predicts 12 sequential speed predictions.There are three types of layers in the encoder and decoder: embedding, spatial attention, and temporal attention layers (Fig. 3B). To allowthe model to extract the spatio-temporal dependencies, we provide aroad network, speed, and observed time as the input features for theembedding layer (Fig. 3B). The road network graph is directed graphG(V,E,A), where a road is represented as a node (i.e., V) and theconnection between roads is shown as a link (i.e., E). Note that roadnetwork G is directed graph, which allow model to distinguish the roaddirections. Note that we provide the order of a given sequence usingthe position embedding method [65].The model captures spatial dependencies among neighbor roads inthe spatial attention layer (Fig. 3B) by using a graph attention network [66, 82], a well-known graph modeling method. In short, thespatial attention layer integrates information among neighboring roadsby directed graph attention. This directed spatial attention improvesdependency modeling and helps developers interpret ST-GRAT.The temporal attention layer (Fig. 3B) models the temporal dependency and trends of given sequences. For modeling, temporal attentionperforms multi-head attention to compute temporal correlations. Theattention type is decided by attention axes; spatial attention aggregatesfeatures among the spatial axis (i.e., neighboring roads), while temporal attention aggregates input features within the temporal axis (i.e.,different time steps of the same roads (Fig. 3C).To sum up, there are two important attention layers\u2013spatial andtemporal attention\u2013in ST-GRAT. Vaswani et al. [65] describe an attention function as mapping a query and a set of key-value pairs to anoutput, where the query, key, values, and output are all vectors. Fromthis perspective, from each key-value pair, we can derive an attentionmatrix from each attention layer, called SA and TA, and create a spatio temporal matrix (ST matrix). Specifically, for each attention head,given X before spatial attention and H after temporal attention, theirrelationship can be written as follows:H = (TA\u2299SA)X, (1)where TA \u2299 SA, named spatio-temporal attention, is the attention thatusers mainly explore for understanding model behaviors.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "In this work, we incorporate two automated methods\u2013warping (DTW) with spectral clustering [6, 57, 73] anddynamic timeGrangercausality tests [14], to allow users to determine which referencesare appropriate for predictions and whether a model employs properreferences for the predictions (R2-3) [38]. We use daily trends of roads as input for DTW computation, whichshould be explored since they are encoded as temporal dependenciesamong roads, which a model learns during training [74]. However,it is difficult to analyze road trends for two reasons. First, there aretoo many trends (i.e., roads) in a traffic dataset. Second, there aredifferent time gaps between the trends of roads due to different levelsof dependencies, which are presented with either lagging or precedingtrends [39]. For example, when congestion occurs on a road, the roadslinked to the congested road also become congested, but there are noconcrete patterns when the neighboring roads are congested. As such,we incorporate dynamic time warping (DTW) [6] in this work, whichcalculates the similarity in two time series over time gaps after findingthe best matching alignment that minimizes the distance, as Le Guenand Thome do for their time-series forecasting model design [35]. Forefficiency, we use FastDTW [57], whose complexity is O(N). Then we perform spectral clustering [47] with the computed DTWscores to further allow cluster-based analysis, which has shown itseffectiveness in analyzing traffic data [5, 73]. For example, users caneasily confirm whether referred roads of a target road have similar dailytrends as the target road using clusters. If the target and referred roadsare in the same cluster, it is highly possible that the model learns thedependencies of the target and referred roads during training [39]. Notethat we use the ELBOW method [1] with visual inspection to choosethe number of clusters.As Wu et al. [71] show in their study, catching preceding patterns intime-series enables effective analysis of deep learning models. Therefore, to help users better explore the preceding patterns in the trafficdata, and to complement DTW that distorts the time during its computation, we incorporate the Granger causality test [14] in this work,a well-known temporal dependency investigation method. We first describe the definition of Granger causality based on two principles: (1) the cause happens prior to its effect and (2) has unique informationabout the future values of its effect. Given these two principles aboutcausality, Granger causality proposes testing the following hypothesisfor the identification of a causal effect.GrangerCausality = P[Y(t ;1) \u2208 A|I (t)] = P[Y(t ;1) \u2208 |I\u2212X (t)]Here, P is probability, A is an arbitrary non-empty set, and I (t) andI\u2212X (t) denote the information available as of time t in the entireuniverse, and in the modified universe where X is excluded, respectively.If the above hypothesis is accepted, we say X Granger-causes Y.", "solution_category": "data_manipulation", "solution_axial": "Modeling;Explainability", "solution_compoent": "", "axial_code": ["Modeling", "Explainability"], "componenet_code": ["modeling", "explainability"]}, {"solution_text": "As models learn dependencies among roads and forecast based on the learned dependencies, it is important to explore what dependencies a model learns with road speed patterns and which roads a model refers to for performance investigation. To help users perform such exploration (R2-1, R2-2), we first present each road as a white dot. Each circle surrounding the dots shows the cluster that the road belongs to. We provide three visualizations in the map view: heatmap, attention arrows (AttnArrows), and cluster visualization. First, to help users overview the relationship between roads\u2019 congestion levels and model performance (Q1), we visualize roads\u2019 congestion levels using heatmaps (Fig. 1D). Here, the redder the heatmaps are, the slower the roads are (heatmap legend: Fig. 1 D2). We also have considered providing heatmaps with a time filter so that users can explore regions with high model error but decided against this because this approach requires many interactions for filtering and memorizing heatmaps that have changed due to filtering. Using Bezier curves, AttnArrows (Fig. 4) link a target road and the roads that the target road attends for prediction. Here, the color represents the amount of attention given to the roads. The darker the head color, the more attention the reference road is given.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Map;Circle;Heatmap", "axial_code": [], "componenet_code": ["heatmap", "map", "circle"]}]}, {"author": "dxf", "index_original": 106, "paper_title": "A Visual Analytics System for Improving Attention-based Traffic Forecasting Models", "pub_year": 2023, "domain": "Traffic Visualization", "requirement": {"requirement_text": "R2-2: data on model behaviors (Q2\u2013Q5), such as which roads a model refers to for forecasting (i.e., which roads influence the prediction for a target road [19]), and which input sequences are importantly used for the prediction.", "requirement_code": {"identify_main_cause_aggregate": 1}}, "data": {"data_text": "In this work, we use the traffic data of two different road networks\u2013theurban and highway road networks to explore the model\u2019s inferenceprocess for speed prediction. For the urban road network, we use ded\\x02icated short range communication (DSRC) data [28] generated fromUlsan, South Korea (range: 9/1/2017\u223c12/28/2017), where more than1.1 million people live with more than 540,000 registered vehicles as of2017. A total of 116 DSRC sensors are used for data collection, whichare installed every 5.7km and cover a total of 68 main roads. For thehighway road network, we use the METR-LA data [24], which werecollected from 207 loop detectors (range: 3/1/2012\u223c6/27/2012) on thehighways of Los Angeles. Note that the highway network data we useare the standard benchmark data for traffic forecasting tasks [40, 77, 82].After discussing with domain experts and reviewing training results, wereplace the missing data and explicit errors with historical data. We alsouse 5-minute aggregated data to mitigate possible effects of outliers, asperformed in many previous studies (e.g., [40]).", "data_code": {"tables": 1, "geometry": 1, "temporal": 1}}, "solution": [{"solution_text": "We describe how we utilize ST-GRAT to demonstrate our VA approach.We choose ST-GRAT because 1) it has demonstrated state-of-the-artperformance and because 2) we can produce attention matrices onspatio-temporal dependency (e.g., Eq. 1, Fig. 3D) [51]. We also consider that the domain experts have expressed that attention models areextensively used at work for not only speed prediction but also othertasks, such as travel time [16] and taxi demand prediction [74]. ST-GRAT (Fig. 3) is a variation of the transformer [65] that usesthe encoder-decoder architecture with self-attention (i.e., temporal attention). Additionally, ST-GRAT utilizes graph attention as spatialattention before temporal attention with a sentinel vector. The sentinel vector acts as weights for skip connection within the same road.ST-GRAT utilizes 12-length sequential historical speed with encodedfeatures for each road and predicts 12 sequential speed predictions.There are three types of layers in the encoder and decoder: embedding, spatial attention, and temporal attention layers (Fig. 3B). To allowthe model to extract the spatio-temporal dependencies, we provide aroad network, speed, and observed time as the input features for theembedding layer (Fig. 3B). The road network graph is directed graphG(V,E,A), where a road is represented as a node (i.e., V) and theconnection between roads is shown as a link (i.e., E). Note that roadnetwork G is directed graph, which allow model to distinguish the roaddirections. Note that we provide the order of a given sequence usingthe position embedding method [65].The model captures spatial dependencies among neighbor roads inthe spatial attention layer (Fig. 3B) by using a graph attention network [66, 82], a well-known graph modeling method. In short, thespatial attention layer integrates information among neighboring roadsby directed graph attention. This directed spatial attention improvesdependency modeling and helps developers interpret ST-GRAT.The temporal attention layer (Fig. 3B) models the temporal dependency and trends of given sequences. For modeling, temporal attentionperforms multi-head attention to compute temporal correlations. Theattention type is decided by attention axes; spatial attention aggregatesfeatures among the spatial axis (i.e., neighboring roads), while temporal attention aggregates input features within the temporal axis (i.e.,different time steps of the same roads (Fig. 3C).To sum up, there are two important attention layers\u2013spatial andtemporal attention\u2013in ST-GRAT. Vaswani et al. [65] describe an attention function as mapping a query and a set of key-value pairs to anoutput, where the query, key, values, and output are all vectors. Fromthis perspective, from each key-value pair, we can derive an attentionmatrix from each attention layer, called SA and TA, and create a spatio temporal matrix (ST matrix). Specifically, for each attention head,given X before spatial attention and H after temporal attention, theirrelationship can be written as follows:H = (TA\u2299SA)X, (1)where TA \u2299 SA, named spatio-temporal attention, is the attention thatusers mainly explore for understanding model behaviors.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "In this work, we incorporate two automated methods\u2013warping (DTW) with spectral clustering [6, 57, 73] anddynamic timeGrangercausality tests [14], to allow users to determine which referencesare appropriate for predictions and whether a model employs properreferences for the predictions (R2-3) [38]. We use daily trends of roads as input for DTW computation, whichshould be explored since they are encoded as temporal dependenciesamong roads, which a model learns during training [74]. However,it is difficult to analyze road trends for two reasons. First, there aretoo many trends (i.e., roads) in a traffic dataset. Second, there aredifferent time gaps between the trends of roads due to different levelsof dependencies, which are presented with either lagging or precedingtrends [39]. For example, when congestion occurs on a road, the roadslinked to the congested road also become congested, but there are noconcrete patterns when the neighboring roads are congested. As such,we incorporate dynamic time warping (DTW) [6] in this work, whichcalculates the similarity in two time series over time gaps after findingthe best matching alignment that minimizes the distance, as Le Guenand Thome do for their time-series forecasting model design [35]. Forefficiency, we use FastDTW [57], whose complexity is O(N). Then we perform spectral clustering [47] with the computed DTWscores to further allow cluster-based analysis, which has shown itseffectiveness in analyzing traffic data [5, 73]. For example, users caneasily confirm whether referred roads of a target road have similar dailytrends as the target road using clusters. If the target and referred roadsare in the same cluster, it is highly possible that the model learns thedependencies of the target and referred roads during training [39]. Notethat we use the ELBOW method [1] with visual inspection to choosethe number of clusters.As Wu et al. [71] show in their study, catching preceding patterns intime-series enables effective analysis of deep learning models. Therefore, to help users better explore the preceding patterns in the trafficdata, and to complement DTW that distorts the time during its computation, we incorporate the Granger causality test [14] in this work,a well-known temporal dependency investigation method. We first describe the definition of Granger causality based on two principles: (1) the cause happens prior to its effect and (2) has unique informationabout the future values of its effect. Given these two principles aboutcausality, Granger causality proposes testing the following hypothesisfor the identification of a causal effect.GrangerCausality = P[Y(t ;1) \u2208 A|I (t)] = P[Y(t ;1) \u2208 |I\u2212X (t)]Here, P is probability, A is an arbitrary non-empty set, and I (t) andI\u2212X (t) denote the information available as of time t in the entireuniverse, and in the modified universe where X is excluded, respectively.If the above hypothesis is accepted, we say X Granger-causes Y.", "solution_category": "data_manipulation", "solution_axial": "Modeling;Explainability", "solution_compoent": "", "axial_code": ["Modeling", "Explainability"], "componenet_code": ["modeling", "explainability"]}, {"solution_text": "As models learn dependencies among roads and forecast based on the learned dependencies, it is important to explore what dependencies a model learns with road speed patterns and which roads a model refers to for performance investigation. To help users perform such exploration (R2-1, R2-2), we first present each road as a white dot. Each circle surrounding the dots shows the cluster that the road belongs to. We provide three visualizations in the map view: heatmap, attention arrows (AttnArrows), and cluster visualization. First, to help users overview the relationship between roads\u2019 congestion levels and model performance (Q1), we visualize roads\u2019 congestion levels using heatmaps (Fig. 1D). Here, the redder the heatmaps are, the slower the roads are (heatmap legend: Fig. 1 D2). We also have considered providing heatmaps with a time filter so that users can explore regions with high model error but decided against this because this approach requires many interactions for filtering and memorizing heatmaps that have changed due to filtering. Using Bezier curves, AttnArrows (Fig. 4) link a target road and the roads that the target road attends for prediction. Here, the color represents the amount of attention given to the roads. The darker the head color, the more attention the reference road is given.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Map;Circle;Heatmap", "axial_code": [], "componenet_code": ["heatmap", "map", "circle"]}]}, {"author": "dxf", "index_original": 107, "paper_title": "A Visual Analytics System for Improving Attention-based Traffic Forecasting Models", "pub_year": 2023, "domain": "Traffic Visualization", "requirement": {"requirement_text": "R2-2: data on model behaviors (Q2\u2013Q5), such as which roads a model refers to for forecasting (i.e., which roads influence the prediction for a target road [19]), and which input sequences are importantly used for the prediction.", "requirement_code": {"identify_main_cause_aggregate": 1}}, "data": {"data_text": "In this work, we use the traffic data of two different road networks\u2013theurban and highway road networks to explore the model\u2019s inferenceprocess for speed prediction. For the urban road network, we use ded\\x02icated short range communication (DSRC) data [28] generated fromUlsan, South Korea (range: 9/1/2017\u223c12/28/2017), where more than1.1 million people live with more than 540,000 registered vehicles as of2017. A total of 116 DSRC sensors are used for data collection, whichare installed every 5.7km and cover a total of 68 main roads. For thehighway road network, we use the METR-LA data [24], which werecollected from 207 loop detectors (range: 3/1/2012\u223c6/27/2012) on thehighways of Los Angeles. Note that the highway network data we useare the standard benchmark data for traffic forecasting tasks [40, 77, 82].After discussing with domain experts and reviewing training results, wereplace the missing data and explicit errors with historical data. We alsouse 5-minute aggregated data to mitigate possible effects of outliers, asperformed in many previous studies (e.g., [40]).", "data_code": {"tables": 1}}, "solution": [{"solution_text": "We describe how we utilize ST-GRAT to demonstrate our VA approach.We choose ST-GRAT because 1) it has demonstrated state-of-the-artperformance and because 2) we can produce attention matrices onspatio-temporal dependency (e.g., Eq. 1, Fig. 3D) [51]. We also consider that the domain experts have expressed that attention models areextensively used at work for not only speed prediction but also othertasks, such as travel time [16] and taxi demand prediction [74]. ST-GRAT (Fig. 3) is a variation of the transformer [65] that usesthe encoder-decoder architecture with self-attention (i.e., temporal attention). Additionally, ST-GRAT utilizes graph attention as spatialattention before temporal attention with a sentinel vector. The sentinel vector acts as weights for skip connection within the same road.ST-GRAT utilizes 12-length sequential historical speed with encodedfeatures for each road and predicts 12 sequential speed predictions.There are three types of layers in the encoder and decoder: embedding, spatial attention, and temporal attention layers (Fig. 3B). To allowthe model to extract the spatio-temporal dependencies, we provide aroad network, speed, and observed time as the input features for theembedding layer (Fig. 3B). The road network graph is directed graphG(V,E,A), where a road is represented as a node (i.e., V) and theconnection between roads is shown as a link (i.e., E). Note that roadnetwork G is directed graph, which allow model to distinguish the roaddirections. Note that we provide the order of a given sequence usingthe position embedding method [65].The model captures spatial dependencies among neighbor roads inthe spatial attention layer (Fig. 3B) by using a graph attention network [66, 82], a well-known graph modeling method. In short, thespatial attention layer integrates information among neighboring roadsby directed graph attention. This directed spatial attention improvesdependency modeling and helps developers interpret ST-GRAT.The temporal attention layer (Fig. 3B) models the temporal dependency and trends of given sequences. For modeling, temporal attentionperforms multi-head attention to compute temporal correlations. Theattention type is decided by attention axes; spatial attention aggregatesfeatures among the spatial axis (i.e., neighboring roads), while temporal attention aggregates input features within the temporal axis (i.e.,different time steps of the same roads (Fig. 3C).To sum up, there are two important attention layers\u2013spatial andtemporal attention\u2013in ST-GRAT. Vaswani et al. [65] describe an attention function as mapping a query and a set of key-value pairs to anoutput, where the query, key, values, and output are all vectors. Fromthis perspective, from each key-value pair, we can derive an attentionmatrix from each attention layer, called SA and TA, and create a spatio temporal matrix (ST matrix). Specifically, for each attention head,given X before spatial attention and H after temporal attention, theirrelationship can be written as follows:H = (TA\u2299SA)X, (1)where TA \u2299 SA, named spatio-temporal attention, is the attention thatusers mainly explore for understanding model behaviors.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "There are two visualization tabs in the attention views: spatio-temporal (ST) and head-cluster views for R2-2. We use pixel-based visualization [27,31] to present the model\u2019s spatio-temporal attention in a matrix form, because of its scalability on the number of items [27, 31]. In the ST attention matrix view (e.g., Fig. 1E), the x-axis indicates different roads, and the y-axis indicates past time steps (5-minute intervals, 12 steps, top: 5 minutes ago, bottom: 60 minutes ago). In the view, users can analyze which roads and time steps the model focuses on with attention intensity, represented by the color (legend: Fig. 1 D2). For example, when users click road 81 on the map, the spatio-temporal view (Fig. 1E) shows that road 81 attends itself (i.e., self-reference) and 73 more than other roads for its prediction.", "solution_category": "visualization", "solution_axial": "mirror", "solution_compoent": "Matrix;Heatmap", "axial_code": [], "componenet_code": ["heatmap", "matrix", "heatmap", "matrix"]}, {"solution_text": "In the view, users can analyze which roads and time steps the model focuses on with attention intensity, represented by the color (legend: Fig. 1 D2). For example, when users click road 81 on the map, the spatio-temporal view (Fig. 1E) shows that road 81 attends itself (i.e., self-reference) and 73 more than other roads for its prediction.", "solution_category": "interaction", "solution_axial": "Selecting;Participation/Collaboration", "solution_compoent": "", "axial_code": ["Selecting", "Participation/Collaboration"], "componenet_code": ["selecting", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 108, "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics", "pub_year": 2023, "domain": "E-commerce", "requirement": {"requirement_text": "R.1: Enable interactive comparison of products. Marketers often compete with peers selling similar products and need information on other complementary products to make promotional decisions accordingly. Therefore, experts expected that our approach can provide an overview of available product data, including but not limited to product ID, category, and brand, to help them observe relationships with other items and quickly retrieve specific products for further analysis.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.", "data_code": {"tables": 1, "sequential": 1, "temporal": 1}}, "solution": [{"solution_text": "We design a Promotion Overview to provide the analysis with the promotions of the selected product and their correlation with the sales volume in the past periods (R.1, R.3). As shown in Fig. 3, there are two rings, each representing a year. The internal one represents the previous year and the external one represents the next year. The internal line graph represents the sales volume of the corresponding promotion offered on that day, and the external bar graph generates information on \u201cwhat\u201d and \u201chow many\u201d promotion strategies were used, with different colors representing a different types of promotions. Also, the height of the bars of the same color indicates the number of promotions using that type.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Pie;Bar;Line", "axial_code": [], "componenet_code": ["line", "bar", "pie"]}]}, {"author": "dxf", "index_original": 109, "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics", "pub_year": 2023, "domain": "E-commerce", "requirement": {"requirement_text": "R.1: Enable interactive comparison of products. Marketers often compete with peers selling similar products and need information on other complementary products to make promotional decisions accordingly. Therefore, experts expected that our approach can provide an overview of available product data, including but not limited to product ID, category, and brand, to help them observe relationships with other items and quickly retrieve specific products for further analysis.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Here, we use t-SNE to project all products into two-dimensional space to see potential clusters and outlier points.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The Product Overview (Fig. 2(A)) enables users to efficiently select a product of interest, observe its general information and examine its relationship with other products (R.1). Dimensionality reduction tech_x0002_niques like t-SNE, PCA, and MDS can generate low-dimensional representations and preserve local similarities to convey neighborhood struc_x0002_tures like potential anomalies and clusters, all of which can be applied to explore and illustrate patterns in higher-dimensional spaces [24, 42]. Here, we use t-SNE to project all products into two-dimensional space to see potential clusters and outlier points. After discussing with do_x0002_main experts and surveying existing e-commerce practices, we used the following six metrics: sales median, standard deviation, quartile range, and the correlation between price, promotion, season and sales. These metrics form an n-dimensional feature vector x1,...,xN. We place the corresponding product statistics glyph, each with six features, in the calculated positions (detailed product statistics glyphs will be presented in the Competitive Analysis View (Fig. 2(D)). To interact with this view, the user should first set the filters that control the data, type, and brand of the product. Then, the system will automatically highlight all prod_x0002_ucts that match the filter criteria for selection. The user can click on these points to select the target product. In addition, if the user knows the product ID, he/she can also enter that ID in the search box and press Search . When a specific product is selected, message tooltip such as product ID, product name and product category will be displayed._x0002_", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "To interact with this view, the user should first set the filters that control the data, type, and brand of the product. Then, the system will automatically highlight all prod_x0002_ucts that match the filter criteria for selection. The user can click on these points to select the target product. In addition, if the user knows the product ID, he/she can also enter that ID in the search box and press Search . When a specific product is selected, message tooltip such as product ID, product name and product category will be displayed._x0002_", "solution_category": "interaction", "solution_axial": "Filtering;Participation/Collaboration", "solution_compoent": "", "axial_code": ["Filtering", "Participation/Collaboration"], "componenet_code": ["filtering", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 110, "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics", "pub_year": 2023, "domain": "E-commerce", "requirement": {"requirement_text": "R.1: Enable interactive comparison of products. Marketers often compete with peers selling similar products and need information on other complementary products to make promotional decisions accordingly. Therefore, experts expected that our approach can provide an overview of available product data, including but not limited to product ID, category, and brand, to help them observe relationships with other items and quickly retrieve specific products for further analysis.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.", "data_code": {"sequential": 1, "tables": 1, "temporal": 1}}, "solution": [{"solution_text": "Designing appropriate product titles is one of the most fundamental ways to attract potential customers by hitting their searches. Theresulting titles are crucial for machine learning models to distinguishbetween various products. As mentioned earlier, product titles arecomposed of descriptive words that may not be logically connected toeach other and can therefore be treated as a set of words rather thanparsed from the perspective of a complete sentence. For this purpose,we utilize a bag-of-words (BOW) [58] method to obtain the informationhidden in the titles. Specifically, we first create a codebook of all thewords that appear in product titles. Then, we split each title into a setof terms and compare them to the codebook to determine which wordsappear in the title. In this way, each title can be encoded as a vectorwith either 0 or 1, where 0 indicates the corresponding keyword in thecodebook is missing from the title and 1 indicates that the keyword ispresent in the title. It should be noted that the dimensionality of theconstructed vector is quite high for a title with only ten words, whichis too sparse for the model to learn features from the title. To solvethe sparsity problem, we follow the idea of word2vec [15] and reducethe title dimension to 8 after several experiments, i.e., we calculate thenumber of semantically valid words in 100 titles. The results show thateight words are sufficient to describe a product. Although promotions can be diverse, they usually follow some basicrules that allow us to translate textual information into numerical values.Based on these rules and the recommendations of experts, we classifyall promotions into two categories, including six promotion types (Table 1), for precise quantification. For direct discounts (value discount,percentage discount, and flash sale), we extract two key features. (1) thediscount rate kd, which is the ratio of the discounted price to originalprice; (2) the trigger amount pt , which is the minimum amount toreceive the discount. For indirect discounts (i.e., loyalty points, freeshipping, and interest-free installment), where the reward is not a directprice reduction, the reward and the original price of the product aretaken out as features. In addition, since the effect of a promotion mayexist before the campaign starts or after it ends, we track the entirelifecycle of the promotion by setting a status flag.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "In order to select the most appropriate model(s), we conduct a series ofexperiments and derive Table 2. To take full advantage of the historicaltime series data, as suggested by E.1, we enter the historical averagesales into the model, including the average daily sales for the last month,quarter, half year and full year. Considering the performance and theinterpretability of the model, we select three representative multivariate time series forecasting models as candidate forecasting techniques(i.e., RandomForest [25], XGBoost [13], and MLP [23]). Our systemcan easily integrate other types of time series forecasting models, e.g.,Vector Auto-Regressive Model (VAR), RNN, and CNN. Feature importance measures the impact of each feature on the prediction results,and explains the model\u2019s decision by calculating the additivity of eachfeature. We apply SHapley Additive exPlanations (SHAP) [38], an interpretation method that calculates Shapley values at the instance level,to estimate the feature importance for the sales prediction results. TheShapley values of these features add up to five values (i.e., descriptions,price, temporal information, competitive information, and promotion),indicating the power from the five relevant aspects. It is worth notingthat the Shapley value can be positive or negative, indicating that thefeature contributes positively or negatively to the prediction.", "solution_category": "data_manipulation", "solution_axial": "Modeling;Explainability", "solution_compoent": "", "axial_code": ["Modeling", "Explainability"], "componenet_code": ["modeling", "explainability"]}, {"solution_text": "The Sales Prediction View in Fig. 2(C) helps users understand the de_x0002_tailed correlation between the five most important factors (i.e., price, competition, temporal variation, title and promotions) and product sell_x0002_ing amount. In particular, at the top of the view is a visualization of the time, which shows the forecast profiles for the three models (Fig. 4(a)), making it easy for the user to adjust the length by dragging the gray slider to determine the time horizon to examine in detail. The main view of the Sales Prediction View shows the results of the three forecast models (R.2) and their interpretation of sales changes. As shown in Fig. 4(b), the graph for each model contains four parts: (1) the black dashed line (Fig. 4(b).1) indicates the ground truth sales volume for the selected time horizon; (2) the pink solid line (Fig. 4(b).2) shows the actual prices; (3) the purple solid step lines (Fig. 4(b).3) represents the comparison of the predicted sales amount with the real amount; (4) five colored bars representing the importance of five characteristics are attached above and below the model prediction line (Fig. 4(b).4) (R.3). It is worth noting that, as shown in Fig. 4(b).3, the daily predicted sales amount is represented as a step line rather than a curve or line segment, so that we can use the step space as the x-axis of daily feature impor_x0002_tance. According to Fig. 4(c), inspired by the design of mTSeer [54], the five feature importance are calculated, normalized, and represented as a stack of bars (R.1, R.3). Bars above the step lines indicate the positive impact on the forecast and vice versa. Further than mTSeer, at the bottom of the Sales Prediction View is a dotted line graph (Fig. 4(d)), which implies the promotion time coded by the length of the dotted line. All the charts mentioned above have the same X-axis, i.e., the time axis. After selecting a specific time horizon, the user can compare the performance of different models and pick up the chart that most closely resembles the ground truth prediction in order to observe the importance of its features and the promotions on the selected days. The user can also check the newly plotted forecasts caused by the updated promotional activity settings in the Strategy Setting View (Fig. 2(E)).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 111, "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics", "pub_year": 2023, "domain": "E-commerce", "requirement": {"requirement_text": "R.1: Enable interactive comparison of products. Marketers often compete with peers selling similar products and need information on other complementary products to make promotional decisions accordingly. Therefore, experts expected that our approach can provide an overview of available product data, including but not limited to product ID, category, and brand, to help them observe relationships with other items and quickly retrieve specific products for further analysis.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.", "data_code": {"tables": 1, "textual": 1, "temporal": 1}}, "solution": [{"solution_text": "The Competitive Analysis View (Fig. 2(D)) provides a comparison between the selected product and several of the most similar products in the same category (R.1), which can be considered as potential competitors. In this work, experts recommend comparing at least five other products of the same type. With this view, the user can find similarities and differences between the target product and its competitors, and thus build a corresponding promotion strategy solidly and confidently.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 112, "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics", "pub_year": 2023, "domain": "E-commerce", "requirement": {"requirement_text": "R.2: Predict future trends for individual items. Traditionally, when estimating and forecasting sales, our domain experts obtain historical sales data from Enterprise Resource Planning (ERP) system and invite some univariate time series models to make forecasts. \u201cThe system doesn\u2019t perform as well as expected, especially during promotional seasons,\u201d said E3. \u201cWe can empirically add a linear factor determined by promotion discount rates, but the results are often unsatisfactory,\u201d said E2. E1 also said that multiple variables combine to determine the amount of sales, of which \u201csome are difficult to quantify.\u201d She also commented that \u201cpromotion is one of the biggest factors that motivate shoppers to press the \u2018buy button\u2019.\u201d Therefore, our approach should support multivariate time series forecasting, consider quantifiable factors, and add promotion features to adjust the prediction results.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.", "data_code": {"sequential": 1, "tables": 1, "temporal": 1}}, "solution": [{"solution_text": "Designing appropriate product titles is one of the most fundamental ways to attract potential customers by hitting their searches. Theresulting titles are crucial for machine learning models to distinguishbetween various products. As mentioned earlier, product titles arecomposed of descriptive words that may not be logically connected toeach other and can therefore be treated as a set of words rather thanparsed from the perspective of a complete sentence. For this purpose,we utilize a bag-of-words (BOW) [58] method to obtain the informationhidden in the titles. Specifically, we first create a codebook of all thewords that appear in product titles. Then, we split each title into a setof terms and compare them to the codebook to determine which wordsappear in the title. In this way, each title can be encoded as a vectorwith either 0 or 1, where 0 indicates the corresponding keyword in thecodebook is missing from the title and 1 indicates that the keyword ispresent in the title. It should be noted that the dimensionality of theconstructed vector is quite high for a title with only ten words, whichis too sparse for the model to learn features from the title. To solvethe sparsity problem, we follow the idea of word2vec [15] and reducethe title dimension to 8 after several experiments, i.e., we calculate thenumber of semantically valid words in 100 titles. The results show thateight words are sufficient to describe a product. Although promotions can be diverse, they usually follow some basicrules that allow us to translate textual information into numerical values.Based on these rules and the recommendations of experts, we classifyall promotions into two categories, including six promotion types (Table 1), for precise quantification. For direct discounts (value discount,percentage discount, and flash sale), we extract two key features. (1) thediscount rate kd, which is the ratio of the discounted price to originalprice; (2) the trigger amount pt , which is the minimum amount toreceive the discount. For indirect discounts (i.e., loyalty points, freeshipping, and interest-free installment), where the reward is not a directprice reduction, the reward and the original price of the product aretaken out as features. In addition, since the effect of a promotion mayexist before the campaign starts or after it ends, we track the entirelifecycle of the promotion by setting a status flag.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "In order to select the most appropriate model(s), we conduct a series ofexperiments and derive Table 2. To take full advantage of the historicaltime series data, as suggested by E.1, we enter the historical averagesales into the model, including the average daily sales for the last month,quarter, half year and full year. Considering the performance and theinterpretability of the model, we select three representative multivariate time series forecasting models as candidate forecasting techniques(i.e., RandomForest [25], XGBoost [13], and MLP [23]). Our systemcan easily integrate other types of time series forecasting models, e.g.,Vector Auto-Regressive Model (VAR), RNN, and CNN. Feature importance measures the impact of each feature on the prediction results,and explains the model\u2019s decision by calculating the additivity of eachfeature. We apply SHapley Additive exPlanations (SHAP) [38], an interpretation method that calculates Shapley values at the instance level,to estimate the feature importance for the sales prediction results. TheShapley values of these features add up to five values (i.e., descriptions,price, temporal information, competitive information, and promotion),indicating the power from the five relevant aspects. It is worth notingthat the Shapley value can be positive or negative, indicating that thefeature contributes positively or negatively to the prediction.", "solution_category": "data_manipulation", "solution_axial": "Modeling;Explainability", "solution_compoent": "", "axial_code": ["Modeling", "Explainability"], "componenet_code": ["modeling", "explainability"]}, {"solution_text": "The Sales Prediction View in Fig. 2(C) helps users understand the de_x0002_tailed correlation between the five most important factors (i.e., price, competition, temporal variation, title and promotions) and product sell_x0002_ing amount. In particular, at the top of the view is a visualization of the time, which shows the forecast profiles for the three models (Fig. 4(a)), making it easy for the user to adjust the length by dragging the gray slider to determine the time horizon to examine in detail. The main view of the Sales Prediction View shows the results of the three forecast models (R.2) and their interpretation of sales changes. As shown in Fig. 4(b), the graph for each model contains four parts: (1) the black dashed line (Fig. 4(b).1) indicates the ground truth sales volume for the selected time horizon; (2) the pink solid line (Fig. 4(b).2) shows the actual prices; (3) the purple solid step lines (Fig. 4(b).3) represents the comparison of the predicted sales amount with the real amount; (4) five colored bars representing the importance of five characteristics are attached above and below the model prediction line (Fig. 4(b).4) (R.3). It is worth noting that, as shown in Fig. 4(b).3, the daily predicted sales amount is represented as a step line rather than a curve or line segment, so that we can use the step space as the x-axis of daily feature impor_x0002_tance. According to Fig. 4(c), inspired by the design of mTSeer [54], the five feature importance are calculated, normalized, and represented as a stack of bars (R.1, R.3). Bars above the step lines indicate the positive impact on the forecast and vice versa. Further than mTSeer, at the bottom of the Sales Prediction View is a dotted line graph (Fig. 4(d)), which implies the promotion time coded by the length of the dotted line. All the charts mentioned above have the same X-axis, i.e., the time axis. After selecting a specific time horizon, the user can compare the performance of different models and pick up the chart that most closely resembles the ground truth prediction in order to observe the importance of its features and the promotions on the selected days. The user can also check the newly plotted forecasts caused by the updated promotional activity settings in the Strategy Setting View (Fig. 2(E)).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 113, "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics", "pub_year": 2023, "domain": "E-commerce", "requirement": {"requirement_text": "R.3: Understand the factors that influence sales volume throughout the product lifecycle. According to the feedback from the domain experts, it is important not only to get the best promotion strategy, but also to find out what causes the diversity of results. E2 and E3 were particularly interested in some rare and unexpected situations in the timeline. Therefore, equipping the visualization system with factor analysis allows them to easily observe and speculate on the influencing factors. E1 also mentioned that advanced analytics should get feedback during the sales evolution of the merchandise and that \u201cretailer should be aware of influencing factors that may affect the sales volume for the whole sales period and not be limited to a certain promotion phase.\u201d", "requirement_code": {"identify_main_cause_aggregate": 1, "explain_differences": 1}}, "data": {"data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.", "data_code": {"sequential": 1, "tables": 1, "media": 1, "temporal": 1}}, "solution": [{"solution_text": "We design a Promotion Overview to provide the analysis with the promotions of the selected product and their correlation with the sales volume in the past periods (R.1, R.3). As shown in Fig. 3, there are two rings, each representing a year. The internal one represents the previous year and the external one represents the next year. The internal line graph represents the sales volume of the corresponding promotion offered on that day, and the external bar graph generates information on \u201cwhat\u201d and \u201chow many\u201d promotion strategies were used, with different colors representing a different types of promotions. Also, the height of the bars of the same color indicates the number of promotions using that type.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Pie;Bar;Line", "axial_code": [], "componenet_code": ["line", "bar", "pie"]}]}, {"author": "dxf", "index_original": 114, "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics", "pub_year": 2023, "domain": "E-commerce", "requirement": {"requirement_text": "R.3: Understand the factors that influence sales volume throughout the product lifecycle. According to the feedback from the domain experts, it is important not only to get the best promotion strategy, but also to find out what causes the diversity of results. E2 and E3 were particularly interested in some rare and unexpected situations in the timeline. Therefore, equipping the visualization system with factor analysis allows them to easily observe and speculate on the influencing factors. E1 also mentioned that advanced analytics should get feedback during the sales evolution of the merchandise and that \u201cretailer should be aware of influencing factors that may affect the sales volume for the whole sales period and not be limited to a certain promotion phase.\u201d", "requirement_code": {"identify_main_cause_aggregate": 1, "explain_differences": 1}}, "data": {"data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.", "data_code": {"tables": 1, "media": 1, "temporal": 1}}, "solution": [{"solution_text": "Designing appropriate product titles is one of the most fundamental ways to attract potential customers by hitting their searches. Theresulting titles are crucial for machine learning models to distinguishbetween various products. As mentioned earlier, product titles arecomposed of descriptive words that may not be logically connected toeach other and can therefore be treated as a set of words rather thanparsed from the perspective of a complete sentence. For this purpose,we utilize a bag-of-words (BOW) [58] method to obtain the informationhidden in the titles. Specifically, we first create a codebook of all thewords that appear in product titles. Then, we split each title into a setof terms and compare them to the codebook to determine which wordsappear in the title. In this way, each title can be encoded as a vectorwith either 0 or 1, where 0 indicates the corresponding keyword in thecodebook is missing from the title and 1 indicates that the keyword ispresent in the title. It should be noted that the dimensionality of theconstructed vector is quite high for a title with only ten words, whichis too sparse for the model to learn features from the title. To solvethe sparsity problem, we follow the idea of word2vec [15] and reducethe title dimension to 8 after several experiments, i.e., we calculate thenumber of semantically valid words in 100 titles. The results show thateight words are sufficient to describe a product. Although promotions can be diverse, they usually follow some basicrules that allow us to translate textual information into numerical values.Based on these rules and the recommendations of experts, we classifyall promotions into two categories, including six promotion types (Table 1), for precise quantification. For direct discounts (value discount,percentage discount, and flash sale), we extract two key features. (1) thediscount rate kd, which is the ratio of the discounted price to originalprice; (2) the trigger amount pt , which is the minimum amount toreceive the discount. For indirect discounts (i.e., loyalty points, freeshipping, and interest-free installment), where the reward is not a directprice reduction, the reward and the original price of the product aretaken out as features. In addition, since the effect of a promotion mayexist before the campaign starts or after it ends, we track the entirelifecycle of the promotion by setting a status flag.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "In order to select the most appropriate model(s), we conduct a series ofexperiments and derive Table 2. To take full advantage of the historicaltime series data, as suggested by E.1, we enter the historical averagesales into the model, including the average daily sales for the last month,quarter, half year and full year. Considering the performance and theinterpretability of the model, we select three representative multivariate time series forecasting models as candidate forecasting techniques(i.e., RandomForest [25], XGBoost [13], and MLP [23]). Our systemcan easily integrate other types of time series forecasting models, e.g.,Vector Auto-Regressive Model (VAR), RNN, and CNN. Feature importance measures the impact of each feature on the prediction results,and explains the model\u2019s decision by calculating the additivity of eachfeature. We apply SHapley Additive exPlanations (SHAP) [38], an interpretation method that calculates Shapley values at the instance level,to estimate the feature importance for the sales prediction results. TheShapley values of these features add up to five values (i.e., descriptions,price, temporal information, competitive information, and promotion),indicating the power from the five relevant aspects. It is worth notingthat the Shapley value can be positive or negative, indicating that thefeature contributes positively or negatively to the prediction.", "solution_category": "data_manipulation", "solution_axial": "Modeling;Explainability", "solution_compoent": "", "axial_code": ["Modeling", "Explainability"], "componenet_code": ["modeling", "explainability"]}, {"solution_text": "The Sales Prediction View in Fig. 2(C) helps users understand the de_x0002_tailed correlation between the five most important factors (i.e., price, competition, temporal variation, title and promotions) and product sell_x0002_ing amount. In particular, at the top of the view is a visualization of the time, which shows the forecast profiles for the three models (Fig. 4(a)), making it easy for the user to adjust the length by dragging the gray slider to determine the time horizon to examine in detail. The main view of the Sales Prediction View shows the results of the three forecast models (R.2) and their interpretation of sales changes. As shown in Fig. 4(b), the graph for each model contains four parts: (1) the black dashed line (Fig. 4(b).1) indicates the ground truth sales volume for the selected time horizon; (2) the pink solid line (Fig. 4(b).2) shows the actual prices; (3) the purple solid step lines (Fig. 4(b).3) represents the comparison of the predicted sales amount with the real amount; (4) five colored bars representing the importance of five characteristics are attached above and below the model prediction line (Fig. 4(b).4) (R.3). It is worth noting that, as shown in Fig. 4(b).3, the daily predicted sales amount is represented as a step line rather than a curve or line segment, so that we can use the step space as the x-axis of daily feature impor_x0002_tance. According to Fig. 4(c), inspired by the design of mTSeer [54], the five feature importance are calculated, normalized, and represented as a stack of bars (R.1, R.3). Bars above the step lines indicate the positive impact on the forecast and vice versa. Further than mTSeer, at the bottom of the Sales Prediction View is a dotted line graph (Fig. 4(d)), which implies the promotion time coded by the length of the dotted line. All the charts mentioned above have the same X-axis, i.e., the time axis. After selecting a specific time horizon, the user can compare the performance of different models and pick up the chart that most closely resembles the ground truth prediction in order to observe the importance of its features and the promotions on the selected days. The user can also check the newly plotted forecasts caused by the updated promotional activity settings in the Strategy Setting View (Fig. 2(E)).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 115, "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics", "pub_year": 2023, "domain": "E-commerce", "requirement": {"requirement_text": "R.4: Identify the similarities and differences of existing promotion strategies. A promotion is a short-term incentive activity designed to encourage people to buy a product or service. Although there are many dazzling promotions such as \u2018\u2018coupons\u201d, \u2018\u2018gifts\u201d, and \u2018\u2018station-wide discounts\u201d, according to E4 and E5, there are many misconceptions in designing promotion strategies. In addition, many existing promotion strategies are relatively similar, and some retailers simply follow their competitors without thinking about why such promotion strategies work. \u201cSome merchants have only one promotional policy, for example, \u2018always discounts\u2019,\u201d said E4. Therefore, domain experts asked our system to support a detailed exploration of the similarities and differences of common promotion strategies available on e-commerce platforms.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Designing appropriate product titles is one of the most fundamental ways to attract potential customers by hitting their searches. Theresulting titles are crucial for machine learning models to distinguishbetween various products. As mentioned earlier, product titles arecomposed of descriptive words that may not be logically connected toeach other and can therefore be treated as a set of words rather thanparsed from the perspective of a complete sentence. For this purpose,we utilize a bag-of-words (BOW) [58] method to obtain the informationhidden in the titles. Specifically, we first create a codebook of all thewords that appear in product titles. Then, we split each title into a setof terms and compare them to the codebook to determine which wordsappear in the title. In this way, each title can be encoded as a vectorwith either 0 or 1, where 0 indicates the corresponding keyword in thecodebook is missing from the title and 1 indicates that the keyword ispresent in the title. It should be noted that the dimensionality of theconstructed vector is quite high for a title with only ten words, whichis too sparse for the model to learn features from the title. To solvethe sparsity problem, we follow the idea of word2vec [15] and reducethe title dimension to 8 after several experiments, i.e., we calculate thenumber of semantically valid words in 100 titles. The results show thateight words are sufficient to describe a product. Although promotions can be diverse, they usually follow some basicrules that allow us to translate textual information into numerical values.Based on these rules and the recommendations of experts, we classifyall promotions into two categories, including six promotion types (Table 1), for precise quantification. For direct discounts (value discount,percentage discount, and flash sale), we extract two key features. (1) thediscount rate kd, which is the ratio of the discounted price to originalprice; (2) the trigger amount pt , which is the minimum amount toreceive the discount. For indirect discounts (i.e., loyalty points, freeshipping, and interest-free installment), where the reward is not a directprice reduction, the reward and the original price of the product aretaken out as features. In addition, since the effect of a promotion mayexist before the campaign starts or after it ends, we track the entirelifecycle of the promotion by setting a status flag.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "In order to select the most appropriate model(s), we conduct a series ofexperiments and derive Table 2. To take full advantage of the historicaltime series data, as suggested by E.1, we enter the historical averagesales into the model, including the average daily sales for the last month,quarter, half year and full year. Considering the performance and theinterpretability of the model, we select three representative multivariate time series forecasting models as candidate forecasting techniques(i.e., RandomForest [25], XGBoost [13], and MLP [23]). Our systemcan easily integrate other types of time series forecasting models, e.g.,Vector Auto-Regressive Model (VAR), RNN, and CNN. Feature importance measures the impact of each feature on the prediction results,and explains the model\u2019s decision by calculating the additivity of eachfeature. We apply SHapley Additive exPlanations (SHAP) [38], an interpretation method that calculates Shapley values at the instance level,to estimate the feature importance for the sales prediction results. TheShapley values of these features add up to five values (i.e., descriptions,price, temporal information, competitive information, and promotion),indicating the power from the five relevant aspects. It is worth notingthat the Shapley value can be positive or negative, indicating that thefeature contributes positively or negatively to the prediction.", "solution_category": "data_manipulation", "solution_axial": "Modeling;Explainability", "solution_compoent": "", "axial_code": ["Modeling", "Explainability"], "componenet_code": ["modeling", "explainability"]}, {"solution_text": "the Strategy Setting View gives a detailed description of the promotion, allowing the domain experts to configure different promotion strategies and simulate their corresponding impact on sales volume (R.4, R.5). The Strategy Setting View is designed to meet the user\u2019s need for promotion strategy assumptions (R.5). When a product is selected in the Product Overview, all promotions within its time frame specified by the user in the sales prediction view are listed here. Users can use the category filters above to quickly locate specific promotions. For each campaign, users can press the Edit button to adjust the campaign status by changing the description in the box with the formatting in Table 1, or by enabling and disabling the checkboxes. Users can also modify the duration of each promotion by dragging the position and length of the slider below each term. Pressing the Delete button will delete an existing campaign, while pressing the Add button will add a campaign. When all promotions are set up, the user can re-run the model by pressing the Refresh button and the system will draw a new step line in purple in the Sales Prediction View and attach an updated feature importance bar to it.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Table;Text", "axial_code": [], "componenet_code": ["text", "table"]}, {"solution_text": "the Strategy Setting View gives a detailed description of the promotion, allowing the domain experts to configure different promotion strategies and simulate their corresponding impact on sales volume (R.4, R.5). The Strategy Setting View is designed to meet the user\u2019s need for promotion strategy assumptions (R.5). When a product is selected in the Product Overview, all promotions within its time frame specified by the user in the sales prediction view are listed here. Users can use the category filters above to quickly locate specific promotions. For each campaign, users can press the Edit button to adjust the campaign status by changing the description in the box with the formatting in Table 1, or by enabling and disabling the checkboxes. Users can also modify the duration of each promotion by dragging the position and length of the slider below each term. Pressing the Delete button will delete an existing campaign, while pressing the Add button will add a campaign. When all promotions are set up, the user can re-run the model by pressing the Refresh button and the system will draw a new step line in purple in the Sales Prediction View and attach an updated feature importance bar to it.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration;Selecting;Connect/Relate", "solution_compoent": "", "axial_code": ["Selecting", "Connect/Relate", "Participation/Collaboration"], "componenet_code": ["selecting", "connect_relate", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 116, "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics", "pub_year": 2023, "domain": "E-commerce", "requirement": {"requirement_text": "R.4: Identify the similarities and differences of existing promotion strategies. A promotion is a short-term incentive activity designed to encourage people to buy a product or service. Although there are many dazzling promotions such as \u2018\u2018coupons\u201d, \u2018\u2018gifts\u201d, and \u2018\u2018station-wide discounts\u201d, according to E4 and E5, there are many misconceptions in designing promotion strategies. In addition, many existing promotion strategies are relatively similar, and some retailers simply follow their competitors without thinking about why such promotion strategies work. \u201cSome merchants have only one promotional policy, for example, \u2018always discounts\u2019,\u201d said E5. Therefore, domain experts asked our system to support a detailed exploration of the similarities and differences of common promotion strategies available on e-commerce platforms.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.", "data_code": {"tables": 1, "textual": 1, "temporal": 1}}, "solution": [{"solution_text": "Designing appropriate product titles is one of the most fundamental ways to attract potential customers by hitting their searches. Theresulting titles are crucial for machine learning models to distinguishbetween various products. As mentioned earlier, product titles arecomposed of descriptive words that may not be logically connected toeach other and can therefore be treated as a set of words rather thanparsed from the perspective of a complete sentence. For this purpose,we utilize a bag-of-words (BOW) [58] method to obtain the informationhidden in the titles. Specifically, we first create a codebook of all thewords that appear in product titles. Then, we split each title into a setof terms and compare them to the codebook to determine which wordsappear in the title. In this way, each title can be encoded as a vectorwith either 0 or 1, where 0 indicates the corresponding keyword in thecodebook is missing from the title and 1 indicates that the keyword ispresent in the title. It should be noted that the dimensionality of theconstructed vector is quite high for a title with only ten words, whichis too sparse for the model to learn features from the title. To solvethe sparsity problem, we follow the idea of word2vec [15] and reducethe title dimension to 8 after several experiments, i.e., we calculate thenumber of semantically valid words in 100 titles. The results show thateight words are sufficient to describe a product. Although promotions can be diverse, they usually follow some basicrules that allow us to translate textual information into numerical values.Based on these rules and the recommendations of experts, we classifyall promotions into two categories, including six promotion types (Table 1), for precise quantification. For direct discounts (value discount,percentage discount, and flash sale), we extract two key features. (1) thediscount rate kd, which is the ratio of the discounted price to originalprice; (2) the trigger amount pt , which is the minimum amount toreceive the discount. For indirect discounts (i.e., loyalty points, freeshipping, and interest-free installment), where the reward is not a directprice reduction, the reward and the original price of the product aretaken out as features. In addition, since the effect of a promotion mayexist before the campaign starts or after it ends, we track the entirelifecycle of the promotion by setting a status flag.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "To select the top five most similar products, we calculate the Euclidean distance between the target product and the other five products in the same category in the Product Overview (Fig. 2(A)).", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "the Competitive Analysis View shows the characteristics of competing products, including their promotional information, and thus helps to compare promotion strategies and motivate domain experts to find better ones (R.1, R.4). The Competitive Analysis View (Fig. 2(D)) provides a comparison be_tween the selected product and several of the most similar products in the same category (R.1), which can be considered as potential competi_x0002_tors. In this work, experts recommend comparing at least five other products of the same type. With this view, the user can find similarities and differences between the target product and its competitors, and thus build a corresponding promotion strategy solidly and confidently. To select the top five most similar products, we calculate the Euclidean distance between the target product and the other five products in the same category in the Product Overview (Fig. 2(A)). To help users get a general idea of the products and compare the differences between them, we designed a novel glyph called product statistics glyph to encode the statistical attributes of the products (e.g., stability of sales amount, price elastics).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line;Text;Pie;Circle", "axial_code": [], "componenet_code": ["circle", "line", "text", "pie"]}]}, {"author": "dxf", "index_original": 117, "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics", "pub_year": 2023, "domain": "E-commerce", "requirement": {"requirement_text": "R.4: Identify the similarities and differences of existing promotion strategies. A promotion is a short-term incentive activity designed to encourage people to buy a product or service. Although there are many dazzling promotions such as \u2018\u2018coupons\u201d, \u2018\u2018gifts\u201d, and \u2018\u2018station-wide discounts\u201d, according to E4 and E5, there are many misconceptions in designing promotion strategies. In addition, many existing promotion strategies are relatively similar, and some retailers simply follow their competitors without thinking about why such promotion strategies work. \u201cSome merchants have only one promotional policy, for example, \u2018always discounts\u2019,\u201d said E4. Therefore, domain experts asked our system to support a detailed exploration of the similarities and differences of common promotion strategies available on e-commerce platforms.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Designing appropriate product titles is one of the most fundamental ways to attract potential customers by hitting their searches. Theresulting titles are crucial for machine learning models to distinguishbetween various products. As mentioned earlier, product titles arecomposed of descriptive words that may not be logically connected toeach other and can therefore be treated as a set of words rather thanparsed from the perspective of a complete sentence. For this purpose,we utilize a bag-of-words (BOW) [58] method to obtain the informationhidden in the titles. Specifically, we first create a codebook of all thewords that appear in product titles. Then, we split each title into a setof terms and compare them to the codebook to determine which wordsappear in the title. In this way, each title can be encoded as a vectorwith either 0 or 1, where 0 indicates the corresponding keyword in thecodebook is missing from the title and 1 indicates that the keyword ispresent in the title. It should be noted that the dimensionality of theconstructed vector is quite high for a title with only ten words, whichis too sparse for the model to learn features from the title. To solvethe sparsity problem, we follow the idea of word2vec [15] and reducethe title dimension to 8 after several experiments, i.e., we calculate thenumber of semantically valid words in 100 titles. The results show thateight words are sufficient to describe a product. Although promotions can be diverse, they usually follow some basicrules that allow us to translate textual information into numerical values.Based on these rules and the recommendations of experts, we classifyall promotions into two categories, including six promotion types (Table 1), for precise quantification. For direct discounts (value discount,percentage discount, and flash sale), we extract two key features. (1) thediscount rate kd, which is the ratio of the discounted price to originalprice; (2) the trigger amount pt , which is the minimum amount toreceive the discount. For indirect discounts (i.e., loyalty points, freeshipping, and interest-free installment), where the reward is not a directprice reduction, the reward and the original price of the product aretaken out as features. In addition, since the effect of a promotion mayexist before the campaign starts or after it ends, we track the entirelifecycle of the promotion by setting a status flag.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "In order to select the most appropriate model(s), we conduct a series ofexperiments and derive Table 2. To take full advantage of the historicaltime series data, as suggested by E.1, we enter the historical averagesales into the model, including the average daily sales for the last month,quarter, half year and full year. Considering the performance and theinterpretability of the model, we select three representative multivariate time series forecasting models as candidate forecasting techniques(i.e., RandomForest [25], XGBoost [13], and MLP [23]). Our systemcan easily integrate other types of time series forecasting models, e.g.,Vector Auto-Regressive Model (VAR), RNN, and CNN. Feature importance measures the impact of each feature on the prediction results,and explains the model\u2019s decision by calculating the additivity of eachfeature. We apply SHapley Additive exPlanations (SHAP) [38], an interpretation method that calculates Shapley values at the instance level,to estimate the feature importance for the sales prediction results. TheShapley values of these features add up to five values (i.e., descriptions,price, temporal information, competitive information, and promotion),indicating the power from the five relevant aspects. It is worth notingthat the Shapley value can be positive or negative, indicating that thefeature contributes positively or negatively to the prediction.", "solution_category": "data_manipulation", "solution_axial": "Modeling;Explainability", "solution_compoent": "", "axial_code": ["Modeling", "Explainability"], "componenet_code": ["modeling", "explainability"]}, {"solution_text": "the Strategy Setting View gives a detailed description of the promotion, allowing the domain experts to configure different promotion strategies and simulate their corresponding impact on sales volume (R.4, R.5). The Strategy Setting View is designed to meet the user\u2019s need for promotion strategy assumptions (R.5). When a product is selected in the Product Overview, all promotions within its time frame specified by the user in the sales prediction view are listed here. Users can use the category filters above to quickly locate specific promotions. For each campaign, users can press the Edit button to adjust the campaign status by changing the description in the box with the formatting in Table 1, or by enabling and disabling the checkboxes. Users can also modify the duration of each promotion by dragging the position and length of the slider below each term. Pressing the Delete button will delete an existing campaign, while pressing the Add button will add a campaign. When all promotions are set up, the user can re-run the model by pressing the Refresh button and the system will draw a new step line in purple in the Sales Prediction View and attach an updated feature importance bar to it.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Table;Text", "axial_code": [], "componenet_code": ["text", "table"]}, {"solution_text": "the Strategy Setting View gives a detailed description of the promotion, allowing the domain experts to configure different promotion strategies and simulate their corresponding impact on sales volume (R.4, R.5). The Strategy Setting View is designed to meet the user\u2019s need for promotion strategy assumptions (R.5). When a product is selected in the Product Overview, all promotions within its time frame specified by the user in the sales prediction view are listed here. Users can use the category filters above to quickly locate specific promotions. For each campaign, users can press the Edit button to adjust the campaign status by changing the description in the box with the formatting in Table 1, or by enabling and disabling the checkboxes. Users can also modify the duration of each promotion by dragging the position and length of the slider below each term. Pressing the Delete button will delete an existing campaign, while pressing the Add button will add a campaign. When all promotions are set up, the user can re-run the model by pressing the Refresh button and the system will draw a new step line in purple in the Sales Prediction View and attach an updated feature importance bar to it.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration;Selecting;Connect/Relate", "solution_compoent": "", "axial_code": ["Selecting", "Connect/Relate", "Participation/Collaboration"], "componenet_code": ["selecting", "connect_relate", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 118, "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics", "pub_year": 2023, "domain": "E-commerce", "requirement": {"requirement_text": "R.4: Identify the similarities and differences of existing promotion strategies. A promotion is a short-term incentive activity designed to encourage people to buy a product or service. Although there are many dazzling promotions such as \u2018\u2018coupons\u201d, \u2018\u2018gifts\u201d, and \u2018\u2018station-wide discounts\u201d, according to E4 and E5, there are many misconceptions in designing promotion strategies. In addition, many existing promotion strategies are relatively similar, and some retailers simply follow their competitors without thinking about why such promotion strategies work. \u201cSome merchants have only one promotional policy, for example, \u2018always discounts\u2019,\u201d said E5. Therefore, domain experts asked our system to support a detailed exploration of the similarities and differences of common promotion strategies available on e-commerce platforms.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.", "data_code": {"tables": 1, "textual": 1, "temporal": 1}}, "solution": [{"solution_text": "Designing appropriate product titles is one of the most fundamental ways to attract potential customers by hitting their searches. Theresulting titles are crucial for machine learning models to distinguishbetween various products. As mentioned earlier, product titles arecomposed of descriptive words that may not be logically connected toeach other and can therefore be treated as a set of words rather thanparsed from the perspective of a complete sentence. For this purpose,we utilize a bag-of-words (BOW) [58] method to obtain the informationhidden in the titles. Specifically, we first create a codebook of all thewords that appear in product titles. Then, we split each title into a setof terms and compare them to the codebook to determine which wordsappear in the title. In this way, each title can be encoded as a vectorwith either 0 or 1, where 0 indicates the corresponding keyword in thecodebook is missing from the title and 1 indicates that the keyword ispresent in the title. It should be noted that the dimensionality of theconstructed vector is quite high for a title with only ten words, whichis too sparse for the model to learn features from the title. To solvethe sparsity problem, we follow the idea of word2vec [15] and reducethe title dimension to 8 after several experiments, i.e., we calculate thenumber of semantically valid words in 100 titles. The results show thateight words are sufficient to describe a product. Although promotions can be diverse, they usually follow some basicrules that allow us to translate textual information into numerical values.Based on these rules and the recommendations of experts, we classifyall promotions into two categories, including six promotion types (Table 1), for precise quantification. For direct discounts (value discount,percentage discount, and flash sale), we extract two key features. (1) thediscount rate kd, which is the ratio of the discounted price to originalprice; (2) the trigger amount pt , which is the minimum amount toreceive the discount. For indirect discounts (i.e., loyalty points, freeshipping, and interest-free installment), where the reward is not a directprice reduction, the reward and the original price of the product aretaken out as features. In addition, since the effect of a promotion mayexist before the campaign starts or after it ends, we track the entirelifecycle of the promotion by setting a status flag.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "To select the top five most similar products, we calculate the Euclidean distance between the target product and the other five products in the same category in the Product Overview (Fig. 2(A)).", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "the Competitive Analysis View shows the characteristics of competing products, including their promotional information, and thus helps to compare promotion strategies and motivate domain experts to find better ones (R.1, R.4). The Competitive Analysis View (Fig. 2(D)) provides a comparison be_tween the selected product and several of the most similar products in the same category (R.1), which can be considered as potential competi_x0002_tors. In this work, experts recommend comparing at least five other products of the same type. With this view, the user can find similarities and differences between the target product and its competitors, and thus build a corresponding promotion strategy solidly and confidently. To select the top five most similar products, we calculate the Euclidean distance between the target product and the other five products in the same category in the Product Overview (Fig. 2(A)). To help users get a general idea of the products and compare the differences between them, we designed a novel glyph called product statistics glyph to encode the statistical attributes of the products (e.g., stability of sales amount, price elastics).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line;Text;Pie;Circle", "axial_code": [], "componenet_code": ["circle", "line", "text", "pie"]}]}, {"author": "dxf", "index_original": 119, "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics", "pub_year": 2023, "domain": "E-commerce", "requirement": {"requirement_text": "R.5: Support \u201cwhat-if\u201d promotion simulations for each commodity. According to E1, there is always a strong need and demand for promotion strategy development. Although previous studies have conducted theoretical and mathematical simulations of specific promotional behaviors, experts still wanted a system that can: enable interactive adjustment of promotional policies, demonstrate the effects of different promotion strategies on a specific commodity, and summarize the effects of promotional behaviors through the performance on various commodities. Thus, our system should support \u201cwhat-if\u201d promotion simulations to examine the model\u2019s response for each commodity.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Designing appropriate product titles is one of the most fundamental ways to attract potential customers by hitting their searches. Theresulting titles are crucial for machine learning models to distinguishbetween various products. As mentioned earlier, product titles arecomposed of descriptive words that may not be logically connected toeach other and can therefore be treated as a set of words rather thanparsed from the perspective of a complete sentence. For this purpose,we utilize a bag-of-words (BOW) [58] method to obtain the informationhidden in the titles. Specifically, we first create a codebook of all thewords that appear in product titles. Then, we split each title into a setof terms and compare them to the codebook to determine which wordsappear in the title. In this way, each title can be encoded as a vectorwith either 0 or 1, where 0 indicates the corresponding keyword in thecodebook is missing from the title and 1 indicates that the keyword ispresent in the title. It should be noted that the dimensionality of theconstructed vector is quite high for a title with only ten words, whichis too sparse for the model to learn features from the title. To solvethe sparsity problem, we follow the idea of word2vec [15] and reducethe title dimension to 8 after several experiments, i.e., we calculate thenumber of semantically valid words in 100 titles. The results show thateight words are sufficient to describe a product. Although promotions can be diverse, they usually follow some basicrules that allow us to translate textual information into numerical values.Based on these rules and the recommendations of experts, we classifyall promotions into two categories, including six promotion types (Table 1), for precise quantification. For direct discounts (value discount,percentage discount, and flash sale), we extract two key features. (1) thediscount rate kd, which is the ratio of the discounted price to originalprice; (2) the trigger amount pt , which is the minimum amount toreceive the discount. For indirect discounts (i.e., loyalty points, freeshipping, and interest-free installment), where the reward is not a directprice reduction, the reward and the original price of the product aretaken out as features. In addition, since the effect of a promotion mayexist before the campaign starts or after it ends, we track the entirelifecycle of the promotion by setting a status flag.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "To select the top five most similar products, we calculate the Euclidean distance between the target product and the other five products in the same category in the Product Overview (Fig. 2(A)).", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "the Competitive Analysis View shows the characteristics of competing products, including their promotional information, and thus helps to compare promotion strategies and motivate domain experts to find better ones (R.1, R.4). The Competitive Analysis View (Fig. 2(D)) provides a comparison be_tween the selected product and several of the most similar products in the same category (R.1), which can be considered as potential competi_x0002_tors. In this work, experts recommend comparing at least five other products of the same type. With this view, the user can find similarities and differences between the target product and its competitors, and thus build a corresponding promotion strategy solidly and confidently. To select the top five most similar products, we calculate the Euclidean distance between the target product and the other five products in the same category in the Product Overview (Fig. 2(A)). To help users get a general idea of the products and compare the differences between them, we designed a novel glyph called product statistics glyph to encode the statistical attributes of the products (e.g., stability of sales amount, price elastics).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line;Text;Pie;Circle", "axial_code": [], "componenet_code": ["circle", "line", "text", "pie"]}]}, {"author": "dxf", "index_original": 120, "paper_title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics", "pub_year": 2023, "domain": "Spatiotemporal dynamics", "requirement": {"requirement_text": "R.1: Explain the temporal dynamics in the industrial and business sectors. The traditional approach is to transfer the change in information from the enterprise level to its corresponding sector level by calculating its contribution to the sector. While understanding the change in ratios for each sector provides an overview of the dynamics of all sectors over time, it only considers absolute numbers and ignores factors that may influence change in RIS dynamics. E1 is interested in explaining specific increases, outbreaks, or decreases on the time axis. Therefore, equipping the system with factor analysis allows to observe and speculate on the factors that influence the RIS temporal dynamics.", "requirement_code": {"identify_main_cause_aggregate": 1}}, "data": {"data_text": "The collaboration with the domain experts provided us with a publiclyavailable dataset of business registration records collected from theNational Enterprise Credit Information Publicity System2, coveringthe period from 1980 to 2015. Each record contains information suchas the enterprise\u2019s primary identification code, name, address, andregistered capital. Based on suggestions from domain experts, wedeveloped and studied the following attributes in this work: 1) address:it records detailed geographic information about a business; 2) startand end dates of operation: it allows us to track the the operations ofbusinesses that existed during a specific period for further analysis;and 3) basic information, including industry category, credit rating,registered capital, and enterprise status, among others. This informationcan be used to describe a specific business.", "data_code": {"geometry": 1, "tables": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "To understand the evolutionary pattern of RIS, we transform theregistration records of existing enterprises into a time-series dataset.Then, we use a multivariant time series prediction machine learningmodel to obtain the basic features of the evolutionary patterns. Formingtime series data from raw business registration data is nontrivial due tothe large volume and complex attributes of raw business registration data. Moreover, the temporal characteristics, i.e., the start and end datesof the business, are hidden in each record independently of each other.Therefore, considering the computational efficiency and speeding upthe processing, we change the index of the data from the main identitycode of the enterprise to the date and construct a hash table to reorganizethe original information. Before modeling the time series data, severalnon-numerical dimensions, such as industry category and credit rating,must be quantified. Since these attributes have a limited and relativelyfixed range of values, it is acceptable to map them to numbers in orderto maintain the information encoded using one-hot encoding.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Dimensionality reduction techniques such as t-SNE, PCA, and MDS have been widely adopted to create lowdimensional representations that preserve local similarity to express neighborhood structure [24,54]. We follow the conventional practice of projecting all RIS snapshots for the entire period into two-dimensional space to see potential clusters and outliers. After discussions with experts, we use the following metrics, namely, enterprise classification code, registered capital, credit rating, enterprise property, and enterprise state, to evaluate the RIS snapshots at the overview level.  We use t-SNE as the dimensional projection because \u201cit reveals meaningful insights about the data and shows superiority in generating two-dimensional projection\u201d [40]. ", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "we design the RIS Projection Overview to obtain the overall distribution of RIS patterns over the entire periods (R.1). The RIS Projection View helps to identify potential anomalies and \u201cclusters\u201d in RIS snapshots. Dimensionality reduction techniques such as t-SNE, PCA, and MDS have been widely adopted to create lowdimensional representations that preserve local similarity to express neighborhood structure [24,54]. We follow the conventional practice of projecting all RIS snapshots for the entire period into two-dimensional space to see potential clusters and outliers. After discussions with experts, we use the following metrics, namely, enterprise classification code, registered capital, credit rating, enterprise property, and enterprise state, to evaluate the RIS snapshots at the overview level. After obtaining the above metrics, we can obtain the corresponding feature vectors for each RIS snapshot. We use t-SNE as the dimensional projection because \u201cit reveals meaningful insights about the data and shows superiority in generating two-dimensional projection\u201d [40]. Similar to [63], as shown in Fig. 3, each RIS snapshot in two-dimensional space is represented by a point, and the color indicates the snapshot. The first and last snapshots are highlighted (red for the first snapshot and blue for the last) and a black curve connects all snapshots in chronological order. The user can hover over any particular RIS snapshot and a tooltip will display a detailed timestamp. The user can lasso any entity on the projection space for interaction and for further \u201clink ; view\u201d analysis.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "The user can hover over any particular RIS snapshot and a tooltip will display a detailed timestamp. The user can lasso any entity on the projection space for interaction and for further \u201clink ; view\u201d analysis.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "dxf", "index_original": 121, "paper_title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics", "pub_year": 2023, "domain": "Spatiotemporal dynamics", "requirement": {"requirement_text": "R.2: Explore general patterns and potential outliers in the dynamics of sector change. Experts need to quickly browse through large amounts of enterprise data to identify areas of interest. For example, sectors with unprecedented growth are more likely to attract the expert\u2019s attention. Further investigation is needed to explain general patterns and potential outliers across historical periods. For example, E1 \u2013 2 would like to find out if there are standard characteristics and outliers between industrial sectors and the possible reasons behind them.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The collaboration with the domain experts provided us with a publiclyavailable dataset of business registration records collected from theNational Enterprise Credit Information Publicity System2, coveringthe period from 1980 to 2015. Each record contains information suchas the enterprise\u2019s primary identification code, name, address, andregistered capital. Based on suggestions from domain experts, wedeveloped and studied the following attributes in this work: 1) address:it records detailed geographic information about a business; 2) startand end dates of operation: it allows us to track the the operations ofbusinesses that existed during a specific period for further analysis;and 3) basic information, including industry category, credit rating,registered capital, and enterprise status, among others. This informationcan be used to describe a specific business.", "data_code": {"sequential": 1, "tables": 1, "temporal": 1}}, "solution": [{"solution_text": "To understand the evolutionary pattern of RIS, we transform theregistration records of existing enterprises into a time-series dataset.Then, we use a multivariant time series prediction machine learningmodel to obtain the basic features of the evolutionary patterns. Formingtime series data from raw business registration data is nontrivial due tothe large volume and complex attributes of raw business registration data. Moreover, the temporal characteristics, i.e., the start and end datesof the business, are hidden in each record independently of each other.Therefore, considering the computational efficiency and speeding upthe processing, we change the index of the data from the main identitycode of the enterprise to the date and construct a hash table to reorganizethe original information. Before modeling the time series data, severalnon-numerical dimensions, such as industry category and credit rating,must be quantified. Since these attributes have a limited and relativelyfixed range of values, it is acceptable to map them to numbers in orderto maintain the information encoded using one-hot encoding.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "We define the input to a time series prediction machine learning modelas a sequence of historical data Xn = xn\u2212L,xn\u2212(L\u22121),...,xn\u22121,xn, whereL is the parameter that is adapted for different datasets and xn is a multidimensional feature vector with timestamp n (i.e., the feature vectorconsists of 7 dimensions, including year, month, enterprise classification code, registered capital, credit rating, enterprise property, andenterprise state) denoted as x fn \u2208 R. The goal of a multivariate timeseries forecasting models is to predict specific values at some futuretime stamp. In our case, we provide only a single-step forecast with anoutput market of yn = xn;1. Considering that different countries dividethe industrial structure in different ways, but basically divided into threemain categories, we first divide the original dataset into three significantindustries based on industry category, namely, primary industry, secondary industry, and tertiary industry. Then, we calculate the numberof enterprises existing in each month from 1980 to 2015. We use thedata from 1980 to 1990 as the first training set to predict the trend in1991, followed by the data before 1991 as the training set to predictthe trend for 1992, and so on. The reason for performing stepwiseforecasting is based on the practical requirements of most real-worldtime series forecasting tasks. Thus, the final prediction curve is spelledout for each time unit. We use the classical MAPE metric to evaluatethe performance of the forecasting model. There are several representative time series forecasting models available for evaluation, such asArima [61], Vector Auto-Regression Model (VAR) [46], Random ForestModel (RF) or Random Decision Forests [26] and Long Short-TermMemory Recurrent Neural Networks (LSTM), covering linear, nonlinearregression methods, and machine learning models [64]. In this work,we considered both model performance and model interpretability, andfinally select two machine learning models for multivariate time seriesprediction, namely, RF and XGBoost [13], as they have sufficientlyhigh model accuracy and good interpretability.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The Registration Prediction View captures the correlation between the predicted registrations and ground truth over time, and understands the latent features driving the changes from the perspective of the time series forecasting models (R.2 \u2013 R.3). The Registration Prediction View (Fig. 2) helps experts understand the detailed associations between RIS influencing factors and registration amounts based on several representative time-series forecasting models. It consists of three subplots, arranged from top to bottom, representing business activities in the primary, secondary and tertiary sectors, respectively. In each subplot, the dotted line represents the ground truth registration of enterprises. The solid black line represents the predicted business activity compared to the ground truth volume. Thus, users can observe the gap that represents the difference between the actual and the predicted value at different timestamps. In addition, the importance of the 7 features is calculated, normalized, and represented with different classification colors stacked on top of each other. In each snapshot, the glyphs of feature importance are placed along the Y-axis. Each bar in the stacked glyphs represents a feature. Bars stacked above the predicted values represent features with negative effects, pushing the predicted value down [64]. The features stacked below the predicted value are positive, pushing the predicted value higher. The user can select other models on the right side for comparison.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "The Registration Prediction View captures the correlation between the predicted registrations and ground truth over time, and understands the latent features driving the changes from the perspective of the time series forecasting models (R.2 \u2013 R.3). The Registration Prediction View (Fig. 2) helps experts understand the detailed associations between RIS influencing factors and registration amounts based on several representative time-series forecasting models. It consists of three subplots, arranged from top to bottom, representing business activities in the primary, secondary and tertiary sectors, respectively. In each subplot, the dotted line represents the ground truth registration of enterprises. The solid black line represents the predicted business activity compared to the ground truth volume. Thus, users can observe the gap that represents the difference between the actual and the predicted value at different timestamps. In addition, the importance of the 7 features is calculated, normalized, and represented with different classification colors stacked on top of each other. In each snapshot, the glyphs of feature importance are placed along the Y-axis. Each bar in the stacked glyphs represents a feature. Bars stacked above the predicted values represent features with negative effects, pushing the predicted value down [64]. The features stacked below the predicted value are positive, pushing the predicted value higher. The user can select other models on the right side for comparison.", "solution_category": "interaction", "solution_axial": "Selecting;Connect", "solution_compoent": "", "axial_code": ["Selecting", "Connect"], "componenet_code": ["selecting", "connect_relate"]}]}, {"author": "dxf", "index_original": 122, "paper_title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics", "pub_year": 2023, "domain": "Spatiotemporal dynamics", "requirement": {"requirement_text": "R.3: Predict the future composition of the RIS and explore the influencing factors behind it. Another conventional approach to RIS analysis is to understand the composition of the industrial sector, for example, the ratio and the appropriate number of establishments. Based on the history and current status of RIS, experts want to know the future trends  and influencing factors of the quality of the composition of each industrial sector. Thus, they can make speculations and assumptions about the future before developing the corresponding strategies.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "The collaboration with the domain experts provided us with a publiclyavailable dataset of business registration records collected from theNational Enterprise Credit Information Publicity System2, coveringthe period from 1980 to 2015. Each record contains information suchas the enterprise\u2019s primary identification code, name, address, andregistered capital. Based on suggestions from domain experts, wedeveloped and studied the following attributes in this work: 1) address:it records detailed geographic information about a business; 2) startand end dates of operation: it allows us to track the the operations ofbusinesses that existed during a specific period for further analysis;and 3) basic information, including industry category, credit rating,registered capital, and enterprise status, among others. This informationcan be used to describe a specific business.", "data_code": {"tables": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "To understand the evolutionary pattern of RIS, we transform theregistration records of existing enterprises into a time-series dataset.Then, we use a multivariant time series prediction machine learningmodel to obtain the basic features of the evolutionary patterns. Formingtime series data from raw business registration data is nontrivial due tothe large volume and complex attributes of raw business registration data. Moreover, the temporal characteristics, i.e., the start and end datesof the business, are hidden in each record independently of each other.Therefore, considering the computational efficiency and speeding upthe processing, we change the index of the data from the main identitycode of the enterprise to the date and construct a hash table to reorganizethe original information. Before modeling the time series data, severalnon-numerical dimensions, such as industry category and credit rating,must be quantified. Since these attributes have a limited and relativelyfixed range of values, it is acceptable to map them to numbers in orderto maintain the information encoded using one-hot encoding.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "We define the input to a time series prediction machine learning modelas a sequence of historical data Xn = xn\u2212L,xn\u2212(L\u22121),...,xn\u22121,xn, whereL is the parameter that is adapted for different datasets and xn is a multidimensional feature vector with timestamp n (i.e., the feature vectorconsists of 7 dimensions, including year, month, enterprise classification code, registered capital, credit rating, enterprise property, andenterprise state) denoted as x fn \u2208 R. The goal of a multivariate timeseries forecasting models is to predict specific values at some futuretime stamp. In our case, we provide only a single-step forecast with anoutput market of yn = xn;1. Considering that different countries dividethe industrial structure in different ways, but basically divided into threemain categories, we first divide the original dataset into three significantindustries based on industry category, namely, primary industry, secondary industry, and tertiary industry. Then, we calculate the numberof enterprises existing in each month from 1980 to 2015. We use thedata from 1980 to 1990 as the first training set to predict the trend in1991, followed by the data before 1991 as the training set to predictthe trend for 1992, and so on. The reason for performing stepwiseforecasting is based on the practical requirements of most real-worldtime series forecasting tasks. Thus, the final prediction curve is spelledout for each time unit. We use the classical MAPE metric to evaluatethe performance of the forecasting model. There are several representative time series forecasting models available for evaluation, such asArima [61], Vector Auto-Regression Model (VAR) [46], Random ForestModel (RF) or Random Decision Forests [26] and Long Short-TermMemory Recurrent Neural Networks (LSTM), covering linear, nonlinearregression methods, and machine learning models [64]. In this work,we considered both model performance and model interpretability, andfinally select two machine learning models for multivariate time seriesprediction, namely, RF and XGBoost [13], as they have sufficientlyhigh model accuracy and good interpretability.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The Registration Prediction View captures the correlation between the predicted registrations and ground truth over time, and understands the latent features driving the changes from the perspective of the time series forecasting models (R.2 \u2013 R.3). The Registration Prediction View (Fig. 2) helps experts understand the detailed associations between RIS influencing factors and registration amounts based on several representative time-series forecasting models. It consists of three subplots, arranged from top to bottom, representing business activities in the primary, secondary and tertiary sectors, respectively. In each subplot, the dotted line represents the ground truth registration of enterprises. The solid black line represents the predicted business activity compared to the ground truth volume. Thus, users can observe the gap that represents the difference between the actual and the predicted value at different timestamps. In addition, the importance of the 7 features is calculated, normalized, and represented with different classification colors stacked on top of each other. In each snapshot, the glyphs of feature importance are placed along the Y-axis. Each bar in the stacked glyphs represents a feature. Bars stacked above the predicted values represent features with negative effects, pushing the predicted value down [64]. The features stacked below the predicted value are positive, pushing the predicted value higher. The user can select other models on the right side for comparison.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "The Registration Prediction View captures the correlation between the predicted registrations and ground truth over time, and understands the latent features driving the changes from the perspective of the time series forecasting models (R.2 \u2013 R.3). The Registration Prediction View (Fig. 2) helps experts understand the detailed associations between RIS influencing factors and registration amounts based on several representative time-series forecasting models. It consists of three subplots, arranged from top to bottom, representing business activities in the primary, secondary and tertiary sectors, respectively. In each subplot, the dotted line represents the ground truth registration of enterprises. The solid black line represents the predicted business activity compared to the ground truth volume. Thus, users can observe the gap that represents the difference between the actual and the predicted value at different timestamps. In addition, the importance of the 7 features is calculated, normalized, and represented with different classification colors stacked on top of each other. In each snapshot, the glyphs of feature importance are placed along the Y-axis. Each bar in the stacked glyphs represents a feature. Bars stacked above the predicted values represent features with negative effects, pushing the predicted value down [64]. The features stacked below the predicted value are positive, pushing the predicted value higher. The user can select other models on the right side for comparison.", "solution_category": "interaction", "solution_axial": "Selecting;Connect", "solution_compoent": "", "axial_code": ["Selecting", "Connect"], "componenet_code": ["selecting", "connect_relate"]}]}, {"author": "dxf", "index_original": 123, "paper_title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics", "pub_year": 2023, "domain": "Spatiotemporal dynamics", "requirement": {"requirement_text": "R.4: Summarize the dynamics and multivariance of enterprises. As mentioned earlier, directly mapping millions of enterprises on a geographic map without any abstraction or simplification is bound to create visual clutter, compromise effective understanding, and affect the performance of the system. In addition, as time goes on, more and more new businesses are being established and added to the existing environment. Integrating these factors requires careful design to summarize the dynamics and multiple attributes of large-scale enterprises.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The collaboration with the domain experts provided us with a publiclyavailable dataset of business registration records collected from theNational Enterprise Credit Information Publicity System2, coveringthe period from 1980 to 2015. Each record contains information suchas the enterprise\u2019s primary identification code, name, address, andregistered capital. Based on suggestions from domain experts, wedeveloped and studied the following attributes in this work: 1) address:it records detailed geographic information about a business; 2) startand end dates of operation: it allows us to track the the operations ofbusinesses that existed during a specific period for further analysis;and 3) basic information, including industry category, credit rating,registered capital, and enterprise status, among others. This informationcan be used to describe a specific business.", "data_code": {"tables": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "To understand the evolutionary pattern of RIS, we transform theregistration records of existing enterprises into a time-series dataset.Then, we use a multivariant time series prediction machine learningmodel to obtain the basic features of the evolutionary patterns. Formingtime series data from raw business registration data is nontrivial due tothe large volume and complex attributes of raw business registration data. Moreover, the temporal characteristics, i.e., the start and end datesof the business, are hidden in each record independently of each other.Therefore, considering the computational efficiency and speeding upthe processing, we change the index of the data from the main identitycode of the enterprise to the date and construct a hash table to reorganizethe original information. Before modeling the time series data, severalnon-numerical dimensions, such as industry category and credit rating,must be quantified. Since these attributes have a limited and relativelyfixed range of values, it is acceptable to map them to numbers in orderto maintain the information encoded using one-hot encoding.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Two challenges exist in tracking the evolution of RIS over long periodsof time and large geographic scales. First, correctly delineating longtime periods is critical to identifying potential patterns in the RIS evolution as reflected by business registration activity. Simply retrievingrecords by year may lead to duplicate analyses if no significant changesoccur in the two subsequent periods. Second, even if we manage todivide the evolution of the RIS into different intervals, for each intervalwe are still confronted with a large amount of data that shows differentspatial locations by the records of the enterprises belonging to that interval. It is important to abstract the characteristics of the correspondingenterprises while maintaining dynamic and multi-attribute details. Inthe following subsections, we perform spatio-temporal clustering interms of time series segmentation and geospatial clustering.Partitioning of Time Series. To address the first challenge, weformulate the partitioning of long periods of RIS evolution as a segmentation problem, which partitions long time series into segments thatcan be formulated as follows: given a time series T, generate the bestrepresentation such that the combined errors (which can be obtainedby calculating the sum of the maximum error for all segments) is lessthan a user-specified threshold. Generating segments for such timeseries data is the key to an efficient and effective solution [36]. Notably,we take a top-down approach, considering every possible partition ofthe time series and segmenting it at the optimal location. The twosubsegments are then tested to see if their approximation error is belowa user-specified threshold. If not, the algorithm recursively splits thesubsequence until the approximation error of all segments is below thethreshold. Thus, the original time series registration is divided intoseveral piecewise linear representations after time series segmentation.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Geospatial Clustering. To address the second challenge of the various enterprise activities within each segment, we analyze the businessdata from a cluster perspective rather than an individual perspective.The goal of clustering is to make the similarity between objects withinthe same category as large as possible and, conversely, the similaritybetween different categories as small as possible. It is worth noting thatwe tried to cluster individual enterprises belonging to different time periods using the division-based KMeans algorithm and the density-basedclustering algorithm DBSCAN, respectively. However, they both exhibitspecific advantages and disadvantages. For example, although KMeanscan achieve clustering results, the determination of K is quite challenging because the location of enterprises is unpredictable. In addition,KMeans is developed based on the centroid location of the aggregatedclusters rather than the actual geographic locations of enterprises. Onthe other hand, DBSCAN requires manual input of two parameters,namely [E ps] and [minPts], so the accuracy of the clustering resultsdirectly depends on the users\u2019 parameter selection. Also, the obtainedclusters are spatially arbitrary shaped and are just sets of geographicalpoints rather than aggregated clusters. In addition, DBSCAN suffers from indistinguishable noisy data. For these reasons, we propose ahybrid algorithm based on DBSCAN and KMeans algorithms. First, thegeographic locations of enterprises are clustered into several clustersusing the density reachability of DBSCAN according to the specificsettings of the two parameters. The data in each cluster is taken asa new input. The centroid positions are then obtained using KMeansto minimize the Sum of Squared Error (SSE) between the data pointsin each cluster and the centroids of the clusters they are in using theiterative aggregation of KMeans and K is set to 1. In addition, we useKANN-DBSCAN [41], to automatically find a stable interval of clusternumber variation by generating candidate [E ps] and [MinPts] usingthe distribution characteristics of the dataset. We take the [E ps] and[MinPts] parameters corresponding to the minimum density thresholdof this interval as the optimal parameters. Specifically, first, the corresponding [E ps] candidate list can be obtained using KANN-DBSCAN.Then, for the given [E ps] candidate list, the number of neighboringobjects can be calculated sequentially and the expected values of these[E ps] are used as the [MinPts] candidate list: MinPts = 1n \u2211ni=1 Pi, Piis the number of [E ps] neighboring objects of the ith object, and n isthe total number of objects of the data. Finally, the list of these twoparameters is given as input to DBSCAN, and then the number of clusters generated with different parameter settings is obtained separatelyand quickly. The result can be considered stable when the number ofclusters is the same three times in a row. After deciding the clusters, we need to further define the centroids of each cluster. To summarize, we first apply KANN-DBSCAN to cluster the geographic locations of enter_x0002_prises into several clusters, and then use KMeans to find the centroid location of each cluster.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "We apply the classical map-centric visual exploration approach torepresent clusters of enterprise geographic locations. As shown inFig. 6, we design radar-like glyphs at the top of the map and placethem in the corresponding geographic regions of the regional clustersgenerated by the geospatial clustering described above. The indicatorsrepresented by the axes in the radar-like glyphs are the same as those inthe RIS evolution view, namely, aggregation index, average registeredcapital, total registered capital, credit rating, livability, and mortality.We did not choose a bar chart because we need to have the radius of theglyphs represent the number of enterprises in the cluster. In addition, inthe Regional Comparative View (see Section 6.5), we need to comparethe same indicators between two or three clusters, and radar-like glyphsare easier to perceive than bar charts for comparison.Note that we place all the regional clusters generated by DBSCANand KMeans on the map at different time intervals, inevitably causingvisual clutter and overlap problems. Therefore, we use a force-directedlayout to pack those overlapping clusters [15]. As shown in Fig. 6(a), aphysics-based simulator will be used to find the optimal circle positionsby 1) optimizing the distance between the circle and the force center,2) attracting each other slightly, and 3) avoiding overlap. We warp thepacked circles with an additional black ring (Fig. 6(a)), indicating thatthese groups of regions have some kind of unavoidable overlap. Toemphasize the representation of region groups from a particular timesnapshot and inspired by [3], we borrow the Bubble Sets technique [14]to provide continuous boundary contours that allow us to examine scenarios with semantically important spatial organization and importantset membership relationships for enterprises belonging to the snapshot.Notably, as shown in Fig. 6(c), the five colored bubble sets representthe geospatial locations in the five snapshots. We also provide regional evolution paths in the form of black curves that connect clusters ofregions in different snapshots in chronological order (Fig. 6(b)).To elaborate more detailed information about region-specific clusters,we provide a separate panel (Fig. 6(1 \u2013 5)) showing the geospatialmetrics (Fig. 6(1)), registration (Fig. 6(2)), livability (Fig. 6(3)), andbusiness category (Fig. 6(4)). The actual geographical coverage of theregional clusters is displayed as a heat map (Fig. 6(5)), where domainexperts can observe the most dense points and the scale of distribution.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Map;Pie;Area", "axial_code": [], "componenet_code": ["area", "map", "pie"]}]}, {"author": "dxf", "index_original": 124, "paper_title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics", "pub_year": 2023, "domain": "Spatiotemporal dynamics", "requirement": {"requirement_text": "R.4: Summarize the dynamics and multivariance of enterprises. As mentioned earlier, directly mapping millions of enterprises on a geographic map without any abstraction or simplification is bound to create visual clutter, compromise effective understanding, and affect the performance of the system. In addition, as time goes on, more and more new businesses are being established and added to the existing environment. Integrating these factors requires careful design to summarize the dynamics and multiple attributes of large-scale enterprises.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The collaboration with the domain experts provided us with a publiclyavailable dataset of business registration records collected from theNational Enterprise Credit Information Publicity System2, coveringthe period from 1980 to 2015. Each record contains information suchas the enterprise\u2019s primary identification code, name, address, andregistered capital. Based on suggestions from domain experts, wedeveloped and studied the following attributes in this work: 1) address:it records detailed geographic information about a business; 2) startand end dates of operation: it allows us to track the the operations ofbusinesses that existed during a specific period for further analysis;and 3) basic information, including industry category, credit rating,registered capital, and enterprise status, among others. This informationcan be used to describe a specific business.", "data_code": {"tables": 1}}, "solution": [{"solution_text": "To understand the evolutionary pattern of RIS, we transform theregistration records of existing enterprises into a time-series dataset.Then, we use a multivariant time series prediction machine learningmodel to obtain the basic features of the evolutionary patterns. Formingtime series data from raw business registration data is nontrivial due tothe large volume and complex attributes of raw business registration data. Moreover, the temporal characteristics, i.e., the start and end datesof the business, are hidden in each record independently of each other.Therefore, considering the computational efficiency and speeding upthe processing, we change the index of the data from the main identitycode of the enterprise to the date and construct a hash table to reorganizethe original information. Before modeling the time series data, severalnon-numerical dimensions, such as industry category and credit rating,must be quantified. Since these attributes have a limited and relativelyfixed range of values, it is acceptable to map them to numbers in orderto maintain the information encoded using one-hot encoding.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "The RIS Evolution View explores the regional features and their evolutionary paths over time (R.4). Although the above RIS Projection View conveys additional information about clusters and potential outliers in all RIS snapshots, it is still unclear how the RIS patterns evolve at the regional level. In addition, domain experts often need to explore the properties of each regional cluster in order to compare and evaluate them in one or more temporal snapshots, which can help them to assess the performance of RIS. Therefore, based on discussions with experts, we adopt and standardize the following indicators for a uniform comparison: 1) number of primary industries; 2) number of secondary industries; 3) number of tertiary industries; 4) aggregation index (AI(x)), which is calculated by the Coefficient of Variation [19], to measure the similarity of the distribution of indicators; 5) average registered capital; 6) total registered capital; 7) credit rating; 8) livability, i.e., the number of surviving enterprises divided by the total number of enterprises; and 9) mortality, complementary to livability.  we design a RIS Evolution View to help domain experts to compare metrics at the regional level. Notably, as shown in Fig. 5, we present the metric values for each region cluster as a combined bar, where the length of a single colored bar indicates the normalized metric value for the corresponding region cluster. We rank the region clusters generated in all consecutive snapshots. The ranking is based on the value of a specific metric. When a region cluster is clicked, all \u201cidentical\u201d region clusters across time snapshots are highlighted. We determine whether a regional cluster is identical between two subsequent snapshots based on the amount of overlaps, and we only depict the evolutionary path between two subsequent \u201cidentical\u201d region clusters with the largest number of overlaps. In the two subsequent snapshots, we place an axis with two different scales along the left and right sides of the axis. The left axis represents the scale of the number of enterprises that transition from one regional cluster to another in the next period. The right axis represents the scale of the distance between the two centroids of the regional clusters in the two subsequent snapshots. For example, as shown in in Fig. 5(1), these two figures indicate that 155,950 enterprises in one regional cluster between 1992 and 2006 stay in the other regional cluster between 2006 and 2012, while the distance between the centroids of the two regional clusters is 1.87km each. Intuitively, a small value indicates that the main distribution of enterprises in the early snapshot is largely consistent with the distribution of enterprises, including the existing enterprises in the early snapshot and newly established ones in the later snapshot.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "When a region cluster is clicked, all \u201cidentical\u201d region clusters across time snapshots are highlighted. ", "solution_category": "interaction", "solution_axial": "Filtering;Selecting", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "dxf", "index_original": 125, "paper_title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics", "pub_year": 2023, "domain": "Spatiotemporal dynamics", "requirement": {"requirement_text": "R.5: Reveal the regional \u201cagglomeration-performance\u201d relationship resided in various groups of enterprises. According to E3 \u2013 4, enterprise-level performance may be strongly related to the geospatial locations of agglomerations. Although theoretical studies have shown predominantly on positive performance effects as an incentive for enterprise collocation \u201cto explain the emergence of agglomerations\u201d, experts have also focused on adverse performance effects, \u201csome enterprises may benefit from agglomeration, while others may be harmed by aggregation and relocate elsewhere,\u201d said E4. Thus, in the face of these conflicting empirical findings, the net performance effects of enterprises located in geographic agglomerations remain ambiguous, calling for clarification of, e.g., \u201cthe agglomeration-performance\u201d relationship hidden in the large-scale activities of enterprises.", "requirement_code": {"discover_observation": 1, "explain_differences": 1}}, "data": {"data_text": "The collaboration with the domain experts provided us with a publiclyavailable dataset of business registration records collected from theNational Enterprise Credit Information Publicity System2, coveringthe period from 1980 to 2015. Each record contains information suchas the enterprise\u2019s primary identification code, name, address, andregistered capital. Based on suggestions from domain experts, wedeveloped and studied the following attributes in this work: 1) address:it records detailed geographic information about a business; 2) startand end dates of operation: it allows us to track the the operations ofbusinesses that existed during a specific period for further analysis;and 3) basic information, including industry category, credit rating,registered capital, and enterprise status, among others. This informationcan be used to describe a specific business.", "data_code": {"tables": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "To understand the evolutionary pattern of RIS, we transform theregistration records of existing enterprises into a time-series dataset.Then, we use a multivariant time series prediction machine learningmodel to obtain the basic features of the evolutionary patterns. Formingtime series data from raw business registration data is nontrivial due tothe large volume and complex attributes of raw business registration data. Moreover, the temporal characteristics, i.e., the start and end datesof the business, are hidden in each record independently of each other.Therefore, considering the computational efficiency and speeding upthe processing, we change the index of the data from the main identitycode of the enterprise to the date and construct a hash table to reorganizethe original information. Before modeling the time series data, severalnon-numerical dimensions, such as industry category and credit rating,must be quantified. Since these attributes have a limited and relativelyfixed range of values, it is acceptable to map them to numbers in orderto maintain the information encoded using one-hot encoding.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Two challenges exist in tracking the evolution of RIS over long periodsof time and large geographic scales. First, correctly delineating longtime periods is critical to identifying potential patterns in the RIS evolution as reflected by business registration activity. Simply retrievingrecords by year may lead to duplicate analyses if no significant changesoccur in the two subsequent periods. Second, even if we manage todivide the evolution of the RIS into different intervals, for each intervalwe are still confronted with a large amount of data that shows differentspatial locations by the records of the enterprises belonging to that interval. It is important to abstract the characteristics of the correspondingenterprises while maintaining dynamic and multi-attribute details. Inthe following subsections, we perform spatio-temporal clustering interms of time series segmentation and geospatial clustering.Partitioning of Time Series. To address the first challenge, weformulate the partitioning of long periods of RIS evolution as a segmentation problem, which partitions long time series into segments thatcan be formulated as follows: given a time series T, generate the bestrepresentation such that the combined errors (which can be obtainedby calculating the sum of the maximum error for all segments) is lessthan a user-specified threshold. Generating segments for such timeseries data is the key to an efficient and effective solution [36]. Notably,we take a top-down approach, considering every possible partition ofthe time series and segmenting it at the optimal location. The twosubsegments are then tested to see if their approximation error is belowa user-specified threshold. If not, the algorithm recursively splits thesubsequence until the approximation error of all segments is below thethreshold. Thus, the original time series registration is divided intoseveral piecewise linear representations after time series segmentation.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Geospatial Clustering. To address the second challenge of the various enterprise activities within each segment, we analyze the businessdata from a cluster perspective rather than an individual perspective.The goal of clustering is to make the similarity between objects withinthe same category as large as possible and, conversely, the similaritybetween different categories as small as possible. It is worth noting thatwe tried to cluster individual enterprises belonging to different time periods using the division-based KMeans algorithm and the density-basedclustering algorithm DBSCAN, respectively. However, they both exhibitspecific advantages and disadvantages. For example, although KMeanscan achieve clustering results, the determination of K is quite challenging because the location of enterprises is unpredictable. In addition,KMeans is developed based on the centroid location of the aggregatedclusters rather than the actual geographic locations of enterprises. Onthe other hand, DBSCAN requires manual input of two parameters,namely [E ps] and [minPts], so the accuracy of the clustering resultsdirectly depends on the users\u2019 parameter selection. Also, the obtainedclusters are spatially arbitrary shaped and are just sets of geographicalpoints rather than aggregated clusters. In addition, DBSCAN suffers from indistinguishable noisy data. For these reasons, we propose ahybrid algorithm based on DBSCAN and KMeans algorithms. First, thegeographic locations of enterprises are clustered into several clustersusing the density reachability of DBSCAN according to the specificsettings of the two parameters. The data in each cluster is taken asa new input. The centroid positions are then obtained using KMeansto minimize the Sum of Squared Error (SSE) between the data pointsin each cluster and the centroids of the clusters they are in using theiterative aggregation of KMeans and K is set to 1. In addition, we useKANN-DBSCAN [41], to automatically find a stable interval of clusternumber variation by generating candidate [E ps] and [MinPts] usingthe distribution characteristics of the dataset. We take the [E ps] and[MinPts] parameters corresponding to the minimum density thresholdof this interval as the optimal parameters. Specifically, first, the corresponding [E ps] candidate list can be obtained using KANN-DBSCAN.Then, for the given [E ps] candidate list, the number of neighboringobjects can be calculated sequentially and the expected values of these[E ps] are used as the [MinPts] candidate list: MinPts = 1n \u2211ni=1 Pi, Piis the number of [E ps] neighboring objects of the ith object, and n isthe total number of objects of the data. Finally, the list of these twoparameters is given as input to DBSCAN, and then the number of clusters generated with different parameter settings is obtained separatelyand quickly. The result can be considered stable when the number ofclusters is the same three times in a row. After deciding the clusters, we need to further define the centroids of each cluster. To summarize, we first apply KANN-DBSCAN to cluster the geographic locations of enter_x0002_prises into several clusters, and then use KMeans to find the centroid location of each cluster.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "We apply the classical map-centric visual exploration approach torepresent clusters of enterprise geographic locations. As shown inFig. 6, we design radar-like glyphs at the top of the map and placethem in the corresponding geographic regions of the regional clustersgenerated by the geospatial clustering described above. The indicatorsrepresented by the axes in the radar-like glyphs are the same as those inthe RIS evolution view, namely, aggregation index, average registeredcapital, total registered capital, credit rating, livability, and mortality.We did not choose a bar chart because we need to have the radius of theglyphs represent the number of enterprises in the cluster. In addition, inthe Regional Comparative View (see Section 6.5), we need to comparethe same indicators between two or three clusters, and radar-like glyphsare easier to perceive than bar charts for comparison.Note that we place all the regional clusters generated by DBSCANand KMeans on the map at different time intervals, inevitably causingvisual clutter and overlap problems. Therefore, we use a force-directedlayout to pack those overlapping clusters [15]. As shown in Fig. 6(a), aphysics-based simulator will be used to find the optimal circle positionsby 1) optimizing the distance between the circle and the force center,2) attracting each other slightly, and 3) avoiding overlap. We warp thepacked circles with an additional black ring (Fig. 6(a)), indicating thatthese groups of regions have some kind of unavoidable overlap. Toemphasize the representation of region groups from a particular timesnapshot and inspired by [3], we borrow the Bubble Sets technique [14]to provide continuous boundary contours that allow us to examine scenarios with semantically important spatial organization and importantset membership relationships for enterprises belonging to the snapshot.Notably, as shown in Fig. 6(c), the five colored bubble sets representthe geospatial locations in the five snapshots. We also provide regional evolution paths in the form of black curves that connect clusters ofregions in different snapshots in chronological order (Fig. 6(b)).To elaborate more detailed information about region-specific clusters,we provide a separate panel (Fig. 6(1 \u2013 5)) showing the geospatialmetrics (Fig. 6(1)), registration (Fig. 6(2)), livability (Fig. 6(3)), andbusiness category (Fig. 6(4)). The actual geographical coverage of theregional clusters is displayed as a heat map (Fig. 6(5)), where domainexperts can observe the most dense points and the scale of distribution.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Map;Pie;Area", "axial_code": [], "componenet_code": ["area", "map", "pie"]}]}, {"author": "dxf", "index_original": 126, "paper_title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics", "pub_year": 2023, "domain": "Spatiotemporal dynamics", "requirement": {"requirement_text": "R.6: Compare and track performance at the region level. Having summarized the dynamics and multiple attributes of enterprise at the regional level, experts want to conduct in-depth analyses of RIS, such as determining regional performance, comparing multiple regions at a specific point in time, and tracking the evolution of a particular region over a long period of time. For example, comparing two regions at a specific timestamp makes it possible to identify differences in their positioning, while tracking their performance over time can facilitate research into the reasons behind booms or busts.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The collaboration with the domain experts provided us with a publiclyavailable dataset of business registration records collected from theNational Enterprise Credit Information Publicity System2, coveringthe period from 1980 to 2015. Each record contains information suchas the enterprise\u2019s primary identification code, name, address, andregistered capital. Based on suggestions from domain experts, wedeveloped and studied the following attributes in this work: 1) address:it records detailed geographic information about a business; 2) startand end dates of operation: it allows us to track the the operations ofbusinesses that existed during a specific period for further analysis;and 3) basic information, including industry category, credit rating,registered capital, and enterprise status, among others. This informationcan be used to describe a specific business.", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "To understand the evolutionary pattern of RIS, we transform theregistration records of existing enterprises into a time-series dataset.Then, we use a multivariant time series prediction machine learningmodel to obtain the basic features of the evolutionary patterns. Formingtime series data from raw business registration data is nontrivial due tothe large volume and complex attributes of raw business registration data. Moreover, the temporal characteristics, i.e., the start and end datesof the business, are hidden in each record independently of each other.Therefore, considering the computational efficiency and speeding upthe processing, we change the index of the data from the main identitycode of the enterprise to the date and construct a hash table to reorganizethe original information. Before modeling the time series data, severalnon-numerical dimensions, such as industry category and credit rating,must be quantified. Since these attributes have a limited and relativelyfixed range of values, it is acceptable to map them to numbers in orderto maintain the information encoded using one-hot encoding.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "We define the input to a time series prediction machine learning modelas a sequence of historical data Xn = xn\u2212L,xn\u2212(L\u22121),...,xn\u22121,xn, whereL is the parameter that is adapted for different datasets and xn is a multidimensional feature vector with timestamp n (i.e., the feature vectorconsists of 7 dimensions, including year, month, enterprise classification code, registered capital, credit rating, enterprise property, andenterprise state) denoted as x fn \u2208 R. The goal of a multivariate timeseries forecasting models is to predict specific values at some futuretime stamp. In our case, we provide only a single-step forecast with anoutput market of yn = xn;1. Considering that different countries dividethe industrial structure in different ways, but basically divided into threemain categories, we first divide the original dataset into three significantindustries based on industry category, namely, primary industry, secondary industry, and tertiary industry. Then, we calculate the numberof enterprises existing in each month from 1980 to 2015. We use thedata from 1980 to 1990 as the first training set to predict the trend in1991, followed by the data before 1991 as the training set to predictthe trend for 1992, and so on. The reason for performing stepwiseforecasting is based on the practical requirements of most real-worldtime series forecasting tasks. Thus, the final prediction curve is spelledout for each time unit. We use the classical MAPE metric to evaluatethe performance of the forecasting model. There are several representative time series forecasting models available for evaluation, such asArima [61], Vector Auto-Regression Model (VAR) [46], Random ForestModel (RF) or Random Decision Forests [26] and Long Short-TermMemory Recurrent Neural Networks (LSTM), covering linear, nonlinearregression methods, and machine learning models [64]. In this work,we considered both model performance and model interpretability, andfinally select two machine learning models for multivariate time seriesprediction, namely, RF and XGBoost [13], as they have sufficientlyhigh model accuracy and good interpretability.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The Registration Prediction View captures the correlation between the predicted registrations and ground truth over time, and understands the latent features driving the changes from the perspective of the time series forecasting models (R.2 \u2013 R.3). The Registration Prediction View (Fig. 2) helps experts understand the detailed associations between RIS influencing factors and registration amounts based on several representative time-series forecasting models. It consists of three subplots, arranged from top to bottom, representing business activities in the primary, secondary and tertiary sectors, respectively. In each subplot, the dotted line represents the ground truth registration of enterprises. The solid black line represents the predicted business activity compared to the ground truth volume. Thus, users can observe the gap that represents the difference between the actual and the predicted value at different timestamps. In addition, the importance of the 7 features is calculated, normalized, and represented with different classification colors stacked on top of each other. In each snapshot, the glyphs of feature importance are placed along the Y-axis. Each bar in the stacked glyphs represents a feature. Bars stacked above the predicted values represent features with negative effects, pushing the predicted value down [64]. The features stacked below the predicted value are positive, pushing the predicted value higher. The user can select other models on the right side for comparison.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "The Registration Prediction View captures the correlation between the predicted registrations and ground truth over time, and understands the latent features driving the changes from the perspective of the time series forecasting models (R.2 \u2013 R.3). The Registration Prediction View (Fig. 2) helps experts understand the detailed associations between RIS influencing factors and registration amounts based on several representative time-series forecasting models. It consists of three subplots, arranged from top to bottom, representing business activities in the primary, secondary and tertiary sectors, respectively. In each subplot, the dotted line represents the ground truth registration of enterprises. The solid black line represents the predicted business activity compared to the ground truth volume. Thus, users can observe the gap that represents the difference between the actual and the predicted value at different timestamps. In addition, the importance of the 7 features is calculated, normalized, and represented with different classification colors stacked on top of each other. In each snapshot, the glyphs of feature importance are placed along the Y-axis. Each bar in the stacked glyphs represents a feature. Bars stacked above the predicted values represent features with negative effects, pushing the predicted value down [64]. The features stacked below the predicted value are positive, pushing the predicted value higher. The user can select other models on the right side for comparison.", "solution_category": "interaction", "solution_axial": "Selecting;Connect", "solution_compoent": "", "axial_code": ["Selecting", "Connect"], "componenet_code": ["selecting", "connect_relate"]}]}, {"author": "dxf", "index_original": 127, "paper_title": "RankAxis: Towards a Systematic Combination of Projection and Ranking in Multi-Attribute Data Exploration", "pub_year": 2023, "domain": "Ranking", "requirement": {"requirement_text": "R.1: Connecting projections and rankings in a seamless context. When experts explore bank projection results, they can observe several clusters of banks, but mapping different clusters to ratings can be a challenge. They encounter similar problems when interpreting the ranking results, as they rely on their expertise to divide the rankings into segments and subjectively use these segments as ratings. Therefore, experts wanted to put dimension reduction and ranking in the same context so that they could explore and compare them more effectively.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "multi-attribute datasets", "data_code": {"clusters_and_sets_and_lists": 1, "geometry": 1}}, "solution": [{"solution_text": "The projection view uses classical dimensionality reduction techniques such as t-SNE to create low-dimensional projections and preserve local similarity to express neighborhood structure [16, 24]. Other techniques such as PCA, MDS and UMAP [30] can also be integrated. We use the same set of weights to normalize the values of the attributes used in the ranked table view to obtain a two-dimensional projection. Specifically, the four projection views are depicted in the Fig. 1. Fig. 1(B) shows the projection using the latest attribute weight vector. Fig. 1(D) shows the projection corresponding to the attribute weight vectors from the first three ranking schemes (R.1). We first introduce the interactive projection view and then the projection view for comparison. In the interactive projection view, there are two main components. First, as shown in Fig. 4, observations on the interactive projection view are coded by a coxcomb digram [20] that shows the distribution of attribute contributions. The color of the dot in the middle of the glyph encodes the ranking score: the higher the ranking score, the darker the color. The size of each pie encodes the corresponding attribute contribution. We did not choose the classic star glyph to encode attribute values because the lines in the star glyph are difficult to detect when the color saturation is low and the glyph is small [52]. A potential drawback of this design is visual clutter, which is a common problem for many reduced-dimensional based visualizations. To mitigate this problem, we first reduced the opacity of the glyphs so that individual glyphs could be observed. When hovering over a glyph, that glyph is zoomed in and displayed in the foreground. In addition, we support panning and semantic zooming to focus on specific areas of the glyph. Second, the interactive projection view can generate a ranking line that explicitly guides the analyst to explore the projection layout and orientation (R.3). We define a ranking line that connects certain sampling points according to their rankings. To reflect the ranking results in the ranking table view, we propose the following three methods to generate ranking lines in the interactive projection view. Other methods, such as clustering based on projection quality or pressure [5] can also be integrated to recommend initial clustering and avoid potential misinterpretations.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The projection view uses classical dimensionality reduction techniques such as t-SNE to create low-dimensional projections and preserve local similarity to express neighborhood structure [16, 24]. Other techniques such as PCA, MDS and UMAP [30] can also be integrated. We use the same set of weights to normalize the values of the attributes used in the ranked table view to obtain a two-dimensional projection. Specifically, the four projection views are depicted in the Fig. 1. Fig. 1(B) shows the projection using the latest attribute weight vector. Fig. 1(D) shows the projection corresponding to the attribute weight vectors from the first three ranking schemes (R.1). We first introduce the interactive projection view and then the projection view for comparison. In the interactive projection view, there are two main components. First, as shown in Fig. 4, observations on the interactive projection view are coded by a coxcomb digram [20] that shows the distribution of attribute contributions. The color of the dot in the middle of the glyph encodes the ranking score: the higher the ranking score, the darker the color. The size of each pie encodes the corresponding attribute contribution. We did not choose the classic star glyph to encode attribute values because the lines in the star glyph are difficult to detect when the color saturation is low and the glyph is small [52]. A potential drawback of this design is visual clutter, which is a common problem for many reduced-dimensional based visualizations. To mitigate this problem, we first reduced the opacity of the glyphs so that individual glyphs could be observed. When hovering over a glyph, that glyph is zoomed in and displayed in the foreground. In addition, we support panning and semantic zooming to focus on specific areas of the glyph. Second, the interactive projection view can generate a ranking line that explicitly guides the analyst to explore the projection layout and orientation (R.3). We define a ranking line that connects certain sampling points according to their rankings. To reflect the ranking results in the ranking table view, we propose the following three methods to generate ranking lines in the interactive projection view. Other methods, such as clustering based on projection quality or pressure [5] can also be integrated to recommend initial clustering and avoid potential misinterpretations.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "The projection view uses classical dimensionality reduction techniques such as t-SNE to create low-dimensional projections and preserve local similarity to express neighborhood structure [16, 24]. Other techniques such as PCA, MDS and UMAP [30] can also be integrated. We use the same set of weights to normalize the values of the attributes used in the ranked table view to obtain a two-dimensional projection. Specifically, the four projection views are depicted in the Fig. 1. Fig. 1(B) shows the projection using the latest attribute weight vector. Fig. 1(D) shows the projection corresponding to the attribute weight vectors from the first three ranking schemes (R.1). We first introduce the interactive projection view and then the projection view for comparison. In the interactive projection view, there are two main components. First, as shown in Fig. 4, observations on the interactive projection view are coded by a coxcomb digram [20] that shows the distribution of attribute contributions. The color of the dot in the middle of the glyph encodes the ranking score: the higher the ranking score, the darker the color. The size of each pie encodes the corresponding attribute contribution. We did not choose the classic star glyph to encode attribute values because the lines in the star glyph are difficult to detect when the color saturation is low and the glyph is small [52]. A potential drawback of this design is visual clutter, which is a common problem for many reduced-dimensional based visualizations. To mitigate this problem, we first reduced the opacity of the glyphs so that individual glyphs could be observed. When hovering over a glyph, that glyph is zoomed in and displayed in the foreground. In addition, we support panning and semantic zooming to focus on specific areas of the glyph. Second, the interactive projection view can generate a ranking line that explicitly guides the analyst to explore the projection layout and orientation (R.3). We define a ranking line that connects certain sampling points according to their rankings. To reflect the ranking results in the ranking table view, we propose the following three methods to generate ranking lines in the interactive projection view. Other methods, such as clustering based on projection quality or pressure [5] can also be integrated to recommend initial clustering and avoid potential misinterpretations.", "solution_category": "interaction", "solution_axial": "OverviewandExplore;Filtering;Reconfigure", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Reconfigure", "Filtering"], "componenet_code": ["overview_and_explore", "reconfigure", "filtering"]}]}, {"author": "dxf", "index_original": 129, "paper_title": "RankAxis: Towards a Systematic Combination of Projection and Ranking in Multi-Attribute Data Exploration", "pub_year": 2023, "domain": "Ranking", "requirement": {"requirement_text": "R.3: Guiding semantic exploration in projections. Dimensional projections inevitably produce \u201cclusters\u201d and \u201coutliers\u201d, and experts want to understand the distribution of observations in the projection space because they sometimes cannot distinguish between the boundaries of clusters and whether an observation is an outlier. That is, when interpreting the layout of the projection results, they want more semantic help to guide their exploration in the projection.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "multi-attribute datasets", "data_code": {"geometry": 1, "tables": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "The interactive projection view can generate a ranking line that explicitly guides the analyst to explore the projection layout and orientation (R.3). We define a ranking line that connects certain sampling points according to their rankings. To reflect the ranking results in the ranking table view, we propose the following three methods to generate ranking lines in the interactive projection view. Other methods, such as clustering based on projection quality or pressure [5] can also be integrated to recommend initial clustering and avoid potential misinterpretations. Sequence Ranking Line. Based on the ranking score of each data item in the ranking tabular view, a line with arrows connects the corresponding observations in the interactive projection view (Fig. 5(A)). We observe that the connected layout may show a trending order, e.g., an ordered ranking line in one direction, or a \u201czigzag\u201d ranking line with backward and forward correspondence. Rating Line. Although a sequence ranking line may indicate an ordered layout, it strings all projection observations and may inevitably introduce visual clutter. For example, if \u201czigzags\u201d occur frequently, then ranking lines do not adequately reflect the sequential semantic information contained in the projection space. It may also be difficult for users to keep track of the order of ranked data items. To alleviate this problem, we first partition the ranking results to obtain a subset of sequences with sequential different ratings. Then, we generate the average of the data items for each rating as the center of the rating in the projected view. These newly generated observations are highlighted in red and linked according to the order of the ratings (Fig. 5(B)). That is, the generation of rating lines to link \u201caverage observations\u201d in the interactive projection view can be considered as a \u201cresampling\u201d of the observations in the sequential ranking line, better reflecting the sequential semantic information contained in the projection. Self-defined Rating Line. The first two methods draw lines based on rating results, but ignore users with extensive domain knowledge. For example, with respect to bank rating questions, joint-stock commercial banks generally outperform private banks. In the interactive projection view, analysts may be inclined to conclude that the regions where the joint-stock banks are located are likely to be the better performers overall because they use the visual metaphor of \u201cproximity \u2248 similarity\u201d. Therefore, analysts can perform customized interactive operations to generate ranking lines based on their judgment of the data. As shown in Fig. 5(C), analysts can lasso a region, and then the system automatically calculates the average of all observations in that region and joins all \u201caverage observations\u201d generated from the lassoed regions in the order of user interaction to form a user-defined rating line.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The interactive projection view can generate a ranking line that explicitly guides the analyst to explore the projection layout and orientation (R.3). We define a ranking line that connects certain sampling points according to their rankings. To reflect the ranking results in the ranking table view, we propose the following three methods to generate ranking lines in the interactive projection view. Other methods, such as clustering based on projection quality or pressure [5] can also be integrated to recommend initial clustering and avoid potential misinterpretations. Sequence Ranking Line. Based on the ranking score of each data item in the ranking tabular view, a line with arrows connects the corresponding observations in the interactive projection view (Fig. 5(A)). We observe that the connected layout may show a trending order, e.g., an ordered ranking line in one direction, or a \u201czigzag\u201d ranking line with backward and forward correspondence. Rating Line. Although a sequence ranking line may indicate an ordered layout, it strings all projection observations and may inevitably introduce visual clutter. For example, if \u201czigzags\u201d occur frequently, then ranking lines do not adequately reflect the sequential semantic information contained in the projection space. It may also be difficult for users to keep track of the order of ranked data items. To alleviate this problem, we first partition the ranking results to obtain a subset of sequences with sequential different ratings. Then, we generate the average of the data items for each rating as the center of the rating in the projected view. These newly generated observations are highlighted in red and linked according to the order of the ratings (Fig. 5(B)). That is, the generation of rating lines to link \u201caverage observations\u201d in the interactive projection view can be considered as a \u201cresampling\u201d of the observations in the sequential ranking line, better reflecting the sequential semantic information contained in the projection. Self-defined Rating Line. The first two methods draw lines based on rating results, but ignore users with extensive domain knowledge. For example, with respect to bank rating questions, joint-stock commercial banks generally outperform private banks. In the interactive projection view, analysts may be inclined to conclude that the regions where the joint-stock banks are located are likely to be the better performers overall because they use the visual metaphor of \u201cproximity \u2248 similarity\u201d. Therefore, analysts can perform customized interactive operations to generate ranking lines based on their judgment of the data. As shown in Fig. 5(C), analysts can lasso a region, and then the system automatically calculates the average of all observations in that region and joins all \u201caverage observations\u201d generated from the lassoed regions in the order of user interaction to form a user-defined rating line.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Line;Scatter", "axial_code": [], "componenet_code": ["scatter", "line"]}, {"solution_text": "The interactive projection view can generate a ranking line that explicitly guides the analyst to explore the projection layout and orientation (R.3). We define a ranking line that connects certain sampling points according to their rankings. To reflect the ranking results in the ranking table view, we propose the following three methods to generate ranking lines in the interactive projection view. Other methods, such as clustering based on projection quality or pressure [5] can also be integrated to recommend initial clustering and avoid potential misinterpretations. Sequence Ranking Line. Based on the ranking score of each data item in the ranking tabular view, a line with arrows connects the corresponding observations in the interactive projection view (Fig. 5(A)). We observe that the connected layout may show a trending order, e.g., an ordered ranking line in one direction, or a \u201czigzag\u201d ranking line with backward and forward correspondence. Rating Line. Although a sequence ranking line may indicate an ordered layout, it strings all projection observations and may inevitably introduce visual clutter. For example, if \u201czigzags\u201d occur frequently, then ranking lines do not adequately reflect the sequential semantic information contained in the projection space. It may also be difficult for users to keep track of the order of ranked data items. To alleviate this problem, we first partition the ranking results to obtain a subset of sequences with sequential different ratings. Then, we generate the average of the data items for each rating as the center of the rating in the projected view. These newly generated observations are highlighted in red and linked according to the order of the ratings (Fig. 5(B)). That is, the generation of rating lines to link \u201caverage observations\u201d in the interactive projection view can be considered as a \u201cresampling\u201d of the observations in the sequential ranking line, better reflecting the sequential semantic information contained in the projection. Self-defined Rating Line. The first two methods draw lines based on rating results, but ignore users with extensive domain knowledge. For example, with respect to bank rating questions, joint-stock commercial banks generally outperform private banks. In the interactive projection view, analysts may be inclined to conclude that the regions where the joint-stock banks are located are likely to be the better performers overall because they use the visual metaphor of \u201cproximity \u2248 similarity\u201d. Therefore, analysts can perform customized interactive operations to generate ranking lines based on their judgment of the data. As shown in Fig. 5(C), analysts can lasso a region, and then the system automatically calculates the average of all observations in that region and joins all \u201caverage observations\u201d generated from the lassoed regions in the order of user interaction to form a user-defined rating line.", "solution_category": "interaction", "solution_axial": "OverviewandExplore;Selecting", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Selecting"], "componenet_code": ["overview_and_explore", "selecting"]}]}, {"author": "dxf", "index_original": 130, "paper_title": "RankAxis: Towards a Systematic Combination of Projection and Ranking in Multi-Attribute Data Exploration", "pub_year": 2023, "domain": "Ranking", "requirement": {"requirement_text": "R.4: Reveal any inconsistencies between projections and rankings. As mentioned earlier, items projected together are not necessarily close in the ranking list, a situation that arouses the curiosity of experts because they regard \u201cproximity\u201d as \u201csimilarity\u201d. Similarly, they often confirm ranking results by observing whether the nearby neighbors of a data item in a ranked list are semantically related. Thus, identifying potential inconsistencies can help them better interpret the meaning of \u201cneighbors\u201d in projections and ranking results, and ultimately identify the underlying data characteristics that lead to inconsistencies.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "multi-attribute datasets", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Step 1: Modeling Ranking SVM. Inspired by Podium [44], we use Ranking SVM to derive attribute weights. Ranking SVM applies the idea of optimizing the SVM hyperplane to the ranking problem with pairwise constraints. A finite set of data points di and dj and a label is used to derive whether di is better or not, instead of a complete set of data points with labels. The input to the Ranking SVM involves a difference vector of data point pairs, e.g. di \u2212dj. Specifically, we transfer a pair (di,dj) and their relative ranks to a tuple based on the following statement: If di is preferred, di \u2212 dj = 1; otherwise, di \u2212dj = \u22121. The generated model can be used to predict which of the given pair of points is better. Nevertheless, the constraints derived from user interactions may be unsatisfying [19]. Therefore, we model all constraints as soft constraints rather than hard constraints to avoid vacuous results. Thus, user interactions can always produce a set of attribute weights that maximize the simulation of user constraints [26].", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Step 2: Deriving constraints. We transfer ranking to a binary classification problem by using the linear separator of SVM. That is, we generate labeled data for Ranking SVM by using the data items that the user has interacted with and dragging these items to a new location (Fig. 3(B)). These items are the k marked rows. Without loss of generality, the k points {dl1 ,...,dlk} are indexed by [l1,...,lk]. Then we create a combination of all pairs of difference vectors as training instances [19], i.e., for i, j \u2208 {1...k}, where i = j, we derive a training tuple based on the above formula, i.e., each training instance is a pair of differences between rows di and dj, classified as y = 1 if di is ranked higher than dj, and y = \u22121 if di is ranked lower than dj. Similar to Podium, we set k = 6 to ensure that the minimum training data amount for the attribute weight vector is derived after the experimental analysis", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "Step 3: Calculating the ranking score. After transforming the user interaction and learning the model, a weight vector w is obtained for us to rank the data items. We compute the individual dot products of w with each data item to generate a rank score as r(di) = w\u00b7 di = \u2211m j=1wjdi j. with the highest one corresponding to the top rank.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Step 4: Transfer ranking to rating. We adapt an entropy discretiza\\x02tion method to transfer rankings to ratings [10]. We first sort ranking scores and consider each score as a split point, and then calculate the entropy of the left and right parts of each point. We consider the split point with the lowest entropy value to be the first split point. We repeat the above procedure until we have n split points (we determine the value of n for each dataset after the experimental results). We round the frac\\x02tion of each data item to multiples of n. We denote the random variable of scores by X and sort the scores of the data items as (x1, x2,...,xn). The P(xi) denotes the probability of the fraction xi. The entropy of X is H(X) = E[\u2212logP(xi)] = \u2212\u2211N i=1 P(xi)logP(xi). Suppose there are k distinct scores among the ranking scores of all data items and k < n. We order k scores as (u1,u2,...uk), and these scores can be considered as x1,x2,...,xn of consecutive values of breakpoints. Then, we select a point with the lowest entropy value from the candidate points. We repeat this process until we have n\u22121 breakpoints, forming n ratings.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The ranking projection axis view allows to compare ranking results from the ranking tabular view with the projection axis generated in the interactive projection view (R.4), which consists of a projection axis, a score axis, a contribution axis and an attribute comparison subviews.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Flow;Circle;Bar", "axial_code": [], "componenet_code": ["flow", "bar", "circle"]}]}, {"author": "dxf", "index_original": 133, "paper_title": "DPVisCreator: Incorporating Pattern Constraints to Privacy-preserving Visualizations via Differential Privacy", "pub_year": 2023, "domain": "Privacy-preserving visualization", "requirement": {"requirement_text": "R3: Explore data patterns of interest. Multidimensional tabular data contains rich information under different attributes. Data custodians need to explore the publishing dataset from different aspects to identify sensitive attributes and representative data patterns. They evaluate the intrinsic values of data patterns and seek ways to sustain the important ones in the privacy protection process. We should support visual exploration because the final output is in a visual form.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "health records with patients\u2019 private information", "data_code": {"tables": 1}}, "solution": [{"solution_text": "A data custodian first loads a dataset and looks for sensitive attributes in the Data View (Fig. 3-A), which shows the general data characteristics. He selects attributes of interest and specifies the desired chart settings in the Pattern View (Fig. 3-B). He can also highlight the data patterns of interest with the data selection tools (R3). Then, these preferences are processed by PriVis to propose a privacy protection scheme (R1)", "solution_category": "interaction", "solution_axial": "Participation/Collaboration;Selecting", "solution_compoent": "", "axial_code": ["Selecting", "Participation/Collaboration"], "componenet_code": ["selecting", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 138, "paper_title": "OBTracker: Visual Analytics of Off-ball Movements in Basketball", "pub_year": 2023, "domain": "Sports visualization", "requirement": {"requirement_text": "S1: Summarize the off-ball movement patterns of a team. Every team uses various types of off-ball movements in their games. An overview of the typical off-ball movement patterns is thus required for experts to identify the tactical style of each team. The overview should present the players\u2019 movement paths, facilitating experts to quickly find and view the off-ball movements of interest.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "basketball tracking data", "data_code": {"geometry": 1, "clusters_and_sets_and_lists": 1, "fields": 1}}, "solution": [{"solution_text": "The summary view (Fig. 5(A)) contains two parts: (1) the team list allows the selection of a team for further investigation (S1); (2) the movement overview summarizes the characteristics of the commonly used off-ball movements for the selected team (S1, S2). Movement overview (Fig. 5(A2)). This part aims to provide a summary of the off-ball movements for the selected team (S1, S2). Each movement glyph represents a type of off-ball movement used by the selected team (i.e., a sequence of functional divisions described in Sect.5). For each glyph, the x-coordinate represents the offensive contribution, and the y-coordinate represents the frequency of the off-ball movement. We use such a scatterplot layout rather than a list to show the most common off-ball movements since it is more efficient when simultaneously comparing two different measures. Team list (Fig. 5(A1)). This part presents a list of basketball teams. In each row, the white bar indicates the number of off-ball movements used by the team, while the black bar indicates the average offensive contribution from the team\u2019s off-ball movements. A sort button is placed in the top right corner, enabling users to sort the teams by the two different attributes. Such a list can help users obtain a quick overview of the tactical style of each team. Movement overview (Fig. 5(A2)). This part aims to provide a sum_x0002_mary of the off-ball movements for the selected team (S1, S2). Each movement glyph represents a type of off-ball movement used by the selected team (i.e., a sequence of functional divisions described in Sect. 5). For each glyph, the x-coordinate represents the offensive contri_x0002_bution, and the y-coordinate represents the frequency of the off-ball movement. We use such a scatterplot layout rather than a list to show the most common off-ball movements since it is more efficient when simultaneously comparing two different measures. For example, users may quickly find off-ball movements with high frequencies and offen_x0002_sive contributions by looking at the movement glyph in the top right. We manage to avoid the overlap issue by following the way in [18].", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Circle;Bar", "axial_code": [], "componenet_code": ["bar", "circle"]}, {"solution_text": "The summary view (Fig. 5(A)) contains two parts: (1) the team list allows the selection of a team for further investigation (S1); (2) the movement overview summarizes the characteristics of the commonly used off-ball movements for the selected team (S1, S2). Movement overview (Fig. 5(A2)). This part aims to provide a summary of the off-ball movements for the selected team (S1, S2). Each movement glyph represents a type of off-ball movement used by the selected team (i.e., a sequence of functional divisions described in Sect.5). For each glyph, the x-coordinate represents the offensive contribution, and the y-coordinate represents the frequency of the off-ball movement. We use such a scatterplot layout rather than a list to show the most common off-ball movements since it is more efficient when simultaneously comparing two different measures. Team list (Fig. 5(A1)). This part presents a list of basketball teams. In each row, the white bar indicates the number of off-ball movements used by the team, while the black bar indicates the average offensive contribution from the team\u2019s off-ball movements. A sort button is placed in the top right corner, enabling users to sort the teams by the two different attributes. Such a list can help users obtain a quick overview of the tactical style of each team. Movement overview (Fig. 5(A2)). This part aims to provide a sum_x0002_mary of the off-ball movements for the selected team (S1, S2). Each movement glyph represents a type of off-ball movement used by the selected team (i.e., a sequence of functional divisions described in Sect. 5). For each glyph, the x-coordinate represents the offensive contri_x0002_bution, and the y-coordinate represents the frequency of the off-ball movement. We use such a scatterplot layout rather than a list to show the most common off-ball movements since it is more efficient when simultaneously comparing two different measures. For example, users may quickly find off-ball movements with high frequencies and offen_x0002_sive contributions by looking at the movement glyph in the top right. We manage to avoid the overlap issue by following the way in [18].", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 140, "paper_title": "OBTracker: Visual Analytics of Off-ball Movements in Basketball", "pub_year": 2023, "domain": "Sports visualization", "requirement": {"requirement_text": "S2: Describe the characteristics of the off-ball movement. Revealing the characteristics of a specific type of off-ball movement, such as its effectiveness, frequency of use, and related game contexts, can provide experts with a multifaceted evaluation of off-ball movements. For example, if two types of off-ball movements are equally effective, the one that is more often used at crucial moments (e.g., when the game is about to end) would be more valuable.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "basketball tracking data", "data_code": {"tables": 1, "fields": 1, "clusters_and_sets_and_lists": 1, "geometry": 1}}, "solution": [{"solution_text": "The summary view (Fig. 5(A)) contains two parts: (1) the team list allows the selection of a team for further investigation (S1); (2) the movement overview summarizes the characteristics of the commonly used off-ball movements for the selected team (S1, S2). Movement overview (Fig. 5(A2)). This part aims to provide a summary of the off-ball movements for the selected team (S1, S2). Each movement glyph represents a type of off-ball movement used by the selected team (i.e., a sequence of functional divisions described in Sect.5). For each glyph, the x-coordinate represents the offensive contribution, and the y-coordinate represents the frequency of the off-ball movement. We use such a scatterplot layout rather than a list to show the most common off-ball movements since it is more efficient when simultaneously comparing two different measures. Team list (Fig. 5(A1)). This part presents a list of basketball teams. In each row, the white bar indicates the number of off-ball movements used by the team, while the black bar indicates the average offensive contribution from the team\u2019s off-ball movements. A sort button is placed in the top right corner, enabling users to sort the teams by the two different attributes. Such a list can help users obtain a quick overview of the tactical style of each team. Movement overview (Fig. 5(A2)). This part aims to provide a sum_x0002_mary of the off-ball movements for the selected team (S1, S2). Each movement glyph represents a type of off-ball movement used by the selected team (i.e., a sequence of functional divisions described in Sect. 5). For each glyph, the x-coordinate represents the offensive contri_x0002_bution, and the y-coordinate represents the frequency of the off-ball movement. We use such a scatterplot layout rather than a list to show the most common off-ball movements since it is more efficient when simultaneously comparing two different measures. For example, users may quickly find off-ball movements with high frequencies and offen_x0002_sive contributions by looking at the movement glyph in the top right. We manage to avoid the overlap issue by following the way in [18].", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Circle;Bar", "axial_code": [], "componenet_code": ["bar", "circle"]}, {"solution_text": "The summary view (Fig. 5(A)) contains two parts: (1) the team list allows the selection of a team for further investigation (S1); (2) the movement overview summarizes the characteristics of the commonly used off-ball movements for the selected team (S1, S2). Movement overview (Fig. 5(A2)). This part aims to provide a summary of the off-ball movements for the selected team (S1, S2). Each movement glyph represents a type of off-ball movement used by the selected team (i.e., a sequence of functional divisions described in Sect.5). For each glyph, the x-coordinate represents the offensive contribution, and the y-coordinate represents the frequency of the off-ball movement. We use such a scatterplot layout rather than a list to show the most common off-ball movements since it is more efficient when simultaneously comparing two different measures. Team list (Fig. 5(A1)). This part presents a list of basketball teams. In each row, the white bar indicates the number of off-ball movements used by the team, while the black bar indicates the average offensive contribution from the team\u2019s off-ball movements. A sort button is placed in the top right corner, enabling users to sort the teams by the two different attributes. Such a list can help users obtain a quick overview of the tactical style of each team. Movement overview (Fig. 5(A2)). This part aims to provide a sum_x0002_mary of the off-ball movements for the selected team (S1, S2). Each movement glyph represents a type of off-ball movement used by the selected team (i.e., a sequence of functional divisions described in Sect. 5). For each glyph, the x-coordinate represents the offensive contri_x0002_bution, and the y-coordinate represents the frequency of the off-ball movement. We use such a scatterplot layout rather than a list to show the most common off-ball movements since it is more efficient when simultaneously comparing two different measures. For example, users may quickly find off-ball movements with high frequencies and offen_x0002_sive contributions by looking at the movement glyph in the top right. We manage to avoid the overlap issue by following the way in [18].", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 143, "paper_title": "OBTracker: Visual Analytics of Off-ball Movements in Basketball", "pub_year": 2023, "domain": "Sports visualization", "requirement": {"requirement_text": "E1: Interpret the dynamic process of an off-ball movement. When experts focus on a specific off-ball movement, its exact process should be outlined to reveal the continuous interactions between attackers and defenders on the court. For instance, showing changes in the defenders\u2019 positions enables an analysis of how this off-ball movement can impact the opponent\u2019s defensive structure.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "basketball tracking data", "data_code": {"tables": 1, "fields": 1, "clusters_and_sets_and_lists": 1, "geometry": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The explanation view (Fig. 5(C)) displays the detailed process of the off-ball movement and illustrates its effectiveness from the perspective of player positioning and team cooperation (E1). This part displays two off-ball moving processes (E1, E2). We use colors to distinguish them. The explanation view (Fig. 5(C)) displays the detailed process of the off-ball movement and illustrates its effectiveness from the perspective of player positioning and team cooperation (E1). We juxtapose the two off-ball moving processes for comparison (E2). Select panel (Fig. 5(C1)). This part displays the information necessary for locating a particular off-ball movement, including the player combination, the game and possession profile. We place this panel so that users can further select and examine the off-ball moving process performed by the player combination they are interested in. Movement flow (Fig. 5(C2)). This part displays two off-ball moving processes (E1, E2). We use colors to distinguish them. As the entire moving process may contain hundreds of frames, we uniformly sample key frames to reduce data complexity. For each key frame, we adopt a Voronoi-based visualization [65] to show the game situation (Fig. 7(A)). Orange dots represent the ball, while the other solid and hollow dots represent attackers and defenders, respectively. The defenders\u2019 cells are filled with gray. The cutters\u2019 cells are colored differently, with luminance indicating the scoring expectation. We arrange the Voronoi diagrams in chronological order from left to right (Fig. 5(C2)). The vertically aligned Voronoi diagrams reflect the game situations of the two moving processes at similar times. This can help users examine the continuous interactions between attackers and defenders.We show the assessment results of the off-ball moving process for the evaluation and explanation (E1). The pie chart (Fig. 7(B)) shows the passing probability for the corresponding Voronoi diagram. For the bar chart in the center, the outer bar denotes the shooting expectation. The inner bar encodes the scoring expectation, which is also the product of the two aforementioned indicators. Interactions. The main interactions in this view are presented below. \u2022 Select an off-ball movement. Users can select an off-ball movement through the dropdown list in the select panel. Then they will see the detailed process displayed in the movement flow. \u2022 Adjust the number of key frames. To view the movement at different levels of detail, users can drag the slider (Fig. 5(C3)) to adjust the number of key frames displayed in the movement flow.", "solution_category": "interaction", "solution_axial": "Filtering;Selecting", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "dxf", "index_original": 144, "paper_title": "OBTracker: Visual Analytics of Off-ball Movements in Basketball", "pub_year": 2023, "domain": "Sports visualization", "requirement": {"requirement_text": "E2: Compare the performance of different players when executing the same off-ball movement. Experts require a comprehensive comparison to uncover the reason why some players can perform better than others in the same type of off-ball movement. Besides the player positions, key indicators, such as the space controlled by the cutter and the probability of receiving the ball, can promote the understanding and comparison of the off-ball moving process.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "basketball tracking data", "data_code": {"tables": 1, "fields": 1, "clusters_and_sets_and_lists": 1, "geometry": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The explanation view (Fig. 5(C)) displays the detailed process of the off-ball movement and illustrates its effectiveness from the perspective of player positioning and team cooperation (E1). This part displays two off-ball moving processes (E1, E2). We use colors to distinguish them. The explanation view (Fig. 5(C)) displays the detailed process of the off-ball movement and illustrates its effectiveness from the perspective of player positioning and team cooperation (E1). We juxtapose the two off-ball moving processes for comparison (E2). Select panel (Fig. 5(C1)). This part displays the information necessary for locating a particular off-ball movement, including the player combination, the game and possession profile. We place this panel so that users can further select and examine the off-ball moving process performed by the player combination they are interested in. Movement flow (Fig. 5(C2)). This part displays two off-ball moving processes (E1, E2). We use colors to distinguish them. As the entire moving process may contain hundreds of frames, we uniformly sample key frames to reduce data complexity. For each key frame, we adopt a Voronoi-based visualization [65] to show the game situation (Fig. 7(A)). Orange dots represent the ball, while the other solid and hollow dots represent attackers and defenders, respectively. The defenders\u2019 cells are filled with gray. The cutters\u2019 cells are colored differently, with luminance indicating the scoring expectation. We arrange the Voronoi diagrams in chronological order from left to right (Fig. 5(C2)). The vertically aligned Voronoi diagrams reflect the game situations of the two moving processes at similar times. This can help users examine the continuous interactions between attackers and defenders.We show the assessment results of the off-ball moving process for the evaluation and explanation (E1). The pie chart (Fig. 7(B)) shows the passing probability for the corresponding Voronoi diagram. For the bar chart in the center, the outer bar denotes the shooting expectation. The inner bar encodes the scoring expectation, which is also the product of the two aforementioned indicators. Interactions. The main interactions in this view are presented below. \u2022 Select an off-ball movement. Users can select an off-ball movement through the dropdown list in the select panel. Then they will see the detailed process displayed in the movement flow. \u2022 Adjust the number of key frames. To view the movement at different levels of detail, users can drag the slider (Fig. 5(C3)) to adjust the number of key frames displayed in the movement flow.", "solution_category": "interaction", "solution_axial": "Filtering;Selecting", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "dxf", "index_original": 145, "paper_title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry", "pub_year": 2023, "domain": "Medicinal", "requirement": {"requirement_text": "R1: Enable intuitive comparison of different targets on different scales. The system should support the comparison of candidate drug targets in different aspects, including the target properties, the research trend and popularity of targets over time, and the individual molecular feature of interest. For example, E3 said that medicinal chemists can directly flter out the candidates that do not satisfy their requirements for research directions by having an overview about the volume and stage of the related research. For the remaining candidate targets, researchers need to check detailed drug compound research information and progress to make their fnal decisions.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "drug targets", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The Drug Target Search view (Fig.1 A) allows users to type the name of a drug target (e.g., \u201cEGFR\u201d, \u201cKRAS\u201d) of interest into the search box and returns a card containing its 2D structure (R1). Information about associated publications also appears in the same row in the Overview and Publication Trend view. Users can hover over a structure to enlarge it. They can also drag the target card up and down to place similar ones next to each other for easier comparison (R6). Corresponding \ninformation in the Overview and the Publication Trend view will also change position accordingly. Upon clicking on a card, the detailed research information of the selected target will be shown in the Detail View. Users can remove a target and all its related information by hitting the delete button on its card.", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "dxf", "index_original": 146, "paper_title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry", "pub_year": 2023, "domain": "Medicinal", "requirement": {"requirement_text": "R1: Enable intuitive comparison of different targets on different scales. The system should support the comparison of candidate drug targets in different aspects, including the target properties, the research trend and popularity of targets over time, and the individual molecular feature of interest. For example, E3 said that medicinal chemists can directly flter out the candidates that do not satisfy their requirements for research directions by having an overview about the volume and stage of the related research. For the remaining candidate targets, researchers need to check detailed drug compound research information and progress to make their fnal decisions.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "publications;Molecular features about drug compounds", "data_code": {"clusters_and_sets_and_lists": 1, "network_and_trees": 1, "tables": 1}}, "solution": [{"solution_text": "The Overview (Fig.1 C) presents the overall performance distributions of existing drug compound research on the candidate targets to help understand the current research progress and diffculty as measured by assorted molecular features and inspire possible areas of improvement (R1, R2, R4, R5). The minimum and maximum feature values are displayed on the x-axis indicating the progress of the research on a particular molecular feature (R1) and hinting researchers about what can be further improved (R4). For each drug target, the Overview (Fig.1 C) aims to provide an overar_x0002_ching picture of the drug compound designs related to the molecular  features using a tabular design (R2). Each row associates with a drug target and aligns with the target\u2019s position in the list of all input candidates in the Drug Target Search view; each column corresponds to a feature introduced in Table 1. The chemistry- (colored in a red theme), pharmacology- (blue), and clinical-pharmacy-related (orange) columns are arranged from left to right following the drug discovery process to show the research progress of the drug target (R2). The background color shading of each cell in the chemistry- and pharmacology-related columns denotes the number of publications whose proposed com_x0002_pounds improved the corresponding molecular feature, while in the clinical-pharmacy-related columns it encodes the number of clinical studies in each phase of clinical trials. Darker color implies more publications fall in the cell; white means no related work exists. The number of publications is shown in the upper right corner of the cell.  To summarize the performance of related work on a molecular feature, we displayed the distributions of reported feature values in these works as a line chart in the corresponding cell (R2). This distribution can also imply how diffcult it is to improve the feature (R5). The x_x0002_dimension represents the published feature values and the y-dimension indicates the number of publications/studies achieving a value. The minimum and maximum feature values are displayed on the x-axis indicating the progress of the research on a particular molecular feature (R1) and hinting researchers about what can be further improved (R4). Hovering over each dot on the plot displays a tooltip of feature value and the number of publications/studies accordingly. Because ongoing or completed but confdential [48] clinical studies can not report their study results, there may be no distribution plot summarizing the clinical trial results even though the cell shows that there are clinical studies on the drug target. This discrepancy might confuse users about the research progress. Thus, hovering over a cell without a distribution plot  pops up a tooltip clarifying the reason (\u201cno results reported\u201d or \u201cno studies completed\u201d). Upon searching a target in the Drug Target Search view, by default the table will add a new row accordingly containing all feature columns. Users can remove columns of features by clicking the delete button in the column headers and add features back from a drop-down menu in the upper right corner of the Overview (R6).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line;Bar", "axial_code": [], "componenet_code": ["line", "bar"]}, {"solution_text": "The Overview (Fig.1 C) presents the overall performance distributions of existing drug compound research on the candidate targets to help understand the current research progress and diffculty as measured by assorted molecular features and inspire possible areas of improvement (R1, R2, R4, R5). The minimum and maximum feature values are displayed on the x-axis indicating the progress of the research on a particular molecular feature (R1) and hinting researchers about what can be further improved (R4). For each drug target, the Overview (Fig.1 C) aims to provide an overar_x0002_ching picture of the drug compound designs related to the molecular  features using a tabular design (R2). Each row associates with a drug target and aligns with the target\u2019s position in the list of all input candidates in the Drug Target Search view; each column corresponds to a feature introduced in Table 1. The chemistry- (colored in a red theme), pharmacology- (blue), and clinical-pharmacy-related (orange) columns are arranged from left to right following the drug discovery process to show the research progress of the drug target (R2). The background color shading of each cell in the chemistry- and pharmacology-related columns denotes the number of publications whose proposed com_x0002_pounds improved the corresponding molecular feature, while in the clinical-pharmacy-related columns it encodes the number of clinical studies in each phase of clinical trials. Darker color implies more publications fall in the cell; white means no related work exists. The number of publications is shown in the upper right corner of the cell.  To summarize the performance of related work on a molecular feature, we displayed the distributions of reported feature values in these works as a line chart in the corresponding cell (R2). This distribution can also imply how diffcult it is to improve the feature (R5). The x_x0002_dimension represents the published feature values and the y-dimension indicates the number of publications/studies achieving a value. The minimum and maximum feature values are displayed on the x-axis indicating the progress of the research on a particular molecular feature (R1) and hinting researchers about what can be further improved (R4). Hovering over each dot on the plot displays a tooltip of feature value and the number of publications/studies accordingly. Because ongoing or completed but confdential [48] clinical studies can not report their study results, there may be no distribution plot summarizing the clinical trial results even though the cell shows that there are clinical studies on the drug target. This discrepancy might confuse users about the research progress. Thus, hovering over a cell without a distribution plot  pops up a tooltip clarifying the reason (\u201cno results reported\u201d or \u201cno studies completed\u201d). Upon searching a target in the Drug Target Search view, by default the table will add a new row accordingly containing all feature columns. Users can remove columns of features by clicking the delete button in the column headers and add features back from a drop-down menu in the upper right corner of the Overview (R6).", "solution_category": "interaction", "solution_axial": "Abstract/Elaborate", "solution_compoent": "", "axial_code": ["Abstract/Elaborate"], "componenet_code": ["abstract_elaborate"]}]}, {"author": "dxf", "index_original": 147, "paper_title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry", "pub_year": 2023, "domain": "Medicinal", "requirement": {"requirement_text": "R1: Enable intuitive comparison of different targets on different scales. The system should support the comparison of candidate drug targets in different aspects, including the target properties, the research trend and popularity of targets over time, and the individual molecular feature of interest. For example, E3 said that medicinal chemists can directly flter out the candidates that do not satisfy their requirements for research directions by having an overview about the volume and stage of the related research. For the remaining candidate targets, researchers need to check detailed drug compound research information and progress to make their fnal decisions.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Publications", "data_code": {"tables": 1}}, "solution": [{"solution_text": "The Publication Trend view (Fig.1 D) shows the research trend of each drug target searched by the user (R1). The Publication Trend view (Fig.1 D) displays the temporal changes in the number of publications related to each candidate drug target in three disciplines (i.e., chemistry, pharmacology, clinical pharmacy), respectively, in area charts.  It helps users explore the research trend and the evolution of each candidate\u2019s popularity over time (R1). ", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Area", "axial_code": [], "componenet_code": ["area"]}]}, {"author": "dxf", "index_original": 148, "paper_title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry", "pub_year": 2023, "domain": "Medicinal", "requirement": {"requirement_text": "R1: Enable intuitive comparison of different targets on different scales. The system should support the comparison of candidate drug targets in different aspects, including the target properties, the research trend and popularity of targets over time, and the individual molecular feature of interest. For example, E3 said that medicinal chemists can directly flter out the candidates that do not satisfy their requirements for research directions by having an overview about the volume and stage of the related research. For the remaining candidate targets, researchers need to check detailed drug compound research information and progress to make their fnal decisions.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Molecular features about drug compounds;Publications", "data_code": {"clusters_and_sets_and_lists": 1, "network_and_trees": 1, "tables": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}]}, {"author": "dxf", "index_original": 149, "paper_title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry", "pub_year": 2023, "domain": "Medicinal", "requirement": {"requirement_text": "R2: Provide a comprehensive picture of the research about each candidate drug target in three relevant disciplines. The system should provide an overarching summary of the drug compound research about each drug target. As mentioned by E1 and E3, each publication has a research goal of designing new drug compounds to enhance certain molecular feature(s). Researchers want to know the number and the overall distribution of the drug compound research focusing on each molecular feature. In addition, E2, E4 and E5 mentioned that as the same drug compound should be studied by different disciplines in different drug discovery stages, the related scholarly documentations are scattered in large-scale online resources from various felds. Since integrating research about the same drug compound manually is diffcult, research data on the same drug should be connected across disciplines and following the drug discovery process to facilitate medicinal chemists to streamline the literature survey process and track the development of each drug compound.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "publications;Molecular features about drug compounds", "data_code": {"network_and_trees": 1, "tables": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "The Overview (Fig.1 C) presents the overall performance distributions of existing drug compound research on the candidate targets to help understand the current research progress and diffculty as measured by assorted molecular features and inspire possible areas of improvement (R1, R2, R4, R5). The minimum and maximum feature values are displayed on the x-axis indicating the progress of the research on a particular molecular feature (R1) and hinting researchers about what can be further improved (R4). For each drug target, the Overview (Fig.1 C) aims to provide an overar_x0002_ching picture of the drug compound designs related to the molecular  features using a tabular design (R2). Each row associates with a drug target and aligns with the target\u2019s position in the list of all input candidates in the Drug Target Search view; each column corresponds to a feature introduced in Table 1. The chemistry- (colored in a red theme), pharmacology- (blue), and clinical-pharmacy-related (orange) columns are arranged from left to right following the drug discovery process to show the research progress of the drug target (R2). The background color shading of each cell in the chemistry- and pharmacology-related columns denotes the number of publications whose proposed com_x0002_pounds improved the corresponding molecular feature, while in the clinical-pharmacy-related columns it encodes the number of clinical studies in each phase of clinical trials. Darker color implies more publications fall in the cell; white means no related work exists. The number of publications is shown in the upper right corner of the cell. \nTo summarize the performance of related work on a molecular feature, we displayed the distributions of reported feature values in these works as a line chart in the corresponding cell (R2). This distribution can also imply how diffcult it is to improve the feature (R5). The x_x0002_dimension represents the published feature values and the y-dimension indicates the number of publications/studies achieving a value. The minimum and maximum feature values are displayed on the x-axis \nindicating the progress of the research on a particular molecular feature (R1) and hinting researchers about what can be further improved (R4). Hovering over each dot on the plot displays a tooltip of feature value and the number of publications/studies accordingly. Because ongoing or completed but confdential [48] clinical studies can not report their study results, there may be no distribution plot summarizing the clinical trial results even though the cell shows that there are clinical studies on the drug target. This discrepancy might confuse users about the research progress. Thus, hovering over a cell without a distribution plot  pops up a tooltip clarifying the reason (\u201cno results reported\u201d or \u201cno studies completed\u201d). Upon searching a target in the Drug Target Search view, by default the table will add a new row accordingly containing all feature columns. Users can remove columns of features by clicking the delete button in the column headers and add features back from a drop-down menu in the upper right corner of the Overview (R6).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line;Bar", "axial_code": [], "componenet_code": ["line", "bar"]}, {"solution_text": "The Overview (Fig.1 C) presents the overall performance distributions of existing drug compound research on the candidate targets to help understand the current research progress and diffculty as measured by assorted molecular features and inspire possible areas of improvement (R1, R2, R4, R5). The minimum and maximum feature values are displayed on the x-axis indicating the progress of the research on a particular molecular feature (R1) and hinting researchers about what can be further improved (R4). For each drug target, the Overview (Fig.1 C) aims to provide an overar_x0002_ching picture of the drug compound designs related to the molecular  features using a tabular design (R2). Each row associates with a drug target and aligns with the target\u2019s position in the list of all input candidates in the Drug Target Search view; each column corresponds to a feature introduced in Table 1. The chemistry- (colored in a red theme), pharmacology- (blue), and clinical-pharmacy-related (orange) columns are arranged from left to right following the drug discovery process to show the research progress of the drug target (R2). The background color shading of each cell in the chemistry- and pharmacology-related columns denotes the number of publications whose proposed com_x0002_pounds improved the corresponding molecular feature, while in the clinical-pharmacy-related columns it encodes the number of clinical studies in each phase of clinical trials. Darker color implies more publications fall in the cell; white means no related work exists. The number of publications is shown in the upper right corner of the cell. \nTo summarize the performance of related work on a molecular feature, we displayed the distributions of reported feature values in these works as a line chart in the corresponding cell (R2). This distribution can also imply how diffcult it is to improve the feature (R5). The x_x0002_dimension represents the published feature values and the y-dimension indicates the number of publications/studies achieving a value. The minimum and maximum feature values are displayed on the x-axis \nindicating the progress of the research on a particular molecular feature (R1) and hinting researchers about what can be further improved (R4). Hovering over each dot on the plot displays a tooltip of feature value and the number of publications/studies accordingly. Because ongoing or completed but confdential [48] clinical studies can not report their study results, there may be no distribution plot summarizing the clinical trial results even though the cell shows that there are clinical studies on the drug target. This discrepancy might confuse users about the research progress. Thus, hovering over a cell without a distribution plot  pops up a tooltip clarifying the reason (\u201cno results reported\u201d or \u201cno studies completed\u201d). Upon searching a target in the Drug Target Search view, by default the table will add a new row accordingly containing all feature columns. Users can remove columns of features by clicking the delete button in the column headers and add features back from a drop-down menu in the upper right corner of the Overview (R6).", "solution_category": "interaction", "solution_axial": "Abstract/Elaborate", "solution_compoent": "", "axial_code": ["Abstract/Elaborate"], "componenet_code": ["abstract_elaborate"]}]}, {"author": "dxf", "index_original": 150, "paper_title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry", "pub_year": 2023, "domain": "Medicinal", "requirement": {"requirement_text": "R2: Provide a comprehensive picture of the research about each candidate drug target in three relevant disciplines. The system should provide an overarching summary of the drug compound research about each drug target. As mentioned by E1 and E3, each publication has a research goal of designing new drug compounds to enhance certain molecular feature(s). Researchers want to know the number and the overall distribution of the drug compound research focusing on each molecular feature. In addition, E2, E4 and E5 mentioned that as the same drug compound should be studied by different disciplines in different drug discovery stages, the related scholarly documentations are scattered in large-scale online resources from various felds. Since integrating research about the same drug compound manually is diffcult, research data on the same drug should be connected across disciplines and following the drug discovery process to facilitate medicinal chemists to streamline the literature survey process and track the development of each drug compound.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Molecular features about drug compounds;Publications", "data_code": {"clusters_and_sets_and_lists": 1, "network_and_trees": 1, "tables": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}]}, {"author": "dxf", "index_original": 151, "paper_title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry", "pub_year": 2023, "domain": "Medicinal", "requirement": {"requirement_text": "R3: Organize scholarly documentations following the practice Researchers require an organization and presentation of the scholarly documentations to show the corresponding research landscape and status in each individual discipline. All researchers pointed out that the chemical structures of the drug compounds are the core fndings of medicinal chemical publications, and it is a common practice for medicinal chemists to organize literature based on chemical structures. In pharmacology, researcher focus on the values of molecular features tested in pharmacological assays. For clinical pharmacy, they want to know the status information for clinical trials, such as \u201chow many organizations are conducting clinical trials\u201d (E1) and \u201cwhy some clinical trials were terminated\u201d (E4). Visual designs should be adapted for different data types to help users process and digest the heterogeneous research data in different disciplines.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Molecular features about drug compounds;Publications", "data_code": {"clusters_and_sets_and_lists": 1, "network_and_trees": 1, "tables": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}]}, {"author": "dxf", "index_original": 157, "paper_title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry", "pub_year": 2023, "domain": "Medicinal", "requirement": {"requirement_text": "R6: Facilitate an interactive and customized data exploration. We observed that individual medicinal chemical researchers or research groups may have different focuses, information needs, and exploration patterns. E2 and E6 added that their research interests, abilities, and available laboratory facilities may also vary. Hence, the system should enable users to customize their preferences on different evaluation metrics and decision-making patterns interactively.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "drug targets", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Information about associated publications also appears in the same row in the Overview and Publication Trend view. Users can hover over a structure to enlarge it. They can also drag the target card up and down to place similar ones next to each other for easier comparison (R6).", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "dxf", "index_original": 158, "paper_title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry", "pub_year": 2023, "domain": "Medicinal", "requirement": {"requirement_text": "R6: Facilitate an interactive and customized data exploration. We observed that individual medicinal chemical researchers or research groups may have different focuses, information needs, and exploration patterns. E2 and E6 added that their research interests, abilities, and available laboratory facilities may also vary. Hence, the system should enable users to customize their preferences on different evaluation metrics and decision-making patterns interactively.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "publications;Molecular features about drug compounds", "data_code": {"network_and_trees": 1, "tables": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Upon searching a target in the Drug Target Search view, by default the table will add a new row accordingly containing all feature columns. Users can remove columns of features by clicking the delete button in the column headers and add features back from a drop-down menu in the upper right corner of the Overview (R6).", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "dxf", "index_original": 159, "paper_title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry", "pub_year": 2023, "domain": "Medicinal", "requirement": {"requirement_text": "R6: Facilitate an interactive and customized data exploration. We observed that individual medicinal chemical researchers or research groups may have different focuses, information needs, and exploration patterns. E2 and E6 added that their research interests, abilities, and available laboratory facilities may also vary. Hence, the system should enable users to customize their preferences on different evaluation metrics and decision-making patterns interactively.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Publications", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Upon searching a target in the Drug Target Search view, an area chart aligning with the target item in the search view appears, showing the publication trend from 1990 to the present by default. Users can adjust the date via a time slider and the area charts will be adjusted accordingly (R6).", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "dxf", "index_original": 160, "paper_title": "MedChemLens: An Interactive Visual Tool to Support Direction Selection in Interdisciplinary Experimental Research of Medicinal Chemistry", "pub_year": 2023, "domain": "Medicinal", "requirement": {"requirement_text": "R6: Facilitate an interactive and customized data exploration. We observed that individual medicinal chemical researchers or research groups may have different focuses, information needs, and exploration patterns. E2 and E6 added that their research interests, abilities, and available laboratory facilities may also vary. Hence, the system should enable users to customize their preferences on different evaluation metrics and decision-making patterns interactively.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Molecular features about drug compounds;Publications", "data_code": {"clusters_and_sets_and_lists": 1, "network_and_trees": 1, "tables": 1}}, "solution": [{"solution_text": "The Pharmacology panel also allows users to sort rows in a heatmap in ascending or descending order of the values in a specifc column (R6).", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 161, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G1: Experts are interested in identifying how specific cell types attract or repel each other (cell-cell interaction). When immune and cancer cells are frequently observed close to one another (in each other\u2019s spatial neighborhood), they are likely to interact. For instance, the interaction of two types of immune cells (B and T-cells) is a central tenet of pro_x005f_x005f_x005f_x0002_tective immunity. These interactions can occur at various scales (cells directly adjacent to each other or in the same large region of tissue).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "After data import and neighborhood quantification(Sec. 6), users can start the analysis with an overview of all specimens in a cohort. Together with the biomedical experts, we chose small multiples of image thumbnails as a sufficient and compact way to compar_x0002_atively summarize (T4) the images in a cohort and their morphology (Fig. 1, b). Users can zoom and pan into these thumbnails to begin ex_x0002_ploration and select a specific dataset for thorough investigation (T1) in the image viewer (a). As analysis progresses, spatial neighborhood patterns identified in a single specimen are detected and visualized in other members of the cohort by highlighting their spatial presence in linked images. Specimens can also be sorted by the number of match_x0002_ing neighborhoods to the pattern currently being investigated or by the statistical significance of that pattern within a specimen (see Sec. 9).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Cluster", "axial_code": [], "componenet_code": ["point"]}, {"solution_text": "After data import and neighborhood quantification(Sec. 6), users can start the analysis with an overview of all specimens in a cohort. Together with the biomedical experts, we chose small multiples of image thumbnails as a sufficient and compact way to compar_x0002_atively summarize (T4) the images in a cohort and their morphology (Fig. 1, b). Users can zoom and pan into these thumbnails to begin ex_x0002_ploration and select a specific dataset for thorough investigation (T1) in the image viewer (a). As analysis progresses, spatial neighborhood patterns identified in a single specimen are detected and visualized in other members of the cohort by highlighting their spatial presence in linked images. Specimens can also be sorted by the number of match_x0002_ing neighborhoods to the pattern currently being investigated or by the statistical significance of that pattern within a specimen (see Sec. 9).", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "dxf", "index_original": 162, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G1: Experts are interested in identifying how specific cell types attract or repel each other (cell-cell interaction). When immune and cancer cells are frequently observed close to one another (in each other\u2019s spatial neighborhood), they are likely to interact. For instance, the interaction of two types of immune cells (B and T-cells) is a central tenet of pro_x0002_tective immunity. These interactions can occur at various scales (cells directly adjacent to each other or in the same large region of tissue).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "To support visual exploration of the spatial neighborhoods in tissue images (T1), we offer a scalable image viewer (Fig. 1, a), allowing navigation via zooming;panning. Through multi-channel rendering, pseudo-colored channels can be blended together into a single view, enabling users to analyze the expression level of multiple markers at once.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "dxf", "index_original": 163, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G1: Experts are interested in identifying how specific cell types attract or repel each other (cell-cell interaction). When immune and cancer cells are frequently observed close to one another (in each other\u2019s spatial neighborhood), they are likely to interact. For instance, the interaction of two types of immune cells (B and T-cells) is a central tenet of pro_x0002_tective immunity. These interactions can occur at various scales (cells directly adjacent to each other or in the same large region of tissue).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "To provide more information about what cell types spatial neighborhoods are composed of (T1), we include a parallel coordinate plot (Fig. 5, b). We chose PC plots over bar charts and box plots to emphasize the occurrence of each cell type in a spatial neighborhood while also encoding the distribution and correlation between the features. Fig. 5, b shows the composition of a selected tonsil region (a). Here, each poly-line represents the neighborhood of a cell. Each individual axis is defined by the influence of a specific cell type in the neighborhood. Two distinct neighborhoods are represented, one containing more B cells and one containing more CD4 T cells.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 164, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G2: With these neighborhoods as building blocks, experts seek to understand their spatial arrangement within the tissue image. These neighborhood patterns can be equally distributed throughout the image or appear in proximity, forming biologically meaningful spatial structures (groups). Germinal centers, which are regions within lymph nodes where B cells proliferate, are examples of such micro-structures.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}]}, {"author": "dxf", "index_original": 165, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G2: With these neighborhoods as building blocks, experts seek to understand their spatial arrangement within the tissue image. These neighborhood patterns can be equally distributed throughout the im_x0002_age or appear in proximity, forming biologically meaningful spatial structures (groups). Germinal centers, which are regions within lymph nodes where B cells proliferate, are examples of such micro-structures.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "To support visual exploration of the spatial neighborhoods in tissue images (T1), we offer a scalable image viewer (Fig. 1, a), allowing navigation via zooming;panning. Through multi-channel rendering, pseudo-colored channels can be blended together into a single view, enabling users to analyze the expression level of multiple markers at once.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "dxf", "index_original": 166, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G2: With these neighborhoods as building blocks, experts seek to understand their spatial arrangement within the tissue image. These neighborhood patterns can be equally distributed throughout the im_x0002_age or appear in proximity, forming biologically meaningful spatial structures (groups). Germinal centers, which are regions within lymph nodes where B cells proliferate, are examples of such micro-structures.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "To provide more information about what cell types spatial neighborhoods are composed of (T1), we include a parallel coordinate plot (Fig. 5, b). We chose PC plots over bar charts and box plots to emphasize the occurrence of each cell type in a spatial neighborhood while also encoding the distribution and correlation between the features. Fig. 5, b shows the composition of a selected tonsil region (a). Here, each poly-line represents the neighborhood of a cell. Each individual axis is defined by the influence of a specific cell type in the neighborhood. Two distinct neighborhoods are represented, one containing more B cells and one containing more CD4 T cells.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 167, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G2: With these neighborhoods as building blocks, experts seek to understand their spatial arrangement within the tissue image. These neighborhood patterns can be equally distributed throughout the im_x0002_age or appear in proximity, forming biologically meaningful spatial structures (groups). Germinal centers, which are regions within lymph nodes where B cells proliferate, are examples of such micro-structures.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "To address the need to explore pairwise interaction between two specific cell types (G1), (T1) we offer a correlation matrix visualization. We chose a matrix over a node-link diagram to avoid clutter when encoding relationships between every pair of cell types.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Matrix", "axial_code": [], "componenet_code": ["matrix"]}]}, {"author": "dxf", "index_original": 168, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G3: Experts seek to validate the statistical significance of identified neighborhood patterns and larger spatial structures (G1, G2) within and between specimens. By understanding how often they appear and how properties (e.g. composition, size) vary between patterns, expers can determine clinical relevance and motivate further investigation.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "We create a feature vector representing each cell\u2019s neighborhood of size 1 x c(where cis the number of cell types in the dataset). Each column corresponds to the fraction of a cell\u2019s overall neighborhood occupied by a specific cell type. These values are linearly weighted such that cells closer to the center of the neighborhood radius contribute more to the overall neighborhood. The resulting vector is normalized. This representation builds on existing approaches [84,85] and was reaffirmed by feedback from our collaborators, who said that it was highly interpretable and fit with the hypotheses they had regarding the cell-cell interactions present in a dataset (T3).", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "dxf", "index_original": 169, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G3: Experts seek to validate the statistical significance of identified neighborhood patterns and larger spatial structures (G1, G2) within and between specimens. By understanding how often they appear and how properties (e.g. composition, size) vary between patterns, expers can determine clinical relevance and motivate further investigation.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "To emphasize correlations between cell types on adjacent axes, we employ an axis reordering strategy [9, 33]. Fig. 8, a shows a negative correlation between CD4 T cells and B cells. The axes can also be reordered by drag&drop, addressing the need of experts to investigate pairwise interactions between cell types (T3).", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 170, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G3: Experts seek to validate the statistical significance of identified neighborhood patterns and larger spatial structures (G1, G2) within and between specimens. By understanding how often they appear and how properties (e.g. composition, size) vary between patterns, expers can determine clinical relevance and motivate further investigation.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "Experts indicated that they often were interested in investigating interactions between specific cell types (T3). To support expression and search for their hypotheses (Fig. 5, f), users can look for specific interactions by defining a custom query vector in the neighborhood composition view by clicking and dragging to create a polyline representing a query vector.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "dxf", "index_original": 171, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G3: Experts seek to validate the statistical significance of identified neighborhood patterns and larger spatial structures (G1, G2) within and between specimens. By understanding how often they appear and how properties (e.g. composition, size) vary between patterns, expers can determine clinical relevance and motivate further investigation.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "Together with the biomedical experts, we chose small multiples of image thumbnails as a sufficient and compact way to comparatively summarize (T4) the images in a cohort and their morphology (Fig. 1, b). Users can zoom and pan into these thumbnails to begin exploration and select a specific dataset for thorough investigation (T1) in the image viewer (a). As analysis progresses, spatial neighborhood patterns identified in a single specimen are detected and visualized in other members of the cohort by highlighting their spatial presence in linked images. Specimens can also be sorted by the number of matching neighborhoods to the pattern currently being investigated or by the statistical significance of that pattern within a specimen (see Sec. 9).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "Together with the biomedical experts, we chose small multiples of image thumbnails as a sufficient and compact way to comparatively summarize (T4) the images in a cohort and their morphology (Fig. 1, b). Users can zoom and pan into these thumbnails to begin exploration and select a specific dataset for thorough investigation (T1) in the image viewer (a). As analysis progresses, spatial neighborhood patterns identified in a single specimen are detected and visualized in other members of the cohort by highlighting their spatial presence in linked images. Specimens can also be sorted by the number of matching neighborhoods to the pattern currently being investigated or by the statistical significance of that pattern within a specimen (see Sec. 9).", "solution_category": "interaction", "solution_axial": "Participation/Collaboration;OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Participation/Collaboration"], "componenet_code": ["overview_and_explore", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 172, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G3: Experts seek to validate the statistical significance of identified neighborhood patterns and larger spatial structures (G1, G2) within and between specimens. By understanding how often they appear and how properties (e.g. composition, size) vary between patterns, expers can determine clinical relevance and motivate further investigation.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "The axes can also be reordered by drag&drop, addressing the need of experts to investigate pairwise interactions between cell types (T3) To compare (T4) the current selection to the overall composition of a specimen or cohort, the PC plot optionally encodes the neighborhoods in the entire specimen or cohort in gray, behind the current selection in orange (Fig. 1, c).", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 173, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G4: Finally, experts want to connect patterns present in an image back to biological and clinical information. They hope to determine how the presence of specific patterns correlates to specific cancer therapies, the growth of tumors, and immune response to those tumors, with an overall goal of improving cancer diagnosis and treatment.", "requirement_code": {"identify_main_cause_aggregate": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "Together with the biomedical experts, we chose small multiples of image thumbnails as a sufficient and compact way to comparatively summarize (T4) the images in a cohort and their morphology (Fig. 1, b). Users can zoom and pan into these thumbnails to begin exploration and select a specific dataset for thorough investigation (T1) in the image viewer (a). As analysis progresses, spatial neighborhood patterns identified in a single specimen are detected and visualized in other members of the cohort by highlighting their spatial presence in linked images. Specimens can also be sorted by the number of matching neighborhoods to the pattern currently being investigated or by the statistical significance of that pattern within a specimen (see Sec. 9).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "Together with the biomedical experts, we chose small multiples of image thumbnails as a sufficient and compact way to comparatively summarize (T4) the images in a cohort and their morphology (Fig. 1, b). Users can zoom and pan into these thumbnails to begin exploration and select a specific dataset for thorough investigation (T1) in the image viewer (a). As analysis progresses, spatial neighborhood patterns identified in a single specimen are detected and visualized in other members of the cohort by highlighting their spatial presence in linked images. Specimens can also be sorted by the number of matching neighborhoods to the pattern currently being investigated or by the statistical significance of that pattern within a specimen (see Sec. 9).", "solution_category": "interaction", "solution_axial": "Participation/Collaboration;OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Participation/Collaboration"], "componenet_code": ["overview_and_explore", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 174, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G4: Finally, experts want to connect patterns present in an image back to biological and clinical information. They hope to determine how the presence of specific patterns correlates to specific cancer therapies, the growth of tumors, and immune response to those tumors, with an overall goal of improving cancer diagnosis and treatment.", "requirement_code": {"identify_main_cause_aggregate": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "The axes can also be reordered by drag&drop, addressing the need of experts to investigate pairwise interactions between cell types (T3) To compare (T4) the current selection to the overall composition of a specimen or cohort, the PC plot optionally encodes the neighborhoods in the entire specimen or cohort in gray, behind the current selection in orange (Fig. 1, c).", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 175, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G1: Experts are interested in identifying how specific cell types attract or repel each other (cell-cell interaction). When immune and cancer cells are frequently observed close to one another (in each other\u2019s spatial neighborhood), they are likely to interact. For instance, the interaction of two types of immune cells (B and T-cells) is a central tenet of pro_x005f_x005f_x005f_x0002_tective immunity. These interactions can occur at various scales (cells directly adjacent to each other or in the same large region of tissue).", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "dxf", "index_original": 176, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G1: Experts are interested in identifying how specific cell types attract or repel each other (cell-cell interaction). When immune and cancer cells are frequently observed close to one another (in each other\u2019s spatial neighborhood), they are likely to interact. For instance, the interaction of two types of immune cells (B and T-cells) is a central tenet of pro_x0002_tective immunity. These interactions can occur at various scales (cells directly adjacent to each other or in the same large region of tissue).", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The axes can also be reordered by drag&drop, addressing the need of experts to investigate pairwise interactions between cell types (T3) To compare (T4) the current selection to the overall composition of a specimen or cohort, the PC plot optionally encodes the neighborhoods in the entire specimen or cohort in gray, behind the current selection in orange (Fig. 1, c).", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 177, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G2: With these neighborhoods as building blocks, experts seek to understand their spatial arrangement within the tissue image. These neighborhood patterns can be equally distributed throughout the im_x005f_x005f_x005f_x0002_age or appear in proximity, forming biologically meaningful spatial structures (groups). Germinal centers, which are regions within lymph nodes where B cells proliferate, are examples of such micro-structures.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "Inspired by similar approaches [77,84], we use permutation testing [30] to determine the patterns\u2019 statistical significance (T5) within a specimen and across a cohort. We count individual neighborhoods that match a given neighborhood pattern based on a user-defined similarity threshold (Sec. 8.2). We then randomly shuffle the assigned cell types in the data, recomputing neighborhood vectors (Steps 1 3), and calculating the number of matching neighborhoods for each permutation.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "dxf", "index_original": 178, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G2: With these neighborhoods as building blocks, experts seek to understand their spatial arrangement within the tissue image. These neighborhood patterns can be equally distributed throughout the im_x0002_age or appear in proximity, forming biologically meaningful spatial structures (groups). Germinal centers, which are regions within lymph nodes where B cells proliferate, are examples of such micro-structures.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "We additionally display the number of results and significance of the query within each specimen above the spatial summary, as discussed in Sec. 9. Users sort related specimens within the list by significance and number of results (T5).", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 179, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G3: Experts seek to validate the statistical significance of identified neighborhood patterns and larger spatial structures (G1, G2) within and between specimens. By understanding how often they appear and how properties (e.g. composition, size) vary between patterns, expers can determine clinical relevance and motivate further investigation.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "Inspired by similar approaches [77,84], we use permutation testing [30] to determine the patterns\u2019 statistical significance (T5) within a specimen and across a cohort. We count individual neighborhoods that match a given neighborhood pattern based on a user-defined similarity threshold (Sec. 8.2). We then randomly shuffle the assigned cell types in the data, recomputing neighborhood vectors (Steps 1 3), and calculating the number of matching neighborhoods for each permutation.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "dxf", "index_original": 180, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G3: Experts seek to validate the statistical significance of identified neighborhood patterns and larger spatial structures (G1, G2) within and between specimens. By understanding how often they appear and how properties (e.g. composition, size) vary between patterns, expers can determine clinical relevance and motivate further investigation.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "We additionally display the number of results and significance of the query within each specimen above the spatial summary, as discussed in Sec. 9. Users sort related specimens within the list by significance and number of results (T5).", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 181, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G3: Experts seek to validate the statistical significance of identified neighborhood patterns and larger spatial structures (G1, G2) within and between specimens. By understanding how often they appear and how properties (e.g. composition, size) vary between patterns, expers can determine clinical relevance and motivate further investigation.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "While using Visinity, users can save and retrieve patterns and label them with clinical or biological context (T6), which are displayed in a pattern list in the interface, as reflected in the proposed workflow (Fig. 3, e).", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 182, "paper_title": "Visinity: Visual Spatial Neighborhood Analysis for Multiplexed Tissue Imaging Data", "pub_year": 2023, "domain": "biology", "requirement": {"requirement_text": "G4: Finally, experts want to connect patterns present in an image back to biological and clinical information. They hope to determine how the presence of specific patterns correlates to specific cancer therapies, the growth of tumors, and immune response to those tumors, with an overall goal of improving cancer diagnosis and treatment.", "requirement_code": {"identify_main_cause_aggregate": 1}}, "data": {"data_text": "Biomedical Imaging Data", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "While using Visinity, users can save and retrieve patterns and label them with clinical or biological context (T6), which are displayed in a pattern list in the interface, as reflected in the proposed workflow (Fig. 3, e).", "solution_category": "interaction", "solution_axial": "History", "solution_compoent": "", "axial_code": ["History"], "componenet_code": ["history"]}]}, {"author": "dxf", "index_original": 183, "paper_title": "RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports", "pub_year": 2023, "domain": "Sports", "requirement": {"requirement_text": "T1: Get an overview of an individual player\u2019s tactics. Experts prefer player-centric tactical analysis, because each player uses unique tactics. Rather than analyzing each tactic of a player directly, experts expect to start with an overview to learn things such as how many tactics result in more wins, and whether the player specializes in a few tactics or adopts more tactics to confuse opponents.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "racket sports data", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "Projection View (Fig. 1E) projects the tactics onto a 2-D plane, helping experts overview tactics and reveal their similarity (T1, T4). We propose a semantic projection method based on a polar coordinate system, which can generate a fixed position for each tactic based on semantics (Fig. 7). For each tactic, the polar angle encodes the top-two relevant attributes (i.e., the two attributes with the most non-null values) so that experts can analyze a specific tactic type by exploring the corresponding direction. Meanwhile, we adopt PCA, which can generate fixed projection results, to project each tactic to a 1-D coordinate system as the radial coordinate. However, tactics vary in length and contain categorical values that are difficult to quantize (e.g., the hitting technique), which cannot be used as inputs for PCA. Thus, we characterize a tactic as a semantic vector based on several basis tactics, constructed in three steps: 1) We required experts to provide ten different typical tactics, such as seesaw battles and net tactics, to be used as basis tactics baked in our system. 2) We calculate the Levenshtein distance of the tactic at hand from each basis tactic to represent their similarity, following TacticFlow. 3) We construct the vector with the similarities in order and normalize it.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Circle", "axial_code": [], "componenet_code": ["circle"]}]}, {"author": "dxf", "index_original": 184, "paper_title": "RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports", "pub_year": 2023, "domain": "Sports", "requirement": {"requirement_text": "T1: Get an overview of an individual player\u2019s tactics. Experts prefer player-centric tactical analysis, because each player uses unique tactics. Rather than analyzing each tactic of a player directly, experts expect to start with an overview to learn things such as how many tactics result in more wins, and whether the player specializes in a few tactics or adopts more tactics to confuse opponents.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "racket sports data", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "Tactic View (Fig. 1C) lists all tactics (T1). Each row represents a tactic and contains six columns as follows. Ranking Columns (Freq., Win%, Imp.). Experts may start by ranking the list to find the most valuable tactics. We use lineups [15] to provide multiple rankings based on the frequency (Freq.), the win rate (Win%), and the tactical importance (Imp.), visualized by the three bars. Experts can select one of the three to satisfy different analysis needs. Tactic Column (Tactic). After selecting a suitable ranking, experts may scan the tactic list top-down and explore the tactics one by one. Each hit in each tactic is encoded by a tailored glyph, which aggregates the corresponding hit in the raw rallies where this tactic was used, with the color encoding the player who hits the ball. The glyph is based on a metaphorical icon, such as the shuttlecock and the tennis ball (Fig. 8), familiar and intuitive to experts [26,40,45,47,50,51]. Each glyph encodes a multivariate hit with multiple non-overlapping components, each encoding a hit feature. For example, in the tennis glyph in Fig. 8, the blue block on the tennis court encodes the position where the ball bounces on the ground. All detailed designs are in the appendix. The main novelty of our glyphs lies in encoding null values with uncertainty, each of which indicates that there exists multiple possible values every time using the tactic. For each null value, we first count the frequency of each possible value. Then, we display the possible value with the most frequency in our glyph, where the opacity of the corresponding component varies on multiple levels based on the frequency percentage. Such a design can help experts see how the tactic most often progresses, as well as how uncertain each value is. Furthermore, experts can expand a tactic to find the two most possible values for each null, where the bars visualize the frequency percentage. Operation Columns (No., Pref.). When experts are satisfied with a tactic, they can click the favorite icon (Pref.) to fix it, avoiding future adjustments. When experts find some tactics to adjust, they can select the tactics (No.) and give suggestions in the Suggestion Panel.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Circle;Bar;Table", "axial_code": [], "componenet_code": ["table", "bar", "circle"]}]}, {"author": "dxf", "index_original": 186, "paper_title": "RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports", "pub_year": 2023, "domain": "Sports", "requirement": {"requirement_text": "T3: Evaluate the adjusted results. Experts\u2019 knowledge comes from past games, and may lose usefulness as a player\u2019s tactics evolve over time. Just as the experts suggest adjustments, experts expect our system to evaluate their suggestions \u2014 providing data-driven metrics and comparing adjusted tactics with the original ones \u2014 to prevent them from making inappropriate adjustments. For example, when experts find a tactic not typical of a certain player and suggest removing it, our system could provide data indicating that this may be a newly developed tactic rather than anomaly, thus reminding experts to exercise caution before discounting it.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "racket sports data", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "Experts can preview the results of an adjustment after giving a suggestion and then determine whether to apply the adjustment (T3), through the preview mode in Projection View (Fig. 1E1) and Tactic View (Fig. 1C1). C1 and E1 demonstrate one example of splitting one tactic (with solid border) into two (with dashed borders). The two new tactics have different colors, indicating different win rates. Tactic View moves the three tactics to the top for comparison, with icons and ; representing the original tactic and the new tactics, respectively. To reveal their differences in detail, experts can compare the glyphs and even click on each tactic to observe the related rallies in Rally View.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Circle", "axial_code": [], "componenet_code": ["circle"]}]}, {"author": "dxf", "index_original": 187, "paper_title": "RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports", "pub_year": 2023, "domain": "Sports", "requirement": {"requirement_text": "T3: Evaluate the adjusted results. Experts\u2019 knowledge comes from past games, and may lose usefulness as a player\u2019s tactics evolve over time. Just as the experts suggest adjustments, experts expect our system to evaluate their suggestions \u2014 providing data-driven metrics and comparing adjusted tactics with the original ones \u2014 to prevent them from making inappropriate adjustments. For example, when experts find a tactic not typical of a certain player and suggest removing it, our system could provide data indicating that this may be a newly developed tactic rather than anomaly, thus reminding experts to exercise caution before discounting it.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "racket sports data", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "Experts can preview the results of an adjustment after giving a suggestion and then determine whether to apply the adjustment (T3), through the preview mode in Projection View (Fig. 1E1) and Tactic View (Fig. 1C1). C1 and E1 demonstrate one example of splitting one tactic (with solid border) into two (with dashed borders). The two new tactics have different colors, indicating different win rates. Tactic View moves the three tactics to the top for comparison, with icons and ; representing the original tactic and the new tactics, respectively. To reveal their differences in detail, experts can compare the glyphs and even click on each tactic to observe the related rallies in Rally View.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Circle;Bar;Table", "axial_code": [], "componenet_code": ["table", "bar", "circle"]}, {"solution_text": "Experts can preview the results of an adjustment after giving a suggestion and then determine whether to apply the adjustment (T3), through the preview mode in Projection View (Fig. 1E1) and Tactic View (Fig. 1C1). C1 and E1 demonstrate one example of splitting one tactic (with solid border) into two (with dashed borders). The two new tactics have different colors, indicating different win rates. Tactic View moves the three tactics to the top for comparison, with icons and ; representing the original tactic and the new tactics, respectively. To reveal their differences in detail, experts can compare the glyphs and even click on each tactic to observe the related rallies in Rally View.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 188, "paper_title": "RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports", "pub_year": 2023, "domain": "Sports", "requirement": {"requirement_text": "T4: Discover similar tactics. Players often achieve different tactical goals (e.g., confusing their opponents) by changing a few hit features of a previous tactic, resulting in a slightly different tactic. Finding these tactics is valuable for tactical analysis because it helps experts study which changes are effective. Meanwhile, merging similar tactics that actually lead to similar outcomes can help experts avoid spending their analysis time unnecessarily.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "racket sports data", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Projection View (Fig. 1E) projects the tactics onto a 2-D plane, helping experts overview tactics and reveal their similarity (T1, T4). We propose a semantic projection method based on a polar coordinate system, which can generate a fixed position for each tactic based on semantics (Fig. 7). For each tactic, the polar angle encodes the top-two relevant attributes (i.e., the two attributes with the most non-null values) so that experts can analyze a specific tactic type by exploring the corresponding direction. Meanwhile, we adopt PCA, which can generate fixed projection results, to project each tactic to a 1-D coordinate system as the radial coordinate. However, tactics vary in length and contain categorical values that are difficult to quantize (e.g., the hitting technique), which cannot be used as inputs for PCA. Thus, we characterize a tactic as a semantic vector based on several basis tactics, constructed in three steps: 1) We required experts to provide ten different typical tactics, such as seesaw battles and net tactics, to be used as basis tactics baked in our system. 2) We calculate the Levenshtein distance of the tactic at hand from each basis tactic to represent their similarity, following TacticFlow. 3) We construct the vector with the similarities in order and normalize it.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Circle", "axial_code": [], "componenet_code": ["circle"]}, {"solution_text": "Projection View (Fig. 1E) projects the tactics onto a 2-D plane, helping experts overview tactics and reveal their similarity (T1, T4). We propose a semantic projection method based on a polar coordinate system, which can generate a fixed position for each tactic based on semantics (Fig. 7). For each tactic, the polar angle encodes the top-two relevant attributes (i.e., the two attributes with the most non-null values) so that experts can analyze a specific tactic type by exploring the corresponding direction. Meanwhile, we adopt PCA, which can generate fixed projection results, to project each tactic to a 1-D coordinate system as the radial coordinate. However, tactics vary in length and contain categorical values that are difficult to quantize (e.g., the hitting technique), which cannot be used as inputs for PCA. Thus, we characterize a tactic as a semantic vector based on several basis tactics, constructed in three steps: 1) We required experts to provide ten different typical tactics, such as seesaw battles and net tactics, to be used as basis tactics baked in our system. 2) We calculate the Levenshtein distance of the tactic at hand from each basis tactic to represent their similarity, following TacticFlow. 3) We construct the vector with the similarities in order and normalize it.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 190, "paper_title": "ECoalVis: Visual Analysis of Control Strategies in Coal-fired Power Plants", "pub_year": 2023, "domain": "Energy", "requirement": {"requirement_text": "R1: Extract the impact of control strategies with time series queries. In the forward analysis, the experts need to find the impact of certain control strategies. Hence, the proposed system should support the intuitive query with partial control strategies that describe the timeseries changes in the sensors of concern (rising, falling, or stable) and the temporal relationships between these changes (the order of propagation). Thereafter, the matching control strategies and their impact should be efficiently extracted from the historical data, and the similarities and differences among these strategies should be visualized to help the experts select a strategy of interest for further analysis.", "requirement_code": {"identify_main_cause_aggregate": 1}}, "data": {"data_text": "dynamic sensor data", "data_code": {"tables": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "Retrieval", "solution_compoent": "", "axial_code": ["Retrieval"], "componenet_code": ["retrieval"]}]}, {"author": "dxf", "index_original": 192, "paper_title": "ECoalVis: Visual Analysis of Control Strategies in Coal-fired Power Plants", "pub_year": 2023, "domain": "Energy", "requirement": {"requirement_text": "R2: Identify responsible control strategies for anomalies in important sensors. In the backward analysis, the experts need to identify anomalies (e.g., sudden rises or drops) of important sensors, such as the efficiency and pollutant emissions, which should be visualized in the proposed system. To find the causes of the anomalies, the experts should be able to select the period of time that comprises an anomaly, and the system should efficiently search through the historical data and extract a responsible control strategy constituted by multiple cascading sensor events, including the selected anomaly. Further analysis of this control strategy can help determine the cause of this anomaly.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "dynamic sensor data", "data_code": {"tables": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "Retrieval", "solution_compoent": "", "axial_code": ["Retrieval"], "componenet_code": ["retrieval"]}]}, {"author": "dxf", "index_original": 194, "paper_title": "ECoalVis: Visual Analysis of Control Strategies in Coal-fired Power Plants", "pub_year": 2023, "domain": "Energy", "requirement": {"requirement_text": "R3: Explore the spatial propagation of control strategy impact. Visualizing the spatial propagation of impact is critical to the analysis of control strategies, where the experts can discover the involved components, units and sensors and identify the propagation paths. We initially adopt a relationship-oriented layout, visualizing the propagation on a distorted schema of the power plant to reflect the strengths of the relationships among the involved sensors. The experts appreciate the intuitiveness of this design, but they also request to add a contextoriented layout without schema distortion, so they can interpret the distribution of the involved sensors faster in a more familiar context.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "dynamic sensor data", "data_code": {"tables": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The graph view adopts a dual-mode design to provide both spatial and contextual information about the coal-fired power plant (R3). Analyzing the relationship between sensors in control strategies helps gain insights into the power plants, so we design a relationship-oriented view. This view highlights the spatial propagation and correlation among the components, units, and sensors. In addition, depicting the complex hierarchical structure and actual spatial location is also very important, so we design a context-oriented view", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Cluster", "axial_code": [], "componenet_code": ["point"]}]}, {"author": "dxf", "index_original": 195, "paper_title": "ECoalVis: Visual Analysis of Control Strategies in Coal-fired Power Plants", "pub_year": 2023, "domain": "Energy", "requirement": {"requirement_text": "R4: Obtain the temporal cascading of control strategy impact. Analyzing how long it takes to propagate impact from one sensor to another in a control strategy can help the experts understand the strategy\u2019s execution flow, such that more control actions can be incorporated into the strategy with confidence. The proposed system should be able to infer such time lags and visualize them along with the topology of the control strategy. This visualization can also reveal that some sensors may be missing from the extracted control strategy based on large time lags. Hence, the system should allow the experts to add or remove sensors from the topology to iteratively guide the extraction model.", "requirement_code": {"identify_main_cause_aggregate": 1}}, "data": {"data_text": "dynamic sensor data", "data_code": {"tables": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "Retrieval", "solution_compoent": "", "axial_code": ["Retrieval"], "componenet_code": ["retrieval"]}, {"solution_text": "The strategy view is designed for analyzing the temporal cascading impact of control strategies (R4). There are two aspects to this analysis task. First, in terms of temporal dimension, the user is concerned about the exact time lag of the impact from sensor A to sensor B. In addition, the users also want to see detailed time series. Second, in terms of topological dimension, the user is concerned about the cascading impact between sensors, especially the correlation and propagating direction.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "dxf", "index_original": 200, "paper_title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections", "pub_year": 2023, "domain": "Ergonomic", "requirement": {"requirement_text": "R2: Provide a way for observing the workers\u2019 movements and postures during several work cycles (T2).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Joint angles over time", "data_code": {"network_and_trees": 1, "temporal": 1}}, "solution": [{"solution_text": "It also supports the evaluation of repeatability and facilitates the elimination of outliers. The main concept of this view is to depict the joint angles by a line chart. In this way, the analysts see the values as a function of time, and can easily spot cycles, Juxtaposition-Similar-Nonsymmetricals, and irregularities (R2, R5).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 204, "paper_title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections", "pub_year": 2023, "domain": "Ergonomic", "requirement": {"requirement_text": "R3: Quickly specify the most compromised postures, critical angle ranges, and highest force-load tasks (T3).", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "joint angles, ergonomic data", "data_code": {"tables": 1}}, "solution": [{"solution_text": "ErgoTimeline also superimposes the curves on colored background (R3). An example is given in Fig. 6(c) to examine joint angles in detail. As limits are given per joint only, in case of comparing several joints a common vertical axis becomes impossible. In this case the vertical axis is split across parts of the view.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 205, "paper_title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections", "pub_year": 2023, "domain": "Ergonomic", "requirement": {"requirement_text": "R3: Quickly specify the most compromised postures, critical angle ranges, and highest force-load tasks (T3).", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "body positions", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Actionable views are essential for some requirements (e.g., R3, R6). Concerning the REBATables, the user can select a single bin in a histogram or make a composite brush by selecting several histograms with the mouse. If a user selects one or more cells in the heatmap, we regard this action as adding new data items to the same brush and not as creating a new brush and adding it into a composite brush (see Fig. 7). Actionable views are essential for some requirements (e.g., R3, R6). Ergonomists appreciated this realization because, in most cases, they select a couple of cells (posture score values). Then they observe the relations in other views to generate hypotheses or valuable clues about the possible causes of actual (R6) or potential ergonomic risks (R8) and suitable feedback recommendations (R7). ", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "dxf", "index_original": 206, "paper_title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections", "pub_year": 2023, "domain": "Ergonomic", "requirement": {"requirement_text": "R4: Locate postures that are held during the longest period of time (T3).", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "joint angles, ergonomic data", "data_code": {"tables": 1}}, "solution": [{"solution_text": "In this regard, the ErgoTimeline presents an easy understandable representation of the ergonomic risks along time. It allows the users to discern the work cycles and all of the movements that characterize them (R2). This is especially relevant to pinpoint a single working cycle that can be taken as a representative of all the other cycles. The goals are to localize inadequate working postures (R3) or movements that repeat or extend for an inappropriately long duration (R4), and to pinpoint outliers or anomalies (R5).", "solution_category": "interaction", "solution_axial": "Connect/Related", "solution_compoent": "", "axial_code": ["Connect/Related"], "componenet_code": ["connect_relate"]}]}, {"author": "dxf", "index_original": 208, "paper_title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections", "pub_year": 2023, "domain": "Ergonomic", "requirement": {"requirement_text": "R5: Provide a way to find (or discard) atypical actions or jointrisk estimations (T4).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "joint angles, ergonomic data", "data_code": {"tables": 1}}, "solution": [{"solution_text": "In this regard, the ErgoTimeline presents an easy understandable representation of the ergonomic risks along time. It allows the users to discern the work cycles and all of the movements that characterize them (R2). This is especially relevant to pinpoint a single working cycle that can be taken as a representative of all the other cycles. The goals are to localize inadequate working postures (R3) or movements that repeat or extend for an inappropriately long duration (R4), and to pinpoint outliers or anomalies (R5).", "solution_category": "interaction", "solution_axial": "Connect/Related", "solution_compoent": "", "axial_code": ["Connect/Related"], "componenet_code": ["connect_relate"]}]}, {"author": "dxf", "index_original": 209, "paper_title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections", "pub_year": 2023, "domain": "Ergonomic", "requirement": {"requirement_text": "R5: Provide a way to find (or discard) atypical actions or jointrisk estimations (T4).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "joint angles, ergonomic data", "data_code": {"tables": 1}}, "solution": [{"solution_text": "The main concept of this view is to depict the joint angles by a line chart. In this way, the analysts see the values as a function of time, and can easily spot cycles, Juxtaposition-Similar-Nonsymmetricals, and irregularities (R2, R5).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 210, "paper_title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections", "pub_year": 2023, "domain": "Ergonomic", "requirement": {"requirement_text": "R5: Provide a way to find (or discard) atypical actions or jointrisk estimations (T4).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "video stream, posture images", "data_code": {"tables": 1, "media": 1}}, "solution": [{"solution_text": "The ErgoMovements view addresses the requirements (such as R5, and R7) to enable image(s) or video preview, for example, to inspect a time window around a specific pose (R1). There is a general trend towards an automatic monitoring in workplaces. Ergonomy experts will be provided with detailed ergonomic data to con_x0002_duct a thorough a-posteriori analysis. However, visual data will still be the most important information in their work. One of the requirements has been that ErgoExplorer should include means to analyze videos and posture images during the assessment task. This can be accom_x0002_plished with the ErgoMovements view, which provides standard video playback options (R7) and also supports further requirements, including R1, R2, R3, R5, R7, and R6, as explained in the following. Compared to the pencil-and-paper-based REBA method, ErgoMovements helps users to create a better mental representation of the analyzed data by linking numerical values in the views together with actual workers\u2019 movements. For example, by examining the quantitative and qualitative data presented in the REBA Tables and displayed in the ErgoView, the ergonomist can quickly conclude the seriousness of the situation and whether to react immediately. In order to help analysts perceive the depicted information more efficiently (especially regarding R5), we provide options to display different sets of pre-selected images (see Fig 5). Images are one of the attributes in our datasets, and each image is associated with a specific time point. The user can select any of the related data tables shown in the ErgoView, to display the corre_x0002_sponding set of images. Each of the images relates to its corresponding REBA score. There is an option to quickly switch between the tables to gain insight into the worker\u2019s actions in relation to the tables\u2019 scores. We have considered different ways of selecting representative images since quite different postures can result in the same overall score in the table. In working conditions where actions are repeated cycli_x0002_cally (as is our case), the experts mentioned that any image of a group with the same score is a good representative of the whole group. In this case, they identified the relevant task to establish a relationship between the results presented in the REBA Tables, the complexity of the work, and the related risk factors to which the worker is exposed. ErgoMovements helps to clarify the observed workers\u2019 ergonomics data in the context of their original work environment (R2, R6, R7). Moreover, ErgoMovements can show examples of unsafe actions as well as good practices previously executed during the workday (R3, R5, R7), which in turn supports the reduction of ergonomic risks. For instance, workers who are at a high ergonomic risk undergo retraining sessions. During this retraining, ErgoMovements depicts repre_x0002_sentations of the currently performed movements and postures (R7), highlighting the aspects that need to improve, and also the progress to achieve safer working practices.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Table", "axial_code": [], "componenet_code": ["table"]}]}, {"author": "dxf", "index_original": 212, "paper_title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections", "pub_year": 2023, "domain": "Ergonomic", "requirement": {"requirement_text": "R5: Provide a way to find (or discard) atypical actions or jointrisk estimations (T4).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "video stream, posture images", "data_code": {"tables": 1}}, "solution": [{"solution_text": "In order to help analysts perceive the depicted information more efficiently (especially regarding R5), we provide options to display different sets of pre-selected images (see Fig 5). ErgoMovements can show examples of unsafe actions as well as good practices previously executed during the workday (R3, R5, R7), which in turn supports the reduction of ergonomic risks. For instance, workers who are at a high ergonomic risk undergo retraining sessions.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 214, "paper_title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections", "pub_year": 2023, "domain": "Ergonomic", "requirement": {"requirement_text": "R6: Provide a way for visual and descriptive identification of the task and the movements performed (T1, T5).", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "body positions", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Actionable views are essential for some requirements (e.g., R3, R6). Concerning the REBATables, the user can select a single bin in a histogram or make a composite brush by selecting several histograms with the mouse. If a user selects one or more cells in the heatmap, we regard this action as adding new data items to the same brush and not as creating a new brush and adding it into a composite brush (see Fig. 7). Actionable views are essential for some requirements (e.g., R3, R6). Ergonomists appreciated this realization because, in most cases, they select a couple of cells (posture score values). Then they observe the relations in other views to generate hypotheses or valuable clues about the possible causes of actual (R6) or potential ergonomic risks (R8) and suitable feedback recommendations (R7). ", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "dxf", "index_original": 215, "paper_title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections", "pub_year": 2023, "domain": "Ergonomic", "requirement": {"requirement_text": "R6: Provide a way for visual and descriptive identification of the task and the movements performed (T1, T5).", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "joint angles, ergonomic data", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Therefore, it has been appreciated by the users to easily link the numerical ergonomic risk estimations in the ErgoTimeline (e.g., peaks) to viewing frames (R6) in the actual video capture where the associated events occurred.", "solution_category": "interaction", "solution_axial": "Connect/Related", "solution_compoent": "", "axial_code": ["Connect/Related"], "componenet_code": ["connect_relate"]}]}, {"author": "dxf", "index_original": 217, "paper_title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections", "pub_year": 2023, "domain": "Ergonomic", "requirement": {"requirement_text": "R7: Provide means to easily locate frames or video portions with risky movements for workers\u2019 retraining (T1, T6).", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "video stream, posture images", "data_code": {"tables": 1}}, "solution": [{"solution_text": "The ErgoMovements view addresses the requirements (such as R5, and R7) to enable image(s) or video preview, for example, to inspect a time window around a specific pose (R1). There is a general trend towards an automatic monitoring in workplaces. Ergonomy experts will be provided with detailed ergonomic data to conduct a thorough a-posteriori analysis. However, visual data will still be the most important information in their work. One of the requirements has been that ErgoExplorer should include means to analyze videos and posture images during the assessment task. This can be accom_x0002_plished with the ErgoMovements view, which provides standard video playback options (R7) and also supports further requirements, including R1, R2, R3, R5, R7, and R6, as explained in the following. Compared to the pencil-and-paper-based REBA method, ErgoMovements helps users to create a better mental representation of the analyzed data by linking numerical values in the views together with actual workers\u2019 movements. For example, by examining the quantitative and qualitative data presented in the REBA Tables and displayed in the ErgoView, the ergonomist can quickly conclude the seriousness of the situation and whether to react immediately. In order to help analysts perceive the depicted information more efficiently (especially regarding R5), we provide options to display different sets of pre-selected images (see Fig 5). Images are one of the attributes in our datasets, and each image is associated with a specific time point. The user can select any of the related data tables shown in the ErgoView, to display the corre_x0002_sponding set of images. Each of the images relates to its corresponding REBA score. There is an option to quickly switch between the tables to gain insight into the worker\u2019s actions in relation to the tables\u2019 scores. We have considered different ways of selecting representative im_x0002_ages since quite different postures can result in the same overall score in the table. In working conditions where actions are repeated cycli_x0002_cally (as is our case), the experts mentioned that any image of a group with the same score is a good representative of the whole group. In this case, they identified the relevant task to establish a relationship between the results presented in the REBA Tables, the complexity of the work, and the related risk factors to which the worker is exposed. ErgoMovements helps to clarify the observed workers\u2019 ergonomics data in the context of their original work environment (R2, R6, R7). Moreover, ErgoMovements can show examples of unsafe actions as well as good practices previously executed during the workday (R3, R5, R7), which in turn supports the reduction of ergonomic risks. For instance, workers who are at a high ergonomic risk undergo retrain_x0002_ing sessions. During this retraining, ErgoMovements depicts repre_x0002_sentations of the currently performed movements and postures (R7), highlighting the aspects that need to improve, and also the progress to achieve safer working practices.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Table", "axial_code": [], "componenet_code": ["table"]}]}, {"author": "dxf", "index_original": 218, "paper_title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections", "pub_year": 2023, "domain": "Ergonomic", "requirement": {"requirement_text": "R8: Incorporate the REBA score tables which provide an action level with an indication of urgency (T3, T6)", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "risk assessment data, ergonomic data", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Brushing in ErgoGauge. (a) A brush was created to select the right-shoulder angles in the range from 62 to 66.8. (b) Corresponding radial lines are highlighted for the right elbow in the linked ErgoGauge (R10).", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "dxf", "index_original": 219, "paper_title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections", "pub_year": 2023, "domain": "Ergonomic", "requirement": {"requirement_text": "R9: Provide comparisons of time-dependent scores for single and multiple joints (T5).", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "risk assessment data, ergonomic data", "data_code": {"tables": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "REBA Tables are arranged around the picture of a human body, where the body silhouette is seen from behind. This orientation makes it easier for ergonomists to quickly associate the scores in the tables positioned on the right side with the corresponding joints on the right side of the human body, and analogously for the left side. The same is true for the ErgoGauges, which are placed in a row above the silhouette. They support a detailed analysis of ergonomic risk for all body joints (R9). Moreover, we propose to augment all tables to allow ergonomists to compare time-dependent scores for single and multiple joints (R9)", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Table", "axial_code": [], "componenet_code": ["table"]}]}, {"author": "dxf", "index_original": 220, "paper_title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections", "pub_year": 2023, "domain": "Ergonomic", "requirement": {"requirement_text": "R9: Provide comparisons of time-dependent scores for single and multiple joints (T5).", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "joint angles, ergonomic data", "data_code": {"tables": 1}}, "solution": [{"solution_text": "We show data for several joints in a single chart to support a visual correlation analysis (R9). Color coding and labels are used to distinguish between joints in this case. simultaneously compares angles & risks of the elbows and shoulders. Different body joints operate in different angle ranges Each vertical axis is scaled according to the range of the corresponding data attribute (R9).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 221, "paper_title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections", "pub_year": 2023, "domain": "Ergonomic", "requirement": {"requirement_text": "R10: Quickly compare the risk distribution for each joint between the two sides of the body (T5, T6).", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "risk assessment data, ergonomic data", "data_code": {"tables": 1}}, "solution": [{"solution_text": "A vast number of scores for an extended period of time can be calculated and deployed for analysis automatically. We display all scoring tables at once, three for the left and three for the right body side (R10).", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Table;Bar;Heatmap", "axial_code": [], "componenet_code": ["heatmap", "bar", "table"]}]}, {"author": "dxf", "index_original": 222, "paper_title": "ErgoExplorer: Interactive Ergonomic Risk Assessment from Video Collections", "pub_year": 2023, "domain": "Ergonomic", "requirement": {"requirement_text": "R10: Quickly compare the risk distribution for each joint between the two sides of the body (T5, T6).", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "joint angles, ergonomic data", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Brushing in ErgoGauge. (a) A brush was created to select the right-shoulder angles in the range from 62 to 66.8. (b) Corresponding radial lines are highlighted for the right elbow in the linked ErgoGauge (R10).", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "dxf", "index_original": 226, "paper_title": "CohortVA: A Visual Analytic System for Interactive Exploration of Cohorts based on Historical Data", "pub_year": 2023, "domain": "Historical", "requirement": {"requirement_text": "T4: Verify the cohort from the organized historical event information. H1-H5 emphasize that they always search for additional evidence to verify the results of automated methods. It is necessary to present historians with rich contexts that historians frequently reference, such as geographic locations and social network relationships. They can cross-check the identified cohorts from the detailed historical event information.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "Historical figure Data", "data_code": {"textual": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "CohortVA lets historians explore and refine the selected cohort from two perspectives: the cohort concept and included figures. The figures are described by the Cohort Exploration Component and the Figure Details Component (Fig. 3B3). Historians can cross-check the cohort composition according to historical events (T4) and detailed figure descriptions (T5). After validation, CohortVA supports historians in excluding figures from the cohort and including related ones in the Figure Label View.", "solution_category": "interaction", "solution_axial": "Selecting;Connect/Relate", "solution_compoent": "", "axial_code": ["Selecting", "Connect/Relate"], "componenet_code": ["selecting", "connect_relate"]}]}, {"author": "dxf", "index_original": 227, "paper_title": "CohortVA: A Visual Analytic System for Interactive Exploration of Cohorts based on Historical Data", "pub_year": 2023, "domain": "Historical", "requirement": {"requirement_text": "T5: Inspect individual figures. Analyzing individual profiles help historians interpret the identified cohort at the most detailed level. All mined features and event evidence can be provided for inspiration. The system should display figure profiles from CBDB with spatial-temporal information and descriptions of social relationships. Directing users to the original sources outside the system should also enhance their trust level in the result.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "Historical figure Data", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "CohortVA lets historians explore and refine the selected cohort from two perspectives: the cohort concept and included figures. The figures are described by the Cohort Exploration Component and the Figure Details Component (Fig. 3B3). Historians can cross-check the cohort composition according to historical events (T4) and detailed figure descriptions (T5). After validation, CohortVA supports historians in excluding figures from the cohort and including related ones in the Figure Label View.", "solution_category": "interaction", "solution_axial": "Selecting;Connect/Relate", "solution_compoent": "", "axial_code": ["Selecting", "Connect/Relate"], "componenet_code": ["selecting", "connect_relate"]}]}, {"author": "dxf", "index_original": 235, "paper_title": "HetVis: A Visual Analysis Approach for Identifying Data Heterogeneity in Horizontal Federated Learning", "pub_year": 2023, "domain": "Federated learning", "requirement": {"requirement_text": "R3.3: Tracking the identified issues during the training process. During the training process, the classification results for certain records may update. An intermediate result of a certain communication round may be randomly influenced (e.g., the training data included outliers accidentally), even after the model has converged. To draw a firm conclusion, analysts need to collect suspected issues and track them in the following communication rounds. Besides, analysts can observe what problems HFL faces and whether it can solve them by tracking the training process. It is significant to evaluate the effectiveness and robustness of the HFL model.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "the records", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "As shown in Figure 2(i), users can annotate the records with suspicious heterogeneity issues and track these records in the following training process (R3.3). If the negative impacts of these records is weakened or even disappears, users can consider that the HFL models can overcome such heterogeneity. If not, this indicates users should pay attention to possible heterogeneity issues, and reassess the cooperation of FL.", "solution_category": "interaction", "solution_axial": "History", "solution_compoent": "", "axial_code": ["History"], "componenet_code": ["history"]}]}, {"author": "dxf", "index_original": 238, "paper_title": "DocFlow: A Visual Analytics System for Question-based Document Retrieval and Categorization", "pub_year": 2023, "domain": "question-based document retrieval", "requirement": {"requirement_text": "DP2: Interactive visualization system for pipeline construction and tracking. DocFlow should support easy user interactions for composing a retrieval pipeline with components and links. Visualization methods can be an integral system for textual data analysis. Furthermore, the system should also visualize the entire pipeline to help users keep track of the retrieval history.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "question-answering(QA) datasets", "data_code": {"network_and_trees": 1, "tables": 1}}, "solution": [{"solution_text": "DocFlow is equipped with user interactions to support users to quickly build and keep track of their retrieval pipeline (DP2). As shown in Figure 3, users can create an information-seeking process on the Pipeline Editing Canvas. We show the instructions below. (1) Create nodes (DocFlow components), resize, and reposition the nodes in an intuitive drag-and-drop manner. A node panel would guide users on how to create a node. (2) Hold and drag data ports of different nodes to create a link for transmitting a document set or the visualization ports for transmitting the visualization parameters. (3) Resize each node to a small view, in which only the necessary icons, questions, and the number of documents are displayed, to get a clear overview of the pipeline, as shown in Figure 3, Figure 7, Figure 5. The document categorizer can further hide all the extracted categories (Figure 8p2) to reduce the space. (4) Save and load the existing pipeline (DP3) for resume and results sharing.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration;Connect/Relate;Selecting;History", "solution_compoent": "", "axial_code": ["Selecting", "History", "Connect/Relate", "Participation/Collaboration"], "componenet_code": ["selecting", "history", "connect_relate", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 239, "paper_title": "DocFlow: A Visual Analytics System for Question-based Document Retrieval and Categorization", "pub_year": 2023, "domain": "question-based document retrieval", "requirement": {"requirement_text": "DP3: Reproducible and sustainable pipeline. DocFlow should support saving and loading user-constructed pipelines to reproduce the document retrieval and categorization process. The saved pipeline should contain all the configuration information, including component layout, links, user input on each component, etc", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "question-answering(QA) datasets", "data_code": {"network_and_trees": 1, "tables": 1}}, "solution": [{"solution_text": "DocFlow is equipped with user interactions to support users to quickly build and keep track of their retrieval pipeline (DP2). As shown in Figure 3, users can create an information-seeking process on the Pipeline Editing Canvas. Save and load the existing pipeline (DP3) for resume and results sharing.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "dxf", "index_original": 240, "paper_title": "PhraseMap: Attention-based Keyphrases Recommendation for Information Seeking", "pub_year": 2023, "domain": "Information Seeking", "requirement": {"requirement_text": "T1: Provide an informative overview of the PhraseMap. Motivated by R3.1, the overview presentation is helpful only if a large amount of information is well-presented to users. Given the enormous amount of information in the PhraseMap, the system should avoid information clutter and simultaneously retain critical information.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "question-answering(QA) datasets", "data_code": {"textual": 1, "fields": 1}}, "solution": [{"solution_text": "Our system aggregates phrases into topics and assigns each topic a color to provide an overview of a corpus (T1). We use colors to help users differentiate semantic clusters. Specifically, we utilize a bottom-up clustering approach to group phrases iteratively \u2013 adjacent cells merge if their semantic distance is smaller than a user-defined threshold. Afterward, we assign each semantic cluster a random color in the palette. However, for the clusters that are too small, we assign them a background color ( ) to prevent visual clutter. Besides, since cells are often more than phrases, a certain number of cells are empty. We colorize these cells in white. Finally, we draw a border between adjacent hexagons if they belong to distinct clusters to enhance readability. ", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "dxf", "index_original": 241, "paper_title": "PhraseMap: Attention-based Keyphrases Recommendation for Information Seeking", "pub_year": 2023, "domain": "Information Seeking", "requirement": {"requirement_text": "T2: Offer goal-specific guidance to explore phrases of interest. To enable efficient exploration over the PhraseMap, the visualization design is required to highlight phrases of interest. The guidance should also consider the specific goals of the users, e.g., a global exploration requires displaying a topic distribution of the whole corpus, whereas a local search based on users\u2019 input needs to highlight relevant phrases.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "question-answering(QA) datasets", "data_code": {"textual": 1, "fields": 1}}, "solution": [{"solution_text": "Our system aggregates phrases into topics and assigns each topic a color to provide an overview of a corpus (T1). Besides, grids can also be colored by relevance to highlight relevant phrases (T2). We use colors to help users differentiate semantic clusters. Specifically, we utilize a bottom-up clustering approach to group phrases iteratively \u2013 adjacent cells merge if their semantic distance is smaller than a user-defined threshold. Afterward, we assign each semantic cluster a random color in the palette. However, for the clusters that are too small, we assign them a background color ( ) to prevent visual clutter. Besides, since cells are often more than phrases, a certain number of cells are empty. We colorize these cells in white. Finally, we draw a border between adjacent hexagons if they belong to distinct clusters to enhance readability. ", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "dxf", "index_original": 243, "paper_title": "PhraseMap: Attention-based Keyphrases Recommendation for Information Seeking", "pub_year": 2023, "domain": "Information Seeking", "requirement": {"requirement_text": "T4: Be responsive to users\u2019 feedback. Knowledge exploration often contains multiple interactions between users and the system. The system must be intuitive for users to provide feedback and respond quickly", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "question-answering(QA) datasets", "data_code": {"clusters_and_sets_and_lists": 1, "textual": 1}}, "solution": [{"solution_text": "Our system aggregates phrases into topics and assigns each topic a color to provide an overview of a corpus (T1). Besides, grids can also be colored by relevance to highlight relevant phrases (T2). We use colors to help users differentiate semantic clusters. Specifically, we utilize a bottom-up clustering approach to group phrases iteratively \u2013 adjacent cells merge if their semantic distance is smaller than a user-defined threshold. Afterward, we assign each semantic cluster a random color in the palette. However, for the clusters that are too small, we assign them a background color ( ) to prevent visual clutter. Besides, since cells are often more than phrases, a certain number of cells are empty. We colorize these cells in white. Finally, we draw a border between adjacent hexagons if they belong to distinct clusters to enhance readability. ", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The second component is a phrase view. It contains a list of phrases ranked by their relevance, and accept/decline buttons which allows users to provide feedback efficiently (T3). Phrase view contains top-20 phrases ranked by the relevance score with two buttons (Figure 5C). It allows users to provide their feedback by clicking the phrases and the accept or decline button. It triggers the multi-modal navigation algorithm based on users\u2019 feedback. As a response to the user input, the other two views are also updated accordingly.", "solution_category": "interaction", "solution_axial": "Selecting;Participation/Collaboration", "solution_compoent": "", "axial_code": ["Selecting", "Participation/Collaboration"], "componenet_code": ["selecting", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 244, "paper_title": "PhraseMap: Attention-based Keyphrases Recommendation for Information Seeking", "pub_year": 2023, "domain": "Information Seeking", "requirement": {"requirement_text": "T5: Support semantic zooming for knowledge exploration. Text analysis involves multiple levels since users can first obtain a high-level idea and then study knowledge details. Besides, limited by the display area, visualizing all phrases without introducing visual clutter is almost impossible. The system should allow users to zoom in and out of the phrase map flexibly and semantically for multi-level analysis.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "question-answering(QA) datasets", "data_code": {"textual": 1, "fields": 1}}, "solution": [{"solution_text": "Our system aggregates phrases into topics and assigns each topic a color to provide an overview of a corpus (T1). Besides, grids can also be colored by relevance to highlight relevant phrases (T2). We use colors to help users differentiate semantic clusters. Specifically, we utilize a bottom-up clustering approach to group phrases iteratively \u2013 adjacent cells merge if their semantic distance is smaller than a user-defined threshold. Afterward, we assign each semantic cluster a random color in the palette. However, for the clusters that are too small, we assign them a background color ( ) to prevent visual clutter. Besides, since cells are often more than phrases, a certain number of cells are empty. We colorize these cells in white. Finally, we draw a border between adjacent hexagons if they belong to distinct clusters to enhance readability. ", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The main component is a map view to present the visualization of PhraseMap. It is a hexagonal grid layout in which each hexagon provides a space for displaying a phrase (T5). The map view presents a grid-based layout for the PhraseMap, which organizes the extracted phrases in a semantic order. As introduced in the usage scenario, we design two color schemes to assist information seeking under two scenarios (1) displaying the global semantic structure when there is no query from users(Figure 6A). (2) coloring based on the relevance score when users specify the information of interest (Figure 6C). Color for Overview. We use colors to help users differentiate semantic clusters. Specifically, we utilize a bottom-up clustering approach to group phrases iteratively \u2013 adjacent cells merge if their semantic distance is smaller than a user-defined threshold. Afterward, we assign each semantic cluster a random color in the palette. However, for the clusters that are too small, we assign them a background color ( ) to prevent visual clutter. Besides, since cells are often more than phrases, a certain number of cells are empty. We colorize these cells in white. Finally, we draw a border between adjacent hexagons if they belong to distinct clusters to enhance readability. In addition to colors, we display the topic label of each cluster at its centroid (Figure 6) to make the map view informative. To avoid using one single label to represent a large cluster, if the cluster size is larger than a threshold t (we set t = 100), we find all connected areas which share a common word and display the 2-gram or keyword sets as a topic label if the area size exceeds a threshold (we set t=60). For small clusters, we generate the topic label by selecting the 2-gram that appears the most in the cluster and use two top keywords instead if 2-gram is not available.", "solution_category": "interaction", "solution_axial": "Filtering;Selecting", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "dxf", "index_original": 245, "paper_title": "PhraseMap: Attention-based Keyphrases Recommendation for Information Seeking", "pub_year": 2023, "domain": "Information Seeking", "requirement": {"requirement_text": "T6: Enable access to the raw information. Original documents provide the most detailed information about concepts and the relations between concepts. The system should allow users to access raw documents after exploration.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "question-answering(QA) datasets", "data_code": {"textual": 1, "tables": 1}}, "solution": [{"solution_text": "Our system aggregates phrases into topics and assigns each topic a color to provide an overview of a corpus (T1). Besides, grids can also be colored by relevance to highlight relevant phrases (T2). We use colors to help users differentiate semantic clusters. Specifically, we utilize a bottom-up clustering approach to group phrases iteratively \u2013 adjacent cells merge if their semantic distance is smaller than a user-defined threshold. Afterward, we assign each semantic cluster a random color in the palette. However, for the clusters that are too small, we assign them a background color ( ) to prevent visual clutter. Besides, since cells are often more than phrases, a certain number of cells are empty. We colorize these cells in white. Finally, we draw a border between adjacent hexagons if they belong to distinct clusters to enhance readability. ", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "the third component is a document view for displaying the retrieved documents (T6). users can click phrases on the map view to retrieve relevant documents in the document view (T6). Document view presents a list of retrieved documents. Users can click each row to see content details. To support efficient information seeking, we also highlight the extracted phrases in each document, as shown in Figure 5D. The document view supports two levels of document retrieval from the other two views respectively:(1) topic-level: Once the user updates the relevance, the document view retrieves papers containing top-20 phrases shown in the phrase view; (2) word-level: users are allowed to click each phrase in the map view to retrieve specific papers in the document view.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 246, "paper_title": "uxSense: Supporting User Experience Analysis with Visualization and Computer Vision", "pub_year": 2023, "domain": "video analytics", "requirement": {"requirement_text": "R1: Semantic key point detection: UX research workflows frequently involve identifying semantically significant or pivotal moments in user sessions upon reviewing session data. As such, the UX system should reduce the time cost of identifying important moments in user sessions.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "audio and video recordings as parallel time-stamped data streams", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Emotion Prediction: Facial expression (F2) emotion classifi-cation labels and prediction probabilities [59] are presented, also for supporting semantic key point detection (R1). ", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Each of the multiple concurrent data streams (F1) are visualized depending on data type (from top to bottom in Figure 2): ? Action Predictions: A plot of discrete events (using hues) based on action labels assigned to video segments over time, with prediction probability represented with rectangle height. Detecting actions support semantic key point detection (R1). ? Emotion Prediction: Facial expression (F2) emotion classifi-cation labels and prediction probabilities [59] are presented, also for supporting semantic key point detection (R1). Speech Rate: Speech (F3) is calculated using the word frequency over fixed time intervals using speech-to-text model output [18]; we argue that this, too, supports a level of semantic key point identification (R1) for the user session. In support of a combination of semantic key point detection (R1), user-defined segment classification (R2), and annotation (R3), the user\u2019s own annotations are visualized as a step function, with step colors signifying the annotation\u2019s timeline, and mouseover details showing the annotation and name of the corresponding timeline.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "Each of the multiple concurrent data streams (F1) are visualized depending on data type (from top to bottom in Figure 2): ? Action Predictions: A plot of discrete events (using hues) based on action labels assigned to video segments over time, with prediction probability represented with rectangle height. Detecting actions support semantic key point detection (R1). ? Emotion Prediction: Facial expression (F2) emotion classifi-cation labels and prediction probabilities [59] are presented, also for supporting semantic key point detection (R1). Speech Rate: Speech (F3) is calculated using the word frequency over fixed time intervals using speech-to-text model output [18]; we argue that this, too, supports a level of semantic key point identification (R1) for the user session. In support of a combination of semantic key point detection (R1), user-defined segment classification (R2), and annotation (R3), the user\u2019s own annotations are visualized as a step function, with step colors signifying the annotation\u2019s timeline, and mouseover details showing the annotation and name of the corresponding timeline.", "solution_category": "interaction", "solution_axial": "Extractionoffeatures;Filtering", "solution_compoent": "", "axial_code": ["Extractionoffeatures", "Filtering"], "componenet_code": ["extraction_of_features", "filtering"]}]}, {"author": "dxf", "index_original": 247, "paper_title": "uxSense: Supporting User Experience Analysis with Visualization and Computer Vision", "pub_year": 2023, "domain": "video analytics", "requirement": {"requirement_text": "R2: User-defined segment classification: Identifying key points (R1) is often followed by classifying or tagging segments of the user session based on recurring or novel patterns in the data. The system should support constructing qualitative classification frameworks for session analysis.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "audio and video recordings as parallel time-stamped data streams", "data_code": {"tables": 1}}, "solution": [{"solution_text": "In support of a combination of semantic key point detection (R1), user-defined segment classification (R2), and annotation (R3), the user\u2019s own annotations are visualized as a step function, with step colors signifying the annotation\u2019s timeline, and mouseover details showing the annotation and name of the corresponding timeline.", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}, {"solution_text": "In support of a combination of semantic key point detection (R1), user-defined segment classification (R2), and annotation (R3), the user\u2019s own annotations are visualized as a step function, with step colors signifying the annotation\u2019s timeline, and mouseover details showing the annotation and name of the corresponding timeline.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "dxf", "index_original": 248, "paper_title": "uxSense: Supporting User Experience Analysis with Visualization and Computer Vision", "pub_year": 2023, "domain": "video analytics", "requirement": {"requirement_text": "R2: User-defined segment classification: Identifying key points (R1) is often followed by classifying or tagging segments of the user session based on recurring or novel patterns in the data. The system should support constructing qualitative classification frameworks for session analysis.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "audio and video recordings as parallel time-stamped data streams", "data_code": {"media": 1, "tables": 1}}, "solution": [{"solution_text": "Beyond the time-marker based track view of each feature data stream, the user can zoom in on a point of interest by brushing the focus interval selection (Figure 3). Once zoomed, the timelines can be dragged to pan through the video and all of the timelines. To support annotation (R3)\u2014and user-defined segment classification (R2) by way of annotation (R3)\u2014the brushed interval of the video can be annotated as a range, or the user can opt to annotate a single point of the video as they code user behavior.", "solution_category": "visualization", "solution_axial": "annotation", "solution_compoent": "Bar;Sankey", "axial_code": [], "componenet_code": ["sankey", "bar"]}, {"solution_text": "Beyond the time-marker based track view of each feature data stream, the user can zoom in on a point of interest by brushing the focus interval selection (Figure 3). Once zoomed, the timelines can be dragged to pan through the video and all of the timelines. To support annotation (R3)\u2014and user-defined segment classification (R2) by way of annotation (R3)\u2014the brushed interval of the video can be annotated as a range, or the user can opt to annotate a single point of the video as they code user behavior.", "solution_category": "interaction", "solution_axial": "Connect/Related;Reconfig;Filtering", "solution_compoent": "", "axial_code": ["Connect/Related", "Filtering", "Reconfig"], "componenet_code": ["connect_relate", "filtering", "reconfigure"]}]}, {"author": "dxf", "index_original": 252, "paper_title": "Anchorage: Visual Analysis of Satisfaction in Customer Service Videos via Anchor Events", "pub_year": 2023, "domain": "video analytics", "requirement": {"requirement_text": "R1: Rank satisfaction by objective metrics. Clients seldom provide satisfaction feedback after services. When they do, their self-reported evaluation is prone to cognitive biases such as the peak-end rule [5] nonetheless. E1-2 added, \u201csome clients rushed to leave, so they randomly clicked any buttons.\u201d The large video collections also require a ranking order to prioritize videos of interest. The system should provide uniform and objective assessments based on users\u2019 behaviors.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "service videos", "data_code": {"sequential": 1, "media": 1, "tables": 1}}, "solution": [{"solution_text": "The behavioral anchors are the multimodal satisfaction evaluation. Similar to [11], [24], we adopt a linear model to generate a customer satisfaction score. We extended the model to cover event duration rather than affective status only (R2). The model combines facial expression v, audio emotion a, and events e to evaluate satisfaction. The customer satisfaction score for a service CSs is calculated by: Funation(1)", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The Service overview (Fig. 3A) provides an overview of all the service videos. It lists all the videos and supports fast comparison over multiple videos to search for a service of interest. Each list item (Fig. 3A1) contains three columns that show different satisfaction metrics (R1). The color encodings are unified for the visual interface (green for visual, red for audio, and purple for event). The left most column displays the basic information of the video. The horizontal bar chart shows the temporal and sequential anomaly scores described in Sec. 4.2. Identified anomalies are represented by filled color, and normal services are in striped color. The rightmost column is a vertical bar chart showing the satisfaction scores CSs of different modalities.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Bar;Circle;Text", "axial_code": [], "componenet_code": ["text", "bar", "circle"]}]}, {"author": "dxf", "index_original": 254, "paper_title": "Anchorage: Visual Analysis of Satisfaction in Customer Service Videos via Anchor Events", "pub_year": 2023, "domain": "video analytics", "requirement": {"requirement_text": "R2: Contextualize the satisfaction evaluation with operations. Customer behaviors should be interpreted with the antecedent events [4]. For example, expecting smooth services, clients would perceive repeated and interrupted operations as troublesome and unsatisfactory, resulting in a negative emotional response. However, clients have diverse affective reactions to provocative actions. E7 pointed out that \u201csome people keep a poker face, but they could be furious,\u201d suggesting the unreliability and inadequacy of using emotional features only. The system should incorporate procedural considerations as the common ground to explain and evaluate clients\u2019 behaviors.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "missing and abundance events", "data_code": {"sequential": 1, "media": 1, "tables": 1}}, "solution": [{"solution_text": "The Markov chain model [64] learns a transition probability distribution P of different discrete states at each time frame in normal sequences. It assigns the service vector E with:Moreover, customer services have predefined procedures (agent guidelines) acting as the normal training set. Although the Markovian model is not designed to identify missing and abundance events [53], its anomaly score would still reflect these conditions as they would appear in the wrong place. The Markovian model is well-suited to detecting repeated and out-of-sync operations (R2). ", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "dxf", "index_original": 255, "paper_title": "Anchorage: Visual Analysis of Satisfaction in Customer Service Videos via Anchor Events", "pub_year": 2023, "domain": "video analytics", "requirement": {"requirement_text": "R2: Contextualize the satisfaction evaluation with operations. Customer behaviors should be interpreted with the antecedent events [4]. For example, expecting smooth services, clients would perceive repeated and interrupted operations as troublesome and unsatisfactory, resulting in a negative emotional response. However, clients have diverse affective reactions to provocative actions. E7 pointed out that \u201csome people keep a poker face, but they could be furious,\u201d suggesting the unreliability and inadequacy of using emotional features only. The system should incorporate procedural considerations as the common ground to explain and evaluate clients\u2019 behaviors.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "semantically meaningful events that describe operations (operational anchors) and behaviors (behavioral anchors)", "data_code": {"sequential": 1, "tables": 1}}, "solution": [{"solution_text": "These anchors can be viewed as an interactive table of content to define the video event structure (R2) for quick navigation to desired segments of satisfaction patterns without searching the whole video (R6). The behavioral anchors are the multimodal satisfaction evaluation. Similar to [11], [24], we adopt a linear model to generate a customer satisfaction score. We extended the model to cover event duration rather than affective status only (R2). The model combines facial expression v, audio emotion a, and events e to evaluate satisfaction. The customer satisfaction score for a service CSs is calculated by:. where N, M, and T denote the total number of frames, utterances, and operations. w is the weights of each channel defaulted as equally weighted. We also obtained the operation\u2019s satisfaction score CSe for each modality by confining the summation scope to individual operation and modality. f is the normal standardization across all services. ze is the z-score for the event duration. We grouped them by operations before standardizing because repeated operations are usually shorter and obfuscate the calculation. For emotional responses, we assigned a magnitude weight m to each discrete emotion and adopted the scheme proposed by previous work [11]. In general, positive emotion has a value of ;1.0, neutral emotions are 0.0, and negative emotions tend to -1.0. We slightly modified the weightings of anger to -1.2 and disgust to -1.0 based on the domain literature [12] and discussions with E5-7. A large value of CSs indicates high satisfaction and vice versa. All of the above settings can be reconfigured to adapt to other needs.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "dxf", "index_original": 257, "paper_title": "Anchorage: Visual Analysis of Satisfaction in Customer Service Videos via Anchor Events", "pub_year": 2023, "domain": "video analytics", "requirement": {"requirement_text": "R3: Show satisfaction progression in a service. Automatic methods usually aggregate frame-level evaluations to model satisfaction [11]. However, the aggregated service score is inferior in differentiating counteracted cases. E5 proposed a satisfied case with a client showing unsatisfied behaviors at first but becoming more satisfied with the service at last. The case would be underrepresented in an accumulative servicelevel satisfaction score. Assessing satisfaction by individual operations naturally magnifies their contributions to the overall evaluation [9]. The system should visualize the dynamic satisfaction progression to reveal the causal relationships between behaviors and satisfaction.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "missing and abundance events", "data_code": {"sequential": 1, "media": 1, "tables": 1}}, "solution": [{"solution_text": "The primary purpose of operational anchors is to lift the burden of status determination (as discussed in Sec. 2.3) for analysts when they watch the videos. The operational anchors can segment operations in service records and grant semantic meanings to segments (R4). They also introduce event structure to summarize video content. To prioritize anomalous events (R5), we employed a similarity-based method to find temporal anomalies and a Markovian technique to detect sequential anomalies from the service records. The service record vector E is piped into the following algorithms to obtain the corresponding anomaly scores. Temporal anomaly locates uncommon durations of operations. The Principle Component Analysis (PCA) [62] is a popular unsupervised method for system log analysis detecting anomalous discrete events. It computes the similarity between the input and the labeled sequences based on the assumption that anomalous sequences should be dissimilar to normal ones. Service records labeled as normal En are further aggregated by the operations to obtain fixed-size vectors. They are reduced to k principal components to formulate the normal space Sn. A service record is said to be anomalous if ||y||2 > Q1?\u03b1, where y is the projection length to Sn, and Q1?\u03b1 is the confidence threshold defaulted at 95%. We had considered another popular method in system log analysis, invariant mining [63]. However, it is tailored to rigorous procedures in software systems and has limited generalizability. Meanwhile, PCA has the advantage of high interpretability and does not require a large training set.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation;AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "SimilarityCalculation"], "componenet_code": ["algorithmic_calculation", "similarity_calculation"]}]}, {"author": "dxf", "index_original": 259, "paper_title": "Anchorage: Visual Analysis of Satisfaction in Customer Service Videos via Anchor Events", "pub_year": 2023, "domain": "video analytics", "requirement": {"requirement_text": "R5: Highlight the anomalous operations. Satisfaction generally follows a steady progression with previous states. A significant turning point could indicate a potential satisfaction pattern induced by internal factors (e.g., exceeding expectations [1]) and external factors (e.g., agents\u2019 misconduct [4]). E4 stated that looking into the \u201cpeaks\u201d and \u201ctroughs\u201d of the satisfaction level would help derive more managerial insights. They are worth more attention to be further investigated. The system should distinguish uncommon satisfaction development to identify critical transition moments.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "the multimodal satisfaction evaluation", "data_code": {"sequential": 1, "tables": 1, "temporal": 1}}, "solution": [{"solution_text": "This section introduces the construction of anchors. Anchors refer to semantically meaningful events that describe operations (operational anchors) and behaviors (behavioral anchors). We identified anomalous events for services and operations as operational anchors with a multi-perspective anomaly detection framework (R5). The primary purpose of operational anchors is to lift the burden of status determination (as discussed in Sec. 2.3) for analysts when they watch the videos. The operational anchors can segment operations in service records and grant semantic meanings to segments (R4). They also introduce event structure to summarize\nvideo content. To prioritize anomalous events (R5), we employed a similarity-based method to find temporal anomalies and a Markovian technique to detect sequential anomalies from the service records. The service record vector E is piped into the following algorithms to obtain the corresponding anomaly scores. Temporal anomaly locates uncommon durations of operations. The Principle Component Analysis (PCA) [62] is a popular unsupervised method for system log analysis detecting anomalous discrete events. It computes the similarity between the input and the labeled sequences based on the assumption that anomalous sequences should be dissimilar to normal ones. Service records labeled as normal En are further aggregated by the operations to obtain fixed-size vectors. They are reduced to k principal components to formulate the normal space Sn. A service record is said to be anomalous if ||y||2 > Q1?\u03b1, where y is the projection length to Sn, and Q1?\u03b1 is the confidence threshold defaulted at 95%. We had considered another popular method in system log analysis, invariant mining [63]. However, it is tailored to rigorous procedures in software systems and has limited generalizability. Meanwhile, PCA has the advantage of high interpretability and does not require a large training set.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation;Modeling", "solution_compoent": "", "axial_code": ["Modeling", "SimilarityCalculation"], "componenet_code": ["modeling", "similarity_calculation"]}]}, {"author": "dxf", "index_original": 260, "paper_title": "Anchorage: Visual Analysis of Satisfaction in Customer Service Videos via Anchor Events", "pub_year": 2023, "domain": "video analytics", "requirement": {"requirement_text": "R5: Highlight the anomalous operations. Satisfaction generally follows a steady progression with previous states. A significant turning point could indicate a potential satisfaction pattern induced by internal factors (e.g., exceeding expectations [1]) and external factors (e.g., agents\u2019 misconduct [4]). E4 stated that looking into the \u201cpeaks\u201d and \u201ctroughs\u201d of the satisfaction level would help derive more managerial insights. They are worth more attention to be further investigated. The system should distinguish uncommon satisfaction development to identify critical transition moments.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "service videos", "data_code": {"sequential": 1, "media": 1, "tables": 1}}, "solution": [{"solution_text": "The primary purpose of operational anchors is to lift the burden of status determination (as discussed in Sec. 2.3) for analysts when they watch the videos. The operational anchors can segment operations in service records and grant semantic meanings to segments (R4). They also introduce event structure to summarize video content. To prioritize anomalous events (R5), we employed a similarity-based method to find temporal anomalies and a Markovian technique to detect sequential anomalies from the service records. The service record vector E is piped into the following algorithms to obtain the corresponding anomaly scores. Temporal anomaly locates uncommon durations of operations. The Principle Component Analysis (PCA) [62] is a popular unsupervised method for system log analysis detecting anomalous discrete events. It computes the similarity between the input and the labeled sequences based on the assumption that anomalous sequences should be dissimilar to normal ones. Service records labeled as normal En are further aggregated by the operations to obtain fixed-size vectors. They are reduced to k principal components to formulate the normal space Sn. A service record is said to be anomalous if ||y||2 > Q1?\u03b1, where y is the projection length to Sn, and Q1?\u03b1 is the confidence threshold defaulted at 95%. We had considered another popular method in system log analysis, invariant mining [63]. However, it is tailored to rigorous procedures in software systems and has limited generalizability. Meanwhile, PCA has the advantage of high interpretability and does not require a large training set.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation;AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "SimilarityCalculation"], "componenet_code": ["algorithmic_calculation", "similarity_calculation"]}]}, {"author": "dxf", "index_original": 261, "paper_title": "Anchorage: Visual Analysis of Satisfaction in Customer Service Videos via Anchor Events", "pub_year": 2023, "domain": "video analytics", "requirement": {"requirement_text": "R5: Highlight the anomalous operations. Satisfaction generally follows a steady progression with previous states. A significant turning point could indicate a potential satisfaction pattern induced by internal factors (e.g., exceeding expectations [1]) and external factors (e.g., agents\u2019 misconduct [4]). E4 stated that looking into the \u201cpeaks\u201d and \u201ctroughs\u201d of the satisfaction level would help derive more managerial insights. They are worth more attention to be further investigated. The system should distinguish uncommon satisfaction development to identify critical transition moments.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "service videos", "data_code": {"sequential": 1, "tables": 1, "temporal": 1}}, "solution": [{"solution_text": "The behavioral anchors are the multimodal satisfaction evaluation. Similar to [11], [24], we adopt a linear model to generate a customer satisfaction score. We extended the model to cover event duration rather than affective status only (R2). The model combines facial expression v, audio emotion a, and events e to evaluate satisfaction. The customer satisfaction score for a service CSs is calculated by: Funation(1)", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The lateral buoy chart (Fig. 3B2) shows the satisfaction progression. The chart\u2019s horizontal encoding follows the timeline above, so all dots are located in the middle of the operation. Its correspondence with the buoy chart is illustrated in Fig. 4B-C. While the two charts share the same metaphor, there are subtle differences. Each operation is represented by three dots, including the event. Here, the vertical position utilizes the z-score to unify all modalities. The visual and audio scores are summed over the operation and further standardized within the selected service. For example, e5 in Fig. 4B contains vastly deviated scores for all modalities, while the e3 counterparts have average scores. An anchor icon denotes higher values than two standard deviations. The scale helps detect the most anomalous service operations (R5). The buoy\u2019s size encodes the absolute deviation rank to draw attention to the most influential anchor. The more significant deviation, the larger the buoy. The drawing order favors smaller buoys to prevent occlusion and visual clutters (see Fig. 5A).", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Circle", "axial_code": [], "componenet_code": ["circle"]}]}, {"author": "dxf", "index_original": 263, "paper_title": "Anchorage: Visual Analysis of Satisfaction in Customer Service Videos via Anchor Events", "pub_year": 2023, "domain": "video analytics", "requirement": {"requirement_text": "R6: Support interactive navigation of original videos. Video recordings are the strongest evidence in evaluating satisfaction. Yet, reviewing the videos from scratch is inefficient. Features extracted by machine learning models are helpful, but they might suffer from model uncertainty and multimodal interactions [35]. E6-7 expressed a need to validate the features when they convey \u201cunreasonable and contradictory meanings.\u201d Also, the dynamics between agents and clients are challenging to define and detect. The system should support various interactions to streamline the fast location of interested events.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "service videos", "data_code": {"sequential": 1, "tables": 1, "temporal": 1}}, "solution": [{"solution_text": "This view supports interactive navigation of the original videos (R6). We adopted the periphery plot [67] as the operation summary (Fig. 3C1). In the middle focused detail view, we fused the facial and audio features to assign an activation value vi = {?1,0,1} to frame i. The fusion favors non-neutral emotions with higher priority given to negative ones because they have a greater impact on satisfaction [12]. The activation values are plotted to show an overview of the operation. Brushing selects the period for the below features. The periphery plots on both sides allow quick navigation to consecutive operations and contextualize the focused operation with neighbors. The three bars show the count of activation values.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "dxf", "index_original": 267, "paper_title": "SDRQuerier: A Visual Querying Framework for Cross-National Survey Data Recycling", "pub_year": 2023, "domain": "Cross-National Survey", "requirement": {"requirement_text": "R1.3: Present an overview of the harmonized dataset. Due to the complexity of harmonized data, it is necessary to present multi-faceted information (e.g., variable types and relationships) in order to fully understand it.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Harmonized Data", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "The Circular Graph is proposed to handle the complexity and dimensionality of the harmonized data, which targets to: (1) indicate diverse types of variables, including source-, target-, harmonization control- or quality-variables; (2) illustrate the relationships of different variables (R1.3). As shown in Fig. 6(a3), the circular bar chart represents the target variables. The length of the bar implies the overall availability of each target variable, i.e., how frequently the corresponding target variable is measured in international surveys. The color indicates the topic of the target variables, which is consistent with the color schema used in the Scatterplot. Once the user triggers the query fromscatterplot, only the predicted bar will be highlighted in orange, while others fade out. Several target variables can describe the same topic from different perspectives. For example, T_HAPPY_11 and T_HAPPY_DISTRIB both measure respondents\u2019 self-reported happiness, but using different specifi-cations. As described in Section III, target variables capturing the same theoretical concept can share one or several harmonization control variables, which record the variance in source variable properties. These controls are visualized as the orange arcs ( ). The number of arcs in the same radial position reflects the number of harmonization control variables in the group. When clicking an arc, the right panel will pop up to show its value distribution with labels of the harmonization control variable (Fig. 6(a4)). As proved by experts, showing this information is extremely helpful when querying data from the SDR portal. The intermediate circle ( ) conveys that all the target variables are related to the quality control variables. The inner network represents the demographics of the respondents. It includes age, birth year, sex of respondents; their color is also consistent with the Scatterplot. For example, for age, surveys can ask about age in many ways as reflected by the numerous red points ( ) in the Scatterplot.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation;Clustering&Grouping", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"], "componenet_code": ["algorithmic_calculation", "clustering_and_grouping"]}, {"solution_text": "The Circular Graph is proposed to handle the complexity and dimensionality of the harmonized data, which targets to: (1) indicate diverse types of variables, including source-, target-, harmonization control- or quality-variables; (2) illustrate the relationships of different variables (R1.3). As shown in Fig. 6(a3), the circular bar chart represents the target variables. The length of the bar implies the overall availability of each target variable, i.e., how frequently the corresponding target variable is measured in international surveys. The color indicates the topic of the target variables, which is consistent with the color schema used in the Scatterplot. Once the user triggers the query fromscatterplot, only the predicted bar will be highlighted in orange, while others fade out. Several target variables can describe the same topic from different perspectives. For example, T_HAPPY_11 and T_HAPPY_DISTRIB both measure respondents\u2019 self-reported happiness, but using different specifi-cations. As described in Section III, target variables capturing the same theoretical concept can share one or several harmonization control variables, which record the variance in source variable properties. These controls are visualized as the orange arcs ( ). The number of arcs in the same radial position reflects the number of harmonization control variables in the group. When clicking an arc, the right panel will pop up to show its value distribution with labels of the harmonization control variable (Fig. 6(a4)). As proved by experts, showing this information is extremely helpful when querying data from the SDR portal. The intermediate circle ( ) conveys that all the target variables are related to the quality control variables. The inner network represents the demographics of the respondents. It includes age, birth year, sex of respondents; their color is also consistent with the Scatterplot. For example, for age, surveys can ask about age in many ways as reflected by the numerous red points ( ) in the Scatterplot.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Bar;Circle", "axial_code": [], "componenet_code": ["bar", "circle"]}]}, {"author": "dxf", "index_original": 269, "paper_title": "SDRQuerier: A Visual Querying Framework for Cross-National Survey Data Recycling", "pub_year": 2023, "domain": "Cross-National Survey", "requirement": {"requirement_text": "R2.2: Assist with decision making. Once available data are identified, it is important to assist researchers in deciding whether these data meet formal requirements for the regression analysis. This can be done by providing multi-faceted information, e.g., which data have quality issues?", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "text, BERT-based model output", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "Fig. 1 displays an overview of our framework. We summarize the domain requirements into three challenges in different stages during the analysis pipeline: understanding, exploring, and evaluating. To solve the challenges, we propose a framework that contains three corresponding modules. First, inspired by conversational Artificial Intelligence, we train a BERT-based model to generate variable recommendations based on the user\u2019s input text, either keywords or sentences describing their information of interest (R1.1).", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The Joint Availability view (Fig. 6(b2)) exhibits the available samples for all the selected target variables, indicating the precise pool of valid samples that can be used to evaluate the multi-variate relations (R2.2). Responsive Bar Chart The meaning of bar can be embedded as either macro-level or micro-level by users, defined as the responsive bar chart. Scientists can select to see how many respondents are available (micro-level) or how many countries are available (macro-level) through the drop-down selector at the top of this module. To present the country coverage, we further allow users to click the bar for the detailed information on a map, where green means covered country (Fig. 6(b3)). Also, we allow two different sorting methods of rows: availability-based and quality-based. For the availability-based method, if a project covers more distinct years, it will have higher availability.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Bar;Map", "axial_code": [], "componenet_code": ["bar", "map"]}]}, {"author": "dxf", "index_original": 270, "paper_title": "SDRQuerier: A Visual Querying Framework for Cross-National Survey Data Recycling", "pub_year": 2023, "domain": "Cross-National Survey", "requirement": {"requirement_text": "R3.1: Validate the selected target variables. Hypotheses propose some associations or causal relationships between variables of interest. Revealing relational patterns from target data can preliminarily evaluate the hypotheses.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "text, BERT-based model output", "data_code": {"tables": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "Fig. 1 displays an overview of our framework. We summarize the domain requirements into three challenges in different stages during the analysis pipeline: understanding, exploring, and evaluating. To solve the challenges, we propose a framework that contains three corresponding modules. First, inspired by conversational Artificial Intelligence, we train a BERT-based model to generate variable recommendations based on the user\u2019s input text, either keywords or sentences describing their information of interest (R1.1).", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Driven by R3.1, our first subview presents the pairwise correlations for user-selected target variables, allowing scientists to check if these variables are correlated with each other and to further determine what to keep for their regression analysis. We compute several necessary and common statistics for pairwise relations,including the Pearson correlation coefficient, p-values, which can be grouped into categories based on expert-suggested p-value thresholds, and standard errors. To flatten the learning curve of visual encoding, as suggested by our domain experts, we show the computed information with texts and only incorporate two visual channels in the matrix, i.e., position for pairwise relation and responsive color. Users are allowed to select one of the computed information and map it to the color interactively. After several key design iterations with the domain experts, we determined to show one-half of the symmetric matrix to reduce redundant information and avoid unnecessary interpretation of the position", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Matrix", "axial_code": [], "componenet_code": ["matrix"]}]}, {"author": "dxf", "index_original": 272, "paper_title": "FraudAuditor: A Visual Analytics Approach for Collusive Fraud in Health Insurance", "pub_year": 2023, "domain": "fraud detection", "requirement": {"requirement_text": "R1: Show attribute distribution of medical records: An overview allows users to understand the dataset and find a starting point to detect fraud. For example, users can learn a reasonable expense range from the distribution of patient expenses. Then, the patients whose expenses exceed the threshold should be reviewed.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "patient attributes", "data_code": {"tables": 1}}, "solution": [{"solution_text": "The network analysis view (Fig. 3-(a)) has two parts: (1) The patient attributes view gives an overview of patients by showing attribute distributions and supports interactive filtering of data of interest (R1, R2). In the patient attributes view (Fig. 3-(a1)), bars indicate the distribution of patient attributes, including the distribution of patients in terms of the number of visits, age, and total fee, as well as the number of visits to different medical institutions. At first, all patients are selected, and users can click on a bar to deselect/re-select the corresponding patients. Patients that are not selected are represented by a translucent background, and a mouse hovering over the corresponding bar will bring up a tooltip showing the total number of patients belonging to the original and current patients in the interval, making it easy for users to compare the distribution of patients under different filtering conditions.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "The network analysis view (Fig. 3-(a)) has two parts: (1) The patient attributes view gives an overview of patients by showing attribute distributions and supports interactive filtering of data of interest (R1, R2). In the patient attributes view (Fig. 3-(a1)), bars indicate the distribution of patient attributes, including the distribution of patients in terms of the number of visits, age, and total fee, as well as the number of visits to different medical institutions. At first, all patients are selected, and users can click on a bar to deselect/re-select the corresponding patients. Patients that are not selected are represented by a translucent background, and a mouse hovering over the corresponding bar will bring up a tooltip showing the total number of patients belonging to the original and current patients in the interval, making it easy for users to compare the distribution of patients under different filtering conditions.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 274, "paper_title": "FraudAuditor: A Visual Analytics Approach for Collusive Fraud in Health Insurance", "pub_year": 2023, "domain": "fraud detection", "requirement": {"requirement_text": "R3: Identify the behavioral connections among patients: Connections of visiting behaviors and drug purchasing behaviors are the basis for detecting fraud groups. Thus, these connections should be identified according to expert knowledge, namely, user-specified restrictions. For instance, patients are considered to be potentially associated only if they visit the same location in less than 15 minutes more than five times.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "patient attributes", "data_code": {"tables": 1}}, "solution": [{"solution_text": "In the first stage, users seek a general understanding of the data by checking the attribute distributions (R1, Fig. 1-(a)). According to data distribution and domain knowledge, users then filter patients for analysis (R2, Fig. 1-(a)). Next, users check connections between the filtered patients interactively (R3, Fig. 1-(b)). Collusive behaviors can be disclosed by time gaps of visits or the number of co-visits. Users are supported to specify the definition of complicit behaviors by setting thresholds for time gaps and the co-visits number.", "solution_category": "interaction", "solution_axial": "Connect/Related", "solution_compoent": "", "axial_code": ["Connect/Related"], "componenet_code": ["connect_relate"]}]}, {"author": "dxf", "index_original": 276, "paper_title": "FraudAuditor: A Visual Analytics Approach for Collusive Fraud in Health Insurance", "pub_year": 2023, "domain": "fraud detection", "requirement": {"requirement_text": "R4: Detect patient groups: Patient groups can be detected based on various user-specified rules (e.g., whether there exist specific behavior connections or whether the total expense exceeds a limit). Automation can be leveraged to guarantee the efficiency of group detection.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Health Insurance data", "data_code": {"tables": 1}}, "solution": [{"solution_text": "To detect suspicious groups with spatio-temporal connections and group action characteristics (see Section III-B), we propose a suspicious group mining method to detect collusive fraud in health insurance (R4). Our method first builds a co-visit network to represent the spatio-temporal relationship among patients. Based on the co-visit network, the method uses a modularity optimization-based community detection algorithm to mine suspicious groups. Co-Visit Network Construction: Patients in a collusive fraud group frequently visit the same medical institution within relatively short time periods. Considering such a characteristic, we construct a co-visit network G among patients to summarize the co-visit behaviors and detect collusive fraud. A node in the net_x0002_work represents a patient. An edge between two patients records the co-visit behaviors between the two patients. If the medical institutions of the two corresponding visits of two patients are the same, and the time gap is less than a threshold \u03b81 (the default is 1 h, which can be adjusted to 6, 12, or 24 hours), it is considered a co-visit. For patients pi and pj , their co-visit behaviors are represented as CV(pi, pj ) = {(vi1, vj1),...,(vis, vjs)} and s is the total number of visits they made together. Edge Weight Calculation: The edge weight indicates the likelihood that the two patients belong to the same group. We calculated the weights of the edge w(pi, pj ) based on the number of co-visits and the visiting time gap. As shown in the (1), the weight of a co-visit is inversely proportional to the visit time gap. To avoid the impact of occasional visits with a small time gap on the weight, inspired by the ReLU activation function, we set the cutoff time to 10 minutes based on expert experience, and weights less than that interval are considered to be the same. The edge weight w(pi, pj ) between two patients is the total of their co-visit weights, defined as (2). An adjustable threshold \u03b82 (default as 4) for the minimum number of co-visits is set here to avoid random factors. The co-visit weight being less than the threshold indicates the low probability of both belonging to the same group. Community Detection: In order to mine suspicious groups from the co-visit network, we use Louvain [26], an community detection algorithm based on modularity optimization. The algo_x0002_rithm is applicable to weighted graphs and supports the exclusion of non-community nodes, which can yield clear detection results since most patients in the healthcare scenario are normal.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "dxf", "index_original": 278, "paper_title": "FraudAuditor: A Visual Analytics Approach for Collusive Fraud in Health Insurance", "pub_year": 2023, "domain": "fraud detection", "requirement": {"requirement_text": "R6: Support suspicious patients verification: Understanding the intra-group similarities of patient behaviors, such as prescribed diseases, drug purchases, and selection of medical institutions, can help users exclude irrelevant patients and examine suspicious patients.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "strategies, patient data", "data_code": {"tables": 1}}, "solution": [{"solution_text": "To verify collusive fraud groups, we need to calculate the similarity among patients based on their prescribed diseases and corresponding drugs (R6). At first, we tried to calculate the similarity according to the string texts of diseases and drugs, but the results were not satisfactory. For example, such a calculation would lead to the headache being similar to the stomachache and not similar to the stroke, when in fact both headaches and strokes are brain disorders with a closer relationship. Later, we found that either diseases or drugs have hierarchical encoding, which can reflect the similarity information. The ICD10 3 coding of diseases and the standard coding of drugs 4 encode diseases and drugs hierarchically by large, medium, and small class. For example, diseases J11 (influenza) and J18 (pneumonia) are similar, but they are very different from M54 (back pain). Hence, we propose a nearest-match-based similarity calculation method that considers the disease/drug codes and the number of visits for the corresponding diseases. For each dis_x0002_ease/drug, we need to find the most similar one in another patient\u2019s disease/drug set, so it is not affected by the specific order of visits. Assume that a patient has been prescribed several diseases D = {d1, d2,...,dl}. The corresponding numbers of visits for each disease are C = {c1, c2,...,cl}.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}]}, {"author": "dxf", "index_original": 279, "paper_title": "FraudAuditor: A Visual Analytics Approach for Collusive Fraud in Health Insurance", "pub_year": 2023, "domain": "fraud detection", "requirement": {"requirement_text": "R6: Support suspicious patients verification: Understanding the intra-group similarities of patient behaviors, such as prescribed diseases, drug purchases, and selection of medical institutions, can help users exclude irrelevant patients and examine suspicious patients.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "patient attribute", "data_code": {"tables": 1}}, "solution": [{"solution_text": "The group comparison view (Fig. 3-(b)) consists of three parts: (1) The group attributes view provides an overview of group-level attributes as well as interactive filtering capabilities. (2) The group projection supports similarity analysis of groups (3) The group rank allows sorting groups across multiple dimensions. Through initial filtering and careful selection, users can identify suspicious groups that need to be focused on for analysis (R5).", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "The group comparison view (Fig. 3-(b)) consists of three parts: (1) The group attributes view provides an overview of group-level attributes as well as interactive filtering capabilities. (2) The group projection supports similarity analysis of groups (3) The group rank allows sorting groups across multiple dimensions. Through initial filtering and careful selection, users can identify suspicious groups that need to be focused on for analysis (R5).", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Bar;Matrix", "axial_code": [], "componenet_code": ["bar", "matrix"]}]}, {"author": "dxf", "index_original": 280, "paper_title": "FraudAuditor: A Visual Analytics Approach for Collusive Fraud in Health Insurance", "pub_year": 2023, "domain": "fraud detection", "requirement": {"requirement_text": "R7: Visualize the visit records of an individual patient: As mentioned earlier, auto-detection can hardly differentiate fraud groups from patients with specific visit needs, which leads to false positives. Users should examine the identi- fied suspicious fraudsters to prove or disprove their suspicions. Visualizing patients\u2019 histories of medical visits could help users gather evidence regarding the continuity and rationality of the visits. In this way, fraudulent groups and false positive groups can be differentiated based on expert knowledge.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "patient data", "data_code": {"sequential": 1, "tables": 1}}, "solution": [{"solution_text": "To verify collusive fraud groups, we need to calculate the similarity among patients based on their prescribed diseases and corresponding drugs (R6). At first, we tried to calculate the similarity according to the string texts of diseases and drugs, but the results were not satisfactory. For example, such a calculation would lead to the headache being similar to the stomachache and not similar to the stroke, when in fact both headaches and strokes are brain disorders with a closer relationship. Later, we found that either diseases or drugs have hierarchical encoding, which can reflect the similarity information. The ICD10 3 coding of diseases and the standard coding of drugs 4 encode diseases and drugs hierarchically by large, medium, and small class. For example, diseases J11 (influenza) and J18 (pneumonia) are similar, but they are very different from M54 (back pain). Hence, we propose a nearest-match-based similarity calculation method that considers the disease/drug codes and the number of visits for the corresponding diseases. For each dis_x0002_ease/drug, we need to find the most similar one in another patient\u2019s disease/drug set, so it is not affected by the specific order of visits. Assume that a patient has been prescribed several diseases D = {d1, d2,...,dl}. The corresponding numbers of visits for each disease are C = {c1, c2,...,cl}.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Bar;Line", "axial_code": [], "componenet_code": ["line", "bar"]}]}, {"author": "dxf", "index_original": 281, "paper_title": "TopicRefiner: Coherence-guided Steerable LDA for Visual Topic Enhancement", "pub_year": 2023, "domain": "Latent Dirichlet Allocation", "requirement": {"requirement_text": "DP1: Show coherence score variations across rounds. The usage of the coherence score in topic editing is twofold. First, it provides essential references for selecting the target topic. The user typically selects a topic with a lower coherence score for editing. Second, the score variations reflect the editing effects. The user can confirm that the refinements are reasonable when most topics\u2019 scores rise. Instead, they need to roll back to the previous topic status when the coherence scores generally decrease.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "documents,News articles", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Coherence [11], [48] is the most widely used metric\nto assess topic quality. It calculates the sum of the co_x0002_occurrence degrees of high-ranking terms (terms with the\nhighest relevance to the topic, obtained from the term-topic\ndistribution) within high-ranking documents (documents\nwith the highest relevance to the topic, obtained from the\ntopic-document distribution)", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "TL is on the left side of the interface, consisting of a group of line charts. Each line chart shows coherence scores of a topic at different rounds of editing (DP1), as in Fig. 5(a). TL assists the user in conducting the four topic-related refinements (RF7-RF10). The user typically needs to select a target topic from TL for editing. TL is blank at the beginning of editing. The user first runs LDA to initialize a few topics and lists them in TL. Each topic corresponds to a line chart that shows its coherence scores at different rounds. The number at the end of each line chart is current coherence score of the topic, as in Fig. 5(a). The user can click on a line chart to edit the corresponding topic using TE and DE. For example, in Fig. 5(a), the user has already conducted three rounds of editing and is editing T4 now. TL supports the rolling back function. When the current round of editing causes the general coherence score de_x0002_creases, the user can click on a previous score on a line chart to edit the corresponding topic from that round. By doing that, TE and DE show high-ranking terms and documents of the topic at that round, respectively. TL supports all four topic-related refinements (RF7-RF10). The user can add topics by clicking on Topic (RF7) to generate a new line chart (without a line). He (or she) can then use TE and DE to add terms or documents to the new topic. The user can delete a topic by sequentially clicking on Topic and the topic (RF8). TL also supports topic splitting(RF9) and merging (RF10) by jointly using RF1, RF7, and RF8 (Section 4.2). TL generates a colored band for each topic, showing gradient colors of its first few high-ranking terms, as in Fig. 5(a). We follow the method proposed by Landesberger et al. [62] to assign each term a unique color according to its position in the latent space formed by projecting word2vec embeddings of all the terms, as in Fig. 6. The color similarity between terms thus reflects their semantic similarity. The better the topic quality, the more consistent the color gradient of the band (see the three topics in Fig. 6). The user should focus on topics with wide color variations in their bands, indicating poor semantic consistency", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 282, "paper_title": "TopicRefiner: Coherence-guided Steerable LDA for Visual Topic Enhancement", "pub_year": 2023, "domain": "Latent Dirichlet Allocation", "requirement": {"requirement_text": "DP2: hncode co-occurrence matrix of high-ranking terms. The coherence degrees between high-ranking terms of a topic directly determine its coherence score (Equation 1). The visual encoding of this information reveals how the current coherence score form. The user can know the contribution of each pair of terms to the current coherence score. They thus can select terms that rank low but contribute much to the score and add them to the topic, and delete terms ranking high but seldom co-occur with other highranking terms from the topic.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "topics", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "LDA, proposed by Blei et al. [2], is currently one of the most\nwidely used topic models in industry and academia. LDA\nholds two assumptions: (1) each document d is a mixture of\nmultiple topics, represented as a multinomial distribution\non topics, denoted as \u03b8d and (2) each topic k is a multinomial\ndistribution on terms, denoted as \u03c6k. Fig. 1 shows the pro_x0002_cess of generating a document d according to a pair of given\n\u03b8d and \u03c6k. The idea is to determine d\u2019s each term, denoted as\nWd,t. For this purpose, we first calculate the topic affiliation\nof Wd,t, denoted as Zd,t, through sampling multinomial\ndistribution \u03b8d (from left to right). Assume Zd,t = k, i.e.,\nWd,t belongs to topic k by sampling \u03b8d, we can determine\nWd,t through sampling multinomial distribution \u03c6k (from\nright to left). The two multinomial distributions \u03b8d and\n\u03c6k can be obtained from two Dirichlet priors according to\nDirichlet-Multinomial conjugation, denoted as \u03b8d \u223c Dir(\u03b1),\n\u03c6k \u223c Dir(\u03b2), where \u03b1 and \u03b2 are the hyperparameters.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "TE enables three term-related refinements (RF1-RF3). It takes up most of the interface, showing a co-occurrence matrix of high-ranking terms of the target topic (DP2), as in Fig. 5(b). TE shows the co-occurrence matrix of high-ranking terms of the selected topic, as in Fig. 5(b), which is the basis for calculating coherence scores (Equation 1). The matrix thus reveals how much a term affects the topic quality. The matrix takes ten terms with the highest relevance to the topic as columns (see Fig. 5(b1)). We put the first m high-ranking terms into the matrix as rows, vertically aligned from the highest relevance to the lowest. Putting too many terms into the matrix is unnecessary and inefficient because low-ranking terms are often topic-irrelevant. Thus, we set m = 60 in this paper, which could cover most semantically relevant terms. The size of each circle encodes the co-occurrence degree of the two terms (the row and the column), providing visual cues for picking relevant or irrelevant terms. There is a bar before each row that encodes the term frequency of the term(see Fig. 5(b2)). TE achieves the three term-related refinements (RF1-RF3). The user can first click on the Term (or Term ), and then drag a term to a topic to add them to the topic (or remove them from the topic) (RF1, RF2). He (or she) can also delete a term from the corpus by sequentially clicking on Term and the term (RF3). There are a few logos at the end of each row, each representing a refinement conducted on the term, as in Fig. 5(b). The inner shape of a logo (;, -, and \u00d7) reflects the refinement type, as in Fig. 7. A logo with a single circle represents the refinement to the current topic, i.e., the term is added to the topic or removed from the topic. Instead, a logo with two concentric circles represents a refinement conducted on any other topic. We mark circles of terms as their assigned colors (see Fig. 7). The user can delete a logo through a mouse click to undo the refinement.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Matrix;Circle", "axial_code": [], "componenet_code": ["circle", "matrix"]}, {"solution_text": "TE enables three term-related refinements (RF1-RF3). It takes up most of the interface, showing a co-occurrence matrix of high-ranking terms of the target topic (DP2), as in Fig. 5(b). TE shows the co-occurrence matrix of high-ranking terms of the selected topic, as in Fig. 5(b), which is the basis for calculating coherence scores (Equation 1). The matrix thus reveals how much a term affects the topic quality. The matrix takes ten terms with the highest relevance to the topic as columns (see Fig. 5(b1)). We put the first m high-ranking terms into the matrix as rows, vertically aligned from the highest relevance to the lowest. Putting too many terms into the matrix is unnecessary and inefficient because low-ranking terms are often topic-irrelevant. Thus, we set m = 60 in this paper, which could cover most semantically relevant terms. The size of each circle encodes the co-occurrence degree of the two terms (the row and the column), providing visual cues for picking relevant or irrelevant terms. There is a bar before each row that encodes the term frequency of the term(see Fig. 5(b2)). TE achieves the three term-related refinements (RF1-RF3). The user can first click on the Term (or Term ), and then drag a term to a topic to add them to the topic (or remove them from the topic) (RF1, RF2). He (or she) can also delete a term from the corpus by sequentially clicking on Term and the term (RF3). There are a few logos at the end of each row, each representing a refinement conducted on the term, as in Fig. 5(b). The inner shape of a logo (;, -, and \u00d7) reflects the refinement type, as in Fig. 7. A logo with a single circle represents the refinement to the current topic, i.e., the term is added to the topic or removed from the topic. Instead, a logo with two concentric circles represents a refinement conducted on any other topic. We mark circles of terms as their assigned colors (see Fig. 7). The user can delete a logo through a mouse click to undo the refinement.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "dxf", "index_original": 283, "paper_title": "TopicRefiner: Coherence-guided Steerable LDA for Visual Topic Enhancement", "pub_year": 2023, "domain": "Latent Dirichlet Allocation", "requirement": {"requirement_text": "DP3: Recommend objects that can significantly improve coherence scores. The practice provides a mechanism to narrow the search ranges of relevant terms or documents for the user. The user only needs to focus on recommended objects that can significantly improve the coherence score. Specifically, the system can recommend (1) a few terms that co-occur with each other frequently on the current high-ranking documents or (2) documents on which the current high-ranking terms frequently co-occur.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "coherence scores, topics", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "LDA, proposed by Blei et al. [2], is currently one of the most\nwidely used topic models in industry and academia. LDA\nholds two assumptions: (1) each document d is a mixture of\nmultiple topics, represented as a multinomial distribution\non topics, denoted as \u03b8d and (2) each topic k is a multinomial\ndistribution on terms, denoted as \u03c6k. Fig. 1 shows the pro_x0002_cess of generating a document d according to a pair of given\n\u03b8d and \u03c6k. The idea is to determine d\u2019s each term, denoted as\nWd,t. For this purpose, we first calculate the topic affiliation\nof Wd,t, denoted as Zd,t, through sampling multinomial\ndistribution \u03b8d (from left to right). Assume Zd,t = k, i.e.,\nWd,t belongs to topic k by sampling \u03b8d, we can determine\nWd,t through sampling multinomial distribution \u03c6k (from\nright to left). The two multinomial distributions \u03b8d and\n\u03c6k can be obtained from two Dirichlet priors according to\nDirichlet-Multinomial conjugation, denoted as \u03b8d \u223c Dir(\u03b1),\n\u03c6k \u223c Dir(\u03b2), where \u03b1 and \u03b2 are the hyperparameters.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "When the user clicks on any term, TE can recommend terms (including the selected one) that co-occur the most (DP3).", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Circle;Table", "axial_code": [], "componenet_code": ["table", "circle"]}, {"solution_text": "When the user clicks on any term, TE can recommend terms (including the selected one) that co-occur the most (DP3).", "solution_category": "interaction", "solution_axial": "Connect/Related;Selelcting", "solution_compoent": "", "axial_code": ["Selelcting", "Connect/Related"], "componenet_code": ["selecting", "connect_relate"]}]}, {"author": "dxf", "index_original": 285, "paper_title": "PanVA: Pangenomic Variant Analysis", "pub_year": 2023, "domain": "pangenomics", "requirement": {"requirement_text": "T2: Examine the sequences and annotations directly, browsing and comparing genomic features to exploresequence variants and contextualize variation.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The data required for PanVA are, in essence, aligned DNA sequences, sequence annotations, metadata and sequence relations", "data_code": {"tables": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "The Locus View (Fig. 2C) is the core view of the design. It enables users to analyze variation in many genomes within the ROI. First, the sequence sub-view \u2460 depicts the genome sequences in the ROI (T2). To identify similarity and patterns of variation, the Locus View depicts an MSA of the sequence segments, where sequences are vertically juxtaposed in a scrollable heat map configuration in ?. Heat maps are commonly used for comparison of measured genomic feature sets [45]. In our case, the sets to compare are the different nucleotide sequences in the pangenome, over which an MSA has been calculated by PanTools. Our design assumes a type of sequence alignment has been performed; we opted to display the global align_x0002_ment because it allows users to judge sequence similarity more accurately than other alignment methods [46]. The main channels used to visually encode sequences are color, shape, letters, text and texture [46]. We use color to encode the different nucleotides in the MSA. Columns arrange nucleotides as the aligned positions, and rows rep_x0002_resent the gene sequence segments. We employ a categorical scheme with complementary colors for the complementary nucleotides: red (A) and green (T), blue (C) and orange (G), and white (for a gap), conforming to the color schemes in widely used MSA tools [47], [48]. We chose to not display the letters to avoid visual clutter in the grid. We provide other coloring schemes such as low-saturated colors, or a multi-channel encoding to highlight chemical properties of pairs, e.g, purines (A,G) vs. pyrimidines (C,T). To provide contextual information about the ROI, the Locus View includes two rows aligned with the top of the heat map of sequence segments in ?, representing the CDS annotation and selected visual reference. The CDS row indicates whether the position falls within a CDS (purple) or not (light grey), helping the user to assess the importance of the genomic variants in the ROI. For example, variants occurring within or at the borders of a CDS are often deemed more important because these are more likely to provoke a change in the encoded protein, thus possibly in the phenotype. The second row shows the visual reference and is just copied from the heat map to ensure guaranteed visibility and to ease association with the CDS annotation.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Matrix;Heatmap", "axial_code": [], "componenet_code": ["heatmap", "matrix"]}]}, {"author": "dxf", "index_original": 286, "paper_title": "PanVA: Pangenomic Variant Analysis", "pub_year": 2023, "domain": "pangenomics", "requirement": {"requirement_text": "T2: Examine the sequences and annotations directly, browsing and comparing genomic features to exploresequence variants and contextualize variation.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The data required for PanVA are, in essence, aligned DNA sequences, sequence annotations, metadata and sequence relations", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Users are enabled to manipulate the integrated views in different ways, facilitating multi-reference (T2) and multifaceted exploration (T3, T4), and the comparison of patterns (T5). Figure 2 shows the gene sequences in ? and metadata in ? linked to the hierarchical grouping ?. We design three main linked interaction strategies supported by tailored visual encodings: selection & filtering, sorting & clustering, and grouping & aggregation. Selections are linked and highlighted across the different views, helping the user to examine how the row-wise relationships change during exploration. Sorting is essential to identify patterns in these visual settings. The sorting operations are simi_x0002_larly linked between views, and animated so the user can keep a mental map. Lastly, for grouping and aggregation, the integrated layout creates a shared space to represent individual genomes as well as visually aggregated groups of genomes, to support comparison (T6, T7) and make the design scalable. In Section 7, we discuss each view\u2019s basic visual encoding, linked interactions, and tailored encodings.", "solution_category": "interaction", "solution_axial": "Selecting;Filtering;Reconfigure", "solution_compoent": "", "axial_code": ["Selecting", "Reconfigure", "Filtering"], "componenet_code": ["selecting", "reconfigure", "filtering"]}]}, {"author": "dxf", "index_original": 287, "paper_title": "PanVA: Pangenomic Variant Analysis", "pub_year": 2023, "domain": "pangenomics", "requirement": {"requirement_text": "T2: Examine the sequences and annotations directly, browsing and comparing genomic features to exploresequence variants and contextualize variation.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The data required for PanVA are, in essence, aligned DNA sequences, sequence annotations, metadata and sequence relations", "data_code": {"tables": 1}}, "solution": [{"solution_text": "The Locus View (Fig. 2C) is the core view of the design. It enables users to analyze variation in many genomes within the ROI. First, the sequence sub-view \u2460 depicts the genome sequences in the ROI (T2). To identify similarity and patterns of variation, the Locus View depicts an MSA of the sequence segments, where sequences are vertically juxtaposed in a scrollable heat map configuration in ?. Heat maps are commonly used for comparison of measured genomic feature sets [45]. In our case, the sets to compare are the different nucleotide sequences in the pangenome, over which an MSA has been calculated by PanTools. Our design assumes a type of sequence alignment has been performed; we opted to display the global align_x0002_ment because it allows users to judge sequence similarity more accurately than other alignment methods [46]. The main channels used to visually encode sequences are color, shape, letters, text and texture [46]. We use color to encode the different nucleotides in the MSA. Columns arrange nucleotides as the aligned positions, and rows rep_x0002_resent the gene sequence segments. We employ a categorical scheme with complementary colors for the complementary nucleotides: red (A) and green (T), blue (C) and orange (G), and white (for a gap), conforming to the color schemes in widely used MSA tools [47], [48]. We chose to not display the letters to avoid visual clutter in the grid. We provide other coloring schemes such as low-saturated colors, or a multi-channel encoding to highlight chemical properties of pairs, e.g, purines (A,G) vs. pyrimidines (C,T). To provide contextual information about the ROI, the Locus View includes two rows aligned with the top of the heat map of sequence segments in ?, representing the CDS annotation and selected visual reference. The CDS row indicates whether the position falls within a CDS (purple) or not (light grey), helping the user to assess the importance of the genomic variants in the ROI. For example, variants occurring within or at the borders of a CDS are often deemed more important because these are more likely to provoke a change in the encoded protein, thus possibly in the phenotype. The second row shows the visual reference and is just copied from the heat map to ensure guaranteed visibility and to ease association with the CDS annotation.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Matrix;Heatmap", "axial_code": [], "componenet_code": ["heatmap", "matrix"]}]}, {"author": "dxf", "index_original": 289, "paper_title": "PanVA: Pangenomic Variant Analysis", "pub_year": 2023, "domain": "pangenomics", "requirement": {"requirement_text": "T3: Explore distribution and dependencies of phenotypes.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data required for PanVA are, in essence, aligned DNA sequences, sequence annotations, metadata and sequence relations", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Users are enabled to manipulate the integrated views in different ways, facilitating multi-reference (T2) and multifaceted exploration (T3, T4), and the comparison of patterns (T5). Figure 2 shows the gene sequences in ? and metadata in ? linked to the hierarchical grouping ?. We design three main linked interaction strategies supported by tailored visual encodings: selection & filtering, sorting & clustering, and grouping & aggregation. Selections are linked and highlighted across the different views, helping the user to examine how the row-wise relationships change during exploration. Sorting is essential to identify patterns in these visual settings. The sorting operations are similarly linked between views, and animated so the user can keep a mental map. Lastly, for grouping and aggregation, the integrated layout creates a shared space to represent individual genomes as well as visually aggregated groups of genomes, to support comparison (T6, T7) and make the design scalable. In Section 7, we discuss each view\u2019s basic visual encoding, linked interactions, and tailored encodings.", "solution_category": "interaction", "solution_axial": "Selecting;Filtering;Reconfigure", "solution_compoent": "", "axial_code": ["Selecting", "Reconfigure", "Filtering"], "componenet_code": ["selecting", "reconfigure", "filtering"]}]}, {"author": "dxf", "index_original": 291, "paper_title": "PanVA: Pangenomic Variant Analysis", "pub_year": 2023, "domain": "pangenomics", "requirement": {"requirement_text": "T4: Explore the (evolutionary) relationships between different sequences in the homology group.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data required for PanVA are, in essence, aligned DNA sequences, sequence annotations, metadata and sequence relations", "data_code": {"tables": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "the relations sub-view ? depicts the hierarchical relations between the genomes (T4, T5). To further understand the similarity of the genomes, the dendrogram in ? shows the hierarchical structure that leads to the ordering or clustering of the sequences and associated metadata. This helps the user hypothesize, for xample, about inheritance from a common ancestor using the phylogenetic tree (T4, T5), providing a higher level view of the variation within the sequence data. The dendrogram is rendered horizontally such that the leaf nodes are aligned with the corresponding rows of the heat map. Different dendrograms can be generated using two data sources: a hierarchical clustering (see Section 7.4) of the sequences or common phylogenetic trees. To link the different sub-views of the Locus View, a bipartite graph is drawn between the dendogram and heat map. This is especially helpful when rows are reordered independently from the dendrogram or vice versa when rendering a dendrogram which is independent from row ordering, i.e., \u201dunlinking\u201d the hierarchy and the ordering (see Section 7.4). The bipartite graph connects, with curved lines, the dendrogram leaves to their corresponding row of the heat map matrix, indicating the difference in order between the two elements. This strategy of showing connections is similar to the slope lines used in Lineage [39], however, we show connecting lines even if they go beyond the screen size. These steep lines may result in more visual clutter, but serve as indicators for dramatic changes when comparing various orderings and genotype-phenotype relationships.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line;Tree", "axial_code": [], "componenet_code": ["tree", "line"]}]}, {"author": "dxf", "index_original": 292, "paper_title": "PanVA: Pangenomic Variant Analysis", "pub_year": 2023, "domain": "pangenomics", "requirement": {"requirement_text": "T4: Explore the (evolutionary) relationships between different sequences in the homology group.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The data required for PanVA are, in essence, aligned DNA sequences, sequence annotations, metadata and sequence relations", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Users are enabled to manipulate the integrated views in different ways, facilitating multi-reference (T2) and multifaceted exploration (T3, T4), and the comparison of patterns (T5).", "solution_category": "interaction", "solution_axial": "Selecting;Filtering;Reconfigure", "solution_compoent": "", "axial_code": ["Selecting", "Reconfigure", "Filtering"], "componenet_code": ["selecting", "reconfigure", "filtering"]}]}, {"author": "dxf", "index_original": 293, "paper_title": "PanVA: Pangenomic Variant Analysis", "pub_year": 2023, "domain": "pangenomics", "requirement": {"requirement_text": "T5: Contextualize similarity/difference within a ROI by identifying sequence patterns and relations.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "The data required for PanVA are, in essence, aligned DNA sequences, sequence annotations, metadata and sequence relations", "data_code": {"tables": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "the relations sub-view ? depicts the hierarchical relations between the genomes (T4, T5). To further understand the similarity of the genomes, the dendrogram in ? shows the hierarchical structure that leads to the ordering or clustering of the sequences and associated metadata. This helps the user hypothesize, for xample, about inheritance from a common ancestor using the phylogenetic tree (T4, T5), providing a higher level view of the variation within the sequence data. The dendrogram is rendered horizontally such that the leaf nodes are aligned with the corresponding rows of the heat map. Different dendrograms can be generated using two data sources: a hierarchical clustering (see Section 7.4) of the sequences or common phylogenetic trees. To link the different sub-views of the Locus View, a bipartite graph is drawn between the dendogram and heat map. This is especially helpful when rows are reordered independently from the dendrogram or vice versa when rendering a dendrogram which is independent from row ordering, i.e., \u201dunlinking\u201d the hierarchy and the ordering (see Section 7.4). The bipartite graph connects, with curved lines, the dendrogram leaves to their corresponding row of the heat map matrix, indicating the difference in order between the two elements. This strategy of showing connections is similar to the slope lines used in Lineage [39], however, we show connecting lines even if they go beyond the screen size. These steep lines may result in more visual clutter, but serve as indicators for dramatic changes when comparing various orderings and genotype-phenotype relationships.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line;Tree", "axial_code": [], "componenet_code": ["tree", "line"]}]}, {"author": "dxf", "index_original": 294, "paper_title": "PanVA: Pangenomic Variant Analysis", "pub_year": 2023, "domain": "pangenomics", "requirement": {"requirement_text": "T5: Contextualize similarity/difference within a ROI by identifying sequence patterns and relations.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "The data required for PanVA are, in essence, aligned DNA sequences, sequence annotations, metadata and sequence relations", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Users are enabled to manipulate the integrated views in different ways, facilitating multi-reference (T2) and multifaceted exploration (T3, T4), and the comparison of patterns (T5). Figure 2 shows the gene sequences in ? and metadata in ? linked to the hierarchical grouping ?. We design three main linked interaction strategies supported by tailored visual encodings: selection & filtering, sorting & clustering, and grouping & aggregation. Selections are linked and highlighted across the different views, helping the user to examine how the row-wise relationships change during exploration. Sorting is essential to identify patterns in these visual settings. The sorting operations are similarly linked between views, and animated so the user can keep a mental map. Lastly, for grouping and aggregation, the integrated layout creates a shared space to represent individual genomes as well as visually aggregated groups of genomes, to support comparison (T6, T7) and make the design scalable. In Section 7, we discuss each view\u2019s basic visual encoding, linked interactions, and tailored encodings.", "solution_category": "interaction", "solution_axial": "Selecting;Filtering;Reconfigure", "solution_compoent": "", "axial_code": ["Selecting", "Reconfigure", "Filtering"], "componenet_code": ["selecting", "reconfigure", "filtering"]}]}, {"author": "dxf", "index_original": 295, "paper_title": "PanVA: Pangenomic Variant Analysis", "pub_year": 2023, "domain": "pangenomics", "requirement": {"requirement_text": "T5: Contextualize similarity/difference within a ROI by identifying sequence patterns and relations.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "The data required for PanVA are, in essence, aligned DNA sequences, sequence annotations, metadata and sequence relations", "data_code": {"tables": 1}}, "solution": [{"solution_text": "The Locus View (Fig. 2C) is the core view of the design. It enables users to analyze variation in many genomes within the ROI. First, the sequence sub-view \u2460 depicts the genome sequences in the ROI (T2). To identify similarity and patterns of variation, the Locus View depicts an MSA of the sequence segments, where sequences are vertically juxtaposed in a scrollable heat map configuration in ?. Heat maps are commonly used for com_x0002_parison of measured genomic feature sets [45]. In our case, the sets to compare are the different nucleotide sequences in the pangenome, over which an MSA has been calculated by PanTools. Our design assumes a type of sequence alignment has been performed; we opted to display the global align_x0002_ment because it allows users to judge sequence similarity more accurately than other alignment methods [46]. The main channels used to visually encode sequences are color, shape, letters, text and texture [46]. We use color to encode the different nucleotides in the MSA. Columns arrange nucleotides as the aligned positions, and rows rep_x0002_resent the gene sequence segments. We employ a categorical\nscheme with complementary colors for the complementary nucleotides: red (A) and green (T), blue (C) and orange (G), and white (for a gap), conforming to the color schemes in widely used MSA tools [47], [48]. We chose to not display the letters to avoid visual clutter in the grid. We provide other coloring schemes such as low-saturated colors, or a multi-channel encoding to highlight chemical properties of pairs, e.g, purines (A,G) vs. pyrimidines (C,T). To provide contextual information about the ROI, the Locus View includes two rows aligned with the top of the heat map of sequence segments in ?, representing the CDS annotation and selected visual reference. The CDS row indicates whether the position falls within a CDS (purple) or not (light grey), helping the user to assess the importance of the genomic variants in the ROI. For example, variants occurring within or at the borders of a CDS are often deemed more important because these are more likely to provoke a change in the encoded protein, thus possibly in the phenotype. The second row shows the visual reference and is just copied from the heat map to ensure guaranteed\nvisibility and to ease association with the CDS annotation.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Matrix;Heatmap", "axial_code": [], "componenet_code": ["heatmap", "matrix"]}]}, {"author": "dxf", "index_original": 296, "paper_title": "PanVA: Pangenomic Variant Analysis", "pub_year": 2023, "domain": "pangenomics", "requirement": {"requirement_text": "T5: Contextualize similarity/difference within a ROI by identifying sequence patterns and relations.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "The data required for PanVA are, in essence, aligned DNA sequences, sequence annotations, metadata and sequence relations", "data_code": {"tables": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "To further understand the similarity of the genomes, the dendrogram in ? shows the hierarchical structure that leads to the ordering or clustering of the sequences and associated metadata. This helps the user hypothesize, for example, about inheritance from a common ancestor using the phylogenetic tree (T4, T5), providing a higher level view of the variation within the sequence data.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Line;Tree", "axial_code": [], "componenet_code": ["tree", "line"]}]}, {"author": "dxf", "index_original": 297, "paper_title": "PanVA: Pangenomic Variant Analysis", "pub_year": 2023, "domain": "pangenomics", "requirement": {"requirement_text": "T5: Contextualize similarity/difference within a ROI by identifying sequence patterns and relations.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "The data required for PanVA are, in essence, aligned DNA sequences, sequence annotations, metadata and sequence relations", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Our design allows interactive selection of a visual reference, a genome or a group, and updates the coloring of the heat map cells to show the differences for comparison (T5)", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 298, "paper_title": "PanVA: Pangenomic Variant Analysis", "pub_year": 2023, "domain": "pangenomics", "requirement": {"requirement_text": "T5: Contextualize similarity/difference within a ROI by identifying sequence patterns and relations.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "The data required for PanVA are, in essence, aligned DNA sequences, sequence annotations, metadata and sequence relations", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Sorting is enabled and linked in all sub-views to help the user identify patterns (T5, T6)", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 300, "paper_title": "PanVA: Pangenomic Variant Analysis", "pub_year": 2023, "domain": "pangenomics", "requirement": {"requirement_text": "T6: Connect relationships by creating groups to understand the variety between sets of genomes.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The data required for PanVA are, in essence, aligned DNA sequences, sequence annotations, metadata and sequence relations", "data_code": {"tables": 1}}, "solution": [{"solution_text": "for grouping and aggregation, the integrated layout creates a shared space to represent individual genomes as well as visually aggregated groups of genomes, to support comparison (T6, T7) and make the design scalable.", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 302, "paper_title": "PanVA: Pangenomic Variant Analysis", "pub_year": 2023, "domain": "pangenomics", "requirement": {"requirement_text": "T6: Connect relationships by creating groups to understand the variety between sets of genomes.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The data required for PanVA are, in essence, aligned DNA sequences, sequence annotations, metadata and sequence relations", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Sorting is enabled and linked in all sub-views to help the user identify patterns (T5, T6). There are three sorting options: by single position, metadata feature, and multiple positions. All sorting described in this section are animated and minimize the number of row swaps to preserve the mental map as much as possible [49], [50]. To achieve minimal swaps when ordering categorical attributes or gen_x0002_erated groups lacking an intrinsic order, we use heuristics based on row indices of the current order. We compute the median row indexes per category. The categories are then sorted on their respective median. Within the categories the initial order is preserved. We experimented with the mean index to guide the sorting order, but the fact that the median is less sensitive to outliers resulted in a better ordering.", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 304, "paper_title": "PanVA: Pangenomic Variant Analysis", "pub_year": 2023, "domain": "pangenomics", "requirement": {"requirement_text": "T7: Summarize groups to examine variations between them.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "The data required for PanVA are, in essence, aligned DNA sequences, sequence annotations, metadata and sequence relations", "data_code": {"tables": 1}}, "solution": [{"solution_text": "for grouping and aggregation, the integrated layout creates a shared space to represent individual genomes as well as visually aggregated groups of genomes, to support comparison (T6, T7) and make the design scalable.", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 305, "paper_title": "PanVA: Pangenomic Variant Analysis", "pub_year": 2023, "domain": "pangenomics", "requirement": {"requirement_text": "T7: Summarize groups to examine variations between them.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "The data required for PanVA are, in essence, aligned DNA sequences, sequence annotations, metadata and sequence relations", "data_code": {"tables": 1, "network_and_trees": 1}}, "solution": [{"solution_text": "The Locus View supports grouping to simultaneously compare the characteristics of multiple selected groups of genomes (T6). In addition, these groups can be visually aggregated, allowing the user to focus on certain groups and data aspects while leaving others as context (T7), and to process more information in the limited screen space. Grouping divides the set of genomes into non-empty sub_x0002_sets, so that every genome is included in exactly one group. The groups can be created from selection as explained in Section 7.3. After creation, group members are highlighted with a unique color to enable tracking by a user. Groups can also be collapsed to summarize and manage vertical scalability of the rows of the Locus View. Collapsed groups appear as a single group row, using tailored encod_x0002_ings to show visual aggregations.", "solution_category": "interaction", "solution_axial": "Selecting;Reconfigure", "solution_compoent": "", "axial_code": ["Selecting", "Reconfigure"], "componenet_code": ["selecting", "reconfigure"]}]}, {"author": "dxf", "index_original": 306, "paper_title": "MediVizor: Visual Mediation Analysis of Nominal Variables", "pub_year": 2023, "domain": "Information Visualization", "requirement": {"requirement_text": "R1: Compare total effects between one or more IVs and DVs and their corresponding direct/indirect effects and MVs. The experts require an overview that enables them to examine and compare the total effects between all IVs and DVs at a glance (N1). In the overview, they can quickly find the strong total effects and locate the effects between particular IVs and DVs. The experts also need to examine and compare the indirect effects and MVs in the total effects (N2).", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "nominal variables, the total and direct/indirect effects", "data_code": {"tables": 1}}, "solution": [{"solution_text": "In this section, we introduce how to model the mediation effects among nominal variables in the raw data. Estimation of Total and Direct/Indirect Effects. Three nominal variables, IV, MV, and DV, are decomposed into three sets of dummy variables {IV1, IV2, ...IVm}, {MV1, MV2, ...MVn}, {DV1, DV2, ...DVl} before analysis. We estimate the mediation effects among the dummy vari_x0002_ables using a general approach proposed by Imai et al. [18], which has been implemented in R [19] and Python. We calculate the direct and indirect effects among each triad {IVi, MVj , DVk}, where i = 1, . . . , m, j = 1, . . . , n, k = 1, . . . , l. When estimating the effects of a specific triad {IVi, MVj , DVk}, we use other IVs and MVs as control variables. The approach outputs the estimated coefficients for the direct and indirect effects and their significance. We use the general linear model as the outcome and mediator models and the logit function as the link function because the DVs are dummy variables. An estimated effect is an odds ratio (OR, which is the ratio between the odds that a DV is one when the IV is one and the odds when the IV is zero. As the interpretation of an odds ratio is not straightforward, we transform the odds ratio into the changed probability (Pchanged) of DV being one when the IV is one compared to that the IV is zero.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The designs aim to support the examination and comparison of the total and direct/indirect effects among multiple IVs, DVs, and MVs (R1, R3). We present the total effects of multiple IVs on multiple DVs (N1) using a matrix view (Fig. 3a4) on the left of the effect view. The row titles and column titles of the matrix represent different IVs and DVs. Each entry in the matrix presents the total effect between the corresponding IV and DV. The area of the circle in an entry encodes the size of the total effect and the color hue encodes whether the effect is positive or negative. Users can browse the matrix to detect strong total effects or find a total effect through its IV and DV. Additionally, a bar is provided for each IV or DV, encoding the average size of the total effects associated with that variable, to aid users in navigating into interesting total effects.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Matrix;Circle", "axial_code": [], "componenet_code": ["circle", "matrix"]}]}, {"author": "dxf", "index_original": 307, "paper_title": "MediVizor: Visual Mediation Analysis of Nominal Variables", "pub_year": 2023, "domain": "Information Visualization", "requirement": {"requirement_text": "R2: Compare direct and indirect effects that compose a total effect and examine the ratios of them versus the total effect. The experts require to compare the direct/indirect effects in a total effect (N3) to find out the strongest indirect effect and its corresponding MV and analyze it further. Experts also need to examine the part-to-whole ratios of the positive and negative direct/indirect effects versus the total effect (N4) to understand their contributions to the total effect.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "nominal variables", "data_code": {"tables": 1}}, "solution": [{"solution_text": "In this section, we introduce how to model the mediation effects among nominal variables in the raw data. Estimation of Total and Direct/Indirect Effects. Three nominal variables, IV, MV, and DV, are decomposed into three sets of dummy variables {IV1, IV2, ...IVm}, {MV1, MV2, ...MVn}, {DV1, DV2, ...DVl} before analysis. We estimate the mediation effects among the dummy vari_x0002_ables using a general approach proposed by Imai et al. [18], which has been implemented in R [19] and Python. We calculate the direct and indirect effects among each triad {IVi, MVj , DVk}, where i = 1, . . . , m, j = 1, . . . , n, k = 1, . . . , l. When estimating the effects of a specific triad {IVi, MVj , DVk}, we use other IVs and MVs as control variables. The approach outputs the estimated coefficients for the direct and indirect effects and their significance. We use the general linear model as the outcome and mediator models and the logit function as the link function because the DVs are dummy variables. An estimated effect is an odds ratio (OR, which is the ratio between the odds that a DV is one when the IV is one and the odds when the IV is zero. As the interpretation of an odds ratio is not straightforward, we transform the odds ratio into the changed probability (Pchanged) of DV being one when the IV is one compared to that the IV is zero.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "For each total effect represented by a circle, we link it to its direct and indirect effects represented by bars in a bar chart using leader lines(Fig. 3a5). The bar chart is placed on the right of the effect view and each row corresponds to a total effect. The first column presents the direct effects, while the subsequent columns display the indirect effects mediated by MVs. In each column, we use a bidirectional bar chart to represent the positive and negative effects. The positive effect is on the right side and the negative effect is on the left side. The direct effect and indirect effects mediated by different MVs are distinguished by color hues, which are from those representing polarity. We assign color hues based on a qualitative color scheme (as shown in the legend in Fig. 1), which is initially generated by a color tool 2 and further improved by the experts and us. There are ten color hues in the effect view in total. If more MVs need to be displayed, we repeat the colors or use similar colors for the MVs with similar meanings. A bar is provided for each MV, encoding the average size of the indirect effects associated with the MV, to help users navigate into interesting indirect effects. A bar is also provided for the direct effects. Users can compare direct and indirect effects within a total effect row (N3) and compare indirect effects mediated by the same MV within a column (N5).", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "dxf", "index_original": 308, "paper_title": "MediVizor: Visual Mediation Analysis of Nominal Variables", "pub_year": 2023, "domain": "Information Visualization", "requirement": {"requirement_text": "R3: Compare indirect effects mediated by one or more MVs and examine the IVs and DVs that these MVs mediate. Different total effects might contain indirect effects mediated by the same MV. The experts need to compare indirect effects through the same MV and compare direct effects in different total effects (N5) and examine the IVs and DVs involved (N6). This information helps experts understand how an MV mediates the effects between various IVs and DVs.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "nominal variables, the total and direct/indirect effects", "data_code": {"tables": 1}}, "solution": [{"solution_text": "In this section, we introduce how to model the mediation effects among nominal variables in the raw data. Estimation of Total and Direct/Indirect Effects. Three nominal variables, IV, MV, and DV, are decomposed into three sets of dummy variables {IV1, IV2, ...IVm}, {MV1, MV2, ...MVn}, {DV1, DV2, ...DVl} before analysis. We estimate the mediation effects among the dummy vari_x0002_ables using a general approach proposed by Imai et al. [18], which has been implemented in R [19] and Python. We calculate the direct and indirect effects among each triad {IVi, MVj , DVk}, where i = 1, . . . , m, j = 1, . . . , n, k = 1, . . . , l. When estimating the effects of a specific triad {IVi, MVj , DVk}, we use other IVs and MVs as control variables. The approach outputs the estimated coefficients for the direct and indirect effects and their significance. We use the general linear model as the outcome and mediator models and the logit function as the link function because the DVs are dummy variables. An estimated effect is an odds ratio (OR, which is the ratio between the odds that a DV is one when the IV is one and the odds when the IV is zero. As the interpretation of an odds ratio is not straightforward, we transform the odds ratio into the changed probability (Pchanged) of DV being one when the IV is one compared to that the IV is zero.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The designs aim to support the examination and comparison of the total and direct/indirect effects among multiple IVs, DVs, and MVs (R1, R3). We present the total effects of multiple IVs on multiple DVs (N1) using a matrix view (Fig. 3a4) on the left of the effect view. The row titles and column titles of the matrix represent different IVs and DVs. Each entry in the matrix presents the total effect between the corresponding IV and DV. The area of the circle in an entry encodes the size of the total effect and the color hue encodes whether the effect is positive or negative. Users can browse the matrix to detect strong total effects or find a total effect through its IV and DV. Additionally, a bar is provided for each IV or DV, encoding the average size of the total effects associated with that variable, to aid users in navigating into interesting total effects.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Matrix;Circle", "axial_code": [], "componenet_code": ["circle", "matrix"]}]}, {"author": "dxf", "index_original": 309, "paper_title": "MediVizor: Visual Mediation Analysis of Nominal Variables", "pub_year": 2023, "domain": "Information Visualization", "requirement": {"requirement_text": "R4: Analysis order and validation. The experts require that the mediation analysis should be conducted in a specific order (N7). The total effects should be examined first. The direct and indirect effects within the same total effect are then investigated. The indirect effects mediated by the same MV are compared finally. As the mediation effects are calculated by the statistical model and are not intuitive, experts hope to validate the identified interesting indirect effects through the conditional frequency distributions of relevant variables (N8).", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "nominal variables", "data_code": {"tables": 1}}, "solution": [{"solution_text": "The workflow of the system is specially designed for the required analysis order of mediation effects, i.e., from total effects to indirect effects (N7). Users can explore the total effects in the matrix view first, and then explore their indirect effects in the bar and paired pie charts conveniently using the system. The system is implemented through React.js. Positive and negative effects are color-coded in green and red, respectively.", "solution_category": "interaction", "solution_axial": "Overviewandexplore", "solution_compoent": "", "axial_code": ["Overviewandexplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "dxf", "index_original": 310, "paper_title": "MediVizor: Visual Mediation Analysis of Nominal Variables", "pub_year": 2023, "domain": "Information Visualization", "requirement": {"requirement_text": "R4: Analysis order and validation. The experts require that the mediation analysis should be conducted in a specific order (N7). The total effects should be examined first. The direct and indirect effects within the same total effect are then investigated. The indirect effects mediated by the same MV are compared finally. As the mediation effects are calculated by the statistical model and are not intuitive, experts hope to validate the identified interesting indirect effects through the conditional frequency distributions of relevant variables (N8).", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "nominal variables", "data_code": {"tables": 1}}, "solution": [{"solution_text": "In this section, we introduce how to model the mediation effects among nominal variables in the raw data. Estimation of Total and Direct/Indirect Effects. Three nominal variables, IV, MV, and DV, are decomposed into three sets of dummy variables {IV1, IV2, ...IVm}, {MV1, MV2, ...MVn}, {DV1, DV2, ...DVl} before analysis. We estimate the mediation effects among the dummy vari_x0002_ables using a general approach proposed by Imai et al. [18], which has been implemented in R [19] and Python. We calculate the direct and indirect effects among each triad {IVi, MVj , DVk}, where i = 1, . . . , m, j = 1, . . . , n, k = 1, . . . , l. When estimating the effects of a specific triad {IVi, MVj , DVk}, we use other IVs and MVs as control variables. The approach outputs the estimated coefficients for the direct and indirect effects and their significance. We use the general linear model as the outcome and mediator models and the logit function as the link function because the DVs are dummy variables. An estimated effect is an odds ratio (OR, which is the ratio between the odds that a DV is one when the IV is one and the odds when the IV is zero. As the interpretation of an odds ratio is not straightforward, we transform the odds ratio into the changed probability (Pchanged) of DV being one when the IV is one compared to that the IV is zero.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The validation view (Fig. 3C) supports visual verification of an indirect effect identified in the effect view through frequency distributions (N8). When the analyst selects an indirect effect in the effect view (Fig. 6D), the encodings in the two bar charts (Fig. 7) vary accordingly. An indirect effect involves an IV, DV, and MV, and re_x0002_quires verification of two conditions. (1) The IV has an effect on the MV. We verify this by comparing MV values of cases that have different IV values. A bar chart is appropriate for this value comparison task. In particular, We divide cases into two groups by the IV value and compared their ratios of cases where the MV is one (represented by the two bars in Fig. 7A). If users find an explicit difference between the two bars, the IV has an effect on the MV. (2) The MV has an effect on the DV. We verify this by comparing the DV values of cases that have different MV values but the same IV value. A grouped bar chart is appropriate for this value comparison task. In particular, We divide cases into two groups by the IV value and each group was divided into two subgroups by the MV value. We compare the ratios of cases where the DV is one in two subgroups in each group (represented by two bars in each bar group Fig. 7B). If users find an explicit difference between the two bars within either bar group, the MV has an effect on the DV when the IV is controlled.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "The validation view (Fig. 3C) supports visual verification of an indirect effect identified in the effect view through frequency distributions (N8). When the analyst selects an indirect effect in the effect view (Fig. 6D), the encodings in the two bar charts (Fig. 7) vary accordingly. An indirect effect involves an IV, DV, and MV, and re_x0002_quires verification of two conditions. (1) The IV has an effect on the MV. We verify this by comparing MV values of cases that have different IV values. A bar chart is appropriate for this value comparison task. In particular, We divide cases into two groups by the IV value and compared their ratios of cases where the MV is one (represented by the two bars in Fig. 7A). If users find an explicit difference between the two bars, the IV has an effect on the MV. (2) The MV has an effect on the DV. We verify this by comparing the DV values of cases that have different MV values but the same IV value. A grouped bar chart is appropriate for this value comparison task. In particular, We divide cases into two groups by the IV value and each group was divided into two subgroups by the MV value. We compare the ratios of cases where the DV is one in two subgroups in each group (represented by two bars in each bar group Fig. 7B). If users find an explicit difference between the two bars within either bar group, the MV has an effect on the DV when the IV is controlled.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 311, "paper_title": "SpectrumVA: Visual Analysis of Astronomical Spectra for Facilitating Classification Inspection", "pub_year": 2023, "domain": "Spectral classification", "requirement": {"requirement_text": "DC1.1: Introduce spectral organization strategies. Allowing the experts to make an initial judgment about a spectrum, such as its potential class, is preferable to random selection based on spectral identifiers. For this purpose, we need to introduce spectral characterization metrics that reveal the relationship between spectraDC1.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Astronomical Spectra", "data_code": {"sequential": 1, "media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "The Projection View (Figure 4a) displays the relationship between reference spectra and spectra to be inspected, using reference spectra as context and spectra to be inspected as focus (DC1.1). As reference spectra serve as ground truth (with accurate redshifts), we first project them using t-SNE [58] and then project the spectra to be inspected. However, since t-SNE is a non_x0002_parametric technique [59], extending the mapping of existing data items is difficult. To achieve this, we use out-of-sample (OOS) extensions to map new data items into the projection generated by a given example set. Several OOS t-SNE have been proposed, such as pt-SNE [60], kernel t-SNE [59], and bi-kernel t-SNE [61]. We adopt bi-kernel t-SNE as it maps new data items well and can reveal differences between inliers and outliers. We visualize the projection of reference spectra as a contour map, which shows the density distribution. The projection of the spectra to be inspected is shown as a scatterplot. Since each spectrum to be inspected contains n candidate redshifts (n pro_x0002_jection points), we only show the projection points of the spectra selected by the PCPs View and the Table View to avoid visual clutter. If more than one spectrum to be inspected is selected, points belonging to the same spectrum are linked to distinguish between different spectra. Color is used to differentiate between different classes of contours and projection points. The size of the projection points encodes the probability that the spectrum is at the corresponding redshift. Points corresponding to the redshifts provided by the pipelines have a maximum and fixed size, while points corresponding to redshifts obtained by traversal are assigned decreasing sizes based on the score. Furthermore, all projection points are outlined to enhance their contrast against the background contours. By default, the outline of the projection point corresponding to the first candidate redshift is black, while the rest are white. And as experts hover over different candidate redshifts in the Spectral Candidate View, the corresponding pro_x0002_jection point outline becomes black, while all others turn white.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The Projection View (Figure 4a) displays the relationship between reference spectra and spectra to be inspected, using reference spectra as context and spectra to be inspected as focus (DC1.1). As reference spectra serve as ground truth (with accurate redshifts), we first project them using t-SNE [58] and then project the spectra to be inspected. However, since t-SNE is a non_x0002_parametric technique [59], extending the mapping of existing data items is difficult. To achieve this, we use out-of-sample (OOS) extensions to map new data items into the projection generated by a given example set. Several OOS t-SNE have been proposed, such as pt-SNE [60], kernel t-SNE [59], and bi-kernel t-SNE [61]. We adopt bi-kernel t-SNE as it maps new data items well and can reveal differences between inliers and outliers. We visualize the projection of reference spectra as a contour map, which shows the density distribution. The projection of the spectra to be inspected is shown as a scatterplot. Since each spectrum to be inspected contains n candidate redshifts (n pro_x0002_jection points), we only show the projection points of the spectra selected by the PCPs View and the Table View to avoid visual clutter. If more than one spectrum to be inspected is selected, points belonging to the same spectrum are linked to distinguish between different spectra. Color is used to differentiate between different classes of contours and projection points. The size of the projection points encodes the probability that the spectrum is at the corresponding redshift. Points corresponding to the redshifts provided by the pipelines have a maximum and fixed size, while points corresponding to redshifts obtained by traversal are assigned decreasing sizes based on the score. Furthermore, all projection points are outlined to enhance their contrast against the background contours. By default, the outline of the projection point corresponding to the first candidate redshift is black, while the rest are white. And as experts hover over different candidate redshifts in the Spectral Candidate View, the corresponding pro_x0002_jection point outline becomes black, while all others turn white.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Area;Circle", "axial_code": [], "componenet_code": ["area", "circle"]}]}, {"author": "dxf", "index_original": 313, "paper_title": "SpectrumVA: Visual Analysis of Astronomical Spectra for Facilitating Classification Inspection", "pub_year": 2023, "domain": "Spectral classification", "requirement": {"requirement_text": "DC1.2: Offer sufficient visual cues for each spectrum. Besides presenting a high-level spectral relationship, we should also provide various visual cues at a more fine-grained level. These cues can help the experts build an initial understanding of each spectrum. For example, if the 1D and independent pipelines assign different classes to a spectrum, it is likely misclassified and requires careful inspection.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "the parameters of spectra", "data_code": {"sequential": 1, "media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "This section describes the data processing procedures of Spec_x0002_trumVA. As shown in Figure 2, we first prepare for the spectral characterization, including spectral normalization and SLC (spec_x0002_tral line candidate) detection. Then the spectra to be inspected and the reference spectra are characterized based on different strategies. In addition, SpectrumVA automatically searches for similar spectra after inspecting a spectrum according to the ex_x0002_perts\u2019 judgment criteria.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "The PCPs View (Figure 4b) is a variant of PCPs, developed to display the parameters of spectra to be inspected, i.e., the classes and redshifts given by the 1D and the independent pipelines (DC1.2).", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line;Area;Bar", "axial_code": [], "componenet_code": ["area", "line", "bar"]}]}, {"author": "dxf", "index_original": 315, "paper_title": "SpectrumVA: Visual Analysis of Astronomical Spectra for Facilitating Classification Inspection", "pub_year": 2023, "domain": "Spectral classification", "requirement": {"requirement_text": "DC2: Provide entry points for inspection. At present, faced with a spectrum containing thousands of wavelength-flux points, the experts have to select spectral lines and adjust the redshift from scratch manually. Relying solely on domain knowledge, they inevitably face the cold start problem. To address this, we should recommend potential redshifts and appropriate spectral lines at each redshift, bridging the gap between spectral selection and inspection while making the process more convenient.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "Astronomical Spectra", "data_code": {"sequential": 1, "media": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "This section describes the data processing procedures of Spec_x0002_trumVA. As shown in Figure 2, we first prepare for the spectral characterization, including spectral normalization and SLC (spec_x0002_tral line candidate) detection. Then the spectra to be inspected and the reference spectra are characterized based on different strategies. In addition, SpectrumVA automatically searches for similar spectra after inspecting a spectrum according to the ex_x0002_perts\u2019 judgment criteria.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "After selecting a spectrum, we design the Spectral Candidate View (Figure 4d) to further ease the inspection burden for experts. This view displays the n candidate redshifts and important spectral lines at each redshift for the selected spectrum (DC2). Additionally, for each candidate redshift, we provide the two most similar reference spectra to its representation. The experts can use these comparisons to determine whether the redshift is an appropriate starting point for inspection. After selecting an entry, they can access the single spectrum inspection interface by clicking the icon in the upper right corner of the corresponding candidate redshift (Figure 4d1). Alternatively, they can click on the icon at the bottom of the view to inspect the spectrum from the start.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 316, "paper_title": "SpectrumVA: Visual Analysis of Astronomical Spectra for Facilitating Classification Inspection", "pub_year": 2023, "domain": "Spectral classification", "requirement": {"requirement_text": "DC3.1: Perform the inspection based on the raw spectrum. We should allow the experts to perceive the raw spectrum, i.e., the relationship between wavelength and flux, to ensure intuitiveness and trust. As sequential data, a spectrum can be visualized in various ways. In astronomy, it is typically presented as a line chart. We choose to follow domain conventions rather than using alternatives or proposing new visualizations to avoid introducing unnecessary learning costs for experts.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "the wavelength-flux relationship of the spectrum ", "data_code": {"sequential": 1, "media": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "This section describes the data processing procedures of Spec_x0002_trumVA. As shown in Figure 2, we first prepare for the spectral characterization, including spectral normalization and SLC (spec_x0002_tral line candidate) detection. Then the spectra to be inspected and the reference spectra are characterized based on different strategies. In addition, SpectrumVA automatically searches for similar spectra after inspecting a spectrum according to the ex_x0002_perts\u2019 judgment criteria.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "The Spectrum View (Figure 6a) displays the wavelength-flux relationship of the spectrum (DC3.1). The Spectral Line View (Figure 6b) shows the spectral lines and their rest frame wavelengths, with common combinations listed in the upper left corner (Figure 6b1). Selected spectral lines or combinations are displayed in the Spectrum View. The experts can access information about the spectrum through the right panel (Figure 6c), such as classification results and redshifts given by the pipelines. They can also perform operations, such as entering the redshift adjustment mode, displaying the smoothed spectrum, and displaying the template spectrum. Upon entering the redshift adjustment mode, the Spectrum View is vertically compressed, making room for four additional views: Horizon Graphs [68], Local Spectrum View, Local Flux View, and Overall Importance View. Redshift adjustment and classification are carried out through three modules: adjustment, recommendation, and verification.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 317, "paper_title": "SpectrumVA: Visual Analysis of Astronomical Spectra for Facilitating Classification Inspection", "pub_year": 2023, "domain": "Spectral classification", "requirement": {"requirement_text": "DC3.2: Provide support for improved inspection decisions. Instead of relying solely on the line chart, as is the current practice, we need to provide additional support for the inspection process, including extra views and instant visual feedback. This support can be offered through evaluations of the experts\u2019 actions and recommendations for the next steps. By doing so, we can help the experts more easily select spectral lines, adjust the redshift, and assess the reasonability of the redshift, i.e., whether the wavelengths of the spectral lines are appropriate.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "the wavelength-flux relationship of the spectrum ", "data_code": {"sequential": 1, "media": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Adjustment module. The module aims to rectify defects in the current inspection process by transforming the wavelength-flux relationship for each spectral line into a redshift-flux relationship (DC3.2). Specifically, we calculate the wavelength range corresponding to the redshift range of each spectral line using \u03bb = (1;z)\u03bb0 and convert the wavelength-flux points within the range into redshift-flux points. The redshift range is based on the 1D pipeline. Fluxes at the boundary without corresponding wavelengths are obtained through interpolation. This transformation eliminates differences in wavelength changes of spectral lines during redshift changes, allowing for a consistent horizontal shift and synchronous redshift adjustment across multiple spectral lines. The Horizon Graphs and Local Flux View are primarily used in this module. The redshift-flux relationship is displayed by Horizon Graphs (Figure 6d), with each graph corresponding to a spectral line. Horizon Graph visualization is a split-space technique [69] that aligns selected spectral lines, effectively compressing the vertical screen space by dividing and layering the raw redshift-flux line chart. The baseline is set to the minimum value of fluxes of all spectral lines. A more appropriate approach would be a semantic_x0002_based baseline [70]. This approach tailors the baseline according to the task at hand.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "Area", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "Adjustment module. The module aims to rectify defects in the current inspection process by transforming the wavelength-flux relationship for each spectral line into a redshift-flux relationship (DC3.2). Specifically, we calculate the wavelength range corresponding to the redshift range of each spectral line using \u03bb = (1;z)\u03bb0 and convert the wavelength-flux points within the range into redshift-flux points. The redshift range is based on the 1D pipeline. Fluxes at the boundary without corresponding wavelengths are obtained through interpolation. This transformation eliminates differences in wavelength changes of spectral lines during redshift changes, allowing for a consistent horizontal shift and synchronous redshift adjustment across multiple spectral lines. The Horizon Graphs and Local Flux View are primarily used in this module. The redshift-flux relationship is displayed by Horizon Graphs (Figure 6d), with each graph corresponding to a spectral line. Horizon Graph visualization is a split-space technique [69] that aligns selected spectral lines, effectively compressing the vertical screen space by dividing and layering the raw redshift-flux line chart. The baseline is set to the minimum value of fluxes of all spectral lines. A more appropriate approach would be a semantic_x0002_based baseline [70]. This approach tailors the baseline according to the task at hand.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Area", "axial_code": [], "componenet_code": ["area"]}]}, {"author": "dxf", "index_original": 318, "paper_title": "SpectrumVA: Visual Analysis of Astronomical Spectra for Facilitating Classification Inspection", "pub_year": 2023, "domain": "Spectral classification", "requirement": {"requirement_text": "DC3.2: Provide support for improved inspection decisions. Instead of relying solely on the line chart, as is the current practice, we need to provide additional support for the inspection process, including extra views and instant visual feedback. This support can be offered through evaluations of the experts\u2019 actions and recommendations for the next steps. By doing so, we can help the experts more easily select spectral lines, adjust the redshift, and assess the reasonability of the redshift, i.e., whether the wavelengths of the spectral lines are appropriate.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "spectral lines", "data_code": {"sequential": 1, "media": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Recommendation module. Spectral lines may not always be present, so the experts need to select other spectral lines if the chosen ones do not work. They can manually select common spectral lines or combinations based on experience, or opt for our recommended spectral lines (DC3.2).  The Spectrum View and Overall Importance View provide access to the recommendation module. The experts can select significant SLCs in the Spectrum View by brushing, and the system automatically identifies the spectral lines within. Multi-brush is supported as the experts typically adjust the redshift based on multiple spectral lines.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 319, "paper_title": "SpectrumVA: Visual Analysis of Astronomical Spectra for Facilitating Classification Inspection", "pub_year": 2023, "domain": "Spectral classification", "requirement": {"requirement_text": "DC3.2: Provide support for improved inspection decisions. Instead of relying solely on the line chart, as is the current practice, we need to provide additional support for the inspection process, including extra views and instant visual feedback. This support can be offered through evaluations of the experts\u2019 actions and recommendations for the next steps. By doing so, we can help the experts more easily select spectral lines, adjust the redshift, and assess the reasonability of the redshift, i.e., whether the wavelengths of the spectral lines are appropriate.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "Astronomical Spectra", "data_code": {"clusters_and_sets_and_lists": 1, "tables": 1}}, "solution": [{"solution_text": "Verification module. The experts can evaluate the results using the verification module (DC3.2), which offers four methods. First, they can examine how well the current spectrum matches the template spectrum in the Spectrum View. Besides, the opacity of each spectral line in the Spectral Line View varies with how well it matches the SLCs. If the opacity of all selected spectral lines at the current redshift is high, the experts\u2019 decision is likely reasonable. Moreover, they can switch the Overall Importance View to show the overall importance of the selected spectral lines (Figure 6g1). A redshift within a high bar would suggest that they may have chosen the correct redshift. Finally, they can utilize the Local Spectrum View (Figure 6e). This view is tailored to meet the experts\u2019 needs, as they need to ascertain if the redshift corresponds to selected, artificial, or real but mislocated spectral lines by inspecting the local spectral profile of each line. Several design alternatives are available(Figure 7). We first establish that the local profile should be embedded within the Horizon Graphs rather than placed to the side, thus avoiding the need for experts to frequently switch focus. We then use the metaphor of lenses, a spatial selection that alters a base visualization [74], to show the profile through familiar line graphs. Two design alternatives are identified: the SignalLens like approach [72] and the ChronoLenses-like approach [73]. The SignalLens-like approach horizontally expands the focus area while compressing the context area by 1D distortion. The ChronoLenses-like approach retains the in-place context area and overlays the expanded focus area onto the context. After finally determining the redshift, the experts can identify the spectral class based on their selected spectral lines and cor_x0002_responding flux variation. They can later click on the icon in the upper right corner (Figure 6h) to record the class and complete the inspection. The pop-up output panel (Figure 8) displays the identified redshift, spectrum, selected spectral lines, and a cascade selector for the experts to choose the spectral class.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration;Filtering", "solution_compoent": "", "axial_code": ["Filtering", "Participation/Collaboration"], "componenet_code": ["filtering", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 320, "paper_title": "SpectrumVA: Visual Analysis of Astronomical Spectra for Facilitating Classification Inspection", "pub_year": 2023, "domain": "Spectral classification", "requirement": {"requirement_text": "DC4: Recommend similar spectra for subsequent inspection. Inspecting spectra individually and in isolation is inefficient. Based on the Juxtaposition-Similar-Nonsymmetrical priming effect [19], [20], i.e., people can indeed process the same or related stimuli in succession faster than they process different stimuli, it would be more effective to support experts in applying similar judgment criteria to multiple spectra consecutively. To accomplish this, we need to recommend spectra with patterns similar to the current one for further inspection.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "spectral lines", "data_code": {"sequential": 1, "media": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "After the experts inspect a spectrum, the system extracts their judgment criteria, i.e., the selected spectral lines and their importance, to search for other similar spectra (DC4). This ensures a seamless inspection process and minimizes disruptions to the experts\u2019 mental map. The results are visualized on the promotion interface (Figure 9), which is updated from the selection interface.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 322, "paper_title": "OW-Adapter: Human-Assisted Open-World Object Detection with a Few Examples", "pub_year": 2024, "domain": "Open world learning", "requirement": {"requirement_text": "R1.1: Cluster unknowns of interest. One dataset may have a large number of unknown objects. Users need an overview to understand the existing structure. The system should cluster unknown objects based on meaningful metrics such as image features.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "images", "data_code": {"media": 1, "tables": 1}}, "solution": [{"solution_text": "We first obtain the feature vector of each proposal box, which is later used for clustering. To this end, we get the feature maps of the images from an intermediate layer (the last layer before the classification head) of the pre-trained detector. For each proposal box, we compute the average of the features in the box from the feature map to obtain a high-dimensional feature representation (Figure 5-a). As a result, each proposal is represented as a high-dimensional vector. The feature representations are then projected into 2D latent vectors with UMAP [47] (Figure 5-b). Users can discover objects with similar semantic meanings in the latent space efficiently. For instance, objects of vehicle groups such as cars, trucks, buses, and motorbikes are close to each other in the latent space.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "We cluster the proposals based on their latent repre_x0002_sentation in the 2D space (Figure 5-c). Specifically, we use k-means clustering with adjustable cluster numbers to cluster the proposals. Note that we perform the clustering in the 2D space instead of the orig_x0002_inal high dimensional space for the following reasons. First, we would like to reduce the influence of the noise from the high-dimensional space following previous work [21, 48]. Second, we want to enforce the consistency of the visualization in the 2D space and the clustering results to reduce confusion.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "Recommended Clusters View (R1) This view summarizes the clusters of unknown objects (shown in Figure 1-a1) to provide an overview of the unknown objects. This view is serving as a starting point to let the users begin the exploration process of unknowns as discussed in R1.1. Moreover, to help users quickly identify potential unknown classes, the clusters ranked to visualize the important unknown classes on the top R1.2. The clusters are generated and ranked based on user-specified parameter settings, which can be adjusted in the control bar. The control bar includes a color legend and various control parameters, such as the minimum false positive rate, the sorting method, and the number of clusters. After adjusting the parameters, users can generate and rank a new set of recommended clusters by clicking on the submit button. We visualize the recommended clusters as a ranked list, such that users can identify important unknown classes on the top. Specifically, the recommended clusters are visualized row by row in the interface (Figure 7-a). Within each row, the left side of the interface presents the summary of the cluster including the total number of proposal boxes and the ranking metric. The stacked bar chart visualizes the number of unknowns misclassified as knowns (orange), the remaining unknowns (light orange), and knowns (green). On the right side of each row, we provide a set of samples to help users understand the types of misclassifications. Specifically, we group the unknown proposals based on the class they are misclassified as (e.g., a zebra misclassified as a horse). We then sort these groups based on the number of proposals within each group and visualize the top-3 groups. Within each group, we further sort the proposals based on their confidence scores and display a set of proposals with high confidence scores. This helps users to identify the most critical misclassifications and take corrective actions to improve the overall accuracy of their model. By identifying the most critical misclassifications and providing users with a highly visual representation of the recommended clusters, users can quickly identify potential unknown classes (R1). The in_x0002_teractivity of the interface further enhances the usability of this view, allowing users to customize their recommended clusters and explore them in real time.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Image", "axial_code": [], "componenet_code": ["image"]}]}, {"author": "dxf", "index_original": 323, "paper_title": "OW-Adapter: Human-Assisted Open-World Object Detection with a Few Examples", "pub_year": 2024, "domain": "Open world learning", "requirement": {"requirement_text": "R1.1: Cluster unknowns of interest. One dataset may have a large number of unknown objects. Users need an overview to understand the existing structure. The system should cluster unknown objects based on meaningful metrics such as image features.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "images", "data_code": {"media": 1, "tables": 1}}, "solution": [{"solution_text": "We first obtain the feature vector of each proposal box, which is later used for clustering. To this end, we get the feature maps of the images from an intermediate layer (the last layer before the classification head) of the pre-trained detector. For each proposal box, we compute the average of the features in the box from the feature map to obtain a high-dimensional feature representation (Figure 5-a). As a result, each proposal is represented as a high-dimensional vector. The feature representations are then projected into 2D latent vectors with UMAP [47] (Figure 5-b). Users can discover objects with similar semantic meanings in the latent space efficiently. For instance, objects of vehicle groups such as cars, trucks, buses, and motorbikes are close to each other in the latent space.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "We cluster the proposals based on their latent repre_x0002_sentation in the 2D space (Figure 5-c). Specifically, we use k-means clustering with adjustable cluster numbers to cluster the proposals. Note that we perform the clustering in the 2D space instead of the orig_x0002_inal high dimensional space for the following reasons. First, we would like to reduce the influence of the noise from the high-dimensional space following previous work [21, 48]. Second, we want to enforce the consistency of the visualization in the 2D space and the clustering results to reduce confusion.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "Projection View The projection view, illustrated in Figure 1-b, visualizes the 2D projection of proposals\u2019 feature vectors as a scatter plot. This view serves as a complementary view to the cluster list to help users understand the structure of the object embeddings (R1.1). The color of each dot indicates the proposal belongs to a known (green) or unknown (orange) object. Additionally, a lasso selection is provided, allowing users to manually select sub-clusters of interest.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Cluster", "axial_code": [], "componenet_code": ["point"]}]}, {"author": "dxf", "index_original": 325, "paper_title": "OW-Adapter: Human-Assisted Open-World Object Detection with a Few Examples", "pub_year": 2024, "domain": "Open world learning", "requirement": {"requirement_text": "R2.1: Recommend unknown labels. Going through all the unknown objects is time-consuming. Experts suggested that given the advance of vision-language models, it will greatly improve the efficiency if the system can recommend labels automatically.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "images", "data_code": {"media": 1, "tables": 1}}, "solution": [{"solution_text": "After users identify an unknown class from a cluster (Figure 6), we introduce two methods to recommend relevant patches (R2.1) to accelerate the annotation process as follows (Figure 6-a and 6-b). CLIP-based Recommendation We use a large vision-language model (e.g., CLIP [54]) to recommend proposals that are likely to belong to the target unknown class (Figure 6-a). Patch Feature-based Recommendation. After users annotated a set of seed unknown proposals, we recommend the proposals that are close to the annotated ones in the feature space (i.e., 2D projection space) as similar objects often share similar feature vectors. Specifically, we find the k-nearest-neighbors of each annotated proposal in the embedding space. Then we merge them and remove duplications.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "dxf", "index_original": 328, "paper_title": "OW-Adapter: Human-Assisted Open-World Object Detection with a Few Examples", "pub_year": 2024, "domain": "Open world learning", "requirement": {"requirement_text": "R2.3: Re-evaluate model on the fly. It is important to provide real-time feedback so users can know the effects of newly labeled objects on the performance of other classes.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "images", "data_code": {"media": 1, "tables": 1}}, "solution": [{"solution_text": "We first obtain the feature vector of each proposal box, which is later used for clustering. To this end, we get the feature maps of the images from an intermediate layer (the last layer before the classification head) of the pre-trained detector. For each proposal box, we compute the average of the features in the box from the feature map to obtain a high-dimensional feature representation (Figure 5-a). As a result, each proposal is represented as a high-dimensional vector. The feature representations are then projected into 2D latent vectors with UMAP [47] (Figure 5-b). Users can discover objects with similar semantic meanings in the latent space efficiently. For instance, objects of vehicle groups such as cars, trucks, buses, and motorbikes are close to each other in the latent space.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "We cluster the proposals based on their latent repre_x0002_sentation in the 2D space (Figure 5-c). Specifically, we use k-means clustering with adjustable cluster numbers to cluster the proposals. Note that we perform the clustering in the 2D space instead of the orig_x0002_inal high dimensional space for the following reasons. First, we would like to reduce the influence of the noise from the high-dimensional space following previous work [21, 48]. Second, we want to enforce the consistency of the visualization in the 2D space and the clustering results to reduce confusion.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "CLIP-based Recommendation We use a large vision-language model (e.g., CLIP [54]) to recommend proposals that are likely to belong to the target unknown class (Figure 6-a). Specifically, we first use the CLIP text encoder to generate the text embedding of the unknown class and use the CLIP image encoder to generate the image embeddings of the unknown proposals. Then, we calculate the cosine similarities between the text and image embeddings. We then sort the similarities and pick the top-k proposals with high similarities, where k is set to 50 by default.", "solution_category": "data_manipulation", "solution_axial": "Modeling;SimilarityCalculation", "solution_compoent": "", "axial_code": ["Modeling", "SimilarityCalculation"], "componenet_code": ["modeling", "similarity_calculation"]}, {"solution_text": "Patch Feature-based Recommendation After users annotated a set of seed unknown proposals, we recommend the proposals that are close to the annotated ones in the fea ture space (i.e., 2D projection space) as similar objects often share similar feature vectors. Specifically, we find the k-nearest-neighbors of each annotated proposal in the embedding space. Then we merge them and remove duplications. Users can select proposals from the recommendations and create annotations. Note that, we just let users annotate the labels of proposals without adjusting the bounding boxes to minimize their efforts. With the annotated examples, we in troduce a lightweight classifier and perform few-shot learning to classify the unknowns followin g [17]. We perform the few-shot learning because of the following reasons. First, to minimize  users\u2019 annotation efforts, we only ask them to annotate a few examples, which is not suf  ficient to fine-tune the entire neural network. Second, following the trend of parameter  -efficient fine-tuning, we train a lightweight adapter to reduce the computation cost an  d provide online feedback to users. Third, we want to adapt the detector to unknown clas  ses without affecting the performance of known classes. Specifical  ly, we train a multi-layer perceptron (MLP) which takes the featur  e vector of each proposal box as input and outputs the probabili  ties to each class. The training data used to train the MLP includes all the boxes that belong to the known classes and the newly annotated boxes of the unknown classes. As users only annotate a few examples for each unknown class, the number of annotations for the known and unknown classes is imbalanced. To perform the few-shot learning, we upsample the unknown annotations during training following [17]. After training, we plug the MLP into the pre-trained object detector to extend its capability in handling unknowns. In this case, we get the predictions of each proposal using both the original classification head and the MLP plugin. Then we classify the proposals as follows. If the MLP classify the proposal as one of the newly added unknown class then we predict it as the unknown class, otherwise, we use the prediction from the original classification head. As the training of the MLP is efficient (training time is less than 10 mins), users can iteratively edit or add annotations and train the MLP on the fly to improve the capability of the MLP as well as the detector", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "After selecting the list of annotated examples, the user can proceed to train the classifier (Section 4.4) and use the trained classifier to evaluate the detector. To provide online feedback to the user (R2.3), we evaluate the performance of the mlp on-the-fly. The evaluation results are shown in the model performance view (Figure 1-e), which provides the history of the performance changes. Users can enhance the model\u2019s performance and detect more unknown objects by selecting additional annotated examples or creating a new list of annotated examples iteratively.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "dxf", "index_original": 331, "paper_title": "VideoPro: A Visual Analytics Approach for Interactive Video Programming", "pub_year": 2024, "domain": "video exploration and analysis", "requirement": {"requirement_text": "R2: Summarize event temporal relationships with templates from multiple facets Given the large set of events in the video dataset, all the experts concurred that it is crucial to summarize event temporal relationships in videos with several compact templates and identify meaningful ones that can serve as labeling functions for video programming. Specifically, a template is a sequence of events shared by several videos, which can potentially help describe the semantics of the labels and define labeling functions for video programming. In addition, the experts expressed interest in exploring the templates from multiple facets, such as data coverage and model performance, to identify meaningful ones.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "video datasets", "data_code": {"sequential": 1, "media": 1, "tables": 1}}, "solution": [{"solution_text": "Given input raw videos, state-of-the-art CV algorithms are leveraged to extract pre-defined events, which vary based on domain-specific requirements and expert needs. For instance, in application scenarios focusing on human behaviors, events of interest may include body movements (e.g., jump and move right). These movements can be captured through analyzing position and angle changes of body parts based on heuristics and object detection models. Each extracted event is represented as a tuple (eventType, t_start, t_end), where eventType denotes the event type, and t_start and t_end are the timestamps of the start and end of the event.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Event sequential patterns, including the order and frequency of event occurrence, are crucial for comprehending and comparing video event sequences during programming. Considering the diversity and complex\\x02ity of event sequential patterns, we adopted a two-stage template mining algorithm (Fig. 2B) to efficiently extract event sequential patterns and characterize the labeling templates. The two-stage template mining algorithm allows for scalable and generalizable analysis of large-scale datasets of varying lengths and diverse event sequential patterns. The first frequent sequential pattern mining algorithm [67] provides a com\\x02prehensive dataset overview and avoids generating unwieldy templates that can be challenging for experts to interpret and define. It also allows users to add self-defined constraints on template compositions flexibly to accommodate their needs [81, 83]. The MinDL algorithm [14, 70] in the later stage further summarizes and distinguishes nuanced se\\x02quence differences within a template to facilitate detailed validation and refinement. After event extraction, each video can be construed as an event se\\x02quence, denoted as an ordered event list S = [e1, e2,....em] where ei belongs to the event set E. The video dataset as a whole can then be ex\\x02pressed as S = [S1,S2,...Sn], where n signifies the total number of video instances. A sequential pattern P = [e1, e2,...e|P| ] is a subsequence of some S \u2208 S if there exist an ordered |P|-tuple m = (m1,m2,...,m|P| ) such that S[mi ] = ei for each ei \u2208 P. For example, the sequential pattern P = [A,D] is a subsequence of S = [A,B,D,C,D] with two ordered 2-tuples (1,3) and (1,5). A sequential pattern is considered frequent if its occurrence exceeds a manually defined threshold. We first em\\x02ployed the seq2pat algorithm [67] to extract frequent sequential patterns from the video dataset S, which were then used as labeling templates T = [T1,T2,T3...]. This algorithm was chosen over other sequential pat\\x02tern mining techniques due to its scalability and efficiency. It utilizes  a multi-valued decision diagram structure [27] to compactly encode video sequences, enabling efficient computation for large volumes of sequences (e.g., thousands) in our scenario. Moreover, the algorithm is highly adaptable, allowing for flexible addition and revision of various constraints, such as sequential pattern length and continuity, based on user needs and task requirements. We then implemented the MinDL algorithm [14, 70] to further ana\\x02lyze sequence nuances within a template. This algorithm applies the minimum description length principle [23] to partition video sequence collections within the selected template into clusters and summarizes each cluster with the most \u201crepresentative\u201d sequential pattern, denoted as sub-template. Events belonging to the selected template are denoted as core events. Events within the sub-template that are not part of the selected template are called focus events, while events outside the sub\\x02template are referred to as context events (Fig. 2B-2). Every individual sequence in the cluster can be restored by editing the sub-template, including adding, deleting, or replacing events. The total description length equals the sum of the sequential pattern length and edit length, and the optimal clustering results are obtained by minimizing the total description length L(C). Here, C denotes the collection of video sequences in a template. s represents the individual video event sequence. The divided sequence clusters are denoted as C = {(P1,G1),(P2,G2),...,(Pn,Gn)} where Pi and Gi are the representative sequential pattern and sequence collection of the i th cluster. The parameters \u03b1 and \u03bb respectively control the information loss importance and the number of clusters. Based on our experiment results, we found that setting \u03b1 as 0.8 and \u03bb as 0 can yield a satisfactory summary for our dataset. We adopted a similar Locality Sensitive Hashing (LSH) strategy [14, 70] to speed up the computa\\x02tion. We also modified the original algorithm to adapt to our problem. Specifically, the computed representative sequential patterns of all clus\\x02ters must include the original template for effective understanding and comparison. The MinDL algorithm excels in partitioning sequences into meaningful clusters based on temporal similarity and identifying representative sequential patterns to provide an informative summary. This is particularly useful for users to compare and understand different video sequence clusters for further labeling template validation and refinement in our scenarios.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The Template View (Fig. 1A) summarizes the frequent and influential labeling templates in an organized table. It facilitates multi-faceted template exploration and comprehension (R1, R2). The first column in the Template View records the template name, which indicates the summarized event sequential patterns. The second column uses a stacked bar chart to encode the class distribution of labeled video instances included in the corresponding template. The length of the bar chart encodes the video instance number, while the color encodes the class type. Hovering over the bars of different colors shows each class\u2019s exact number of labeled video instances, providing a clear understanding of the class distribution within the template. The bar charts will be updated after each labeling round. Newly labeled instances are visually distinguished from previously labeled ones using the corresponding class color and a check texture. The third and fourth columns respectively display the overall prediction accuracy of labeled video instances and the number of unlabeled instances within the template, which will also be updated after each labeling round. A control panel on the top of the template table offers multiple in_x0002_teraction options, where users can choose to aggregate templates in different ways, including by prefix, by degree (i.e., template length), and by set (i.e., event collections in template). By default, templates are aggregated by prefix. Users can expand templates for further explo_x0002_ration by clicking the \u201c;\u201d symbol. Users can customize the Template View based on their specific needs by setting frequency and degree threshold to filter templates. They can also sort the templates by multiple predefined metrics, including overall prediction accuracy, unlabeled video instance number, and label purity in ascending or descending order. In addition, users can manually input and search for templates based on their domain knowledge in the search box above the table.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Matrix;Bar", "axial_code": [], "componenet_code": ["bar", "matrix"]}]}, {"author": "dxf", "index_original": 332, "paper_title": "VideoPro: A Visual Analytics Approach for Interactive Video Programming", "pub_year": 2024, "domain": "video exploration and analysis", "requirement": {"requirement_text": "R3: Support efficient and scalable template-guided video data programming The experts expected the system to support interactive validation and refinement of templates to achieve efficient and scalable video programming. They pointed out that comprehending the semantic implications of templates and verifying their correctness is crucial to ensuring high-quality labeling outcomes. Moreover, the system should allow experts to refine or manually compose templates based on their domain knowledge and new insights that emerge during the exploration process. Additionally, the system should automatically retrieve the most relevant videos for programming. This will allow users to apply selected and refined templates to program a large number of videos efficiently, as E5 commented, \u201cit would save much effort if we could apply the knowledge to a batch of videos simultaneously.\u201d", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "video datasets", "data_code": {"sequential": 1, "media": 1, "tables": 1}}, "solution": [{"solution_text": "Given input raw videos, state-of-the-art CV algorithms are leveraged to extract pre-defined events, which vary based on domain-specific requirements and expert needs. For instance, in application scenarios focusing on human behaviors, events of interest may include body movements (e.g., jump and move right). These movements can be captured through analyzing position and angle changes of body parts based on heuristics and object detection models. Each extracted event is represented as a tuple (eventType, t_start, t_end), where eventType denotes the event type, and t_start and t_end are the timestamps of the start and end of the event.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Event sequential patterns, including the order and frequency of event occurrence, are crucial for comprehending and comparing video event sequences during programming. Considering the diversity and complex\\x02ity of event sequential patterns, we adopted a two-stage template mining algorithm (Fig. 2B) to efficiently extract event sequential patterns and characterize the labeling templates. The two-stage template mining algorithm allows for scalable and generalizable analysis of large-scale datasets of varying lengths and diverse event sequential patterns. The first frequent sequential pattern mining algorithm [67] provides a com\\x02prehensive dataset overview and avoids generating unwieldy templates that can be challenging for experts to interpret and define. It also allows users to add self-defined constraints on template compositions flexibly to accommodate their needs [81, 83]. The MinDL algorithm [14, 70] in the later stage further summarizes and distinguishes nuanced se\\x02quence differences within a template to facilitate detailed validation and refinement. After event extraction, each video can be construed as an event se\\x02quence, denoted as an ordered event list S = [e1, e2,....em] where ei belongs to the event set E. The video dataset as a whole can then be ex\\x02pressed as S = [S1,S2,...Sn], where n signifies the total number of video instances. A sequential pattern P = [e1, e2,...e|P| ] is a subsequence of some S \u2208 S if there exist an ordered |P|-tuple m = (m1,m2,...,m|P| ) such that S[mi ] = ei for each ei \u2208 P. For example, the sequential pattern P = [A,D] is a subsequence of S = [A,B,D,C,D] with two ordered 2-tuples (1,3) and (1,5). A sequential pattern is considered frequent if its occurrence exceeds a manually defined threshold. We first em\\x02ployed the seq2pat algorithm [67] to extract frequent sequential patterns from the video dataset S, which were then used as labeling templates T = [T1,T2,T3...]. This algorithm was chosen over other sequential pat\\x02tern mining techniques due to its scalability and efficiency. It utilizes  a multi-valued decision diagram structure [27] to compactly encode video sequences, enabling efficient computation for large volumes of sequences (e.g., thousands) in our scenario. Moreover, the algorithm is highly adaptable, allowing for flexible addition and revision of various constraints, such as sequential pattern length and continuity, based on user needs and task requirements. We then implemented the MinDL algorithm [14, 70] to further ana\\x02lyze sequence nuances within a template. This algorithm applies the minimum description length principle [23] to partition video sequence collections within the selected template into clusters and summarizes each cluster with the most \u201crepresentative\u201d sequential pattern, denoted as sub-template. Events belonging to the selected template are denoted as core events. Events within the sub-template that are not part of the selected template are called focus events, while events outside the sub\\x02template are referred to as context events (Fig. 2B-2). Every individual sequence in the cluster can be restored by editing the sub-template, including adding, deleting, or replacing events. The total description length equals the sum of the sequential pattern length and edit length, and the optimal clustering results are obtained by minimizing the total description length L(C). Here, C denotes the collection of video sequences in a template. s represents the individual video event sequence. The divided sequence clusters are denoted as C = {(P1,G1),(P2,G2),...,(Pn,Gn)} where Pi and Gi are the representative sequential pattern and sequence collection of the i th cluster. The parameters \u03b1 and \u03bb respectively control the information loss importance and the number of clusters. Based on our experiment results, we found that setting \u03b1 as 0.8 and \u03bb as 0 can yield a satisfactory summary for our dataset. We adopted a similar Locality Sensitive Hashing (LSH) strategy [14, 70] to speed up the computa\\x02tion. We also modified the original algorithm to adapt to our problem. Specifically, the computed representative sequential patterns of all clus\\x02ters must include the original template for effective understanding and comparison. The MinDL algorithm excels in partitioning sequences into meaningful clusters based on temporal similarity and identifying representative sequential patterns to provide an informative summary. This is particularly useful for users to compare and understand different video sequence clusters for further labeling template validation and refinement in our scenarios.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Upon selecting a template in the Template View, users can validate and refine the selected template, as well as examine the videos that match the template for scalable labeling in the Labeling View (R1, R3). The upper part of the view (Fig. 1B) consists of three parts from left to right: the summary figures, the cluster heatmaps, and the connected Sankey diagrams. The summary figures (Fig. 1B-1 and Fig. 3A), inspired by the periphery plots [48], provide an overview of the tempo_x0002_ral event distributions within the corresponding clusters. The middle stacked line charts depict the aggregated temporal distribution of the sub-template events across the entire video clusters, while the histograms on either side illustrate the frequency of context events occur_x0002_ring before and after the subtemplate events. This design enables users to compare the event temporal distribution of sub-templates and observe the differences in contextual events between and within clusters. The middle cluster heatmaps show the temporal distribution of the labeled videos belonging to the clusters. Each row represents an indi_x0002_vidual video sequence, and each grid represents a fixed time interval(Fig. 1B-2). For example, if one video is 10 seconds long and there are 10 grids, then each grid represents 1 second time interval. To facilitate cross-video temporal comparisons, the time duration of all video sequences is normalized so that they contain the same number of grids. Videos belonging to the same cluster are vertically stacked together, with larger clusters having larger heights. The color of each grid indicates the types of events occurring during the corresponding time interval, including core events from the selected template, the focus events in the sub-template, and other context events. Users can hover over the grid to inspect the specific event. Furthermore, a Sankey diagram-based design (Fig. 1B-(3-4)) is adopted to visualize the label distribution across different clusters. The colored bar at the end of each video sequence indicates its label class. Therefore, the height of the colored bars at the end of each cluster (Fig. 1B-3) reflects the number of video instances belonging to the corresponding class in the cluster. The rightmost colored rectangles (Fig. 1B-4) represent corresponding classes and are linked with their contained video instances (i.e., the colored bars) through flows of different widths. The width of the flows equals the bar height, thereby encoding the total number of video instances for each class. Hovering over a rectangle will highlight all associated flows. Additionally, users can click on each rectangle to stack videos of the same class together for efficient comparison. Additionally, users can select a group of videos by clicking on the corresponding colored bar.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Area;Matrix;Sankey", "axial_code": [], "componenet_code": ["area", "sankey", "matrix"]}]}, {"author": "dxf", "index_original": 334, "paper_title": "VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions", "pub_year": 2024, "domain": "Causal Analysis", "requirement": {"requirement_text": "T1: As a cause-outcome relationship might be spurious when additional confounders are involved (Fig. 1B), a visual analytic tool should guide users in reflecting causal relationships among variables, i.e., which covariates might simultaneously affect the preference for treatment and for outcome. It should also provide quantitative measurements and intuitive visualizations for human users to locate the most likely confounders.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "Lalonde dataset", "data_code": {"tables": 1}}, "solution": [{"solution_text": "The CONFOUNDER DASHBOARD interface in Fig. 2A enables users to set up cause/outcome variables and to locate the most likely confounders that are distorting the specified cause-outcome relationship (T1). The flexibility of the system allows users to tailor their investigations to their specific research interests. For example, if someone is curious about the effectiveness of the job training programs in boosting annual earnings, they can select take_training_program as the cause and yr1978_earning as the outcome. But if users are interested in the impact of marriage status on income, they can choose married as the cause and yr1978_earnings as the outcome. To guide users in reflecting the causal relationships among covariates with the cause and outcome, CONFOUNDER DASHBOARD provides a textual explanation of confounders\u2014\u201ca confounder is a third variable that influences both cause and outcome yet does not lie on a causal pathway between cause and outcome\u201d\u2014when hovering over the question mark next to confounder selection box in Fig. 2A. ", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "dxf", "index_original": 335, "paper_title": "VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions", "pub_year": 2024, "domain": "Causal Analysis", "requirement": {"requirement_text": "T2: The system should facilitate both manual (hypothesis-driven) and automatic (data-driven) subgroup discovery. Users should be able to define subgroups manually using one or multiple covariates. It should also incorporate algorithms to automatically search for subgroups that are internally homogeneous but differ from each other.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Lalonde dataset", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Our visual analytic system incorporates ML methods to automatically discover subgroups to be shown on the visualization interfaces. Several existing works have used tree-based methods for searching and esti_x0002_mating subgroup-level treatment effects [4, 69]. These methods start by recursively splitting the feature space until they have partitioned  data into a set of leaves (subgroups) L , each of which, l \u2208 L , con_x0002_tains a subset of data points. One might consider that the data points belonging to the same leaf, i \u2208 l, act as if they had come from a ran_x0002_domized experiment. Then a leaf-specific effect size \u03c4(l) is computed by comparing the difference of outcomes \u03c4(l) = \u00afYl,1 \u2212 \u00afYl,0, or learning a dose-response relation Yl = g\u03b8 (Xl). Suppose g\u03b8 takes a logistic re_x0002_gression form g\u03b8 (Xl) = 1/(1;e\u2212(\u03b20(l);\u03b21(l)Xl)), the coefficient \u03b21(l) could be used to represent the effect size \u03c4(l) for the leaf l. VISPUR exploits the state-of-the-art subgroup partition algorithm: propensity tree [4,69]. It combines decision tree techniques with propensity scores in causal inference. It partitions data based on features Z while using\nX as the target to ensure a balanced distribution of treated and control units within each subgroup. This approach helps address confounding and enables reliable estimation of causal effects within specific sub_x0002_groups. It is different from decision tree in a way that an estimated\neffect size is attached to each of the leaves rather than predicted values\nof treatment. We adopt the propensity tree algorithm because: (i) It is\nparticularly useful in observational studies, where we want to minimize\nconfounding bias due to variance in treatment propensity; (ii) It is able\nto handle a large size of features by automatically selecting the most\n\u201cimportant\u201d features to split on.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The SUBGROUP PARTITION interface in Fig.2B allows users to create subgroups using two methods\u2014MANUAL and AUTO\u2014based on a set of features4 (T2). Previous subgroup analysis systems have utilized attribute-value pairs to construct subgroups [16, 33, 47, 49]. VISPUR employs a similar design, enabling users to add or remove covariates and specify the corresponding cut points. As shown in Fig. 2B, two variables, black and yr1975_earning, are selected, multi-thumb sliders are utilized to determine the cut points. A histogram distribution is displayed above the sliders, providing users with a visual reference for selecting appropriate cut points. Alternatively, users can opt for the AUTO option, which uses algorithm-supported automated partitioning (ref. Section4.4). By specifying a few configurations, such as the expected number of subgroups and the minimum size of subgroups, users can easily obtain an algorithm-generated partition that mitigates within-subgroup confounding bias. When clicking the Submit button, subgroups based on the partition are generated in SUBGROUP VIEWER.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Table;Bar", "axial_code": [], "componenet_code": ["bar", "table"]}, {"solution_text": "The SUBGROUP PARTITION interface in Fig.2B allows users to create subgroups using two methods\u2014MANUAL and AUTO\u2014based on a set of features4 (T2). Previous subgroup analysis systems have utilized attribute-value pairs to construct subgroups [16, 33, 47, 49]. VISPUR employs a similar design, enabling users to add or remove covariates and specify the corresponding cut points. As shown in Fig. 2B, two variables, black and yr1975_earning, are selected, multi-thumb sliders are utilized to determine the cut points. A histogram distribution is displayed above the sliders, providing users with a visual reference for selecting appropriate cut points. Alternatively, users can opt for the AUTO option, which uses algorithm-supported automated partitioning (ref. Section4.4). By specifying a few configurations, such as the expected number of subgroups and the minimum size of subgroups, users can easily obtain an algorithm-generated partition that mitigates within-subgroup confounding bias. When clicking the Submit button, subgroups based on the partition are generated in SUBGROUP VIEWER.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "dxf", "index_original": 336, "paper_title": "VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions", "pub_year": 2024, "domain": "Causal Analysis", "requirement": {"requirement_text": "T3: The system should enable users to examine the heterogeneity of subgroups from a variety of aspects, such as feature properties, to what extent a subgroup is likely to take certain treatment actions (propensity), to what extent a subgroup is to take certain outcome scenarios (base effect), what are the subgroup-level associations, whether they are distorted because of nested confounding bias(causal effect).", "requirement_code": {"explain_differences": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "Lalonde dataset", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Our visual analytic system incorporates ML methods to automatically discover subgroups to be shown on the visualization interfaces. Several existing works have used tree-based methods for searching and esti_x0002_mating subgroup-level treatment effects [4, 69]. These methods start by recursively splitting the feature space until they have partitioned  data into a set of leaves (subgroups) L , each of which, l \u2208 L , con_x0002_tains a subset of data points. One might consider that the data points belonging to the same leaf, i \u2208 l, act as if they had come from a ran_x0002_domized experiment. Then a leaf-specific effect size \u03c4(l) is computed by comparing the difference of outcomes \u03c4(l) = \u00afYl,1 \u2212 \u00afYl,0, or learning a dose-response relation Yl = g\u03b8 (Xl). Suppose g\u03b8 takes a logistic re_x0002_gression form g\u03b8 (Xl) = 1/(1;e\u2212(\u03b20(l);\u03b21(l)Xl)), the coefficient \u03b21(l) could be used to represent the effect size \u03c4(l) for the leaf l. VISPUR exploits the state-of-the-art subgroup partition algorithm: propensity tree [4,69]. It combines decision tree techniques with propensity scores in causal inference. It partitions data based on features Z while using\nX as the target to ensure a balanced distribution of treated and control units within each subgroup. This approach helps address confounding and enables reliable estimation of causal effects within specific sub_x0002_groups. It is different from decision tree in a way that an estimated\neffect size is attached to each of the leaves rather than predicted values\nof treatment. We adopt the propensity tree algorithm because: (i) It is\nparticularly useful in observational studies, where we want to minimize\nconfounding bias due to variance in treatment propensity; (ii) It is able\nto handle a large size of features by automatically selecting the most\n\u201cimportant\u201d features to split on.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The SUBGROUP VIEWER interface in Fig.2C provides a comprehensive overview of subgroup patterns, enabling users to understand their heterogeneous characteristics (T3). Our design considers subgroup differences not only at the attribute level, but also at the causal behavioral patterns. To achieve this, we have created two views in this panel: Causality Space for exploring causal patterns and Covariate Space for analyzing attributes. To ensure a seamless user experience, these two spaces are coordinated with consistent color codings over subgroups. Users can select a subgroup in either space and the interactions will be reflected in both spaces, or hover over subgroups to examine more detailed information.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Table;Bar", "axial_code": [], "componenet_code": ["bar", "table"]}, {"solution_text": "The SUBGROUP VIEWER interface in Fig.2C provides a comprehensive overview of subgroup patterns, enabling users to understand their heterogeneous characteristics (T3). Our design considers subgroup differences not only at the attribute level, but also at the causal behavioral patterns. To achieve this, we have created two views in this panel: Causality Space for exploring causal patterns and Covariate Space for analyzing attributes. To ensure a seamless user experience, these two spaces are coordinated with consistent color codings over subgroups. Users can select a subgroup in either space and the interactions will be reflected in both spaces, or hover over subgroups to examine more detailed information.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "dxf", "index_original": 337, "paper_title": "VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions", "pub_year": 2024, "domain": "Causal Analysis", "requirement": {"requirement_text": "T4: The visual analytic system should enable users to understand why the aggregation of subgroups with different characteristics could lead to a paradoxical phenomenon. In particular, the two causal mechanisms \u2013 confounding bias, and subgroup heterogeneity \u2013 should be explained in a clear and intuitive way.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "Lalonde dataset", "data_code": {"tables": 1}}, "solution": [{"solution_text": "Our visual analytic system incorporates ML methods to automatically discover subgroups to be shown on the visualization interfaces. Several existing works have used tree-based methods for searching and esti_x0002_mating subgroup-level treatment effects [4, 69]. These methods start by recursively splitting the feature space until they have partitioned  data into a set of leaves (subgroups) L , each of which, l \u2208 L , con_x0002_tains a subset of data points. One might consider that the data points belonging to the same leaf, i \u2208 l, act as if they had come from a ran_x0002_domized experiment. Then a leaf-specific effect size \u03c4(l) is computed by comparing the difference of outcomes \u03c4(l) = \u00afYl,1 \u2212 \u00afYl,0, or learning a dose-response relation Yl = g\u03b8 (Xl). Suppose g\u03b8 takes a logistic re_x0002_gression form g\u03b8 (Xl) = 1/(1;e\u2212(\u03b20(l);\u03b21(l)Xl)), the coefficient \u03b21(l) could be used to represent the effect size \u03c4(l) for the leaf l. VISPUR exploits the state-of-the-art subgroup partition algorithm: propensity tree [4,70]. It combines decision tree techniques with propensity scores in causal inference. It partitions data based on features Z while using\nX as the target to ensure a balanced distribution of treated and control units within each subgroup. This approach helps address confounding and enables reliable estimation of causal effects within specific sub_x0002_groups. It is different from decision tree in a way that an estimated\neffect size is attached to each of the leaves rather than predicted values\nof treatment. We adopt the propensity tree algorithm because: (i) It is\nparticularly useful in observational studies, where we want to minimize\nconfounding bias due to variance in treatment propensity; (ii) It is able\nto handle a large size of features by automatically selecting the most\n\u201cimportant\u201d features to split on.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "To aid in understanding association conflicts, we design REASONING STORYBOARD, depicted in Fig. 2D, which complements SUBGROUP VIEWER by providing a narrative for the appearance of a conflicting/paradoxical phenomenon (T4). The diagram comprises three layers: subgroup, cause, and outcome. The subgroup node is a rectangle scaled to the size of the chosen subgroup (e.g., the number of participants). In the cause layer, multiple nodes depict possible treatments. For a dichotomous treatment, nodes are limited to treated and untreated; For a continuous treatment, the values are discretized into L (L = 4 in our demonstration) bins based on percentiles and ranked in a descending order. The height of a cause node represents the percentage of participants taking that action. The outcome layer is a vertical axis, where the top endpoint indicates the maximum value of the outcome and the bottom indicates the minimum. Pathways originate from the group node, traverse through relevant cause nodes, and terminate at a specific point along the outcome axis. The height of each pathway represents the proportion of participants who took a particular treatment action at each cause node. The endpoint of each pathway indicates the average outcome achieved (e.g., average earnings in 1978).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Sankey", "axial_code": [], "componenet_code": ["sankey"]}, {"solution_text": "To aid in understanding association conflicts, we design REASONING STORYBOARD, depicted in Fig. 2D, which complements SUBGROUP VIEWER by providing a narrative for the appearance of a conflicting/paradoxical phenomenon (T4). The diagram comprises three layers: subgroup, cause, and outcome. The subgroup node is a rectangle scaled to the size of the chosen subgroup (e.g., the number of participants). In the cause layer, multiple nodes depict possible treatments. For a dichotomous treatment, nodes are limited to treated and untreated; For a continuous treatment, the values are discretized into L (L = 4 in our demonstration) bins based on percentiles and ranked in a descending order. The height of a cause node represents the percentage of participants taking that action. The outcome layer is a vertical axis, where the top endpoint indicates the maximum value of the outcome and the bottom indicates the minimum. Pathways originate from the group node, traverse through relevant cause nodes, and terminate at a specific point along the outcome axis. The height of each pathway represents the proportion of participants who took a particular treatment action at each cause node. The endpoint of each pathway indicates the average outcome achieved (e.g., average earnings in 1978).", "solution_category": "interaction", "solution_axial": "Abstract/Elaborate", "solution_compoent": "", "axial_code": ["Abstract/Elaborate"], "componenet_code": ["abstract_elaborate"]}]}, {"author": "dxf", "index_original": 339, "paper_title": "HoopInSight: Analyzing and Comparing Basketball Shooting Performance Through Visualization", "pub_year": 2024, "domain": "Sports", "requirement": {"requirement_text": "DR1: Design explicit visual encodings that facilitate preattentive multivariate spatial comparison. This can be broken down into four sub-requirements. First, the locations from which shots are taken are often associated with players\u2019 skill sets and teams\u2019 game strategies. Therefore, we prioritize displaying areas where the number of shots has increased or decreased across scenarios (DR1a), as well as the specific increased/decreased quantities at each location (DR1b). Furthermore, the system should assist in detecting spatial patterns and trends in the data, which often signify a shift in team strategy or playing style (DR1c).  Finally, although changes in efficiency may be inconsistent across neighboring locations, they can provide insights into a player/team\u2019s shot-making ability, i.e., whether or not they improve their shooting performance from certain areas. Thus, the system also must visualize values and patterns of quantitative efficiency (DR1d).", "requirement_code": {"explain_differences": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "The fundamental unit of data we are exploring is a shot taken by a player. Each shot is either made or missed, and we can aggregate these shots temporally (e.g., minute, game, season) and spatially (coordinates on the court).", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The central Comparison View provides a visual comparison of the two displayed Shot Charts with a primary goal of communicating the differences (?) between the two selections, thus fulfilling design goals (DR1). As we sought to display the changes in both frequency and efficiency, we chose to compare the cells from the Cell shot encoding, and thus the Comparison View uses the same cell units. For each corresponding comparison cell, we use the data for S1 (D1) and the data for S2 (D2) to calculate the ? (data for the comparison cell). The resulting ? has three main attributes: its index on the court ?(i, j), the difference in frequency ?F , and the difference in efficiency ?E. ?(i, j) translates to the [x, y] position on the court, and the ?E and ?F are two quantitative variables we encode for each cell.", "solution_category": "visualization", "solution_axial": "mirror", "solution_compoent": "Others", "axial_code": [], "componenet_code": ["others"]}]}, {"author": "dxf", "index_original": 340, "paper_title": "HoopInSight: Analyzing and Comparing Basketball Shooting Performance Through Visualization", "pub_year": 2024, "domain": "Sports", "requirement": {"requirement_text": "DR1: Design explicit visual encodings that facilitate preattentive multivariate spatial comparison. This can be broken down into four sub-requirements. First, the locations from which shots are taken are often associated with players\u2019 skill sets and teams\u2019 game strategies. Therefore, we prioritize displaying areas where the number of shots has increased or decreased across scenarios (DR1a), as well as the specific increased/decreased quantities at each location (DR1b). Furthermore, the system should assist in detecting spatial patterns and trends in the data, which often signify a shift in team strategy or playing style (DR1c).  Finally, although changes in efficiency may be inconsistent across neighboring locations, they can provide insights into a player/team\u2019s shot-making ability, i.e., whether or not they improve their shooting performance from certain areas. Thus, the system also must visualize values and patterns of quantitative efficiency (DR1d).", "requirement_code": {"explain_differences": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "The fundamental unit of data we are exploring is a shot taken by a player. Each shot is either made or missed, and we can aggregate these shots temporally (e.g., minute, game, season) and spatially (coordinates on the court).", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We merged and transformed the collected data into shot datasets grouped by players, teams, and seasons. Each data item (an individual shot event) contains 31 attributes, including the court coordinates (spatial), the result (numeric), the type (categorical) of each shot, team rotation data (textual), a video link connecting to the specific shot, and metadata. The data is updated on a daily basis. We also fetched supplemental data, such as player/team information.This data pipeline(retrieval, aggregation, and partition) helps us fulfill DR2a.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "We presented these candidates (Figure 3 & Figure 4b) again to our local group and gathered second-round feedback. One design (Figure 3A) used one consolidated view with two different shapes: circles denoted cells where frequency increased from Shot Chart 1 to Shot Chart 2, and diamonds denoted cells where the frequency decreased. The size of the glyph denoted the amount of frequency change and its color represented the change in efficiency between the two scenarios. A second design placed a four-quadrant axis on each cell, with a sector\u2019s appearance indicating one of the four results (increasing decreasing \u00d7 ?E/?F ) and the radius indicating the quantity. With both of these designs, viewers felt that each glyph was comprehensible, but it was relatively difficult to discern trends and patterns across the entire view of glyphs showing all cells (DR1c).", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Circle", "axial_code": [], "componenet_code": ["circle"]}]}, {"author": "dxf", "index_original": 341, "paper_title": "HoopInSight: Analyzing and Comparing Basketball Shooting Performance Through Visualization", "pub_year": 2024, "domain": "Sports", "requirement": {"requirement_text": "DR1: Design explicit visual encodings that facilitate preattentive multivariate spatial comparison. This can be broken down into four sub-requirements. First, the locations from which shots are taken are often associated with players\u2019 skill sets and teams\u2019 game strategies. Therefore, we prioritize displaying areas where the number of shots has increased or decreased across scenarios (DR1a), as well as the specific increased/decreased quantities at each location (DR1b). Furthermore, the system should assist in detecting spatial patterns and trends in the data, which often signify a shift in team strategy or playing style (DR1c).  Finally, although changes in efficiency may be inconsistent across neighboring locations, they can provide insights into a player/team\u2019s shot-making ability, i.e., whether or not they improve their shooting performance from certain areas. Thus, the system also must visualize values and patterns of quantitative efficiency (DR1d).", "requirement_code": {"explain_differences": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "The fundamental unit of data we are exploring is a shot taken by a player. Each shot is either made or missed, and we can aggregate these shots temporally (e.g., minute, game, season) and spatially (coordinates on the court).", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We merged and transformed the collected data into shot datasets grouped by players, teams, and seasons. Each data item (an individual shot event) contains 31 attributes, including the court coordinates (spatial), the result (numeric), the type (categorical) of each shot, team rotation data (textual), a video link connecting to the specific shot, and metadata. The data is updated on a daily basis. We also fetched supplemental data, such as player/team information.This data pipeline(retrieval, aggregation, and partition) helps us fulfill DR2a.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Both designs also employ the idea of two sub-views, as was done in the earlier alternative designs shown in Figure 3C&D above. The top sub-view indicates all cells where the shot frequency increased from the first to the second selection, while the bottom sub-view indicates cells where the shot frequency decreased. Additionally, triangles in the upper Arrowhead sub-view (frequency increase from Selection View 1 to 2) are drawn pointing upwards, while those in the lower sub-view (frequency decrease) point downwards. Similarly, lines in the upper Needle sub-view slope to the upper-right, while those in the lower sub-view slope to the upper-left. We feel that this separated view makes the frequency and efficiency differences between the two periods more pre-attentively clear, enabling rapid determination of areas of shot increases (glyph locations on the court in the upper view) versus shot decreases (glyph locations in the bottom view), fulfilling one of our primary design goals (DR1a).", "solution_category": "visualization", "solution_axial": "mirror", "solution_compoent": "Others", "axial_code": [], "componenet_code": ["others"]}]}, {"author": "dxf", "index_original": 342, "paper_title": "HoopInSight: Analyzing and Comparing Basketball Shooting Performance Through Visualization", "pub_year": 2024, "domain": "Sports", "requirement": {"requirement_text": "DR2:  Enable users to create different comparison scenarios. We can model the different types of comparisons via a comparison cross-table (entity\u00d7scenario). In this context, entity refers to players, teams, and opponents, while scenario involves lineups, shot types, time windows, and other variables. This can be simply described as \u201ccomparing entities under different scenarios.\u201d For instance, one could compare Michael Jordan\u2019s (entity) shooting performance with and without Scottie Pippen on the court (lineup scenarios), or compare Jordan\u2019s (entity) shooting pattern in his early career vs. later years (temporal scenarios). This design requirement can be divided into two sub-requirements: 1) obtain necessary data and structure the data to allow such filtering activities (DR2a); 2) design interactions to afford different entity selection and comparison scenario creation (DR2b).", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The fundamental unit of data we are exploring is a shot taken by a player. Each shot is either made or missed, and we can aggregate these shots temporally (e.g., minute, game, season) and spatially (coordinates on the court).", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We merged and transformed the collected data into shot datasets grouped by players, teams, and seasons. Each data item (an individual shot event) contains 31 attributes, including the court coordinates (spatial), the result (numeric), the type (categorical) of each shot, team rotation data (textual), a video link connecting to the specific shot, and metadata. The data is updated on a daily basis. We also fetched supplemental data, such as player/team information.This data pipeline(retrieval, aggregation, and partition) helps us fulfill DR2a.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "dxf", "index_original": 343, "paper_title": "HoopInSight: Analyzing and Comparing Basketball Shooting Performance Through Visualization", "pub_year": 2024, "domain": "Sports", "requirement": {"requirement_text": "DR2:  Enable users to create different comparison scenarios. We can model the different types of comparisons via a comparison cross-table (entity\u00d7scenario). In this context, entity refers to players, teams, and opponents, while scenario involves lineups, shot types, time windows, and other variables. This can be simply described as \u201ccomparing entities under different scenarios.\u201d For instance, one could compare Michael Jordan\u2019s (entity) shooting performance with and without Scottie Pippen on the court (lineup scenarios), or compare Jordan\u2019s (entity) shooting pattern in his early career vs. later years (temporal scenarios). This design requirement can be divided into two sub-requirements: 1) obtain necessary data and structure the data to allow such filtering activities (DR2a); 2) design interactions to afford different entity selection and comparison scenario creation (DR2b).", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The fundamental unit of data we are exploring is a shot taken by a player. Each shot is either made or missed, and we can aggregate these shots temporally (e.g., minute, game, season) and spatially (coordinates on the court).", "data_code": {"tables": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Within the system, users can select to examine a particular NBA player\u2019s or team\u2019s data and which season to display. We also support a special third focus entity (Opponents) that aggregates all opponents\u2019shooting data from games played against the selected team. This is a novel addition that enables users to analyze a team\u2019s defensive capabilities from the \u201cshots allowed\u201d perspective. Together, these capabilities partially fulfill DR2b (i.e., entity selection).", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "dxf", "index_original": 345, "paper_title": "LiberRoad: Probing into the Journey of Chinese Classics through Visual Analytics", "pub_year": 2024, "domain": "Historical", "requirement": {"requirement_text": "T1: Providing an overview. An overview of the basic statistics about the data is appreciated. Domain experts are interested in the overall collection of classics at the Imperial Household Agency, namely the number of books, the proportion of different book categories, and their printing time (P1, P4). The distribution of the libraries and the time these classics arrived in Japan is important as well.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Our domain collaborators manually collated the transmission events of 64 books collected in the Syoryobu Department by carefully examining inscriptions, postscripts, and bookplates within the printed versions of Song-Yuan editions of Chinese books, combined with relevant historical knowledge. As is shown in Figure 2, each book contains basic information such as the title, category, author, version, place and time of printing. At the same time, each book includes a sequence of transmission events, with each event including the order, time, location, book repository, intermediary, and printed materials used to infer the attributes of the transmission event. Since many records in the books are incomplete and much historical information is difficult to verify or investigate due to the distant period, even with our collaborator possessing profound pro_x0002_fessional knowledge, accurate attributes of many transmission events cannot be inferred, resulting in significant uncertainty in the data.", "data_code": {"geometry": 1, "temporal": 1, "tables": 1}}, "solution": [{"solution_text": "Nodes on the extracted tree can be classified into\nthree categories: region nodes, certain library nodes, and uncertain\nlibrary nodes. The region nodes refer to the non-leaf nodes (dashed cir_x0002_cles in Figure 4c), representing the known administrative regions. The\ncertain library nodes refer to the leaf nodes without non-leaf siblings,\nrepresenting libraries with known locations. In contrast, the uncertain\nlibrary nodes refer to the leaf nodes with non-leaf siblings, representing\nlibraries with unknown locations. The region nodes and certain library\nnodes are positioned by circular packing, and the uncertain library\nnodes are placed on the border of their parent nodes (Figure 4d). We\nrefer to the region nodes and the certain library nodes as inner nodes,\nand the uncertain library nodes as border nodes when introducing the\nlayout algorithm. Taking the hierarchical constraints, graph readability,\nand geographic similarity into consideration, the node layout is divided\ninto two stages, which is illustrated in Figure 5. In the first stage, the\napproximate positions of inner nodes are determined, with the goal of approximating geographic positions while maintaining node compact_x0002_ness. In the second stage, the positions of border nodes are placed with\nthe goal of reducing path length. ", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The Location Graph view is a novel visualization method proposed in this work. This view aims to provide an uncertain-aware overview of the institutional-level movement (T1), which is powerful in revealing movement patterns such as the spatial concentration of multiple books into a collecting institution (T4). The Location Graph provides an overview of spatial movement between institutions and locations. Due to the spatial uncertainty and disparity in the area of regions, it is non-trivial to visualize the institutional level movement while preserving the overall context. On a map, it is necessary to zoom in extensively to see specific institutions. The locations of many historical collecting institutions are unknown, making it difficult to place them on the map in a way that produces little misinformation. While node-link diagrams directly show the nodes and edges, institutions in the same location would be dispersed in different positions on the screen, making it difficult to associate the institutions with geospatial relations. In the context of geographical information, scholars prioritize the concept of regions over real spatial distance. As a result, we abstract the movement between discrete locations into a graph, on which the locations are packed into hierarchical circles. The positions of these circles maintain the relative geographical directions. The institutions with uncertain accurate locations are placed on the border of the most precise known area. f each trajectory include ancient locations, modern locations, and\nholding libraries. Due to changes in ancient administrative divisions\nand difficulties in standardizing their expression, we established the\nhierarchical structure based on modern locations. Figure 4a shows the\nideal hierarchy, where the libraries are attached to the cities where they\nare located. However, the inferred locations have different levels of\nuncertainty (Figure 4b). For example, for some libraries, we know the\nprefecture but don\u2019t know the exact city, while for some other libraries,\nthe prefecture is unknown as well. Figure 4c shows the reorganized\nhierarchy according to the known spatial information. Nodes with\nhigher uncertainty have lower depth.\nPositioning nodes. Nodes on the extracted tree can be classified into\nthree categories: region nodes, certain library nodes, and uncertain\nlibrary nodes. The region nodes refer to the non-leaf nodes (dashed cir_x0002_cles in Figure 4c), representing the known administrative regions. The\ncertain library nodes refer to the leaf nodes without non-leaf siblings,\nrepresenting libraries with known locations. In contrast, the uncertain\nlibrary nodes refer to the leaf nodes with non-leaf siblings, representing\nlibraries with unknown locations. The region nodes and certain library\nnodes are positioned by circular packing, and the uncertain library\nnodes are placed on the border of their parent nodes (Figure 4d). We\nrefer to the region nodes and the certain library nodes as inner nodes,\nand the uncertain library nodes as border nodes when introducing the\nlayout algorithm. Taking the hierarchical constraints, graph readability,\nand geographic similarity into consideration, the node layout is divided\ninto two stages, which is illustrated in Figure 5. In the first stage, the\napproximate positions of inner nodes are determined, with the goal of", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Circle;Line", "axial_code": [], "componenet_code": ["line", "circle"]}]}, {"author": "dxf", "index_original": 348, "paper_title": "LiberRoad: Probing into the Journey of Chinese Classics through Visual Analytics", "pub_year": 2024, "domain": "Historical", "requirement": {"requirement_text": "T2: Inspecting single book circulation. All interviewers emphasized the requirement of inspecting the detailed trajectory of the book they are interested in. Each movement is of great importance. Experts will investigate the starting point, when and where it is collected, as well as the libraries and collectors it encountered along the journey. Presenting the details would \u201cbenefit the general humanities scholars studying literature or history\u201d (P5) as well, supplementing their studies.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "Our domain collaborators manually collated the transmission events of 64 books collected in the Syoryobu Department by carefully examining inscriptions, postscripts, and bookplates within the printed versions of Song-Yuan editions of Chinese books, combined with relevant historical knowledge. As is shown in Figure 5, each book contains basic information such as the title, category, author, version, place and time of printing. At the same time, each book includes a sequence of transmission events, with each event including the order, time, location, book repository, intermediary, and printed materials used to infer the attributes of the transmission event. Since many records in the books are incomplete and much historical information is difficult to verify or investigate due to the distant period, even with our collaborator possessing profound pro_x0002_fessional knowledge, accurate attributes of many transmission events cannot be inferred, resulting in significant uncertainty in the data.", "data_code": {"tables": 1, "temporal": 1}}, "solution": [{"solution_text": "The interactable visual elements, including editions, event nodes, institution nodes, locations, collectors, and paths, support focus on different facets (T3). The LiberRoad system provides three types of actions, namely selection, filtering, and transforming, allowing users to change their focus and perspectives flexibly. Selection targets entities in the view, including six types: books, editions, collectors, locations, institutions, and paths. Clicking multiple library nodes results in the selection of a path, and the trajectories of the books which have gone through the path will be highlighted. Filtering targets a set of related books. There are five types of filters: book type, collector type, institution type, time range, and special events. The special events include a book\u2019s printing,\nthe last known location in China (which probably indicates where the Japanese obtained the book, reflecting their search strategies), and the first arrival in Japan (which indicates the main cohort that brought Chinese classics to Japan). Transforming allows users to change their perspectives and results in a smooth transition from the current view to the desired view. Among the three layouts, the Event Timeline has the lowest level of aggregation, followed by the Location Graph (aggregated by institutions), and the Geomap (by geographical location). When switching from a higher-level aggregated view to a lower-level one, the elements split and move to their corresponding positions.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The Event Timeline view presents the detailed temporal circulation sequence of each book (T2), which is enhanced by interactive dynamic ranking to show temporal relations of movement events (T4). The timeline view emphasizes the chronological distribution and sequential arrangement of events pertaining to each classic circulation. In this view, each book is allocated a row, movement events are represented by circles on the timeline, and the degree of uncertainty is represented by bold lines.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Circle;Table;Text", "axial_code": [], "componenet_code": ["text", "table", "circle"]}, {"solution_text": "The Event Timeline view presents the detailed temporal circulation sequence of each book (T2), which is enhanced by interactive dynamic ranking to show temporal relations of movement events (T4). The timeline view emphasizes the chronological distribution and sequential arrangement of events pertaining to each classic circulation. In this view, each book is allocated a row, movement events are represented by circles on the timeline, and the degree of uncertainty is represented by bold lines.", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 349, "paper_title": "LiberRoad: Probing into the Journey of Chinese Classics through Visual Analytics", "pub_year": 2024, "domain": "Historical", "requirement": {"requirement_text": "T3: Searching with facets. P2 put forward the demand for complex retrieval, hoping to query data from multiple perspectives (book type, time, location, library, agent) and combined attributes (e.g., books move from one wanted library to another). Experts not only care about a few specific books, but also a group of books of the same category, the collection of a person, the collection by libraries of the same type, the circulation within a specific period of time, and so on.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Our domain collaborators manually collated the transmission events of 64 books collected in the Syoryobu Department by carefully examining inscriptions, postscripts, and bookplates within the printed versions of Song-Yuan editions of Chinese books, combined with relevant historical knowledge. As is shown in Figure 6, each book contains basic information such as the title, category, author, version, place and time of printing. At the same time, each book includes a sequence of transmission events, with each event including the order, time, location, book repository, intermediary, and printed materials used to infer the attributes of the transmission event. Since many records in the books are incomplete and much historical information is difficult to verify or investigate due to the distant period, even with our collaborator possessing profound pro_x0002_fessional knowledge, accurate attributes of many transmission events cannot be inferred, resulting in significant uncertainty in the data.", "data_code": {"geometry": 1, "temporal": 1, "tables": 1}}, "solution": [{"solution_text": "The interactable visual elements, including editions, event nodes, institution nodes, locations, collectors, and paths, support focus on different facets (T3). The LiberRoad system provides three types of actions, namely selection, filtering, and transforming, allowing users to change their focus and perspectives flexibly. Selection targets entities in the view, including six types: books, editions, collectors, locations, institutions, and paths. Clicking multiple library nodes results in the selection of a path, and the trajectories of the books which have gone through the path will be highlighted. Filtering targets a set of related books. There are five types of filters: book type, collector type, institution type, time range, and special events. The special events include a book\u2019s printing,\nthe last known location in China (which probably indicates where the Japanese obtained the book, reflecting their search strategies), and the first arrival in Japan (which indicates the main cohort that brought Chinese classics to Japan). Transforming allows users to change their perspectives and results in a smooth transition from the current view to the desired view. Among the three layouts, the Event Timeline has the lowest level of aggregation, followed by the Location Graph (aggregated by institutions), and the Geomap (by geographical location). When switching from a higher-level aggregated view to a lower-level one, the elements split and move to their corresponding positions.", "solution_category": "interaction", "solution_axial": "Selecting;Filtering", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}, {"solution_text": "The interactable visual elements, including editions, event nodes, institution nodes, locations, collectors, and paths, support focus on different facets (T3). The LiberRoad system provides three types of actions, namely selection, filtering, and transforming, allowing users to change their focus and perspectives flexibly. Selection targets entities in the view, including six types: books, editions, collectors, locations, institutions, and paths. Clicking multiple library nodes results in the selection of a path, and the trajectories of the books which have gone through the path will be highlighted. Filtering targets a set of related books. There are five types of filters: book type, collector type, institution type, time range, and special events. The special events include a book\u2019s printing,\nthe last known location in China (which probably indicates where the Japanese obtained the book, reflecting their search strategies), and the first arrival in Japan (which indicates the main cohort that brought Chinese classics to Japan). Transforming allows users to change their perspectives and results in a smooth transition from the current view to the desired view. Among the three layouts, the Event Timeline has the lowest level of aggregation, followed by the Location Graph (aggregated by institutions), and the Geomap (by geographical location). When switching from a higher-level aggregated view to a lower-level one, the elements split and move to their corresponding positions.", "solution_category": "interaction", "solution_axial": "Selecting;Filtering;Reconfigure", "solution_compoent": "", "axial_code": ["Selecting", "Reconfigure", "Filtering"], "componenet_code": ["selecting", "reconfigure", "filtering"]}]}, {"author": "dxf", "index_original": 350, "paper_title": "LiberRoad: Probing into the Journey of Chinese Classics through Visual Analytics", "pub_year": 2024, "domain": "Historical", "requirement": {"requirement_text": "T4: Identifying patterns. Visualization offers opportunities for uncovering the intricate intersection of time, location, and collectors (P2). In visualization, the intersecting relationships can be understood as patterns of trajectories [17]. When examining book circulation, the main focus is on compound patterns, including divergence, convergence, and propagation. Books may be \u201cconcentrated in one place during a certain period\u201d (P1), or they might disperse to various locations.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Our domain collaborators manually collated the transmission events of 64 books collected in the Syoryobu Department by carefully examining inscriptions, postscripts, and bookplates within the printed versions of Song-Yuan editions of Chinese books, combined with relevant historical knowledge. As is shown in Figure 7, each book contains basic information such as the title, category, author, version, place and time of printing. At the same time, each book includes a sequence of transmission events, with each event including the order, time, location, book repository, intermediary, and printed materials used to infer the attributes of the transmission event. Since many records in the books are incomplete and much historical information is difficult to verify or investigate due to the distant period, even with our collaborator possessing profound pro_x0002_fessional knowledge, accurate attributes of many transmission events cannot be inferred, resulting in significant uncertainty in the data.", "data_code": {"geometry": 1, "temporal": 1, "tables": 1}}, "solution": [{"solution_text": "The interactable visual elements, including editions, event nodes, institution nodes, locations, collectors, and paths, support focus on different facets (T3). The LiberRoad system provides three types of actions, namely selection, filtering, and transforming, allowing users to change their focus and perspectives flexibly. Selection targets entities in the view, including six types: books, editions, collectors, locations, institutions, and paths. Clicking multiple library nodes results in the selection of a path, and the trajectories of the books which have gone through the path will be highlighted. Filtering targets a set of related books. There are five types of filters: book type, collector type, institution type, time range, and special events. The special events include a book\u2019s printing,\nthe last known location in China (which probably indicates where the Japanese obtained the book, reflecting their search strategies), and the first arrival in Japan (which indicates the main cohort that brought Chinese classics to Japan). Transforming allows users to change their perspectives and results in a smooth transition from the current view to the desired view. Among the three layouts, the Event Timeline has the lowest level of aggregation, followed by the Location Graph (aggregated by institutions), and the Geomap (by geographical location). When switching from a higher-level aggregated view to a lower-level one, the elements split and move to their corresponding positions.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The Location Graph view is a novel visualization method proposed in this work. This view aims to provide an uncertain-aware overview of the institutional-level movement (T1), which is powerful in revealing movement patterns such as the spatial concentration of multiple books into a collecting institution (T4). The Location Graph provides an overview of spatial movement between institutions and locations. Due to the spatial uncertainty and disparity in the area of regions, it is non-trivial to visualize the institutional level movement while preserving the overall context. On a map, it is necessary to zoom in extensively to see specific institutions. The locations of many historical collecting institutions are unknown, making it difficult to place them on the map in a way that produces little misinformation. While node-link diagrams directly show the nodes and edges, institutions in the same location would be dispersed in different positions on the screen, making it difficult to associate the institutions with geospatial relations. In the context of geographical information, scholars prioritize the concept of regions over real spatial distance. As a result, we abstract the movement between discrete locations into a graph, on which the locations are packed into hierarchical circles. The positions of these circles maintain the relative geographical directions. The institutions with uncertain accurate locations are placed on the border of the most precise known area. f each trajectory include ancient locations, modern locations, and\nholding libraries. Due to changes in ancient administrative divisions\nand difficulties in standardizing their expression, we established the\nhierarchical structure based on modern locations. Figure 4a shows the\nideal hierarchy, where the libraries are attached to the cities where they\nare located. However, the inferred locations have different levels of\nuncertainty (Figure 4b). For example, for some libraries, we know the\nprefecture but don\u2019t know the exact city, while for some other libraries,\nthe prefecture is unknown as well. Figure 4c shows the reorganized\nhierarchy according to the known spatial information. Nodes with\nhigher uncertainty have lower depth.\nPositioning nodes. Nodes on the extracted tree can be classified into\nthree categories: region nodes, certain library nodes, and uncertain\nlibrary nodes. The region nodes refer to the non-leaf nodes (dashed cir_x0002_cles in Figure 4c), representing the known administrative regions. The\ncertain library nodes refer to the leaf nodes without non-leaf siblings,\nrepresenting libraries with known locations. In contrast, the uncertain\nlibrary nodes refer to the leaf nodes with non-leaf siblings, representing\nlibraries with unknown locations. The region nodes and certain library\nnodes are positioned by circular packing, and the uncertain library\nnodes are placed on the border of their parent nodes (Figure 4d). We\nrefer to the region nodes and the certain library nodes as inner nodes,\nand the uncertain library nodes as border nodes when introducing the\nlayout algorithm. Taking the hierarchical constraints, graph readability,\nand geographic similarity into consideration, the node layout is divided\ninto two stages, which is illustrated in Figure 5. In the first stage, the\napproximate positions of inner nodes are determined, with the goal of", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Circle;Line", "axial_code": [], "componenet_code": ["line", "circle"]}]}, {"author": "dxf", "index_original": 351, "paper_title": "LiberRoad: Probing into the Journey of Chinese Classics through Visual Analytics", "pub_year": 2024, "domain": "Historical", "requirement": {"requirement_text": "T4: Identifying patterns. Visualization offers opportunities for uncovering the intricate intersection of time, location, and collectors (P2). In visualization, the intersecting relationships can be understood as patterns of trajectories [17]. When examining book circulation, the main focus is on compound patterns, including divergence, convergence, and propagation. Books may be \u201cconcentrated in one place during a certain period\u201d (P1), or they might disperse to various locations.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Our domain collaborators manually collated the transmission events of 64 books collected in the Syoryobu Department by carefully examining inscriptions, postscripts, and bookplates within the printed versions of Song-Yuan editions of Chinese books, combined with relevant historical knowledge. As is shown in Figure 8, each book contains basic information such as the title, category, author, version, place and time of printing. At the same time, each book includes a sequence of transmission events, with each event including the order, time, location, book repository, intermediary, and printed materials used to infer the attributes of the transmission event. Since many records in the books are incomplete and much historical information is difficult to verify or investigate due to the distant period, even with our collaborator possessing profound pro_x0002_fessional knowledge, accurate attributes of many transmission events cannot be inferred, resulting in significant uncertainty in the data.", "data_code": {"geometry": 1, "temporal": 1, "tables": 1}}, "solution": [{"solution_text": "The speculated time of each event is obtained separately based on information such as the collection institution and is mostly semi-bounded or bounded. In fact, taking the constraints of the event order into consideration, we can narrow down the uncertain range, which is represented by l and h. \u2022 Sequential constraints. The basic constraint for the normalized bounds is that the preceding events must be earlier than the sub\\x02sequent events. Thus, l and h should satisfy li \u2264 lj ,hi \u2264 hj ,\u2200i < j. Also, l and h are bounded within the temporal scope, i.e, from 960 (the beginning of the Song Dynasty) to 1960 (the latest time that the Syoryobu Department introduced Song-Yuan editions). \u2022 Assumption on the uncertainty levels. We assume that the four types of uncertainty can be arranged in ascending order of uncertainty degree as follows: certain, semi-bounded, bounded, and unbounded. Though, intuitively, the semi-bounded uncertainty is more significant than the bounded uncertainty, domain experts suggested that the semi-bounded time can be set as the known bound since the real time is more likely to be close to the bound. \u2022 Iterative estimation. We calculate the time in the order of the uncer\\x02tainty level. The time of the certain type and the semi-bounded type is set by the known bound. If event ei has certain time, ti = Li ; If it is semi-bounded, ti = \\x1a li ;1, |li | < \u221e hi \u22121, |hi | < \u221e . Then, for bounded event e j , the compact bounds (lj ,hj) is updated and the estimated time is the midpoint of the interval: tj = (lj ; hj)/2. Lastly, we update the bounds of the unbounded event ek and distribute the unknown time points evenly in the interval. tk = lk\u2212m ; m m;n (hk;n \u2212lk\u2212m), where k \u2212m is the order of the earliest event among the consecutive unbounded events to which ek belongs, and k ;n is the last one. \u2022 Visual encoding. Bold line segments are used to encode uncertain times. When Li or Hi is infinity, the line is represented with dots, otherwise a solid line is used.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The Event Timeline view presents the detailed temporal circulation sequence of each book (T2), which is enhanced by interactive dynamic ranking to show temporal relations of movement events (T4). The timeline view emphasizes the chronological distribution and sequential arrangement of events pertaining to each classic circulation. In this view, each book is allocated a row, movement events are represented by circles on the timeline, and the degree of uncertainty is represented by bold lines.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Circle;Table;Text", "axial_code": [], "componenet_code": ["text", "table", "circle"]}, {"solution_text": "The Event Timeline view presents the detailed temporal circulation sequence of each book (T2), which is enhanced by interactive dynamic ranking to show temporal relations of movement events (T4). The timeline view emphasizes the chronological distribution and sequential arrangement of events pertaining to each classic circulation. In this view, each book is allocated a row, movement events are represented by circles on the timeline, and the degree of uncertainty is represented by bold lines.", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 353, "paper_title": "LiberRoad: Probing into the Journey of Chinese Classics through Visual Analytics", "pub_year": 2024, "domain": "Historical", "requirement": {"requirement_text": "T5: Comparing features. The academic characteristics of different periods and regions are reflected in the categories of books and the types of libraries that collected them. Comparison of the features of book movement in different time and space \u201cmight drive a more fine-grained temporal and regional division\u201d (P2) regarding cultural history.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Our domain collaborators manually collated the transmission events of 64 books collected in the Syoryobu Department by carefully examining inscriptions, postscripts, and bookplates within the printed versions of Song-Yuan editions of Chinese books, combined with relevant historical knowledge. As is shown in Figure 10, each book contains basic information such as the title, category, author, version, place and time of printing. At the same time, each book includes a sequence of transmission events, with each event including the order, time, location, book repository, intermediary, and printed materials used to infer the attributes of the transmission event. Since many records in the books are incomplete and much historical information is difficult to verify or investigate due to the distant period, even with our collaborator possessing profound pro_x0002_fessional knowledge, accurate attributes of many transmission events cannot be inferred, resulting in significant uncertainty in the data.", "data_code": {"geometry": 1, "temporal": 1, "tables": 1}}, "solution": [{"solution_text": "With further selection and filtering, users can compare different patterns to analyze the collection and cultural features of different time and space (T5). By filtering the time range, users can compare the features of the trajectories in different periods (T5).", "solution_category": "interaction", "solution_axial": "Selecting;Filtering", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}, {"solution_text": "With further selection and filtering, users can compare different patterns to analyze the collection and cultural features of different time and space (T5). By filtering the time range, users can compare the features of the trajectories in different periods (T5).", "solution_category": "interaction", "solution_axial": "Selecting;Filtering", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "dxf", "index_original": 354, "paper_title": "LiberRoad: Probing into the Journey of Chinese Classics through Visual Analytics", "pub_year": 2024, "domain": "Historical", "requirement": {"requirement_text": "T6: Presenting uncertainty and provenance. In view of the ambiguity of the source data, the visualization system should present where the uncertainty lies. Moreover, \u201cit\u2019s these unclear areas that are where to find problems and solve problems\u201d (P2, P3). At the same time, provenance information is required, including the original data attributes and page images from which the circulation event is extracted, in order to facilitate scholars\u2019 examination and correction.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "Our domain collaborators manually collated the transmission events of 64 books collected in the Syoryobu Department by carefully examining inscriptions, postscripts, and bookplates within the printed versions of Song-Yuan editions of Chinese books, combined with relevant historical knowledge. As is shown in Figure 11, each book contains basic information such as the title, category, author, version, place and time of printing. At the same time, each book includes a sequence of transmission events, with each event including the order, time, location, book repository, intermediary, and printed materials used to infer the attributes of the transmission event. Since many records in the books are incomplete and much historical information is difficult to verify or investigate due to the distant period, even with our collaborator possessing profound pro_x0002_fessional knowledge, accurate attributes of many transmission events cannot be inferred, resulting in significant uncertainty in the data.", "data_code": {"geometry": 1, "temporal": 1, "tables": 1}}, "solution": [{"solution_text": "The Geomap view equips users with an intuitive geospatial perspective, providing an overview of movement between geographical regions (T1) as well as detailed single book trajectories (T2). The interactable visual elements, including editions, event nodes, institution nodes, locations, collectors, and paths, sup_port focus on different facets (T3). The Geomap view provides both an overview of multiple trajectories\nand a detailed single trajectory view. A Mercator-projected map of\nEast Asia served as the base map. The movement and stop events are\nrepresented as circles and lines on the base map.\nMultiple book movement. In the overview, trajectories are ag_x0002_gregated to reduce clutter. Due to the missing and uncertainty in the\nepisodic data, we aggregate the location of the atomic events instead of\nusing compound features like the Origin-Destination (OD) movements\nor sub-paths. Spatial uncertainty makes trajectory clustering challeng_x0002_ing. Firstly, clustering locations with different scales of uncertainty\nmight cause misleading. In our data, all locations in book trajectories\nare described with administrative districts, where higher level admin_x0002_istrative districts are of wider range and mean huger uncertainty. An\nordinary method is to represent each location as a point on the capital\nor centroid of corresponding administrative districts. However, with\nlarge numbers of uncertain locations, this approach would mislead the\nusers to believe that trajectories are concentrated in the centroid.\nTo avoid misleading and perverse more accurate uncertainty in the\nclustering, we adopted a layered clustering process (Figure 6a). We\ndiscriminate uncertainty in each location according to the administrative\ndistrict\u2019s division and cluster locations of different levels separately.\nFrom the finer level to the coarse level, we start from a threshold\nlevel according to the screen sizes (city by default), where we think the\nuncertainty under such a level is precise enough under the map scale.\nWe use the FairPair algorithm [18] to take hierarchical clustering on\neach level. The clustered locations will be rendered with a blur filter at\ndifferent extents to indicate the scale of uncertainty. The second challenge is the labeling of clustered locations. Due to\nspace limitations, we select several representative place names. Direct\nmethods contain sampling the place names with the highest frequencies\nor just adopting the place name of the lowest common ancestor on the\nlocation tree. The result would suffer from co-exists of place names\nwith containment relationship, while the latter might give over a vague\ndescription of the cluster. We propose a dynamic programming-based\nalgorithm (Figure 6b) to find a balance between precision and cover_x0002_age while avoiding the coexistence of place names with containment\nrelationships. For all locations clustered in a group, the place names of\ntheir representative administrative districts form a forest. The problem\nis formulated as a tree knapsack problem [13], which can be solved in\nO(nL) time complexity. The multiple trajectory overview is rendered\nas a geo-network [44] after the location is cluttered. The size of nodes\nand the width of edges encode the number of stops and movements. We\nadopt the force-directed edge bundling [27] to decrease visual clutter.\nThe labels are placed with a force-directed label placement algorithm.\nSingle book movement. In the single trajectory view, users inspect\nthe geographical information on the map as well as data provenance on\nthe side panel. To allow users to view trajectory segments in different\nscales at one glance, the system automatically recommends magnifying\nlenses on the map to display fine local movements that are too small to\nsee on a uniformly scaled map. To implement lens recommendations,\nwe detect all point pairs that are too close to draw separately on the map.\nWe use a disjoint set to group these points into clusters, where points\nin the same cluster are expected to be accommodated by a lens. For\neach cluster, we compute the magnification of its lens to fit the extent\nof its points. The position of the lens is determined by searching for\nthe nearest available place on the map that is not occluded by countries\nrelated to our data.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction;Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping", "DimensionalityReduction"], "componenet_code": ["clustering_and_grouping", "dimensionality_reduction"]}, {"solution_text": "The speculated time of each event is obtained separately based on information such as the collection institution and is mostly semi-bounded or bounded. In fact, taking the constraints of the event order into consideration, we can narrow down the uncertain range, which is represented by l and h. \u2022 Sequential constraints. The basic constraint for the normalized bounds is that the preceding events must be earlier than the subsequent events. Thus,  l and h should satisfy li \u2264 lj ,hi \u2264 hj ,\u2200i < j. Also, l and h are bounded within the temporal scope, i.e, from 960 (the beginning of the Song Dynasty) to 1960 (the latest time that the Syoryobu Department introduced Song-Yuan editions). \u2022 Assumption on the uncertainty levels. We assume that the four types of uncertainty can be arranged in ascending order of uncertainty degree as follows: certain, semi-bounded, bounded, and unbounded. Though, intuitively, the semi-bounded uncertainty is more significant than the bounded uncertainty, domain experts suggested that the semi-bounded time can be set as the known bound since the real time is more likely to be close to the bound. \u2022 Iterative estimation. We calculate the time in the order of the uncertainty level. The time of the certain type and the semi-bounded type is set by the known bound. If event ei has certain time, ti = Li ; If it is semi-bounded, ti = li ;1, |li | < \u221e hi \u22121, |hi | < \u221e . Then, for bounded event e j , the compact bounds (lj ,hj) is updated and the estimated time is the midpoint of the interval: tj = (lj ; hj)/2. Lastly, we update the bounds of the unbounded event ek and distribute the unknown time points evenly in the interval. tk = lk\u2212m ; m m;n (hk;n \u2212lk\u2212m), where k \u2212m is the order of the earliest event among the consecutive unbounded events to which ek belongs, and k ;n is the last one. \u2022 Visual encoding. Bold line segments are used to encode uncertain times. When Li or Hi is infinity, the line is represented with dots, otherwise a solid line is used.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction;Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping", "DimensionalityReduction"], "componenet_code": ["clustering_and_grouping", "dimensionality_reduction"]}, {"solution_text": "The speculated time of each event is obtained separately based on information such as the collection institution and is mostly semi-bounded or bounded. In fact, taking the constraints of the event order into consideration, we can narrow down the uncertain range, which is represented by l and h. \u2022 Sequential constraints. The basic constraint for the normalized bounds is that the preceding events must be earlier than the subsequent events. Thus,  l and h should satisfy li \u2264 lj ,hi \u2264 hj ,\u2200i < j. Also, l and h are bounded within the temporal scope, i.e, from 960 (the beginning of the Song Dynasty) to 1960 (the latest time that the Syoryobu Department introduced Song-Yuan editions). \u2022 Assumption on the uncertainty levels. We assume that the four types of uncertainty can be arranged in ascending order of uncertainty degree as follows: certain, semi-bounded, bounded, and unbounded. Though, intuitively, the semi-bounded uncertainty is more significant than the bounded uncertainty, domain experts suggested that the semi-bounded time can be set as the known bound since the real time is more likely to be close to the bound. \u2022 Iterative estimation. We calculate the time in the order of the uncertainty level. The time of the certain type and the semi-bounded type is set by the known bound. If event ei has certain time, ti = Li ; If it is semi-bounded, ti = li ;1, |li | < \u221e hi \u22121, |hi | < \u221e . Then, for bounded event e j , the compact bounds (lj ,hj) is updated and the estimated time is the midpoint of the interval: tj = (lj ; hj)/2. Lastly, we update the bounds of the unbounded event ek and distribute the unknown time points evenly in the interval. tk = lk\u2212m ; m m;n (hk;n \u2212lk\u2212m), where k \u2212m is the order of the earliest event among the consecutive unbounded events to which ek belongs, and k ;n is the last one. \u2022 Visual encoding. Bold line segments are used to encode uncertain times. When Li or Hi is infinity, the line is represented with dots, otherwise a solid line is used.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Circle;Table;Text", "axial_code": [], "componenet_code": ["text", "table", "circle"]}, {"solution_text": "The speculated time of each event is obtained separately based on information such as the collection institution and is mostly semi-bounded or bounded. In fact, taking the constraints of the event order into consideration, we can narrow down the uncertain range, which is represented by l and h. \u2022 Sequential constraints. The basic constraint for the normalized bounds is that the preceding events must be earlier than the subsequent events. Thus,  l and h should satisfy li \u2264 lj ,hi \u2264 hj ,\u2200i < j. Also, l and h are bounded within the temporal scope, i.e, from 960 (the beginning of the Song Dynasty) to 1960 (the latest time that the Syoryobu Department introduced Song-Yuan editions). \u2022 Assumption on the uncertainty levels. We assume that the four types of uncertainty can be arranged in ascending order of uncertainty degree as follows: certain, semi-bounded, bounded, and unbounded. Though, intuitively, the semi-bounded uncertainty is more significant than the bounded uncertainty, domain experts suggested that the semi-bounded time can be set as the known bound since the real time is more likely to be close to the bound. \u2022 Iterative estimation. We calculate the time in the order of the uncertainty level. The time of the certain type and the semi-bounded type is set by the known bound. If event ei has certain time, ti = Li ; If it is semi-bounded, ti = li ;1, |li | < \u221e hi \u22121, |hi | < \u221e . Then, for bounded event e j , the compact bounds (lj ,hj) is updated and the estimated time is the midpoint of the interval: tj = (lj ; hj)/2. Lastly, we update the bounds of the unbounded event ek and distribute the unknown time points evenly in the interval. tk = lk\u2212m ; m m;n (hk;n \u2212lk\u2212m), where k \u2212m is the order of the earliest event among the consecutive unbounded events to which ek belongs, and k ;n is the last one. \u2022 Visual encoding. Bold line segments are used to encode uncertain times. When Li or Hi is infinity, the line is represented with dots, otherwise a solid line is used.", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 355, "paper_title": "Marjorie: Visualizing Type 1 Diabetes Data  to Support Pattern Exploration", "pub_year": 2024, "domain": "diabetes", "requirement": {"requirement_text": "R1: Make periods of low and high glucose easy to identify. As can be derived from Tab. 1, the primary focus in the analysis ses_x005f_x005f_x0002_sion are periods of blood glucose outside the target range, namely hypoglycemia, hyperglycemia, and high blood glucose fuctuation (al_x0002_ternating hypo- and hyperglycemia within short intervals). Swift identifcation of these segments in the dataset is crucial for the analyst.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The attributes we incorporated into our approach encompass blood glucose levels, carbohydrate consumption, and insulin (both bolus and basal insulin). Sleep, nutrition, and exercise data requested by the diabetologists (see R3) are not included due to the diffculty in obtaining such information", "data_code": {"tables": 1}}, "solution": [{"solution_text": "To generate the visualizations in the Marjorie frontend, various data processing steps are executed in the backend. The glucose data is aggre_x0002_gated by minute for the AGP, the bolus insulin event data is transformed into a continuous insulin activity series, and the carbohydrate and bolus insulin data is clustered to avoid overplotting. Additionally, for the Insights view, periods of meals and hypoglycemia are detected, which are later clustered into patterns. In the following, we provide details on the clustering of event entries and on pattern detection.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "By clicking on the Explore days in detail button located below the AGP, the size of the AGP is reduced, and a detailed visualization of the days used to calculate it is presented below (see Fig. 1). The data is partitioned into distinct days which are juxtaposed underneath each other, sharing the same time axis. The glucose data is represented using adapted horizon charts, where the baseline is not a single value, but instead the target range of 70 mg/dL to 180 mg/dL. The values outside the target range are displayed like in typical horizon charts, while the values within the range are shown as a green band. This makes it simple to pinpoint days and times that require closer examination, helping to achieve T1 (identify extreme values). Clicking the right arrow ( ) expands the graph to show a detailed line plot of the glucose data.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Area;Bar", "axial_code": [], "componenet_code": ["area", "bar"]}, {"solution_text": "By clicking on the Explore days in detail button located below the AGP, the size of the AGP is reduced, and a detailed visualization of the days used to calculate it is presented below (see Fig. 1). The data is partitioned into distinct days which are juxtaposed underneath each other, sharing the same time axis. The glucose data is represented using adapted horizon charts, where the baseline is not a single value, but instead the target range of 70 mg/dL to 180 mg/dL. The values outside the target range are displayed like in typical horizon charts, while the values within the range are shown as a green band. This makes it simple to pinpoint days and times that require closer examination, helping to achieve T1 (identify extreme values). Clicking the right arrow ( ) expands the graph to show a detailed line plot of the glucose data.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "dxf", "index_original": 356, "paper_title": "Marjorie: Visualizing Type 1 Diabetes Data  to Support Pattern Exploration", "pub_year": 2024, "domain": "diabetes", "requirement": {"requirement_text": "R2: Support finding recurring daily and weekly patterns. After segments of hypo- and hyperglycemia have been identifed, the diabetologist usually browses the AGP and daily detail views to make out seasonal patterns such as daily or weekly recurrences in the data (see Tab. 1). These patterns are easy to identify and effcient to eliminate, as they are usually caused by unrefected habits of the patients. D1: \u201cFor example, I had a patient who always had high blood sugar levels after lunch and gave a big amount of correction insulin every day to get back into the target range. So I suggested him to adjust the insulin factor for lunch and give higher doses right away.\u201d", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The attributes we incorporated into our approach encompass blood glucose levels, carbohydrate consumption, and insulin (both bolus and basal insulin). Sleep, nutrition, and exercise data requested by the diabetologists (see R3) are not included due to the diffculty in obtaining such information", "data_code": {"tables": 1}}, "solution": [{"solution_text": "To generate the visualizations in the Marjorie frontend, various data processing steps are executed in the backend. The glucose data is aggre_x0002_gated by minute for the AGP, the bolus insulin event data is transformed into a continuous insulin activity series, and the carbohydrate and bolus insulin data is clustered to avoid overplotting. Additionally, for the Insights view, periods of meals and hypoglycemia are detected, which are later clustered into patterns. In the following, we provide details on the clustering of event entries and on pattern detection.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The AGP view (Fig. 3) extends the original AGP visualization [9, 22]. It supports T2 (identify seasonal patterns) by letting the user explore hourly patterns. Like the original AGP, it aggregates the ranges between the 10th and 90th, and the 25th and 75th percentiles of two weeks of CGM data, supporting T2a (aggregate by temporal granularity). The aggregated data is displayed as superimposed area charts across a 24-To support hour period. On top, the 50th percentile is visualized as a line graph. T2b (identify extreme values in aggregated data), the AGP highlights areas that fall outside the target glucose range in different colors, according to the color scheme described above.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Area;Bar", "axial_code": [], "componenet_code": ["area", "bar"]}]}, {"author": "dxf", "index_original": 357, "paper_title": "Marjorie: Visualizing Type 1 Diabetes Data  to Support Pattern Exploration", "pub_year": 2024, "domain": "diabetes", "requirement": {"requirement_text": "R3: Visualize all variables in one place. Another requirement is to collect and visualize multivariate and het_x005f_x005f_x0002_erogenous datasets in one place. When asked about the limitations of current diabetes data analysis tools, the diabetologists stated that they require a comprehensive view of both glycemic data and additional data such as meal and insulin information, to better understand the connections between blood glucose fuctuations and potential causes. D1: \u201cIn the AGP there is only glucose data, and I have to click back and forth 10 times to understand why the patient, for example, always has high values in the morning. I always go into the daily views to see what he ate around that time.\u201d Moreover, the diabetologists expressed the wish to load more attributes into the tool. D2 stated: \u201cAdditional health data such as exercise or sleep would really help to understand the blood sugar\u2019s behavior and to advise patients how to handle specifc situations better.\u201d. D1 wished to see more detailed nutrition data.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The attributes we incorporated into our approach encompass blood glucose levels, carbohydrate consumption, and insulin (both bolus and basal insulin). Sleep, nutrition, and exercise data requested by the diabetologists (see R3) are not included due to the diffculty in obtaining such information", "data_code": {"tables": 1}}, "solution": [{"solution_text": "To generate the visualizations in the Marjorie frontend, various data processing steps are executed in the backend. The glucose data is aggre_x0002_gated by minute for the AGP, the bolus insulin event data is transformed into a continuous insulin activity series, and the carbohydrate and bolus insulin data is clustered to avoid overplotting. Additionally, for the Insights view, periods of meals and hypoglycemia are detected, which are later clustered into patterns. In the following, we provide details on the clustering of event entries and on pattern detection.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "We enhance the original AGP by adding carbohydrate and bolus in-sulin data, supporting T3 (compare multiple attributes), by adding three  vertically juxtaposed charts sharing the same time axis.  We use two  barcode plots for carbohydrate and insulin administration events, where  the position on the horizontal axis encodes the timing of the events,  and the opacity of the bars corresponds to the amount. This enables the  user to identify time periods during which the frequency and/or amount  of carbohydrate or insulin consumption was high. We chose barcode  plots because they use little screen space while conveying most of the  important information.  As a third chart, we added a qualitative line  chart visualization of the insulin activity in the bolus section to simplify  the search for correlations with the glucose data.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Area;Bar", "axial_code": [], "componenet_code": ["area", "bar"]}]}, {"author": "dxf", "index_original": 358, "paper_title": "Marjorie: Visualizing Type 1 Diabetes Data  to Support Pattern Exploration", "pub_year": 2024, "domain": "diabetes", "requirement": {"requirement_text": "R3: Visualize all variables in one place. Another requirement is to collect and visualize multivariate and het_x0002_erogenous datasets in one place. When asked about the limitations of current diabetes data analysis tools, the diabetologists stated that they require a comprehensive view of both glycemic data and additional data such as meal and insulin information, to better understand the connections between blood glucose fuctuations and potential causes. D1: \u201cIn the AGP there is only glucose data, and I have to click back and forth 10 times to understand why the patient, for example, always has high values in the morning. I always go into the daily views to see what he ate around that time.\u201d Moreover, the diabetologists expressed the wish to load more attributes into the tool. D2 stated: \u201cAdditional health data such as exercise or sleep would really help to understand the blood sugar\u2019s behavior and to advise patients how to handle specifc situations better.\u201d. D1 wished to see more detailed nutrition data.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The attributes we incorporated into our approach encompass blood glucose levels, carbohydrate consumption, and insulin (both bolus and basal insulin). Sleep, nutrition, and exercise data requested by the diabetologists (see R3) are not included due to the diffculty in obtaining such information", "data_code": {"tables": 1}}, "solution": [{"solution_text": "To generate the visualizations in the Marjorie frontend, various data processing steps are executed in the backend. The glucose data is aggre_x0002_gated by minute for the AGP, the bolus insulin event data is transformed into a continuous insulin activity series, and the carbohydrate and bolus insulin data is clustered to avoid overplotting. Additionally, for the Insights view, periods of meals and hypoglycemia are detected, which are later clustered into patterns. In the following, we provide details on the clustering of event entries and on pattern detection.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The carbohydrate and bolus insulin data are presented in a heatmap  below the glucose data, addressing T3 (compare multiple attributes).  It often happens that event entries occur within short time frames, for  instance when a patient administers multiple insulin corrections within  half  an  hour  to  address  high  blood  glucose  levels.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Matrix;Heatmap", "axial_code": [], "componenet_code": ["heatmap", "matrix"]}]}, {"author": "dxf", "index_original": 359, "paper_title": "Marjorie: Visualizing Type 1 Diabetes Data  to Support Pattern Exploration", "pub_year": 2024, "domain": "diabetes", "requirement": {"requirement_text": "R3: Visualize all variables in one place. Another requirement is to collect and visualize multivariate and het_x0002_erogenous datasets in one place. When asked about the limitations of current diabetes data analysis tools, the diabetologists stated that they require a comprehensive view of both glycemic data and additional data such as meal and insulin information, to better understand the connections between blood glucose fuctuations and potential causes. D1: \u201cIn the AGP there is only glucose data, and I have to click back and forth 10 times to understand why the patient, for example, always has high values in the morning. I always go into the daily views to see what he ate around that time.\u201d Moreover, the diabetologists expressed the wish to load more attributes into the tool. D2 stated: \u201cAdditional health data such as exercise or sleep would really help to understand the blood sugar\u2019s behavior and to advise patients how to handle specifc situations better.\u201d. D1 wished to see more detailed nutrition data.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The attributes we incorporated into our approach encompass blood glucose levels, carbohydrate consumption, and insulin (both bolus and basal insulin). Sleep, nutrition, and exercise data requested by the diabetologists (see R3) are not included due to the diffculty in obtaining such information", "data_code": {"tables": 1}}, "solution": [{"solution_text": "To generate the visualizations in the Marjorie frontend, various data processing steps are executed in the backend. The glucose data is aggre_x0002_gated by minute for the AGP, the bolus insulin event data is transformed into a continuous insulin activity series, and the carbohydrate and bolus insulin data is clustered to avoid overplotting. Additionally, for the Insights view, periods of meals and hypoglycemia are detected, which are later clustered into patterns. In the following, we provide details on the clustering of event entries and on pattern detection.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The daily view (Fig. 5) is designed to support both T5 (lookup time  segments of interest) and T3 (compare multiple attributes). Users can  select a specifc day with a date picker, and the view displays juxta-posed glucose, carbohydrate, bolus, and basal data arranged on a single  time axis. Glucose is presented as a scatter plot, while carbohydrates  and bolus insulin are represented by bar charts, and basal insulin by an  area plot. To avoid overplotting with overlapping bars, we cluster the carbohydrate and bolus insulin data in the same way as the data in the  overview. When hovering over a bar, a tooltip window displays the dis-tinct events that are summarized within it. We incorporated a semantic  zoom feature that adjusts the level of summarization of carbohydrate  and bolus insulin data based on the zoom level. Upon zooming into the  graph, the summarized bars split up into their components, providing  more details.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line;Bar", "axial_code": [], "componenet_code": ["line", "bar"]}, {"solution_text": "The daily view (Fig. 5) is designed to support both T5 (lookup time  segments of interest) and T3 (compare multiple attributes). Users can  select a specifc day with a date picker, and the view displays juxta-posed glucose, carbohydrate, bolus, and basal data arranged on a single  time axis. Glucose is presented as a scatter plot, while carbohydrates  and bolus insulin are represented by bar charts, and basal insulin by an  area plot. To avoid overplotting with overlapping bars, we cluster the carbohydrate and bolus insulin data in the same way as the data in the  overview. When hovering over a bar, a tooltip window displays the dis-tinct events that are summarized within it. We incorporated a semantic  zoom feature that adjusts the level of summarization of carbohydrate  and bolus insulin data based on the zoom level. Upon zooming into the  graph, the summarized bars split up into their components, providing  more details.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 360, "paper_title": "Marjorie: Visualizing Type 1 Diabetes Data  to Support Pattern Exploration", "pub_year": 2024, "domain": "diabetes", "requirement": {"requirement_text": "R4: Bundle and compare similar data. The ability to collect similar situations of the patient and make them comparable helps the physician to identify recurring behavior that leads to blood sugar imbalances. D2 stated: \u201cA single area to analyze sport days would be great for patients; many wonder how they can better regulate their blood sugar during these activities. For example, if they tend to end up in a hypoglycemia two hours after sports, I would suggest them to eat a banana in between.\u201d In another statement, D2 also said: \u201cIt would be really great if it was possible to collect and compare all instances of hypoglycemia in order to understand the different reasons contributing to them.\u201d", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The attributes we incorporated into our approach encompass blood glucose levels, carbohydrate consumption, and insulin (both bolus and basal insulin). Sleep, nutrition, and exercise data requested by the diabetologists (see R3) are not included due to the diffculty in obtaining such information", "data_code": {"tables": 1}}, "solution": [{"solution_text": "To generate the visualizations in the Marjorie frontend, various data processing steps are executed in the backend. The glucose data is aggre_x0002_gated by minute for the AGP, the bolus insulin event data is transformed into a continuous insulin activity series, and the carbohydrate and bolus insulin data is clustered to avoid overplotting. Additionally, for the Insights view, periods of meals and hypoglycemia are detected, which are later clustered into patterns. In the following, we provide details on the clustering of event entries and on pattern detection.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The Insights view addresses T4 (summarize and compare multiple similar time segments). Here, situations from a selected category (Meals or Hypoglycemia) are clustered by glucose similarity (T4a; cluster similar time segments) and presented to the user in the form of typically occurring glucose profles (T4b; summarize similar time segments). This allows users to discover non-periodic glucose patterns without the need to manually browse and compare multiple individual daily views.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 361, "paper_title": "Marjorie: Visualizing Type 1 Diabetes Data  to Support Pattern Exploration", "pub_year": 2024, "domain": "diabetes", "requirement": {"requirement_text": "R5: Display a detailed view of specifc days on request. The diabetologists mentioned that patients sometimes remember a specifc day that they want to analyze further. D1: \u201cRecently a patient told me about his half marathon in April. He had low blood sugar for the entire two hours and had to constantly eat while running. We looked up the day and found out that he had lunch only 1.5 hours before the run. With foresight on the exercise, he had reduced the insulin amount for the meal, but obviously not enough for such a strenuous exercise.\u201d", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The attributes we incorporated into our approach encompass blood glucose levels, carbohydrate consumption, and insulin (both bolus and basal insulin). Sleep, nutrition, and exercise data requested by the diabetologists (see R3) are not included due to the diffculty in obtaining such information", "data_code": {"tables": 1}}, "solution": [{"solution_text": "To generate the visualizations in the Marjorie frontend, various data processing steps are executed in the backend. The glucose data is aggre_x0002_gated by minute for the AGP, the bolus insulin event data is transformed into a continuous insulin activity series, and the carbohydrate and bolus insulin data is clustered to avoid overplotting. Additionally, for the Insights view, periods of meals and hypoglycemia are detected, which are later clustered into patterns. In the following, we provide details on the clustering of event entries and on pattern detection.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The daily view (Fig. 5) is designed to support both T5 (lookup time  segments of interest) and T3 (compare multiple attributes). Users can  select a specifc day with a date picker, and the view displays juxta-posed glucose, carbohydrate, bolus, and basal data arranged on a single  time axis. Glucose is presented as a scatter plot, while carbohydrates  and bolus insulin are represented by bar charts, and basal insulin by an  area plot. To avoid overplotting with overlapping bars, we cluster the carbohydrate and bolus insulin data in the same way as the data in the  overview. When hovering over a bar, a tooltip window displays the dis-tinct events that are summarized within it. We incorporated a semantic  zoom feature that adjusts the level of summarization of carbohydrate  and bolus insulin data based on the zoom level. Upon zooming into the  graph, the summarized bars split up into their components, providing  more details.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line;Bar", "axial_code": [], "componenet_code": ["line", "bar"]}, {"solution_text": "The daily view (Fig. 5) is designed to support both T5 (lookup time  segments of interest) and T3 (compare multiple attributes). Users can  select a specifc day with a date picker, and the view displays juxta-posed glucose, carbohydrate, bolus, and basal data arranged on a single  time axis. Glucose is presented as a scatter plot, while carbohydrates  and bolus insulin are represented by bar charts, and basal insulin by an  area plot. To avoid overplotting with overlapping bars, we cluster the carbohydrate and bolus insulin data in the same way as the data in the  overview. When hovering over a bar, a tooltip window displays the dis-tinct events that are summarized within it. We incorporated a semantic  zoom feature that adjusts the level of summarization of carbohydrate  and bolus insulin data based on the zoom level. Upon zooming into the  graph, the summarized bars split up into their components, providing  more details.", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 362, "paper_title": "HealthPrism: A Visual Analytics System for Exploring Children\u2019s Physical and Mental Health Profiles with Multimodal Data", "pub_year": 2024, "domain": "Health", "requirement": {"requirement_text": "R1: Enable efficient presentation of multi-modal input features from multiple levels. In this work, there are two modalities of input data (tabular and sensor data). E1 and E4-E6 hope the system can present these data efficiently. D1 said that data with different modalities usually have different suitable graphs; mixing this information together may confuse users and require users to pay extra effort to understand. D2 added that, some traditional graphs that have a long history and are frequently used in our daily life, are easier for users to grasp information quickly. E3 and E8 wished to explore the multimodal input features from different levels (i.e., overall, group, and individual).", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Health", "data_code": {"tables": 1}}, "solution": [{"solution_text": "In Categorical Feature Summary (Fig. 1-A1), the Sankey diagram depicts the flow between different categorical features(i.e., gender and learning modes) (R1). Tooltips are used to provide values for each category and flow (Fig. 1-A1-1). Two lists are used to customize the categorical features of interests (Fig. 1-A1-2). ", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Sankey", "axial_code": [], "componenet_code": ["sankey"]}, {"solution_text": "In Categorical Feature Summary (Fig. 1-A1), the Sankey diagram depicts the flow between different categorical features(i.e., gender and learning modes) (R1). Tooltips are used to provide values for each category and flow (Fig. 1-A1-1). Two lists are used to customize the categorical features of interests (Fig. 1-A1-2). ", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "dxf", "index_original": 363, "paper_title": "HealthPrism: A Visual Analytics System for Exploring Children\u2019s Physical and Mental Health Profiles with Multimodal Data", "pub_year": 2024, "domain": "Health", "requirement": {"requirement_text": "R1: Enable efficient presentation of multi-modal input features from multiple levels. In this work, there are two modalities of input data (tabular and sensor data). E1 and E4-E6 hope the system can present these data efficiently. D1 said that data with different modalities usually have different suitable graphs; mixing this information together may confuse users and require users to pay extra effort to understand. D2 added that, some traditional graphs that have a long history and are frequently used in our daily life, are easier for users to grasp information quickly. E3 and E8 wished to explore the multimodal input features from different levels (i.e., overall, group, and individual).", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Health", "data_code": {"tables": 1}}, "solution": [{"solution_text": "We extract context and motion patterns for each participant. The context pattern is extracted from children\u2019s context features. In this study, there are 47 different context features, detailed in Appendix A. K_x0002_nearest neighbor algorithm is used for data imputation [46] and then Min-Max Scale [28] is used for data normalization. We further use one-hot encoding to process the categorical features, including gender (binary gender) and learning modes (three learning modes). Thus, we obtain a 1 \u00d7 50 context pattern for each participant. The motion pattern is extracted from the tri-axial accelerometer data collected using wristbands. More specifically, we aggregate and average the values for each minute in a week to generate the weekly pattern (a sequence with 7\u00d724\u00d760 = 10080 entries). Here we extract the pattern in a weekly manner according to the mean and standard deviation of participants\u2019 wear duration. In this way, for tri-axial accelerometer data, we obtain a 3\u00d710080 motion pattern for each participant. The number of channels can be extended when there are other signals.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "The health profiling model (Fig. 2) consists of two streams, including two gates, one context encoder, one motion encoder, a stack of fully connected layers (FCN), the rectified linear unit layers (ReLU), and Dropout layers (Dropout). The gate consists of a 1D convolution layer, a ReLU layer, and a Sigmoid layer [24]. The context encoder consists of a stack of FCN, ReLU, and Dropout. The motion encoder followthe similar structure of the IMU encoder in [52, 53], consisting of a GroupNorm layer, several blocks of a 1D-CNN and Max Pooling, another GroupNorm layer, and a GRU combining the CNN output and\ngenerating the motion embedding. The detailed configuration of the model can be found in Appendix D.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The health profiling model (Fig. 2) consists of two streams, including two gates, one context encoder, one motion encoder, a stack of fully connected layers (FCN), the rectified linear unit layers (ReLU), and Dropout layers (Dropout). The gate consists of a 1D convolution layer, a ReLU layer, and a Sigmoid layer [24]. The context encoder consists of a stack of FCN, ReLU, and Dropout. The motion encoder followthe similar structure of the IMU encoder in [52, 53], consisting of a GroupNorm layer, several blocks of a 1D-CNN and Max Pooling, another GroupNorm layer, and a GRU combining the CNN output and\ngenerating the motion embedding. The detailed configuration of the model can be found in Appendix D.", "solution_category": "data_manipulation", "solution_axial": "Explainability", "solution_compoent": "", "axial_code": ["Explainability"], "componenet_code": ["explainability"]}, {"solution_text": "The health profiling model (Fig. 2) consists of two streams, including two gates, one context encoder, one motion encoder, a stack of fully connected layers (FCN), the rectified linear unit layers (ReLU), and Dropout layers (Dropout). The gate consists of a 1D convolution layer, a ReLU layer, and a Sigmoid layer [24]. The context encoder consists of a stack of FCN, ReLU, and Dropout. The motion encoder followthe similar structure of the IMU encoder in [52, 53], consisting of a GroupNorm layer, several blocks of a 1D-CNN and Max Pooling, another GroupNorm layer, and a GRU combining the CNN output and\ngenerating the motion embedding. The detailed configuration of the model can be found in Appendix D.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}, {"solution_text": "The health profiling model (Fig. 2) consists of two streams, including two gates, one context encoder, one motion encoder, a stack of fully connected layers (FCN), the rectified linear unit layers (ReLU), and Dropout layers (Dropout). The gate consists of a 1D convolution layer, a ReLU layer, and a Sigmoid layer [24]. The context encoder consists of a stack of FCN, ReLU, and Dropout. The motion encoder followthe similar structure of the IMU encoder in [52, 53], consisting of a GroupNorm layer, several blocks of a 1D-CNN and Max Pooling, another GroupNorm layer, and a GRU combining the CNN output and\ngenerating the motion embedding. The detailed configuration of the model can be found in Appendix D.", "solution_category": "interaction", "solution_axial": "Filtering;Selecting", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "dxf", "index_original": 364, "paper_title": "HealthPrism: A Visual Analytics System for Exploring Children\u2019s Physical and Mental Health Profiles with Multimodal Data", "pub_year": 2024, "domain": "Health", "requirement": {"requirement_text": "R1: Enable efficient presentation of multi-modal input features from multiple levels. In this work, there are two modalities of input data (tabular and sensor data). E1 and E4-E6 hope the system can present these data efficiently. D1 said that data with different modalities usually have different suitable graphs; mixing this information together may confuse users and require users to pay extra effort to understand. D2 added that, some traditional graphs that have a long history and are frequently used in our daily life, are easier for users to grasp information quickly. E3 and E8 wished to explore the multimodal input features from different levels (i.e., overall, group, and individual).", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Health", "data_code": {"tables": 1}}, "solution": [{"solution_text": "We extract context and motion patterns for each participant. The context pattern is extracted from children\u2019s context features. In this study, there are 47 different context features, detailed in Appendix A. K_x0002_nearest neighbor algorithm is used for data imputation [46] and then Min-Max Scale [28] is used for data normalization. We further use one-hot encoding to process the categorical features, including gender (binary gender) and learning modes (three learning modes). Thus, we obtain a 1 \u00d7 50 context pattern for each participant. The motion pattern is extracted from the tri-axial accelerometer data collected using wristbands. More specifically, we aggregate and average the values for each minute in a week to generate the weekly pattern (a sequence with 7\u00d724\u00d760 = 10080 entries). Here we extract the pattern in a weekly manner according to the mean and standard deviation of participants\u2019 wear duration. In this way, for tri-axial accelerometer data, we obtain a 3\u00d710080 motion pattern for each participant. The number of channels can be extended when there are other signals.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "As for the Individual Context Feature Summary (Fig. 1-C4), we use separated trees to distinguish between different categories of context features. The value of each context feature is directly presented at the leaf (R1). If the value is imputed, it would be marked with pink and the symbol \u2018*\u2019. Users can click on the nodes of feature categories to pack up those uninteresting features (Fig. 1-C4-1) and focus on those of interest. When there are two individuals, it can also be scrolled down to see the information of the other individual.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Tree", "axial_code": [], "componenet_code": ["tree"]}, {"solution_text": "As for the Individual Context Feature Summary (Fig. 1-C4), we use separated trees to distinguish between different categories of context features. The value of each context feature is directly presented at the leaf (R1). If the value is imputed, it would be marked with pink and the symbol \u2018*\u2019. Users can click on the nodes of feature categories to pack up those uninteresting features (Fig. 1-C4-1) and focus on those of interest. When there are two individuals, it can also be scrolled down to see the information of the other individual.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "dxf", "index_original": 365, "paper_title": "HealthPrism: A Visual Analytics System for Exploring Children\u2019s Physical and Mental Health Profiles with Multimodal Data", "pub_year": 2024, "domain": "Health", "requirement": {"requirement_text": "R1: Enable efficient presentation of multi-modal input features from multiple levels. In this work, there are two modalities of input data (tabular and sensor data). E1 and E4-E6 hope the system can present these data efficiently. D1 said that data with different modalities usually have different suitable graphs; mixing this information together may confuse users and require users to pay extra effort to understand. D2 added that, some traditional graphs that have a long history and are frequently used in our daily life, are easier for users to grasp information quickly. E3 and E8 wished to explore the multimodal input features from different levels (i.e., overall, group, and individual).", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Health", "data_code": {"tables": 1}}, "solution": [{"solution_text": "We extract context and motion patterns for each participant. The context pattern is extracted from children\u2019s context features. In this study, there are 47 different context features, detailed in Appendix A. K_x0002_nearest neighbor algorithm is used for data imputation [46] and then Min-Max Scale [28] is used for data normalization. We further use one-hot encoding to process the categorical features, including gender (binary gender) and learning modes (three learning modes). Thus, we obtain a 1 \u00d7 50 context pattern for each participant. The motion pattern is extracted from the tri-axial accelerometer data collected using wristbands. More specifically, we aggregate and average the values for each minute in a week to generate the weekly pattern (a sequence with 7\u00d724\u00d760 = 10080 entries). Here we extract the pattern in a weekly manner according to the mean and standard deviation of participants\u2019 wear duration. In this way, for tri-axial accelerometer data, we obtain a 3\u00d710080 motion pattern for each participant. The number of channels can be extended when there are other signals.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "The health profiling model (Fig. 2) consists of two streams, including two gates, one context encoder, one motion encoder, a stack of fully connected layers (FCN), the rectified linear unit layers (ReLU), and Dropout layers (Dropout). The gate consists of a 1D convolution layer, a ReLU layer, and a Sigmoid layer [24]. The context encoder consists of a stack of FCN, ReLU, and Dropout. The motion encoder followthe similar structure of the IMU encoder in [52, 53], consisting of a GroupNorm layer, several blocks of a 1D-CNN and Max Pooling, another GroupNorm layer, and a GRU combining the CNN output and\ngenerating the motion embedding. The detailed configuration of the model can be found in Appendix D.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The health profiling model (Fig. 2) consists of two streams, including two gates, one context encoder, one motion encoder, a stack of fully connected layers (FCN), the rectified linear unit layers (ReLU), and Dropout layers (Dropout). The gate consists of a 1D convolution layer, a ReLU layer, and a Sigmoid layer [24]. The context encoder consists of a stack of FCN, ReLU, and Dropout. The motion encoder followthe similar structure of the IMU encoder in [52, 53], consisting of a GroupNorm layer, several blocks of a 1D-CNN and Max Pooling, another GroupNorm layer, and a GRU combining the CNN output and\ngenerating the motion embedding. The detailed configuration of the model can be found in Appendix D.", "solution_category": "data_manipulation", "solution_axial": "Explainability", "solution_compoent": "", "axial_code": ["Explainability"], "componenet_code": ["explainability"]}, {"solution_text": "The health profiling model (Fig. 2) consists of two streams, including two gates, one context encoder, one motion encoder, a stack of fully connected layers (FCN), the rectified linear unit layers (ReLU), and Dropout layers (Dropout). The gate consists of a 1D convolution layer, a ReLU layer, and a Sigmoid layer [24]. The context encoder consists of a stack of FCN, ReLU, and Dropout. The motion encoder followthe similar structure of the IMU encoder in [52, 53], consisting of a GroupNorm layer, several blocks of a 1D-CNN and Max Pooling, another GroupNorm layer, and a GRU combining the CNN output and\ngenerating the motion embedding. The detailed configuration of the model can be found in Appendix D.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}, {"solution_text": "The health profiling model (Fig. 2) consists of two streams, including two gates, one context encoder, one motion encoder, a stack of fully connected layers (FCN), the rectified linear unit layers (ReLU), and Dropout layers (Dropout). The gate consists of a 1D convolution layer, a ReLU layer, and a Sigmoid layer [24]. The context encoder consists of a stack of FCN, ReLU, and Dropout. The motion encoder followthe similar structure of the IMU encoder in [52, 53], consisting of a GroupNorm layer, several blocks of a 1D-CNN and Max Pooling, another GroupNorm layer, and a GRU combining the CNN output and\ngenerating the motion embedding. The detailed configuration of the model can be found in Appendix D.", "solution_category": "interaction", "solution_axial": "Filtering;Selecting", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "dxf", "index_original": 366, "paper_title": "HealthPrism: A Visual Analytics System for Exploring Children\u2019s Physical and Mental Health Profiles with Multimodal Data", "pub_year": 2024, "domain": "Health", "requirement": {"requirement_text": "R2: Provide an interactive analysis environment to explore the correlation between different input features. There is a wide variety of context features. Presenting the correlation between these features can help researchers identify patterns and relationships between context features, which can provide insights into the underlying factors that affect children\u2019s health status. It can also help the development of targeted interventions to improve health outcomes. Many interviewed experts (E1-E3 and E6-E8) mentioned that they want to see the correlation of features in an efficient way. To meet this requirement, the system should provide an interactive analysis interface for the correlation.", "requirement_code": {"discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "Health", "data_code": {"tables": 1}}, "solution": [{"solution_text": "In Fig. 1-A2, users can inspect the details (i.e., correlation coefficient \u03c1with positive or negative signs indicating the positive or negative correlation and p-value) by hovering over the heatmap (R2). The table on the right of the heatmap displays the top n pairs of different features with the strongest correlation, and users can adjust n using the slider over the table. By clicking on a grid in the heatmap, the corresponding pairs will be added to the table. Users can add and delete the pairs in the table freely to compare the pairs of interest. The table is scrollable, allowing users to view as many pairs as they choose.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "Heatmap;Table", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "dxf", "index_original": 367, "paper_title": "HealthPrism: A Visual Analytics System for Exploring Children\u2019s Physical and Mental Health Profiles with Multimodal Data", "pub_year": 2024, "domain": "Health", "requirement": {"requirement_text": "R3: Facilitate the exploration and comparison of health profiles from multiple levels. Visualizing a comprehensive picture of health profiles can help users explore the data distribution and characteristics quickly. For example, E1 and E2 said the overview visualization would be helpful in describing the population characteristics in their research area. E5 mentioned challenges in identifying common patterns with only an individual view due to significant differences between individuals. Besides, as mentioned by D4 and E7, the system should present a multi-dimensional health profile for each individual effectively for indepth investigation. D1-D3 pointed out that radar charts are a common and useful visualization technique for multi-dimensional indicators. E7 also hopes the system can facilitate an efficient comparison of health profiles between different individuals.", "requirement_code": {"compare_entities": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "Health", "data_code": {"tables": 1}}, "solution": [{"solution_text": "The Health Profile Distribution Graph (Fig. 1-B1) will be updated according to the groups and health indicators selected, as shown in Fig. 1-B1-3. In the graph, each node represents a participant in the group, and the link represents the similarity of the health profile between two participants (i.e., the Euclidian distance between two profiles). The nodes in the graph are draggable to make sure that each node can be clicked on, even though some nodes have overlapped. For each node, we retain its links to the top ten closest nodes. Note that even though some links of a node are not included in the top ten closest, they might be included in the top ten closest of its neighbors so that each node may have more than ten links. The size and color of nodes are determined by the profile score (R3).", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation", "solution_compoent": "", "axial_code": ["SimilarityCalculation"], "componenet_code": ["similarity_calculation"]}, {"solution_text": "The Health Profile Distribution Graph (Fig. 1-B1) will be updated according to the groups and health indicators selected, as shown in Fig. 1-B1-3. In the graph, each node represents a participant in the group, and the link represents the similarity of the health profile between two participants (i.e., the Euclidian distance between two profiles). The nodes in the graph are draggable to make sure that each node can be clicked on, even though some nodes have overlapped. For each node, we retain its links to the top ten closest nodes. Note that even though some links of a node are not included in the top ten closest, they might be included in the top ten closest of its neighbors so that each node may have more than ten links. The size and color of nodes are determined by the profile score (R3).", "solution_category": "visualization", "solution_axial": "large_panel", "solution_compoent": "Scatter;Circle;Table", "axial_code": [], "componenet_code": ["scatter", "table", "circle"]}, {"solution_text": "The Health Profile Distribution Graph (Fig. 1-B1) will be updated according to the groups and health indicators selected, as shown in Fig. 1-B1-3. In the graph, each node represents a participant in the group, and the link represents the similarity of the health profile between two participants (i.e., the Euclidian distance between two profiles). The nodes in the graph are draggable to make sure that each node can be clicked on, even though some nodes have overlapped. For each node, we retain its links to the top ten closest nodes. Note that even though some links of a node are not included in the top ten closest, they might be included in the top ten closest of its neighbors so that each node may have more than ten links. The size and color of nodes are determined by the profile score (R3).", "solution_category": "interaction", "solution_axial": "OverviewandExplore;Extractionoffeatures", "solution_compoent": "", "axial_code": ["OverviewandExplore", "Extractionoffeatures"], "componenet_code": ["overview_and_explore", "extraction_of_features"]}]}, {"author": "dxf", "index_original": 369, "paper_title": "HealthPrism: A Visual Analytics System for Exploring Children\u2019s Physical and Mental Health Profiles with Multimodal Data", "pub_year": 2024, "domain": "Health", "requirement": {"requirement_text": "R4: Support quick screening and comparison between the participants according to their gender and age groups. Gender and age are critical in health-related analysis, and it is crucial to support population screening with different gender and age groups. Researchers commonly compare behaviors and health outcomes between different genders and age groups in their research, as noted by E1 and E2. According to E3, health disparities can arise due to gender or age, resulting in inequitable health outcomes. Additionally, E7 stated that different gender and age groups often have different health-related baselines.", "requirement_code": {"compare_entities": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "Health", "data_code": {"tables": 1}}, "solution": [{"solution_text": "We extract context and motion patterns for each participant. The context pattern is extracted from children\u2019s context features. In this study, there are 47 different context features, detailed in Appendix A. K_x0002_nearest neighbor algorithm is used for data imputation [46] and then Min-Max Scale [28] is used for data normalization. We further use one-hot encoding to process the categorical features, including gender (binary gender) and learning modes (three learning modes). Thus, we obtain a 1 \u00d7 50 context pattern for each participant. The motion pattern is extracted from the tri-axial accelerometer data collected using wristbands. More specifically, we aggregate and average the values for each minute in a week to generate the weekly pattern (a sequence with 7\u00d724\u00d760 = 10080 entries). Here we extract the pattern in a weekly manner according to the mean and standard deviation of participants\u2019 wear duration. In this way, for tri-axial accelerometer data, we obtain a 3\u00d710080 motion pattern for each participant. The number of channels can be extended when there are other signals.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "The health profiling model (Fig. 2) consists of two streams, including two gates, one context encoder, one motion encoder, a stack of fully connected layers (FCN), the rectified linear unit layers (ReLU), and Dropout layers (Dropout). The gate consists of a 1D convolution layer, a ReLU layer, and a Sigmoid layer [24]. The context encoder consists of a stack of FCN, ReLU, and Dropout. The motion encoder followthe similar structure of the IMU encoder in [52, 53], consisting of a GroupNorm layer, several blocks of a 1D-CNN and Max Pooling, another GroupNorm layer, and a GRU combining the CNN output and\ngenerating the motion embedding. The detailed configuration of the model can be found in Appendix D.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The health profiling model (Fig. 2) consists of two streams, including two gates, one context encoder, one motion encoder, a stack of fully connected layers (FCN), the rectified linear unit layers (ReLU), and Dropout layers (Dropout). The gate consists of a 1D convolution layer, a ReLU layer, and a Sigmoid layer [24]. The context encoder consists of a stack of FCN, ReLU, and Dropout. The motion encoder followthe similar structure of the IMU encoder in [52, 53], consisting of a GroupNorm layer, several blocks of a 1D-CNN and Max Pooling, another GroupNorm layer, and a GRU combining the CNN output and\ngenerating the motion embedding. The detailed configuration of the model can be found in Appendix D.", "solution_category": "data_manipulation", "solution_axial": "Explainability", "solution_compoent": "", "axial_code": ["Explainability"], "componenet_code": ["explainability"]}, {"solution_text": "The health profiling model (Fig. 2) consists of two streams, including two gates, one context encoder, one motion encoder, a stack of fully connected layers (FCN), the rectified linear unit layers (ReLU), and Dropout layers (Dropout). The gate consists of a 1D convolution layer, a ReLU layer, and a Sigmoid layer [24]. The context encoder consists of a stack of FCN, ReLU, and Dropout. The motion encoder followthe similar structure of the IMU encoder in [52, 53], consisting of a GroupNorm layer, several blocks of a 1D-CNN and Max Pooling, another GroupNorm layer, and a GRU combining the CNN output and\ngenerating the motion embedding. The detailed configuration of the model can be found in Appendix D.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}, {"solution_text": "The health profiling model (Fig. 2) consists of two streams, including two gates, one context encoder, one motion encoder, a stack of fully connected layers (FCN), the rectified linear unit layers (ReLU), and Dropout layers (Dropout). The gate consists of a 1D convolution layer, a ReLU layer, and a Sigmoid layer [24]. The context encoder consists of a stack of FCN, ReLU, and Dropout. The motion encoder followthe similar structure of the IMU encoder in [52, 53], consisting of a GroupNorm layer, several blocks of a 1D-CNN and Max Pooling, another GroupNorm layer, and a GRU combining the CNN output and\ngenerating the motion embedding. The detailed configuration of the model can be found in Appendix D.", "solution_category": "interaction", "solution_axial": "Filtering;Selecting", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "dxf", "index_original": 370, "paper_title": "HealthPrism: A Visual Analytics System for Exploring Children\u2019s Physical and Mental Health Profiles with Multimodal Data", "pub_year": 2024, "domain": "Health", "requirement": {"requirement_text": "R4: Support quick screening and comparison between the participants according to their gender and age groups. Gender and age are critical in health-related analysis, and it is crucial to support population screening with different gender and age groups. Researchers commonly compare behaviors and health outcomes between different genders and age groups in their research, as noted by E1 and E2. According to E3, health disparities can arise due to gender or age, resulting in inequitable health outcomes. Additionally, E7 stated that different gender and age groups often have different health-related baselines.", "requirement_code": {"compare_entities": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "Health", "data_code": {"tables": 1}}, "solution": [{"solution_text": "We extract context and motion patterns for each participant. The context pattern is extracted from children\u2019s context features. In this study, there are 47 different context features, detailed in Appendix A. K_x0002_nearest neighbor algorithm is used for data imputation [46] and then Min-Max Scale [28] is used for data normalization. We further use one-hot encoding to process the categorical features, including gender (binary gender) and learning modes (three learning modes). Thus, we obtain a 1 \u00d7 50 context pattern for each participant. The motion pattern is extracted from the tri-axial accelerometer data collected using wristbands. More specifically, we aggregate and average the values for each minute in a week to generate the weekly pattern (a sequence with 7\u00d724\u00d760 = 10080 entries). Here we extract the pattern in a weekly manner according to the mean and standard deviation of participants\u2019 wear duration. In this way, for tri-axial accelerometer data, we obtain a 3\u00d710080 motion pattern for each participant. The number of channels can be extended when there are other signals.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "The health profiling model (Fig. 2) consists of two streams, including two gates, one context encoder, one motion encoder, a stack of fully connected layers (FCN), the rectified linear unit layers (ReLU), and Dropout layers (Dropout). The gate consists of a 1D convolution layer, a ReLU layer, and a Sigmoid layer [24]. The context encoder consists of a stack of FCN, ReLU, and Dropout. The motion encoder followthe similar structure of the IMU encoder in [52, 53], consisting of a GroupNorm layer, several blocks of a 1D-CNN and Max Pooling, another GroupNorm layer, and a GRU combining the CNN output and\ngenerating the motion embedding. The detailed configuration of the model can be found in Appendix D.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The health profiling model (Fig. 2) consists of two streams, including two gates, one context encoder, one motion encoder, a stack of fully connected layers (FCN), the rectified linear unit layers (ReLU), and Dropout layers (Dropout). The gate consists of a 1D convolution layer, a ReLU layer, and a Sigmoid layer [24]. The context encoder consists of a stack of FCN, ReLU, and Dropout. The motion encoder followthe similar structure of the IMU encoder in [52, 53], consisting of a GroupNorm layer, several blocks of a 1D-CNN and Max Pooling, another GroupNorm layer, and a GRU combining the CNN output and\ngenerating the motion embedding. The detailed configuration of the model can be found in Appendix D.", "solution_category": "data_manipulation", "solution_axial": "Explainability", "solution_compoent": "", "axial_code": ["Explainability"], "componenet_code": ["explainability"]}, {"solution_text": "The health profiling model (Fig. 2) consists of two streams, including two gates, one context encoder, one motion encoder, a stack of fully connected layers (FCN), the rectified linear unit layers (ReLU), and Dropout layers (Dropout). The gate consists of a 1D convolution layer, a ReLU layer, and a Sigmoid layer [24]. The context encoder consists of a stack of FCN, ReLU, and Dropout. The motion encoder followthe similar structure of the IMU encoder in [52, 53], consisting of a GroupNorm layer, several blocks of a 1D-CNN and Max Pooling, another GroupNorm layer, and a GRU combining the CNN output and\ngenerating the motion embedding. The detailed configuration of the model can be found in Appendix D.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}, {"solution_text": "The health profiling model (Fig. 2) consists of two streams, including two gates, one context encoder, one motion encoder, a stack of fully connected layers (FCN), the rectified linear unit layers (ReLU), and Dropout layers (Dropout). The gate consists of a 1D convolution layer, a ReLU layer, and a Sigmoid layer [24]. The context encoder consists of a stack of FCN, ReLU, and Dropout. The motion encoder followthe similar structure of the IMU encoder in [52, 53], consisting of a GroupNorm layer, several blocks of a 1D-CNN and Max Pooling, another GroupNorm layer, and a GRU combining the CNN output and\ngenerating the motion embedding. The detailed configuration of the model can be found in Appendix D.", "solution_category": "interaction", "solution_axial": "Filtering;Selecting", "solution_compoent": "", "axial_code": ["Selecting", "Filtering"], "componenet_code": ["selecting", "filtering"]}]}, {"author": "dxf", "index_original": 371, "paper_title": "HealthPrism: A Visual Analytics System for Exploring Children\u2019s Physical and Mental Health Profiles with Multimodal Data", "pub_year": 2024, "domain": "Health", "requirement": {"requirement_text": "R4: Support quick screening and comparison between the participants according to their gender and age groups. Gender and age are critical in health-related analysis, and it is crucial to support population screening with different gender and age groups. Researchers commonly compare behaviors and health outcomes between different genders and age groups in their research, as noted by E1 and E2. According to E3, health disparities can arise due to gender or age, resulting in inequitable health outcomes. Additionally, E7 stated that different gender and age groups often have different health-related baselines.", "requirement_code": {"compare_entities": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "Health", "data_code": {"tables": 1}}, "solution": [{"solution_text": "We extract context and motion patterns for each participant. The context pattern is extracted from children\u2019s context features. In this study, there are 47 different context features, detailed in Appendix A. K_x0002_nearest neighbor algorithm is used for data imputation [46] and then Min-Max Scale [28] is used for data normalization. We further use one-hot encoding to process the categorical features, including gender (binary gender) and learning modes (three learning modes). Thus, we obtain a 1 \u00d7 50 context pattern for each participant. The motion pattern is extracted from the tri-axial accelerometer data collected using wristbands. More specifically, we aggregate and average the values for each minute in a week to generate the weekly pattern (a sequence with 7\u00d724\u00d760 = 10080 entries). Here we extract the pattern in a weekly manner according to the mean and standard deviation of participants\u2019 wear duration. In this way, for tri-axial accelerometer data, we obtain a 3\u00d710080 motion pattern for each participant. The number of channels can be extended when there are other signals.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "The context features are presented using a parallel coordinates chart (R1, Fig. 1-B3). Each y-axis represents one context feature. Users can select the group of interest (or include all participants) by clicking on the legend (R4) and select the features of interest using the list below the chart (Fig. 1-B3-1). The motion features of groups are presented using a line graph (R1, Fig. 1-B3-2), which follows the same structure as that in Summary View.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}, {"solution_text": "The context features are presented using a parallel coordinates chart (R1, Fig. 1-B3). Each y-axis represents one context feature. Users can select the group of interest (or include all participants) by clicking on the legend (R4) and select the features of interest using the list below the chart (Fig. 1-B3-1). The motion features of groups are presented using a line graph (R1, Fig. 1-B3-2), which follows the same structure as that in Summary View.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "dxf", "index_original": 372, "paper_title": "HealthPrism: A Visual Analytics System for Exploring Children\u2019s Physical and Mental Health Profiles with Multimodal Data", "pub_year": 2024, "domain": "Health", "requirement": {"requirement_text": "R5: Facilitate the exploration of feature importance and influence for each health indicator from multiple levels. Analyzing the importance and influence of input context and motion features facilitates the identification of key predictors, which can help researchers focus their efforts on the most influential factors and develop targeted interventions to improve health outcomes (E1, E2, E6, and E8). It can also help researchers interpret model results and make their analysis more practical [43]. By eliminating irrelevant or redundant features, the model can be optimized [16]. Furthermore, since there exist disparities between individuals of different genders, ages, etc., some experts also hope the system can present the personalized feature importance and influence for each individual (E3 and E4).", "requirement_code": {"identify_main_cause_aggregate": 1, "explain_differences": 1}}, "data": {"data_text": "Health", "data_code": {"tables": 1}}, "solution": [{"solution_text": "In Overall Feature Importance and Influence Analysis (Fig. 1-A3), stacked bars represent the top 10 most important features for each health indicator (R5). The weights of the ten features are first divided by their sum and multiplied by 100% for normalization and then sorted by values. As shown in Fig. 1-A3, the most important one among the ten features is on the left (dark blue), and the least one is on the right (pink). Since motion feature importance is influenced by the time window, a slider is provided for the customized time window selection (Fig. 1-A31). By clicking on the bar, the influence of the corresponding feature on the health indicator will be displayed on the right (R5).", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "In Overall Feature Importance and Influence Analysis (Fig. 1-A3), stacked bars represent the top 10 most important features for each health indicator (R5). The weights of the ten features are first divided by their sum and multiplied by 100% for normalization and then sorted by values. As shown in Fig. 1-A3, the most important one among the ten features is on the left (dark blue), and the least one is on the right (pink). Since motion feature importance is influenced by the time window, a slider is provided for the customized time window selection (Fig. 1-A31). By clicking on the bar, the influence of the corresponding feature on the health indicator will be displayed on the right (R5).", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "In Overall Feature Importance and Influence Analysis (Fig. 1-A3), stacked bars represent the top 10 most important features for each health indicator (R5). The weights of the ten features are first divided by their sum and multiplied by 100% for normalization and then sorted by values. As shown in Fig. 1-A3, the most important one among the ten features is on the left (dark blue), and the least one is on the right (pink). Since motion feature importance is influenced by the time window, a slider is provided for the customized time window selection (Fig. 1-A31). By clicking on the bar, the influence of the corresponding feature on the health indicator will be displayed on the right (R5).", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 373, "paper_title": "HealthPrism: A Visual Analytics System for Exploring Children\u2019s Physical and Mental Health Profiles with Multimodal Data", "pub_year": 2024, "domain": "Health", "requirement": {"requirement_text": "R5: Facilitate the exploration of feature importance and influence for each health indicator from multiple levels. Analyzing the importance and influence of input context and motion features facilitates the identification of key predictors, which can help researchers focus their efforts on the most influential factors and develop targeted interventions to improve health outcomes (E1, E2, E6, and E8). It can also help researchers interpret model results and make their analysis more practical [43]. By eliminating irrelevant or redundant features, the model can be optimized [16]. Furthermore, since there exist disparities between individuals of different genders, ages, etc., some experts also hope the system can present the personalized feature importance and influence for each individual (E3 and E4).", "requirement_code": {"identify_main_cause_aggregate": 1, "explain_differences": 1}}, "data": {"data_text": "Health", "data_code": {"tables": 1}}, "solution": [{"solution_text": "The individual feature importance and influence are displayed on the right of the radar chart (Fig. 1-C2), following the same workflow as those in Summary View and Group View (R5). When there are two individuals, the feature importance part can be scrolled to see the full content (R3). Also, users can click on the feature of interest to see its influence on the corresponding indicator of an individual.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Heatmap;Line", "axial_code": [], "componenet_code": ["heatmap", "line"]}]}, {"author": "dxf", "index_original": 375, "paper_title": "LiveRetro: Visual Analytics for Strategic Retrospect in Livestream E-Commerce", "pub_year": 2024, "domain": "E-commerce", "requirement": {"requirement_text": "R2: Capture features of streamers\u2019 live performance from multiple channels. Experts agreed that relying on repetitive raw video observation to evaluate streamers\u2019 live performance is tedious and time-consuming. A more efficient and effective approach is required to examine streamers\u2019 live performance. Therefore, they suggested using features from multiple channels to create a more comprehensive and detailed picture of the streamer\u2019s performance.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "livestream e-commerce data", "data_code": {"sequential": 1, "media": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The Segment View (Fig. 1- C ) allows users to investigate the temporal distribution of streamers\u2019 live performance (R2) for different time segments. Inspired by time curves [2], to unfold the temporal variation pattern in live performance, we partition live sessions into time segments of varying granularities (i.e., 1-minute and 5-minute) and project performance features (detailed in Tab.1 in the appendix) of each seg_x0002_ment as a glyph onto a 2D plane using t-SNE, because it offers \u201cthe best overall quality in terms of producing low errors on aver_x0002_age\u201d [15]. To construct the feature vector, we extract each audio feature by taking its minimum, median, and maximum values (e.g., Vvolume = [Volumemin,Volumemedian,Volumemax]), text features are ex_x0002_tracted by calculating the total number of words in each pitch\u2019s sentence, and facial features are extracted by determining the primary expression category and its frequency. We normalize each group of features before concatenation them to prevent bias, resulting in a 25-dimensional vector. Each selected glyph point is then connected by curves in chronological order, with the hue of the curves varying. To effectively display the performance features of each time segment, we design a glyph based on a pie chart. Within the glyph, the central pie chart displays the relative value of GPM. Both the pie\u2019s and outer ring\u2019s colors represent the chronological order. To encode audio, text, and facial features, this glyph is equally divided into three sectors of a circle. Specifically, the top-left sector represents audio features, the top-right sector represents text features, and the bottom sector represents facial features. As these features are heterogeneous, we adopt different designs to differentiate each group of features. Audio features are encoded in a rose chart-based design, where the petal length represents the feature\u2019s average value of the time segment. Text features are encoded in a pie chart-based design, where the angle of the pie represents the relative amount of corresponding pitch in the time segment. Finally, facial features are encoded in a donut chart-based design, where the color represents the type of primary expression and the angle of the arc represents the intensity.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Circle;Pie", "axial_code": [], "componenet_code": ["circle", "pie"]}]}, {"author": "dxf", "index_original": 376, "paper_title": "LiveRetro: Visual Analytics for Strategic Retrospect in Livestream E-Commerce", "pub_year": 2024, "domain": "E-commerce", "requirement": {"requirement_text": "R3: Summarize viewers\u2019 comments for the chosen time period. Viewer comments during live streams offer direct feedback on the streamer and the promoted merchandise and are influenced by live room activity and streamer guidance. Therefore, it is important to synthesize and summarize viewer comments during the relevant time period to address pre-sales inquiries and post-sales concerns.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "livestream e-commerce data", "data_code": {"textual": 1, "tables": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "The Comment Reference part (Fig. 3C3 ) facilitates users in retrospectively accessing the real-time comments that were generated during live streaming, as articulated in requirements R3 and R6. It includes a timeline and a summary of comments. In order to present both the overarching temporal patterns of the comments and informative details contained therein (such as opinion on merchandise), the summary is presented using a volume chart-based design with four components: a background volume chart, a zig-zag curve, a foreground with individual comment data points, and a floating keyword box. The background volume chart illustrates the trend of comment intensity, and a zig-zag curve that is bounded by the volume chart is constructed along the timeline.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 378, "paper_title": "LiveRetro: Visual Analytics for Strategic Retrospect in Livestream E-Commerce", "pub_year": 2024, "domain": "E-commerce", "requirement": {"requirement_text": "R5: Visualize merchandise arrangement. Streamers promote merchandise in a specific order during live sessions, categorizing them as \u201ctraffic-type\u201d for visibility, \u201cwelfare-type\u201d for interaction, and \u201cprofit-type\u201d for maximum profit. An overview of the merchandise arrangement is needed to evaluate its effectiveness in driving sales.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "livestream e-commerce data", "data_code": {"tables": 1}}, "solution": [{"solution_text": "The Session View (Fig. 1A ) provides an overview of live sessions within a specific time range, along with sales data for the merchandise promoted during each session (R5). This view presents a table consisting of four columns that detail fundamental information and key indicators, namely the time, duration, GPM, and GMV (Gross Merchandise Volume). The display of these four attributes allows users to obtain a swift summary of each live session. Additionally, users have the option to sort sessions according to these attributes by clicking on the header. Users can expand the row of a specific session to view details of the merchandise by clicking on the left. Sales data for each promoted merchandise is presented in chronological order within an expanded table along a vertical timeline, with the launch time displayed on the left and sales details grouped in boxes on the right. Each box lists the thumbnail and title of the merchandise on the left, while on the right, the overall Exposure-Click Ratio (i.e., the click number of the purchase interface to online counts) and Click-Turnover Ratio (i.e., the number of transactions to click count of the purchase interface) distribution is depicted using two opposing beeswarm charts. Highlighted dots indicate the exact value of two attributes for the merchandise. The size of the rectangle on the right encodes the volume, while the color encodes sales. In terms of design, the amount of sales for each merchandise follows the same encoding scheme as the Exploration View ( DP1 ). Lastly, to facilitate comparison, the area graphs and sales & volume rectangles are aligned vertically ( DP2 )", "solution_category": "visualization", "solution_axial": "mirror", "solution_compoent": "Table;Image;Text;Scatter", "axial_code": [], "componenet_code": ["image", "text", "scatter", "table", "image", "text", "scatter", "table"]}, {"solution_text": "The Session View (Fig. 1A ) provides an overview of live sessions within a specific time range, along with sales data for the merchandise promoted during each session (R5). This view presents a table consisting of four columns that detail fundamental information and key indicators, namely the time, duration, GPM, and GMV (Gross Merchandise Volume). The display of these four attributes allows users to obtain a swift summary of each live session. Additionally, users have the option to sort sessions according to these attributes by clicking on the header. Users can expand the row of a specific session to view details of the merchandise by clicking on the left. Sales data for each promoted merchandise is presented in chronological order within an expanded table along a vertical timeline, with the launch time displayed on the left and sales details grouped in boxes on the right. Each box lists the thumbnail and title of the merchandise on the left, while on the right, the overall Exposure-Click Ratio (i.e., the click number of the purchase interface to online counts) and Click-Turnover Ratio (i.e., the number of transactions to click count of the purchase interface) distribution is depicted using two opposing beeswarm charts. Highlighted dots indicate the exact value of two attributes for the merchandise. The size of the rectangle on the right encodes the volume, while the color encodes sales. In terms of design, the amount of sales for each merchandise follows the same encoding scheme as the Exploration View ( DP1 ). Lastly, to facilitate comparison, the area graphs and sales & volume rectangles are aligned vertically ( DP2 )", "solution_category": "interaction", "solution_axial": "Reconfigure;Filtering;Extractionoffeatures;Abstract/Exlaborate", "solution_compoent": "", "axial_code": ["Reconfigure", "Extractionoffeatures", "Filtering", "Abstract/Exlaborate"], "componenet_code": ["reconfigure", "extraction_of_features", "filtering", "abstract_elaborate"]}]}, {"author": "dxf", "index_original": 379, "paper_title": "LiveRetro: Visual Analytics for Strategic Retrospect in Livestream E-Commerce", "pub_year": 2024, "domain": "E-commerce", "requirement": {"requirement_text": "R6: Provide an organized joint analysis for live replay and feedback. Experts agreed on the need to connect live performances with their corresponding feedback, which is difficult to discern from video analysis alone. Thus, the system must harmonize multi-modal features and streaming statistics and present them in an organized way", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "livestream e-commerce data", "data_code": {"textual": 1, "tables": 1}}, "solution": [{"solution_text": "NONE", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "The Comment Reference part (Fig. 3C3 ) facilitates users in retrospectively accessing the real-time comments that were generated during live streaming, as articulated in requirements R3 and R6. It includes a timeline and a summary of comments. In order to present both the overarching temporal patterns of the comments and informative details contained therein (such as opinion on merchandise), the summary is presented using a volume chart-based design with four components: a background volume chart, a zig-zag curve, a foreground with individual comment data points, and a floating keyword box. The background volume chart illustrates the trend of comment intensity, and a zig-zag curve that is bounded by the volume chart is constructed along the timeline.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Line", "axial_code": [], "componenet_code": ["line"]}]}, {"author": "dxf", "index_original": 380, "paper_title": "LiveRetro: Visual Analytics for Strategic Retrospect in Livestream E-Commerce", "pub_year": 2024, "domain": "E-commerce", "requirement": {"requirement_text": "R7: Identify the influences of different channels and features for a live session. E2, E4, and E5 expressed a notable interest in examining the impact of specific channels or features on streaming statistics. They aimed to determine the key elements that streamers should prioritize in order to enhance sales, such as providing detailed explanations or creating an engaging atmosphere.", "requirement_code": {"identify_main_cause_aggregate": 1}}, "data": {"data_text": "livestream e-commerce data", "data_code": {"sequential": 1, "media": 1}}, "solution": [{"solution_text": "project performance features (detailed in Tab.1 in the appendix) of each seg_x0002_ment as a glyph onto a 2D plane using t-SNE, because it offers \u201cthe best overall quality in terms of producing low errors on aver_x0002_age\u201d [15]. To construct the feature vector, we extract each audio feature by taking its minimum, median, and maximum values (e.g., Vvolume = [Volumemin,Volumemedian,Volumemax]), text features are ex_x0002_tracted by calculating the total number of words in each pitch\u2019s sentence, and facial features are extracted by determining the primary expression category and its frequency.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The Segment View (Fig. 1- C ) allows users to investigate the temporal distribution of streamers\u2019 live performance (R2) for different time segments. Inspired by time curves [2], to unfold the temporal variation pattern in live performance, we partition live sessions into time segments of varying granularities (i.e., 1-minute and 5-minute) and project per_x0002_formance features (detailed in Tab.1 in the appendix) of each seg_x0002_ment as a glyph onto a 2D plane using t-SNE, because it offers \u201cthe best overall quality in terms of producing low errors on aver_x0002_age\u201d [15]. To construct the feature vector, we extract each audio feature by taking its minimum, median, and maximum values (e.g., Vvolume = [Volumemin,Volumemedian,Volumemax]), text features are ex_x0002_tracted by calculating the total number of words in each pitch\u2019s sentence, and facial features are extracted by determining the primary expression category and its frequency. We normalize each group of features before concatenation them to prevent bias, resulting in a 25-dimensional vector. Each selected glyph point is then connected by curves in chronological order, with the hue of the curves varying. To effectively display the performance features of each time segment, we design a glyph based on a pie chart. Within the glyph, the central pie chart displays the relative value of GPM. Both the pie\u2019s and outer ring\u2019s colors represent the chronological order. To encode audio, text, and facial features, this glyph is equally divided into three sectors of a circle. Specifically, the top-left sector represents audio features, the top-right sector represents text features, and the bottom sector represents facial features. As these features are heterogeneous, we adopt different designs to differentiate each group of features. Audio features are encoded in a rose chart-based design, where the petal length represents the feature\u2019s average value of the time segment. Text features are encoded in a pie chart-based design, where the angle of the pie represents the relative amount of corresponding pitch in the time segment. Finally, facial features are encoded in a donut chart-based design, where the color represents the type of primary expression and the angle of the arc represents the intensity.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Circle;Pie", "axial_code": [], "componenet_code": ["circle", "pie"]}]}, {"author": "dxf", "index_original": 382, "paper_title": "Leveraging Historical Medical Records as a Proxy via Multimodal Modeling and Visualization to Enrich Medical Diagnostic Learning", "pub_year": 2024, "domain": "Medical", "requirement": {"requirement_text": "R.1: Describe the focal medical diagnostic tasks and present data statistics. According to the experts, it is essential to provide statistics regarding the multimodal dataset, including data quality and sources for each modality. Additionally, a clear definition of medical diagnostic tasks should be provided.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "DiagnosisAssistant, is designed to process heterogeneous clinical data, construct robust models, and provide diagnostic insights based on in_x0002_terpretability techniques. To evaluate the system\u2019s performance, we collaborate closely with physicians from a prominent local hospital and used a real-life clinical dataset for diagnosing cervical spine disorders. The dataset comprises 750 patient records collected during hospital visits for cervical spine discomfort between 2012 and 2013. The pa_x0002_tients\u2019 ages range from 21 to 82 years, with a male-to-female ratio of 1.16 : 1. Each patient has a unique CardID that corresponds to a set of clinical records, including demographic information, laboratory test results, magnetic resonance imaging (MRI) images, clinical reports, and diagnostic findings. We organize the data into three modalities, namely, the indicator modality, text modality, and image modality. The indicator modality consists of laboratory test results and demographic information, including gender, age, height, and weight. The unstruc_x0002_tured data includes clinical reports and MRI images. Prior to model training, the dataset was carefully preprocessed and checked, resulting in 626 retained instances.", "data_code": {"textual": 1, "tables": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Based on the physicians\u2019 suggestions, we extract 34 pertinent indicators (R.1). These indicators are structured in a collapsible table, where each row corresponds to one indicator. In the collapsed mode, each row includes the indicator name, an approximate distribution of indicators concerning the lassoed patient groups, and the mean value, thereby enabling the user to gain an overview of the indicator modality. The overall distribution of indicator data is exhibited using strip charts.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}]}, {"author": "dxf", "index_original": 384, "paper_title": "Leveraging Historical Medical Records as a Proxy via Multimodal Modeling and Visualization to Enrich Medical Diagnostic Learning", "pub_year": 2024, "domain": "Medical", "requirement": {"requirement_text": "R.1: Describe the focal medical diagnostic tasks and present data statistics. According to the experts, it is essential to provide statistics regarding the multimodal dataset, including data quality and sources for each modality. Additionally, a clear definition of medical diagnostic tasks should be provided.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "DiagnosisAssistant, is designed to process heterogeneous clinical data, construct robust models, and provide diagnostic insights based on in_x0002_terpretability techniques. To evaluate the system\u2019s performance, we collaborate closely with physicians from a prominent local hospital and used a real-life clinical dataset for diagnosing cervical spine disorders. The dataset comprises 750 patient records collected during hospital visits for cervical spine discomfort between 2012 and 2013. The pa_x0002_tients\u2019 ages range from 21 to 82 years, with a male-to-female ratio of 1.16 : 1. Each patient has a unique CardID that corresponds to a set of clinical records, including demographic information, laboratory test results, magnetic resonance imaging (MRI) images, clinical reports, and diagnostic findings. We organize the data into three modalities, namely, the indicator modality, text modality, and image modality. The indicator modality consists of laboratory test results and demographic information, including gender, age, height, and weight. The unstruc_x0002_tured data includes clinical reports and MRI images. Prior to model training, the dataset was carefully preprocessed and checked, resulting in 626 retained instances.", "data_code": {"textual": 1, "tables": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Based on the physicians\u2019 suggestions, we extract 34 pertinent indicators (R.1). These indicators are structured in a collapsible table, where each row corresponds to one indicator. In the collapsed mode, each row includes the indicator name, an approximate distribution of indicators concerning the lassoed patient groups, and the mean value, thereby enabling the user to gain an overview of the indicator modality. The overall distribution of indicator data is exhibited using strip charts.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}]}, {"author": "dxf", "index_original": 385, "paper_title": "Leveraging Historical Medical Records as a Proxy via Multimodal Modeling and Visualization to Enrich Medical Diagnostic Learning", "pub_year": 2024, "domain": "Medical", "requirement": {"requirement_text": "R.2: Develop a reliable and interpretable ML model that can capture the diagnosis process. According to the experts, ML techniques, especially deep learning methods, are highly effective in this regard. Data scientist E5 affirmed that their prior modeling experiments have also yielded satisfactory outcomes. However, they emphasized that certain aspects need to be addressed if advanced models are to be implemented in actual diagnostic scenarios. Specifically, the model\u2019s accuracy must be sufficiently high, and it must also capture the diagnostic process accurately. E5 further stated that \u201cit is crucial to understand how the model arrived at its conclusions and whether its decision-making process aligns with medical findings\u201d.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "DiagnosisAssistant, is designed to process heterogeneous clinical data, construct robust models, and provide diagnostic insights based on in_x0002_terpretability techniques. To evaluate the system\u2019s performance, we collaborate closely with physicians from a prominent local hospital and used a real-life clinical dataset for diagnosing cervical spine disorders. The dataset comprises 750 patient records collected during hospital visits for cervical spine discomfort between 2012 and 2013. The pa_x0002_tients\u2019 ages range from 21 to 82 years, with a male-to-female ratio of 1.16 : 1. Each patient has a unique CardID that corresponds to a set of clinical records, including demographic information, laboratory test results, magnetic resonance imaging (MRI) images, clinical reports, and diagnostic findings. We organize the data into three modalities, namely, the indicator modality, text modality, and image modality. The indicator modality consists of laboratory test results and demographic information, including gender, age, height, and weight. The unstruc_x0002_tured data includes clinical reports and MRI images. Prior to model training, the dataset was carefully preprocessed and checked, resulting in 626 retained instances.", "data_code": {"textual": 1, "tables": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Our objective is to provide users with an understanding of the data distribution and model behaviors for each modality by extracting and visually summarizing the data embeddings. For the indicator data, we directly utilize the raw data without additional processing due to its low dimensionality. In the case of ClinicalBERT, which follows the architecture of the BERT model and comprises 12 transformer layers, each layer generates 768-dimensional embeddings that capture seman_x0002_tic features at various levels. To retain the maximum amount of valid information, we employ common techniques such as summing, averaging, and concatenating selected or all layer embeddings. We adopt the bit-wise sum of all token embeddings as the final 768-dimensional text representation, striking a balance between computational efficiency and information retention. As for ConvNeXt, an optimized CNN that follows the classic CNN architecture, we extract the output from the penultimate layer (i.e., the input of the classifier layer) to obtain a 768-dimensional image representation. To encompass the model\u2019s overall understanding of the data distribution, we define a fusion embedding by concatenating the three embedding vectors and weighting each vector element according to its modality.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "To examine the patient collections\u2019 features in various embedding spaces, an embedding transition view (Fig. 2(B)) has been developed. This view connects the fusion model embedding and the three modality data (i.e., indicators, text, images) embedding spaces (R.2). Each patient is represented by a node, and the connections between each projection allow users to trace the same group of patients across different embeddings. To generate two-dimensional projections, the t-SNE method [70] is chosen as it\u201creveals meaningful insights about the data and shows superiority in generating two-dimensional projection [43].\u201d The users can use the lasso tool to select a group of patients, and the system will display the distribution of the selected patients by class. Moreover, each modality data for each patient contributes to the final model output, where the contribution is a probability estimate for each category in the resulting model output (R.4, R.5). To show each modal_x0002_ity data\u2019s contributions to different prediction classes (three classes in the running example), bar charts are used. The contribution value is determined by the probability mass function (PMF) [66], which charac_x0002_terizes the distribution of discrete random variables. The contribution values attached to the fusion embedding projection are calculated as weighted sums of the three modalities.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "To examine the patient collections\u2019 features in various embedding spaces, an embedding transition view (Fig. 2(B)) has been developed. This view connects the fusion model embedding and the three modality data (i.e., indicators, text, images) embedding spaces (R.2). Each patient is represented by a node, and the connections between each projection allow users to trace the same group of patients across different embeddings. To generate two-dimensional projections, the t-SNE method [70] is chosen as it\u201creveals meaningful insights about the data and shows superiority in generating two-dimensional projection [43].\u201d The users can use the lasso tool to select a group of patients, and the system will display the distribution of the selected patients by class. Moreover, each modality data for each patient contributes to the final model output, where the contribution is a probability estimate for each category in the resulting model output (R.4, R.5). To show each modal_x0002_ity data\u2019s contributions to different prediction classes (three classes in the running example), bar charts are used. The contribution value is determined by the probability mass function (PMF) [66], which charac_x0002_terizes the distribution of discrete random variables. The contribution values attached to the fusion embedding projection are calculated as weighted sums of the three modalities.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Line;Scatter", "axial_code": [], "componenet_code": ["scatter", "line"]}, {"solution_text": "To examine the patient collections\u2019 features in various embedding spaces, an embedding transition view (Fig. 2(B)) has been developed. This view connects the fusion model embedding and the three modality data (i.e., indicators, text, images) embedding spaces (R.2). Each patient is represented by a node, and the connections between each projection allow users to trace the same group of patients across different embeddings. To generate two-dimensional projections, the t-SNE method [70] is chosen as it\u201creveals meaningful insights about the data and shows superiority in generating two-dimensional projection [43].\u201d The users can use the lasso tool to select a group of patients, and the system will display the distribution of the selected patients by class. Moreover, each modality data for each patient contributes to the final model output, where the contribution is a probability estimate for each category in the resulting model output (R.4, R.5). To show each modal_x0002_ity data\u2019s contributions to different prediction classes (three classes in the running example), bar charts are used. The contribution value is determined by the probability mass function (PMF) [66], which charac_x0002_terizes the distribution of discrete random variables. The contribution values attached to the fusion embedding projection are calculated as weighted sums of the three modalities.", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "dxf", "index_original": 388, "paper_title": "Leveraging Historical Medical Records as a Proxy via Multimodal Modeling and Visualization to Enrich Medical Diagnostic Learning", "pub_year": 2024, "domain": "Medical", "requirement": {"requirement_text": "R.3: Convert the diagnostic process into a user-friendly representation. Once a reliable ML model has been developed, the focus shifts toward creating an intuitive representation of the diagnostic process for historical cases. The experts highlighted the importance of demonstrating the model\u2019s functionality in a clear and easily understandable manner. As E3 noted, conveying the abstract experience in a user-friendly manner would be beneficial for all involved.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "DiagnosisAssistant, is designed to process heterogeneous clinical data, construct robust models, and provide diagnostic insights based on in_x0002_terpretability techniques. To evaluate the system\u2019s performance, we collaborate closely with physicians from a prominent local hospital and used a real-life clinical dataset for diagnosing cervical spine disorders. The dataset comprises 750 patient records collected during hospital visits for cervical spine discomfort between 2012 and 2013. The pa_x0002_tients\u2019 ages range from 21 to 82 years, with a male-to-female ratio of 1.16 : 1. Each patient has a unique CardID that corresponds to a set of clinical records, including demographic information, laboratory test results, magnetic resonance imaging (MRI) images, clinical reports, and diagnostic findings. We organize the data into three modalities, namely, the indicator modality, text modality, and image modality. The indicator modality consists of laboratory test results and demographic information, including gender, age, height, and weight. The unstruc_x0002_tured data includes clinical reports and MRI images. Prior to model training, the dataset was carefully preprocessed and checked, resulting in 626 retained instances.", "data_code": {"textual": 1, "tables": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Our objective is to provide users with an understanding of the data distribution and model behaviors for each modality by extracting and visually summarizing the data embeddings. For the indicator data, we directly utilize the raw data without additional processing due to its low dimensionality. In the case of ClinicalBERT, which follows the architecture of the BERT model and comprises 12 transformer layers, each layer generates 768-dimensional embeddings that capture seman_x0002_tic features at various levels. To retain the maximum amount of valid information, we employ common techniques such as summing, averaging, and concatenating selected or all layer embeddings. We adopt the bit-wise sum of all token embeddings as the final 768-dimensional text representation, striking a balance between computational efficiency and information retention. As for ConvNeXt, an optimized CNN that follows the classic CNN architecture, we extract the output from the penultimate layer (i.e., the input of the classifier layer) to obtain a 768-dimensional image representation. To encompass the model\u2019s overall understanding of the data distribution, we define a fusion embedding by concatenating the three embedding vectors and weighting each vector element according to its modality.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "We use two representations to show the text modality. The top part represents a boxplot demonstrating the word weight of medical text for chosen patients. The words are presented in descending order based on their average weight, as depicted in Fig. 3(D). The boxplot is a superior alternative to word clouds, as it can be aligned to enable a comparative analysis of word weights among diverse patient groups. Physicians can easily identify high-weighted terms, which can serve as visual cues for further exploration (R.3).", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Box", "axial_code": [], "componenet_code": ["boxplot"]}]}, {"author": "dxf", "index_original": 389, "paper_title": "Leveraging Historical Medical Records as a Proxy via Multimodal Modeling and Visualization to Enrich Medical Diagnostic Learning", "pub_year": 2024, "domain": "Medical", "requirement": {"requirement_text": "R.3: Convert the diagnostic process into a user-friendly representation. Once a reliable ML model has been developed, the focus shifts toward creating an intuitive representation of the diagnostic process for historical cases. The experts highlighted the importance of demonstrating the model\u2019s functionality in a clear and easily understandable manner. As E3 noted, conveying the abstract experience in a user-friendly manner would be beneficial for all involved.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "DiagnosisAssistant, is designed to process heterogeneous clinical data, construct robust models, and provide diagnostic insights based on in_x0002_terpretability techniques. To evaluate the system\u2019s performance, we collaborate closely with physicians from a prominent local hospital and used a real-life clinical dataset for diagnosing cervical spine disorders. The dataset comprises 750 patient records collected during hospital visits for cervical spine discomfort between 2012 and 2013. The pa_x0002_tients\u2019 ages range from 21 to 82 years, with a male-to-female ratio of 1.16 : 1. Each patient has a unique CardID that corresponds to a set of clinical records, including demographic information, laboratory test results, magnetic resonance imaging (MRI) images, clinical reports, and diagnostic findings. We organize the data into three modalities, namely, the indicator modality, text modality, and image modality. The indicator modality consists of laboratory test results and demographic information, including gender, age, height, and weight. The unstruc_x0002_tured data includes clinical reports and MRI images. Prior to model training, the dataset was carefully preprocessed and checked, resulting in 626 retained instances.", "data_code": {"textual": 1, "tables": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The treemap is employed to categorize images based on various categories, facilitating a panoramic view of the gallery of selected patients. By clicking the button shown in Fig. 3(F), the gallery can be switched between RAW image mode and CAM (Class Activation Mapping) mode. While browsing through the thumbnails in the gallery, if a user desires to examine a specific image, they can click on it to view the details on a larger scale. Both RAW and CAM information are displayed in parallel, providing users with a comprehensive understanding of the diagnostic focus of the image modality (R.3).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Image", "axial_code": [], "componenet_code": ["image"]}, {"solution_text": "The treemap is employed to categorize images based on various categories, facilitating a panoramic view of the gallery of selected patients. By clicking the button shown in Fig. 3(F), the gallery can be switched between RAW image mode and CAM (Class Activation Mapping) mode. While browsing through the thumbnails in the gallery, if a user desires to examine a specific image, they can click on it to view the details on a larger scale. Both RAW and CAM information are displayed in parallel, providing users with a comprehensive understanding of the diagnostic focus of the image modality (R.3).", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "dxf", "index_original": 390, "paper_title": "Leveraging Historical Medical Records as a Proxy via Multimodal Modeling and Visualization to Enrich Medical Diagnostic Learning", "pub_year": 2024, "domain": "Medical", "requirement": {"requirement_text": "R.3: Convert the diagnostic process into a user-friendly representation. Once a reliable ML model has been developed, the focus shifts toward creating an intuitive representation of the diagnostic process for historical cases. The experts highlighted the importance of demonstrating the model\u2019s functionality in a clear and easily understandable manner. As E3 noted, conveying the abstract experience in a user-friendly manner would be beneficial for all involved.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "DiagnosisAssistant, is designed to process heterogeneous clinical data, construct robust models, and provide diagnostic insights based on in_x0002_terpretability techniques. To evaluate the system\u2019s performance, we collaborate closely with physicians from a prominent local hospital and used a real-life clinical dataset for diagnosing cervical spine disorders. The dataset comprises 750 patient records collected during hospital visits for cervical spine discomfort between 2012 and 2013. The pa_x0002_tients\u2019 ages range from 21 to 82 years, with a male-to-female ratio of 1.16 : 1. Each patient has a unique CardID that corresponds to a set of clinical records, including demographic information, laboratory test results, magnetic resonance imaging (MRI) images, clinical reports, and diagnostic findings. We organize the data into three modalities, namely, the indicator modality, text modality, and image modality. The indicator modality consists of laboratory test results and demographic information, including gender, age, height, and weight. The unstruc_x0002_tured data includes clinical reports and MRI images. Prior to model training, the dataset was carefully preprocessed and checked, resulting in 626 retained instances.", "data_code": {"textual": 1, "tables": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "In this subsection, we first present our approach for developing a multi_x0002_modal model and subsequently outline our method for unfolding the diagnostic insights derived from the constructed model. Specifically, the design of our model revolves around the classification of patients into three distinct categories, namely, normal, herniated, bulging, as suggested by domain experts. We explore various machine learning techniques for modeling the data across multiple modalities, as summarized in Tab. 1. Subsequently, we select three models for each modality, XGBoost [16] for indicator data, ClinicalBERT [4] for textural data, and ConvNeXt [45] for image data, owing to their superior performance. We fine-tune the hyper_x0002_parameters of the three models using grid search [22] with k-fold cross-validation [67]. The dataset is divided into training and validation sets using a random selection method with a ratio of 75:25 for training and validation, respectively. Table 1 lists the performance of the models on the validation set with the optimal hyper-parameter settings. Fusion Strategy. Fusing heterogeneous information from multi_x0002_modal data is a common strategy to improve model performance [20]. In this study, we investigate two fusion strategies: decision-level fusion and feature-level fusion (discussed in subsection 2.2). In feature-level fusion, we concatenate the raw indicator data with the output from the penultimate layer in the ConvNeXt and ClinicalBERT models and then feed the concatenated features into a 12-head, 12-layer transformer. In decision-level fusion, we adopt a weighted voting strategy and employ multiclass perception to learn the weights of each modality. As shown in Tab. 2, the two strategies perform similarly. However, the decision_x0002_level fusion strategy aligns better with physicians\u2019 diagnostic process and is more robust and scalable in the absence of modalities. Therefore we select the decision-level fusion strategy for our model. Interpretability Towards Diagnosis. To improve users\u2019 learning experience, we apply multiple post-hoc interpretability techniques. Specifically, Guided Grad-CAM [59] is utilized to generate saliency maps on each MRI image and highlight important areas. The method proposed by Chefer et al. [15] is adopted to interpret the transformer model and highlight important words. For the indicator-modal model, SHAP is used to determine the contribution of each feature to a par_x0002_ticular decision. By providing saliency maps, key text highlighting, and feature contribution quantification, comprehensive hints about the diagnostic focus of each case are given to the users to aid in their diag_x0002_nostic learning process. All these explanations are combined to offer interpretability to the model\u2019s decisions.", "solution_category": "data_manipulation", "solution_axial": "Modeling;Explainability", "solution_compoent": "", "axial_code": ["Modeling", "Explainability"], "componenet_code": ["modeling", "explainability"]}, {"solution_text": "Our objective is to provide users with an understanding of the data distribution and model behaviors for each modality by extracting and visually summarizing the data embeddings. For the indicator data, we directly utilize the raw data without additional processing due to its low dimensionality. In the case of ClinicalBERT, which follows the architecture of the BERT model and comprises 12 transformer layers, each layer generates 768-dimensional embeddings that capture seman_x0002_tic features at various levels. To retain the maximum amount of valid information, we employ common techniques such as summing, averaging, and concatenating selected or all layer embeddings. We adopt the bit-wise sum of all token embeddings as the final 768-dimensional text representation, striking a balance between computational efficiency and information retention. As for ConvNeXt, an optimized CNN that follows the classic CNN architecture, we extract the output from the penultimate layer (i.e., the input of the classifier layer) to obtain a 768-dimensional image representation. To encompass the model\u2019s overall understanding of the data distribution, we define a fusion embedding by concatenating the three embedding vectors and weighting each vector element according to its modality.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "To present the diverse modalities of data, each with a unique structure for each patient, an aggregated approach is employed (R.3). The multimodal information is organized into a collapsible table, as shown in Fig. 2(D1).", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Table;Bar;Image;Pie;Text", "axial_code": [], "componenet_code": ["bar", "table", "text", "pie", "image"]}]}, {"author": "dxf", "index_original": 392, "paper_title": "Leveraging Historical Medical Records as a Proxy via Multimodal Modeling and Visualization to Enrich Medical Diagnostic Learning", "pub_year": 2024, "domain": "Medical", "requirement": {"requirement_text": "R.5: Reveal the relationship between different modalities. When only one type of data is available, inexperienced medical professionals can easily make assessments based on that single modality. However, when faced with data from multiple modalities, they may encounter difficulties and confusion. According to E2, this difficulty arises from a lack of proficiency in correlating information across different modalities. It is crucial for them to establish connections between these modalities, considering that some modalities may present contradictory findings. For example, while a clinically recommended modality may indicate the presence of Benign Prostatic Hyperplasia, no abnormalities may be detected through a medical ultrasound. In such cases, a comprehensive patient analysis, including factors like age and relevant symptoms, should guide the diagnosis to favor the ultrasound results. It is important to note that different diseases may require different interpretations of the relationships between modalities.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "DiagnosisAssistant, is designed to process heterogeneous clinical data, construct robust models, and provide diagnostic insights based on in_x0002_terpretability techniques. To evaluate the system\u2019s performance, we collaborate closely with physicians from a prominent local hospital and used a real-life clinical dataset for diagnosing cervical spine disorders. The dataset comprises 750 patient records collected during hospital visits for cervical spine discomfort between 2012 and 2013. The pa_x0002_tients\u2019 ages range from 21 to 82 years, with a male-to-female ratio of 1.16 : 1. Each patient has a unique CardID that corresponds to a set of clinical records, including demographic information, laboratory test results, magnetic resonance imaging (MRI) images, clinical reports, and diagnostic findings. We organize the data into three modalities, namely, the indicator modality, text modality, and image modality. The indicator modality consists of laboratory test results and demographic information, including gender, age, height, and weight. The unstruc_x0002_tured data includes clinical reports and MRI images. Prior to model training, the dataset was carefully preprocessed and checked, resulting in 626 retained instances.", "data_code": {"textual": 1, "tables": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "In this subsection, we first present our approach for developing a multi_x0002_modal model and subsequently outline our method for unfolding the diagnostic insights derived from the constructed model. Specifically, the design of our model revolves around the classification of patients into three distinct categories, namely, normal, herniated, bulging, as suggested by domain experts. We explore various machine learning techniques for modeling the data across multiple modalities, as summarized in Tab. 1. Subsequently, we select three models for each modality, XGBoost [16] for indicator data, ClinicalBERT [4] for textural data, and ConvNeXt [45] for image data, owing to their superior performance. We fine-tune the hyper_x0002_parameters of the three models using grid search [22] with k-fold cross-validation [67]. The dataset is divided into training and validation sets using a random selection method with a ratio of 75:25 for training and validation, respectively. Table 1 lists the performance of the models on the validation set with the optimal hyper-parameter settings. Fusion Strategy. Fusing heterogeneous information from multi_x0002_modal data is a common strategy to improve model performance [20]. In this study, we investigate two fusion strategies: decision-level fusion and feature-level fusion (discussed in subsection 2.2). In feature-level fusion, we concatenate the raw indicator data with the output from the penultimate layer in the ConvNeXt and ClinicalBERT models and then feed the concatenated features into a 12-head, 12-layer transformer. In decision-level fusion, we adopt a weighted voting strategy and employ multiclass perception to learn the weights of each modality. As shown in Tab. 2, the two strategies perform similarly. However, the decision_x0002_level fusion strategy aligns better with physicians\u2019 diagnostic process and is more robust and scalable in the absence of modalities. Therefore we select the decision-level fusion strategy for our model. Interpretability Towards Diagnosis. To improve users\u2019 learning experience, we apply multiple post-hoc interpretability techniques. Specifically, Guided Grad-CAM [59] is utilized to generate saliency maps on each MRI image and highlight important areas. The method proposed by Chefer et al. [15] is adopted to interpret the transformer model and highlight important words. For the indicator-modal model, SHAP is used to determine the contribution of each feature to a par_x0002_ticular decision. By providing saliency maps, key text highlighting, and feature contribution quantification, comprehensive hints about the diagnostic focus of each case are given to the users to aid in their diag_x0002_nostic learning process. All these explanations are combined to offer interpretability to the model\u2019s decisions.", "solution_category": "data_manipulation", "solution_axial": "Modeling;Explainability", "solution_compoent": "", "axial_code": ["Modeling", "Explainability"], "componenet_code": ["modeling", "explainability"]}, {"solution_text": "each modality data for each patient contributes to the final model output, where the contribution is a probability estimate for each category in the resulting model output (R.4, R.5). To show each modality data\u2019s contributions to different prediction classes (three classes in the running example), bar charts are used. The contribution value is determined by the probability mass function (PMF) [66], which charac_x0002_terizes the distribution of discrete random variables. The contribution values attached to the fusion embedding projection are calculated as weighted sums of the three modalities.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "dxf", "index_original": 394, "paper_title": "Leveraging Historical Medical Records as a Proxy via Multimodal Modeling and Visualization to Enrich Medical Diagnostic Learning", "pub_year": 2024, "domain": "Medical", "requirement": {"requirement_text": "R.5: Reveal the relationship between different modalities. When only one type of data is available, inexperienced medical professionals can easily make assessments based on that single modality. However, when faced with data from multiple modalities, they may encounter difficulties and confusion. According to E2, this difficulty arises from a lack of proficiency in correlating information across different modalities. It is crucial for them to establish connections between these modalities, considering that some modalities may present contradictory findings. For example, while a clinically recommended modality may indicate the presence of Benign Prostatic Hyperplasia, no abnormalities may be detected through a medical ultrasound. In such cases, a comprehensive patient analysis, including factors like age and relevant symptoms, should guide the diagnosis to favor the ultrasound results. It is important to note that different diseases may require different interpretations of the relationships between modalities.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "DiagnosisAssistant, is designed to process heterogeneous clinical data, construct robust models, and provide diagnostic insights based on in_x0002_terpretability techniques. To evaluate the system\u2019s performance, we collaborate closely with physicians from a prominent local hospital and used a real-life clinical dataset for diagnosing cervical spine disorders. The dataset comprises 750 patient records collected during hospital visits for cervical spine discomfort between 2012 and 2013. The pa_x0002_tients\u2019 ages range from 21 to 82 years, with a male-to-female ratio of 1.16 : 1. Each patient has a unique CardID that corresponds to a set of clinical records, including demographic information, laboratory test results, magnetic resonance imaging (MRI) images, clinical reports, and diagnostic findings. We organize the data into three modalities, namely, the indicator modality, text modality, and image modality. The indicator modality consists of laboratory test results and demographic information, including gender, age, height, and weight. The unstruc_x0002_tured data includes clinical reports and MRI images. Prior to model training, the dataset was carefully preprocessed and checked, resulting in 626 retained instances.", "data_code": {"textual": 1, "tables": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "In this subsection, we first present our approach for developing a multi_x0002_modal model and subsequently outline our method for unfolding the diagnostic insights derived from the constructed model. Specifically, the design of our model revolves around the classification of patients into three distinct categories, namely, normal, herniated, bulging, as suggested by domain experts. We explore various machine learning techniques for modeling the data across multiple modalities, as summarized in Tab. 1. Subsequently, we select three models for each modality, XGBoost [16] for indicator data, ClinicalBERT [4] for textural data, and ConvNeXt [45] for image data, owing to their superior performance. We fine-tune the hyper_x0002_parameters of the three models using grid search [22] with k-fold cross-validation [67]. The dataset is divided into training and validation sets using a random selection method with a ratio of 75:25 for training and validation, respectively. Table 1 lists the performance of the models on the validation set with the optimal hyper-parameter settings. Fusion Strategy. Fusing heterogeneous information from multi_x0002_modal data is a common strategy to improve model performance [20]. In this study, we investigate two fusion strategies: decision-level fusion and feature-level fusion (discussed in subsection 2.2). In feature-level fusion, we concatenate the raw indicator data with the output from the penultimate layer in the ConvNeXt and ClinicalBERT models and then feed the concatenated features into a 12-head, 12-layer transformer. In decision-level fusion, we adopt a weighted voting strategy and employ multiclass perception to learn the weights of each modality. As shown in Tab. 2, the two strategies perform similarly. However, the decision_x0002_level fusion strategy aligns better with physicians\u2019 diagnostic process and is more robust and scalable in the absence of modalities. Therefore we select the decision-level fusion strategy for our model. Interpretability Towards Diagnosis. To improve users\u2019 learning experience, we apply multiple post-hoc interpretability techniques. Specifically, Guided Grad-CAM [59] is utilized to generate saliency maps on each MRI image and highlight important areas. The method proposed by Chefer et al. [15] is adopted to interpret the transformer model and highlight important words. For the indicator-modal model, SHAP is used to determine the contribution of each feature to a par_x0002_ticular decision. By providing saliency maps, key text highlighting, and feature contribution quantification, comprehensive hints about the diagnostic focus of each case are given to the users to aid in their diag_x0002_nostic learning process. All these explanations are combined to offer interpretability to the model\u2019s decisions.", "solution_category": "data_manipulation", "solution_axial": "Modeling;Explainability", "solution_compoent": "", "axial_code": ["Modeling", "Explainability"], "componenet_code": ["modeling", "explainability"]}, {"solution_text": "Our objective is to provide users with an understanding of the data distribution and model behaviors for each modality by extracting and visually summarizing the data embeddings. For the indicator data, we directly utilize the raw data without additional processing due to its low dimensionality. In the case of ClinicalBERT, which follows the architecture of the BERT model and comprises 12 transformer layers, each layer generates 768-dimensional embeddings that capture seman_x0002_tic features at various levels. To retain the maximum amount of valid information, we employ common techniques such as summing, averaging, and concatenating selected or all layer embeddings. We adopt the bit-wise sum of all token embeddings as the final 768-dimensional text representation, striking a balance between computational efficiency and information retention. As for ConvNeXt, an optimized CNN that follows the classic CNN architecture, we extract the output from the penultimate layer (i.e., the input of the classifier layer) to obtain a 768-dimensional image representation. To encompass the model\u2019s overall understanding of the data distribution, we define a fusion embedding by concatenating the three embedding vectors and weighting each vector element according to its modality.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "For each modality, the second and third columns display key information for each of the two patients (R.5). This information includes indicator distribution and shapely value data for the indicator modality, comparisons of medical text and their associated weights for the text modality, and RAW images alongside their CAM modes for the image modality", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Table;Bar;Image;Pie;Text", "axial_code": [], "componenet_code": ["bar", "table", "text", "pie", "image"]}]}, {"author": "dxf", "index_original": 395, "paper_title": "Leveraging Historical Medical Records as a Proxy via Multimodal Modeling and Visualization to Enrich Medical Diagnostic Learning", "pub_year": 2024, "domain": "Medical", "requirement": {"requirement_text": "R.6: Support comparative analysis of individual patients and maintain data provenance. Conducting comparative analysis on typical cases of specific diseases holds significant importance in the realm of clinical practice. \u201cBy comparing individual patients from diverse groups, we can broaden our knowledge and comprehension of varied disease pathologies and patient cohorts\u201d, said I2. This aids interns and novice practitioners in developing their diagnostic skills and enables them to make accurate diagnoses in real-life scenarios. Additionally, the system should maintain a log of actions to monitor and track the comparison process. In summary, the comparative analysis of individual patient cases plays a crucial role in enhancing diagnostic accuracy and minimizing the risk of misdiagnosis and missed diagnoses.", "requirement_code": {"collect_evidence": 1, "compare_entities": 1}}, "data": {"data_text": "DiagnosisAssistant, is designed to process heterogeneous clinical data, construct robust models, and provide diagnostic insights based on in_x0002_terpretability techniques. To evaluate the system\u2019s performance, we collaborate closely with physicians from a prominent local hospital and used a real-life clinical dataset for diagnosing cervical spine disorders. The dataset comprises 750 patient records collected during hospital visits for cervical spine discomfort between 2012 and 2013. The pa_x0002_tients\u2019 ages range from 21 to 82 years, with a male-to-female ratio of 1.16 : 1. Each patient has a unique CardID that corresponds to a set of clinical records, including demographic information, laboratory test results, magnetic resonance imaging (MRI) images, clinical reports, and diagnostic findings. We organize the data into three modalities, namely, the indicator modality, text modality, and image modality. The indicator modality consists of laboratory test results and demographic information, including gender, age, height, and weight. The unstruc_x0002_tured data includes clinical reports and MRI images. Prior to model training, the dataset was carefully preprocessed and checked, resulting in 626 retained instances.", "data_code": {"textual": 1, "tables": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "In this subsection, we first present our approach for developing a multi_x0002_modal model and subsequently outline our method for unfolding the diagnostic insights derived from the constructed model. Specifically, the design of our model revolves around the classification of patients into three distinct categories, namely, normal, herniated, bulging, as suggested by domain experts. We explore various machine learning techniques for modeling the data across multiple modalities, as summarized in Tab. 1. Subsequently, we select three models for each modality, XGBoost [16] for indicator data, ClinicalBERT [4] for textural data, and ConvNeXt [45] for image data, owing to their superior performance. We fine-tune the hyper_x0002_parameters of the three models using grid search [22] with k-fold cross-validation [67]. The dataset is divided into training and validation sets using a random selection method with a ratio of 75:25 for training and validation, respectively. Table 1 lists the performance of the models on the validation set with the optimal hyper-parameter settings. Fusion Strategy. Fusing heterogeneous information from multi_x0002_modal data is a common strategy to improve model performance [20]. In this study, we investigate two fusion strategies: decision-level fusion and feature-level fusion (discussed in subsection 2.2). In feature-level fusion, we concatenate the raw indicator data with the output from the penultimate layer in the ConvNeXt and ClinicalBERT models and then feed the concatenated features into a 12-head, 12-layer transformer. In decision-level fusion, we adopt a weighted voting strategy and employ multiclass perception to learn the weights of each modality. As shown in Tab. 2, the two strategies perform similarly. However, the decision_x0002_level fusion strategy aligns better with physicians\u2019 diagnostic process and is more robust and scalable in the absence of modalities. Therefore we select the decision-level fusion strategy for our model. Interpretability Towards Diagnosis. To improve users\u2019 learning experience, we apply multiple post-hoc interpretability techniques. Specifically, Guided Grad-CAM [59] is utilized to generate saliency maps on each MRI image and highlight important areas. The method proposed by Chefer et al. [15] is adopted to interpret the transformer model and highlight important words. For the indicator-modal model, SHAP is used to determine the contribution of each feature to a par_x0002_ticular decision. By providing saliency maps, key text highlighting, and feature contribution quantification, comprehensive hints about the diagnostic focus of each case are given to the users to aid in their diag_x0002_nostic learning process. All these explanations are combined to offer interpretability to the model\u2019s decisions.", "solution_category": "data_manipulation", "solution_axial": "Modeling;Explainability", "solution_compoent": "", "axial_code": ["Modeling", "Explainability"], "componenet_code": ["modeling", "explainability"]}, {"solution_text": "Our objective is to provide users with an understanding of the data distribution and model behaviors for each modality by extracting and visually summarizing the data embeddings. For the indicator data, we directly utilize the raw data without additional processing due to its low dimensionality. In the case of ClinicalBERT, which follows the architecture of the BERT model and comprises 12 transformer layers, each layer generates 768-dimensional embeddings that capture seman_x0002_tic features at various levels. To retain the maximum amount of valid information, we employ common techniques such as summing, averaging, and concatenating selected or all layer embeddings. We adopt the bit-wise sum of all token embeddings as the final 768-dimensional text representation, striking a balance between computational efficiency and information retention. As for ConvNeXt, an optimized CNN that follows the classic CNN architecture, we extract the output from the penultimate layer (i.e., the input of the classifier layer) to obtain a 768-dimensional image representation. To encompass the model\u2019s overall understanding of the data distribution, we define a fusion embedding by concatenating the three embedding vectors and weighting each vector element according to its modality.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "the Comparison View allows for the comparative analysis of individual patients, aiding in the improved understanding of specific diseases (R.6). For the purpose of clinical learning and reasoning, the Comparison View facilitates a more detailed analysis of patient-specific similarities and differences (R.6). Through this view, users can perform a more finegrained comparison of individual patients, considering both aggregated multimodal data and detailed modality fusion. Additionally, users have the option to record notes for future reference.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Table;Bar;Image;Pie;Text", "axial_code": [], "componenet_code": ["bar", "table", "text", "pie", "image"]}]}, {"author": "dxf", "index_original": 396, "paper_title": "Leveraging Historical Medical Records as a Proxy via Multimodal Modeling and Visualization to Enrich Medical Diagnostic Learning", "pub_year": 2024, "domain": "Medical", "requirement": {"requirement_text": "R.6: Support comparative analysis of individual patients and maintain data provenance. Conducting comparative analysis on typical cases of specific diseases holds significant importance in the realm of clinical practice. \u201cBy comparing individual patients from diverse groups, we can broaden our knowledge and comprehension of varied disease pathologies and patient cohorts\u201d, said I2. This aids interns and novice practitioners in developing their diagnostic skills and enables them to make accurate diagnoses in real-life scenarios. Additionally, the system should maintain a log of actions to monitor and track the comparison process. In summary, the comparative analysis of individual patient cases plays a crucial role in enhancing diagnostic accuracy and minimizing the risk of misdiagnosis and missed diagnoses.", "requirement_code": {"collect_evidence": 1, "compare_entities": 1}}, "data": {"data_text": "DiagnosisAssistant, is designed to process heterogeneous clinical data, construct robust models, and provide diagnostic insights based on in_x0002_terpretability techniques. To evaluate the system\u2019s performance, we collaborate closely with physicians from a prominent local hospital and used a real-life clinical dataset for diagnosing cervical spine disorders. The dataset comprises 750 patient records collected during hospital visits for cervical spine discomfort between 2012 and 2013. The pa_x0002_tients\u2019 ages range from 21 to 82 years, with a male-to-female ratio of 1.16 : 1. Each patient has a unique CardID that corresponds to a set of clinical records, including demographic information, laboratory test results, magnetic resonance imaging (MRI) images, clinical reports, and diagnostic findings. We organize the data into three modalities, namely, the indicator modality, text modality, and image modality. The indicator modality consists of laboratory test results and demographic information, including gender, age, height, and weight. The unstruc_x0002_tured data includes clinical reports and MRI images. Prior to model training, the dataset was carefully preprocessed and checked, resulting in 626 retained instances.", "data_code": {"textual": 1, "tables": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The details can be viewed by clicking and expanding each feature, revealing a Beeswarm plot illustrating the data value of each instance (R.6). When the user hovers over a point, the patient information (card-id and class) and the indicator value are displayed (Fig. 3(A)(B))", "solution_category": "interaction", "solution_axial": "Abstract/Elaborate;Selecting", "solution_compoent": "", "axial_code": ["Selecting", "Abstract/Elaborate"], "componenet_code": ["selecting", "abstract_elaborate"]}]}, {"author": "dxf", "index_original": 398, "paper_title": "Quantivine: A Visualization Approach for Large-scale Quantum Circuit Representation and Analysis", "pub_year": 2024, "domain": "Quantum circuit", "requirement": {"requirement_text": "R1: Clarify the components of quantum circuits. When designing quantum algorithms, it is common to use some typical sub-circuits as components of entire circuits. Quantum researchers could easily recognize a typical component in an isolate circuit. However, in a complex quantum circuit that consists of numerous quantum gates, it is challenging to identify and distinguish components from the general quantum circuit diagram. Therefore, new diagrams should clarify the structure of quantum circuits with multiple components.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Quantum Circuit Data", "data_code": {"media": 1, "tables": 1}}, "solution": [{"solution_text": "To incorporate R1, we present a three-step approach for breaking down a typical quantum circuit diagram into hierarchical representations (Fig. 4-B). Quantum Gate Grouping. Qubit Bundling. Layout Strategy. Our approach draws inspiration from techniques such as node grouping and edge bundling that are effective for graph summarization [47]. Afterwards, a semantic-preserving layout strategy is introduced to arrange the new diagram. Quantum Gate Grouping. The semantic tree (Fig. 4-A2) is first employed to guide the segmentation of circuit components. We label attributes on quantum gates based on a user-customized semantic tree. Before grouping, users can interactively fold or unfold the tree nodes to control the level of detail (Sec. 5.2). Then, all quantum gates will be labeled with two attributes. (1) Tree node label: Each gate will be labeled in accord with the nearest unfolded node to which it belongs. For example, if the tree node S1 is folded while S3 is unfolded, then Gate1 and Gate2 would be labeled with S3. (2) Loop time label: As a tree node may be repeated in compilation due to loops, two groups of gates may correspond to the same tree node. We thus label each gate with a time stamp to differentiate separated gate groups that are built from the same function. Subsequently, the gates will be aggregated by attributes to compose super-gates (Fig. 4-B1), analogous to a supernode in a summarized graph. As a result, the quantum circuit is broken down to a series of primitive and component gates. Qubit Bundling. For a circuit with hundreds of qubits, a typical dia_x005f_x0002_gram will display a line for each qubit, which causes massive overlaps when displaying multi-qubit gates. An effective solution to alleviate the problem is adopting the edge bundling technique [30] for combining neighboring edges. However, the classic method needs to be modified for the quantum circuit. Due to the fact that each wire represents a distinct qubit throughout the whole circuit, solely sharing end nodes within a local scope is not a sufficient reason to bundle two qubits together. They may play a different role in other portions of the circuit. Therefore, the bundled qubits must be contiguous and of the same provenance throughout the circuit. The same provenance means these qubits go through the same sequence of primitive or component gates. For example, all the qubits in Fig. 4-B2 go through S1 \u2192 S2 \u2192 S1, so they can be bundled as one super-bit. Layout Strategy. Since the grouping and bundling process breaks up the placements of quantum gates, we present a bottom-up layout strategy to reorganize the placements with minimum circuit length and maximum semantic-preserving. First, we order the sequence of gates by their insertion time stamps (Fig. 4-A3). The time stamp of the primitive gates has been calculated in the node alignment process (Sec. 4.1). On this basis, the time stamp of the component gate is retrieved from its subcomponents. Secondly, we arrange the layout referring to the semantic structure from the bottom to the top. The gates in the same tree node are arranged in a local circuit space following the order of their time stamp. They are laid out on the left most idle place of its correlated qubit wires to compress the length of circuit. If two gates are intersected at the same column, the later gate would be moved backward. Subsequently, the local layouts of sibling tree nodes will be concatenated while calculating layout for their parent node. In this way, we could construct a layout from the bottom to the top that shortens circuit length and preserves the semantic structure.", "solution_category": "data_manipulation", "solution_axial": "Wrangling", "solution_compoent": "", "axial_code": ["Wrangling"], "componenet_code": ["wrangling"]}]}, {"author": "dxf", "index_original": 399, "paper_title": "Quantivine: A Visualization Approach for Large-scale Quantum Circuit Representation and Analysis", "pub_year": 2024, "domain": "Quantum circuit", "requirement": {"requirement_text": "R1: Clarify the components of quantum circuits. When designing quantum algorithms, it is common to use some typical sub-circuits as components of entire circuits. Quantum researchers could easily recognize a typical component in an isolate circuit. However, in a complex quantum circuit that consists of numerous quantum gates, it is challenging to identify and distinguish components from the general quantum circuit diagram. Therefore, new diagrams should clarify the structure of quantum circuits with multiple components.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Quantum Circuit Data, the semantic structure of the quantum circuit", "data_code": {"media": 1, "tables": 1}}, "solution": [{"solution_text": "To incorporate R1, we present a three-step approach for breaking down a typical quantum circuit diagram into hierarchical representations (Fig. 4-B). Quantum Gate Grouping. Qubit Bundling. Layout Strategy. Our approach draws inspiration from techniques such as node grouping and edge bundling that are effective for graph summarization [47]. Afterwards, a semantic-preserving layout strategy is introduced to arrange the new diagram. Quantum Gate Grouping. The semantic tree (Fig. 4-A2) is first employed to guide the segmentation of circuit components. We label attributes on quantum gates based on a user-customized semantic tree. Before grouping, users can interactively fold or unfold the tree nodes to control the level of detail (Sec. 5.2). Then, all quantum gates will be labeled with two attributes. (1) Tree node label: Each gate will be labeled in accord with the nearest unfolded node to which it belongs. For example, if the tree node S1 is folded while S3 is unfolded, then Gate1 and Gate2 would be labeled with S3. (2) Loop time label: As a tree node may be repeated in compilation due to loops, two groups of gates may correspond to the same tree node. We thus label each gate with a time stamp to differentiate separated gate groups that are built from the same function. Subsequently, the gates will be aggregated by attributes to compose super-gates (Fig. 4-B1), analogous to a supernode in a summarized graph. As a result, the quantum circuit is broken down to a series of primitive and component gates. Qubit Bundling. For a circuit with hundreds of qubits, a typical dia_x005f_x0002_gram will display a line for each qubit, which causes massive overlaps when displaying multi-qubit gates. An effective solution to alleviate the problem is adopting the edge bundling technique [30] for combining neighboring edges. However, the classic method needs to be modified for the quantum circuit. Due to the fact that each wire represents a distinct qubit throughout the whole circuit, solely sharing end nodes within a local scope is not a sufficient reason to bundle two qubits together. They may play a different role in other portions of the circuit. Therefore, the bundled qubits must be contiguous and of the same provenance throughout the circuit. The same provenance means these qubits go through the same sequence of primitive or component gates. For example, all the qubits in Fig. 4-B2 go through S1 \u2192 S2 \u2192 S1, so they can be bundled as one super-bit. Layout Strategy. Since the grouping and bundling process breaks up the placements of quantum gates, we present a bottom-up layout strategy to reorganize the placements with minimum circuit length and maximum semantic-preserving. First, we order the sequence of gates by their insertion time stamps (Fig. 4-A3). The time stamp of the primitive gates has been calculated in the node alignment process (Sec. 4.1). On this basis, the time stamp of the component gate is retrieved from its subcomponents. Secondly, we arrange the layout referring to the semantic structure from the bottom to the top. The gates in the same tree node are arranged in a local circuit space following the order of their time stamp. They are laid out on the left most idle place of its correlated qubit wires to compress the length of circuit. If two gates are intersected at the same column, the later gate would be moved backward. Subsequently, the local layouts of sibling tree nodes will be concatenated while calculating layout for their parent node. In this way, we could construct a layout from the bottom to the top that shortens circuit length and preserves the semantic structure.", "solution_category": "data_manipulation", "solution_axial": "Wrangling", "solution_compoent": "", "axial_code": ["Wrangling"], "componenet_code": ["wrangling"]}, {"solution_text": "The Structure View (Fig. 8-A) presents the semantic structure of the quantum circuit as a tree diagram. This view serves to provide an overview of the circuit (R1) and allows for flexible customization of the circuit visualizations (R4). The tree structure corresponds to the semantic tree extracted from the source code (Sec. 4.1). The branches of the tree outline the hierarchical components and repetitive patterns of the circuit. Three interactions based on this view are also designed and further elaborated in Sec. 5.2.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Tree", "axial_code": [], "componenet_code": ["tree"]}]}, {"author": "dxf", "index_original": 400, "paper_title": "Quantivine: A Visualization Approach for Large-scale Quantum Circuit Representation and Analysis", "pub_year": 2024, "domain": "Quantum circuit", "requirement": {"requirement_text": "R1: Clarify the components of quantum circuits. When designing quantum algorithms, it is common to use some typical sub-circuits as components of entire circuits. Quantum researchers could easily recognize a typical component in an isolate circuit. However, in a complex quantum circuit that consists of numerous quantum gates, it is challenging to identify and distinguish components from the general quantum circuit diagram. Therefore, new diagrams should clarify the structure of quantum circuits with multiple components.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Quantum Circuit Data", "data_code": {"media": 1, "tables": 1}}, "solution": [{"solution_text": "To incorporate R1, we present a three-step approach for breaking down a typical quantum circuit diagram into hierarchical representations (Fig. 4-B). Quantum Gate Grouping. Qubit Bundling. Layout Strategy. Our approach draws inspiration from techniques such as node grouping and edge bundling that are effective for graph summarization [47]. Afterwards, a semantic-preserving layout strategy is introduced to arrange the new diagram. Quantum Gate Grouping. The semantic tree (Fig. 4-A2) is first employed to guide the segmentation of circuit components. We label attributes on quantum gates based on a user-customized semantic tree. Before grouping, users can interactively fold or unfold the tree nodes to control the level of detail (Sec. 5.2). Then, all quantum gates will be labeled with two attributes. (1) Tree node label: Each gate will be labeled in accord with the nearest unfolded node to which it belongs. For example, if the tree node S1 is folded while S3 is unfolded, then Gate1 and Gate2 would be labeled with S3. (2) Loop time label: As a tree node may be repeated in compilation due to loops, two groups of gates may correspond to the same tree node. We thus label each gate with a time stamp to differentiate separated gate groups that are built from the same function. Subsequently, the gates will be aggregated by attributes to compose super-gates (Fig. 4-B1), analogous to a supernode in a summarized graph. As a result, the quantum circuit is broken down to a series of primitive and component gates. Qubit Bundling. For a circuit with hundreds of qubits, a typical dia_x005f_x0002_gram will display a line for each qubit, which causes massive overlaps when displaying multi-qubit gates. An effective solution to alleviate the problem is adopting the edge bundling technique [30] for combining neighboring edges. However, the classic method needs to be modified for the quantum circuit. Due to the fact that each wire represents a distinct qubit throughout the whole circuit, solely sharing end nodes within a local scope is not a sufficient reason to bundle two qubits together. They may play a different role in other portions of the circuit. Therefore, the bundled qubits must be contiguous and of the same provenance throughout the circuit. The same provenance means these qubits go through the same sequence of primitive or component gates. For example, all the qubits in Fig. 4-B2 go through S1 \u2192 S2 \u2192 S1, so they can be bundled as one super-bit. Layout Strategy. Since the grouping and bundling process breaks up the placements of quantum gates, we present a bottom-up layout strategy to reorganize the placements with minimum circuit length and maximum semantic-preserving. First, we order the sequence of gates by their insertion time stamps (Fig. 4-A3). The time stamp of the primitive gates has been calculated in the node alignment process (Sec. 4.1). On this basis, the time stamp of the component gate is retrieved from its subcomponents. Secondly, we arrange the layout referring to the semantic structure from the bottom to the top. The gates in the same tree node are arranged in a local circuit space following the order of their time stamp. They are laid out on the left most idle place of its correlated qubit wires to compress the length of circuit. If two gates are intersected at the same column, the later gate would be moved backward. Subsequently, the local layouts of sibling tree nodes will be concatenated while calculating layout for their parent node. In this way, we could construct a layout from the bottom to the top that shortens circuit length and preserves the semantic structure.", "solution_category": "data_manipulation", "solution_axial": "Wrangling", "solution_compoent": "", "axial_code": ["Wrangling"], "componenet_code": ["wrangling"]}, {"solution_text": "The Component View (Fig. 8-B) presents a quantum circuit diagram in a generalized form where subsets of quantum gates are aggregated into components. This view aims to provide a high-level abstraction of the circuit structure, emphasizing the modularity and reusability of the components (R1). The components are grouped and arranged based on their execution time and semantics, as extracted from the source code (Sec. 4.2). The users can use interactions in the Structure View, as detailed in Sec. 5.2, to customize the level of detail in this view (R4). We implement two interactions in the Structure View to enable users to customize the circuit diagrams (R4): folding and highlighting. Folding allows users to fold/unfold items in the Structure View. Initially, all items in the structure tree are collapsed, providing a high-level summary of the circuit. Users can expand the items of interest in the tree to explore specific components in more detail. This allows users to obtain a more detailed view of the circuit by expanding more low-level items. With the highlighting operation, when users select a particular tree item in the Structure View, the corresponding gate will be highlighted in the circuit diagrams in both the Component and Abstraction Views(Fig. 8-B1,C1). This reduces the manual effort required to match items between code structure and the quantum circuit, and the hierarchical highlights provide a clear division of complex circuit diagrams.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "dxf", "index_original": 401, "paper_title": "Quantivine: A Visualization Approach for Large-scale Quantum Circuit Representation and Analysis", "pub_year": 2024, "domain": "Quantum circuit", "requirement": {"requirement_text": "R2: Simplify the patterns of quantum gates. Performing batch operations increases the scale of quantum circuits, and results in repeated patterns in circuit diagrams. In practice, quantum researchers need to identify patterns in diagrams to understand the circuits. Nevertheless, repeatedly examining patterns imposes a substantial cognitive load. Therefore, new diagrams should reveal the patterns of quantum gates and reduce unnecessary Juxtaposition-Similar-Nonsymmetricals.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "Quantum Circuit Data", "data_code": {"media": 1, "tables": 1}}, "solution": [{"solution_text": "To incorporate R1, we present a three-step approach for breaking down a typical quantum circuit diagram into hierarchical representations (Fig. 4-B). Quantum Gate Grouping. Qubit Bundling. Layout Strategy. Our approach draws inspiration from techniques such as node grouping and edge bundling that are effective for graph summarization [47]. Afterwards, a semantic-preserving layout strategy is introduced to arrange the new diagram. Quantum Gate Grouping. The semantic tree (Fig. 4-A2) is first employed to guide the segmentation of circuit components. We label attributes on quantum gates based on a user-customized semantic tree. Before grouping, users can interactively fold or unfold the tree nodes to control the level of detail (Sec. 5.2). Then, all quantum gates will be labeled with two attributes. (1) Tree node label: Each gate will be labeled in accord with the nearest unfolded node to which it belongs. For example, if the tree node S1 is folded while S3 is unfolded, then Gate1 and Gate2 would be labeled with S3. (2) Loop time label: As a tree node may be repeated in compilation due to loops, two groups of gates may correspond to the same tree node. We thus label each gate with a time stamp to differentiate separated gate groups that are built from the same function. Subsequently, the gates will be aggregated by attributes to compose super-gates (Fig. 4-B1), analogous to a supernode in a summarized graph. As a result, the quantum circuit is broken down to a series of primitive and component gates. Qubit Bundling. For a circuit with hundreds of qubits, a typical dia_x005f_x0002_gram will display a line for each qubit, which causes massive overlaps when displaying multi-qubit gates. An effective solution to alleviate the problem is adopting the edge bundling technique [30] for combining neighboring edges. However, the classic method needs to be modified for the quantum circuit. Due to the fact that each wire represents a distinct qubit throughout the whole circuit, solely sharing end nodes within a local scope is not a sufficient reason to bundle two qubits together. They may play a different role in other portions of the circuit. Therefore, the bundled qubits must be contiguous and of the same provenance throughout the circuit. The same provenance means these qubits go through the same sequence of primitive or component gates. For example, all the qubits in Fig. 4-B2 go through S1 \u2192 S2 \u2192 S1, so they can be bundled as one super-bit. Layout Strategy. Since the grouping and bundling process breaks up the placements of quantum gates, we present a bottom-up layout strategy to reorganize the placements with minimum circuit length and maximum semantic-preserving. First, we order the sequence of gates by their insertion time stamps (Fig. 4-A3). The time stamp of the primitive gates has been calculated in the node alignment process (Sec. 4.1). On this basis, the time stamp of the component gate is retrieved from its subcomponents. Secondly, we arrange the layout referring to the semantic structure from the bottom to the top. The gates in the same tree node are arranged in a local circuit space following the order of their time stamp. They are laid out on the left most idle place of its correlated qubit wires to compress the length of circuit. If two gates are intersected at the same column, the later gate would be moved backward. Subsequently, the local layouts of sibling tree nodes will be concatenated while calculating layout for their parent node. In this way, we could construct a layout from the bottom to the top that shortens circuit length and preserves the semantic structure.", "solution_category": "data_manipulation", "solution_axial": "Wrangling", "solution_compoent": "", "axial_code": ["Wrangling"], "componenet_code": ["wrangling"]}, {"solution_text": "Based on a survey of 18 benchmarks of quantum algorithms and expert interviews, we distilled common patterns and abstraction designs of quantum circuits. The classification and visual representations for these patterns are summarized in Fig. 5. The statistic result demonstrates that repetitions are necessary pat_x0002_terns in the design of scalable quantum circuits (Fig. 5-A). One obser_x0002_vation is that researchers utilize loop statements to repeat sub-circuits for creating scalable circuits. The repeated sub-circuits are represented as periodic visual patterns in the general quantum circuit diagrams. We\ncategorize these repetitions owing to their direction, including vertical, horizontal, and diagonal repetitions (Fig. 5-B). In abstractions, each repeated sub-circuit is regarded as a group. To reveal the frequency and the detail of groups, we preserve the first two sub-circuits and the last sub-circuit for each directional repetition, whilst the intermediate units are simplified as dots.", "solution_category": "data_manipulation", "solution_axial": "FeatureSelection", "solution_compoent": "", "axial_code": ["FeatureSelection"], "componenet_code": ["feature_selection"]}, {"solution_text": "The Abstraction View (Fig. 8-C) presents the patterns of quantum gates. It aims to provide a global perspective of the circuit, highlighting the repetitive patterns and simplifying the visual representation (R2). Even though the Component View enables scaling down the circuit diagram to a certain extent, it might still take up a large space due to the numerous repeated patterns. Hence, we identify and visually abstract such patterns using our approach outlined in Sec. 4.3. The level of detail in this view is consistent with the Component View.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "dxf", "index_original": 403, "paper_title": "Quantivine: A Visualization Approach for Large-scale Quantum Circuit Representation and Analysis", "pub_year": 2024, "domain": "Quantum circuit", "requirement": {"requirement_text": "R3: Explicate the context of qubits and quantum gates. Quantum researchers have different concerns in quantum circuit analysis, such as qubit provenance [31], idling [10], quantum gate parallelisms [17] and the entanglement of quantum circuits [32, 35]. These considerations are essential for programming and debugging circuits. However, when exploring large circuit diagrams, these contextual details become intricate and challenging to comprehend. Therefore, new diagrams should explicitly present context information, enabling the analysis of idling, parallelism, and entanglement in quantum circuits.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "Quantum Circuit Data", "data_code": {"media": 1, "tables": 1}}, "solution": [{"solution_text": "We implement two interactions in the Structure View to enable users to customize the circuit diagrams (R4): folding and highlighting. Folding allows users to fold/unfold items in the Structure View. Initially, all items in the structure tree are collapsed, providing a high-level summary of the circuit. Users can expand the items of interest in the tree to explore specific components in more detail. This allows users to obtain a more detailed view of the circuit by expanding more low-level items. With the highlighting operation, when users select a particular tree item in the Structure View, the corresponding gate will be highlighted in the circuit diagrams in both the Component and Abstraction Views (Fig. 8-B1,C1). This reduces the manual effort required to match items between code structure and the quantum circuit, and the hierarchical highlights provide a clear division of complex circuit diagrams. To facilitate exploration of context information, we design a series of interactions for the Context Views in accordance with R3. Users can select a specific qubit in the structure view to display its provenance(Fig. 8-D1). Clicking on the gates associated with this qubit enables navigation to its placement within the context. In the placement view, users can adjust the threshold of parallelism level via a scroll bar. Click_x0002_ing on a specific column of gates shows potential adjustment places for current parallel operations. In terms of connectivity, Quantivine allows users to specify a component and highlight its connections and entanglement behaviors for a more comprehensive view of the circuit.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration;Reconfigure;Filtering", "solution_compoent": "", "axial_code": ["Reconfigure", "Filtering", "Participation/Collaboration"], "componenet_code": ["reconfigure", "filtering", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 404, "paper_title": "Quantivine: A Visualization Approach for Large-scale Quantum Circuit Representation and Analysis", "pub_year": 2024, "domain": "Quantum circuit", "requirement": {"requirement_text": "R4: Adopt familiar visual designs and flexible interactions. As our users are specialized in quantum computing, they have no experience in visual analysis. Thus, a concise and familiar design is preferred. Also, they need flexibly-customized visualizations on demand. For example, some users focus on the outline of the circuit, while other users are interested in details of qubits. New diagrams should thus use visual designs that are intuitive and familiar to quantum researchers.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "Quantum Circuit Data, the semantic structure of the quantum circuit", "data_code": {"media": 1, "tables": 1}}, "solution": [{"solution_text": "To incorporate R1, we present a three-step approach for breaking down a typical quantum circuit diagram into hierarchical representations (Fig. 4-B). Quantum Gate Grouping. Qubit Bundling. Layout Strategy. Our approach draws inspiration from techniques such as node grouping and edge bundling that are effective for graph summarization [47]. Afterwards, a semantic-preserving layout strategy is introduced to arrange the new diagram. Quantum Gate Grouping. The semantic tree (Fig. 4-A2) is first employed to guide the segmentation of circuit components. We label attributes on quantum gates based on a user-customized semantic tree. Before grouping, users can interactively fold or unfold the tree nodes to control the level of detail (Sec. 5.2). Then, all quantum gates will be labeled with two attributes. (1) Tree node label: Each gate will be labeled in accord with the nearest unfolded node to which it belongs. For example, if the tree node S1 is folded while S3 is unfolded, then Gate1 and Gate2 would be labeled with S3. (2) Loop time label: As a tree node may be repeated in compilation due to loops, two groups of gates may correspond to the same tree node. We thus label each gate with a time stamp to differentiate separated gate groups that are built from the same function. Subsequently, the gates will be aggregated by attributes to compose super-gates (Fig. 4-B1), analogous to a supernode in a summarized graph. As a result, the quantum circuit is broken down to a series of primitive and component gates. Qubit Bundling. For a circuit with hundreds of qubits, a typical dia_x005f_x0002_gram will display a line for each qubit, which causes massive overlaps when displaying multi-qubit gates. An effective solution to alleviate the problem is adopting the edge bundling technique [30] for combining neighboring edges. However, the classic method needs to be modified for the quantum circuit. Due to the fact that each wire represents a distinct qubit throughout the whole circuit, solely sharing end nodes within a local scope is not a sufficient reason to bundle two qubits together. They may play a different role in other portions of the circuit. Therefore, the bundled qubits must be contiguous and of the same provenance throughout the circuit. The same provenance means these qubits go through the same sequence of primitive or component gates. For example, all the qubits in Fig. 4-B2 go through S1 \u2192 S2 \u2192 S1, so they can be bundled as one super-bit. Layout Strategy. Since the grouping and bundling process breaks up the placements of quantum gates, we present a bottom-up layout strategy to reorganize the placements with minimum circuit length and maximum semantic-preserving. First, we order the sequence of gates by their insertion time stamps (Fig. 4-A3). The time stamp of the primitive gates has been calculated in the node alignment process (Sec. 4.1). On this basis, the time stamp of the component gate is retrieved from its subcomponents. Secondly, we arrange the layout referring to the semantic structure from the bottom to the top. The gates in the same tree node are arranged in a local circuit space following the order of their time stamp. They are laid out on the left most idle place of its correlated qubit wires to compress the length of circuit. If two gates are intersected at the same column, the later gate would be moved backward. Subsequently, the local layouts of sibling tree nodes will be concatenated while calculating layout for their parent node. In this way, we could construct a layout from the bottom to the top that shortens circuit length and preserves the semantic structure.", "solution_category": "data_manipulation", "solution_axial": "Wrangling", "solution_compoent": "", "axial_code": ["Wrangling"], "componenet_code": ["wrangling"]}, {"solution_text": "The Structure View (Fig. 8-A) presents the semantic structure of the quantum circuit as a tree diagram. This view serves to provide an overview of the circuit (R1) and allows for flexible customization of the circuit visualizations (R4). The tree structure corresponds to the semantic tree extracted from the source code (Sec. 4.1). The branches of the tree outline the hierarchical components and repetitive patterns of the circuit. Three interactions based on this view are also designed and further elaborated in Sec. 5.2.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Tree", "axial_code": [], "componenet_code": ["tree"]}]}, {"author": "dxf", "index_original": 405, "paper_title": "Quantivine: A Visualization Approach for Large-scale Quantum Circuit Representation and Analysis", "pub_year": 2024, "domain": "Quantum circuit", "requirement": {"requirement_text": "R4: Adopt familiar visual designs and flexible interactions. As our users are specialized in quantum computing, they have no experience in visual analysis. Thus, a concise and familiar design is preferred. Also, they need flexibly-customized visualizations on demand. For example, some users focus on the outline of the circuit, while other users are interested in details of qubits. New diagrams should thus use visual designs that are intuitive and familiar to quantum researchers.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "Quantum Circuit Data", "data_code": {"media": 1, "tables": 1}}, "solution": [{"solution_text": "To incorporate R1, we present a three-step approach for breaking down a typical quantum circuit diagram into hierarchical representations (Fig. 4-B). Quantum Gate Grouping. Qubit Bundling. Layout Strategy. Our approach draws inspiration from techniques such as node grouping and edge bundling that are effective for graph summarization [47]. Afterwards, a semantic-preserving layout strategy is introduced to arrange the new diagram. Quantum Gate Grouping. The semantic tree (Fig. 4-A2) is first employed to guide the segmentation of circuit components. We label attributes on quantum gates based on a user-customized semantic tree. Before grouping, users can interactively fold or unfold the tree nodes to control the level of detail (Sec. 5.2). Then, all quantum gates will be labeled with two attributes. (1) Tree node label: Each gate will be labeled in accord with the nearest unfolded node to which it belongs. For example, if the tree node S1 is folded while S3 is unfolded, then Gate1 and Gate2 would be labeled with S3. (2) Loop time label: As a tree node may be repeated in compilation due to loops, two groups of gates may correspond to the same tree node. We thus label each gate with a time stamp to differentiate separated gate groups that are built from the same function. Subsequently, the gates will be aggregated by attributes to compose super-gates (Fig. 4-B1), analogous to a supernode in a summarized graph. As a result, the quantum circuit is broken down to a series of primitive and component gates. Qubit Bundling. For a circuit with hundreds of qubits, a typical dia_x005f_x0002_gram will display a line for each qubit, which causes massive overlaps when displaying multi-qubit gates. An effective solution to alleviate the problem is adopting the edge bundling technique [30] for combining neighboring edges. However, the classic method needs to be modified for the quantum circuit. Due to the fact that each wire represents a distinct qubit throughout the whole circuit, solely sharing end nodes within a local scope is not a sufficient reason to bundle two qubits together. They may play a different role in other portions of the circuit. Therefore, the bundled qubits must be contiguous and of the same provenance throughout the circuit. The same provenance means these qubits go through the same sequence of primitive or component gates. For example, all the qubits in Fig. 4-B2 go through S1 \u2192 S2 \u2192 S1, so they can be bundled as one super-bit. Layout Strategy. Since the grouping and bundling process breaks up the placements of quantum gates, we present a bottom-up layout strategy to reorganize the placements with minimum circuit length and maximum semantic-preserving. First, we order the sequence of gates by their insertion time stamps (Fig. 4-A3). The time stamp of the primitive gates has been calculated in the node alignment process (Sec. 4.1). On this basis, the time stamp of the component gate is retrieved from its subcomponents. Secondly, we arrange the layout referring to the semantic structure from the bottom to the top. The gates in the same tree node are arranged in a local circuit space following the order of their time stamp. They are laid out on the left most idle place of its correlated qubit wires to compress the length of circuit. If two gates are intersected at the same column, the later gate would be moved backward. Subsequently, the local layouts of sibling tree nodes will be concatenated while calculating layout for their parent node. In this way, we could construct a layout from the bottom to the top that shortens circuit length and preserves the semantic structure.", "solution_category": "data_manipulation", "solution_axial": "Wrangling", "solution_compoent": "", "axial_code": ["Wrangling"], "componenet_code": ["wrangling"]}, {"solution_text": "The Component View (Fig. 8-B) presents a quantum circuit diagram in a generalized form where subsets of quantum gates are aggregated into components. This view aims to provide a high-level abstraction of the circuit structure, emphasizing the modularity and reusability of the components (R1). The components are grouped and arranged based on their execution time and semantics, as extracted from the source code (Sec. 4.2). The users can use interactions in the Structure View, as detailed in Sec. 5.2, to customize the level of detail in this view (R4). We implement two interactions in the Structure View to enable users to customize the circuit diagrams (R4): folding and highlighting. Folding allows users to fold/unfold items in the Structure View. Initially, all items in the structure tree are collapsed, providing a high-level summary of the circuit. Users can expand the items of interest in the tree to explore specific components in more detail. This allows users to obtain a more detailed view of the circuit by expanding more low-level items. With the highlighting operation, when users select a particular tree item in the Structure View, the corresponding gate will be highlighted in the circuit diagrams in both the Component and Abstraction Views(Fig. 8-B1,C1). This reduces the manual effort required to match items between code structure and the quantum circuit, and the hierarchical highlights provide a clear division of complex circuit diagrams.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}]}, {"author": "dxf", "index_original": 408, "paper_title": "InnovationInsights: A Visual Analytics Approach for Understanding the Dual Frontiers of Science and Technology", "pub_year": 2024, "domain": "Science and Technology", "requirement": {"requirement_text": "E: Interplay Exploration. Exploring the interplay between science and technology is another goal frequently mentioned by experts. The exploration requires not only the extraction of relational patterns (e.g., paper-patent citations) but also the interpretation of these connections in the context of knowledge transfer.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We leverage the Microsoft Academic Graph (MAG) dataset [67] to retrieve information about scientific research. The dataset consists of 270M research papers and their corresponding meta information, including the title, publication year, topic keywords, doi, author list, author affiliations, and citations.We use the patent records collected in PatentsView [3] to capture the technical inventions and reveal the development of technologies. This dataset contains over 7.9M patents filed through the United States Patent and Trademark Of_x0002_fice (USPTO [4]). A subset of the most relevant patent attributes is carefully selected for analysis, including patent ID, title, application year, assignee name (i.e., the owner of the patent), and cooperative patent classification (CPC, i.e., the category of the patent [1]). The CPC category we used consists of three levels: section, subsection, and group (the lowest level). Private data (e.g., invention disclosures and patents collected by research institutions) are also used.", "data_code": {"sequential": 1, "tables": 1, "temporal": 1}}, "solution": [{"solution_text": "We analyze the data to capture the scientific facts for each research paper and each individual researcher based on a set of carefully defined metrics. Specifically, given a research paper P, the following metrics are designed to help analysts estimate the quality and impact of P in the context of knowledge transfer (Fig. 2(B)),", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "In our work, the input feature matrix X is com_x0002_posed of paper title embeddings using SPECTER [20], a popular API to generate embeddings for research papers. The input graph A represents the citation graph between papers. Given a patent CPC category g, the label matrix Y provides information on whether a paper is cited by a patent from CPC category g within 5 years of publication. We selected a 5-year duration for the experiment because our experts prioritized recently published papers and aimed to determine if a paper would be cited by patents soon after its publication. We split the papers published\nbetween 2001 and 2014 into a training set (70%) and a validation set(30%), and use papers published in 2015 as the test set. For each paper published between 2016 and 2020, we predict its likelihood of being cited by patents in the CPC group g. We use the PyTorch Geometric implementation of GCN [30] and follow the experimental setup pro_x0002_posed by Kipf and Welling [40]. Our model has 200 epochs of training iterations, a learning rate of 0.01, a dropout rate of 0.5, and 16 hidden units. The weights of the neural network (W0 and W1) are trained using gradient descent to minimize the loss L . To determine the likelihood of a paper P published between 2016 and 2020 being cited by patents in the CPC group g within 5 years of publication, we use the predicted probabilities in the softmax output matrix Z obtained from the final model. In addition, to assess its relative importance within a specific range of papers (e.g., those within a research institution), we further convert its probability to a percentile, denoted as Patentabilityg P, which is a scalar ranging from 0 to 100. We apply the above prediction pipeline to the top K patent CPC groups g (denoted as G) based on the number of patents citing our target papers. To assess a paper\u2019s overall patentability across different CPC groups, we compute its average likelihood of being cited by patent CPC groups g in G (i.e., Patentabilityg P), denoted as PatentabilityP. To evaluate a researcher\u2019s overall performance, we calculate the average PatentabilityP of all their papers published between 2016 and 2020. This aggregated value is called the P-index. To the best of our knowledge, the P-index is the first index proposed in the SciSci litera_x0002_ture that measures the extent to which a researcher\u2019s recent papers will be cited by patents in the future, acting as an indicator of a researcher\u2019s potential for commercial success. The higher the P-index, the higher the commercialization potential of the researcher.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The Interplay Graph (Fig. 3(C3)) is the primary visualization component that enables users to explore the detailed citations from patents to papers and reveal the interplay between scientific research and technological inventions (E1, P1). It consists of three components Paper Matrix, Patent Icicle Plot, and Citation Flow, whose design is inspired by a river metaphor (Fig. 4). The Paper Matrix symbolizes the knowledge landscape, and the Citation Flow illustrates the diverse branches of knowledge that ultimately merge into the Patent Icicle Plot, representing vast technological rivers.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Tree", "axial_code": [], "componenet_code": ["tree"]}]}, {"author": "dxf", "index_original": 409, "paper_title": "InnovationInsights: A Visual Analytics Approach for Understanding the Dual Frontiers of Science and Technology", "pub_year": 2024, "domain": "Science and Technology", "requirement": {"requirement_text": "E: Interplay Exploration. Exploring the interplay between science and technology is another goal frequently mentioned by experts. The exploration requires not only the extraction of relational patterns (e.g., paper-patent citations) but also the interpretation of these connections in the context of knowledge transfer.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We leverage the Microsoft Academic Graph (MAG) dataset [67] to retrieve information about scientific research. The dataset consists of 270M research papers and their corresponding meta information, including the title, publication year, topic keywords, doi, author list, author affiliations, and citations.We use the patent records collected in PatentsView [3] to capture the technical inventions and reveal the development of technologies. This dataset contains over 7.9M patents filed through the United States Patent and Trademark Of_x0002_fice (USPTO [4]). A subset of the most relevant patent attributes is carefully selected for analysis, including patent ID, title, application year, assignee name (i.e., the owner of the patent), and cooperative patent classification (CPC, i.e., the category of the patent [1]). The CPC category we used consists of three levels: section, subsection, and group (the lowest level). Private data (e.g., invention disclosures and patents collected by research institutions) are also used.", "data_code": {"sequential": 1, "tables": 1, "temporal": 1}}, "solution": [{"solution_text": "We analyze the data to capture the scientific facts for each research paper and each individual researcher based on a set of carefully defined metrics. Specifically, given a research paper P, the following metrics are designed to help analysts estimate the quality and impact of P in the context of knowledge transfer (Fig. 2(B)),", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Field Timeline. The time dimension is also essential to help users identify trending topics in the dual frontiers. Thus, we designed two groups of horizon graphs to reveal the temporal evolution of different fields in science and technology (E2, Fig. 3(C1)). Each paper or patent field is represented by a horizon graph which shows the temporal evolution in a space-saving way. The x-axis is the timeline. The saturation of the area encodes the papers or patents published or granted in each field every year. Darker color indicates a higher value.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Area", "axial_code": [], "componenet_code": ["area"]}]}, {"author": "dxf", "index_original": 410, "paper_title": "InnovationInsights: A Visual Analytics Approach for Understanding the Dual Frontiers of Science and Technology", "pub_year": 2024, "domain": "Science and Technology", "requirement": {"requirement_text": "E: Interplay Exploration. Exploring the interplay between science and technology is another goal frequently mentioned by experts. The exploration requires not only the extraction of relational patterns (e.g., paper-patent citations) but also the interpretation of these connections in the context of knowledge transfer.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We leverage the Microsoft Academic Graph (MAG) dataset [67] to retrieve information about scientific research. The dataset consists of 270M research papers and their corresponding meta information, including the title, publication year, topic keywords, doi, author list, author affiliations, and citations.We use the patent records collected in PatentsView [3] to capture the technical inventions and reveal the development of technologies. This dataset contains over 7.9M patents filed through the United States Patent and Trademark Of_x0002_fice (USPTO [4]). A subset of the most relevant patent attributes is carefully selected for analysis, including patent ID, title, application year, assignee name (i.e., the owner of the patent), and cooperative patent classification (CPC, i.e., the category of the patent [1]). The CPC category we used consists of three levels: section, subsection, and group (the lowest level). Private data (e.g., invention disclosures and patents collected by research institutions) are also used.", "data_code": {"sequential": 1, "tables": 1, "temporal": 1}}, "solution": [{"solution_text": "We analyze the data to capture the scientific facts for each research paper and each individual researcher based on a set of carefully defined metrics. Specifically, given a research paper P, the following metrics are designed to help analysts estimate the quality and impact of P in the context of knowledge transfer (Fig. 2(B)),", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Technology Inspection View. This view shows additional context information about patent categories (E3, Fig. 3(D)). In addition to patent categories displayed in the Interplay Graph, the experts wanted more details on the assignee distribution and patent topics to aid in decision-making (e.g., identifying potential partners and directions for invention commercialization). Thus, this view includes a two-level sunburst showing the proportion of assignees and a word cloud in the center showing the keywords of the selected patents.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Circle;Text", "axial_code": [], "componenet_code": ["text", "circle"]}]}, {"author": "dxf", "index_original": 411, "paper_title": "InnovationInsights: A Visual Analytics Approach for Understanding the Dual Frontiers of Science and Technology", "pub_year": 2024, "domain": "Science and Technology", "requirement": {"requirement_text": "E: Interplay Exploration. Exploring the interplay between science and technology is another goal frequently mentioned by experts. The exploration requires not only the extraction of relational patterns (e.g., paper-patent citations) but also the interpretation of these connections in the context of knowledge transfer.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We leverage the Microsoft Academic Graph (MAG) dataset [67] to retrieve information about scientific research. The dataset consists of 270M research papers and their corresponding meta information, including the title, publication year, topic keywords, doi, author list, author affiliations, and citations.We use the patent records collected in PatentsView [3] to capture the technical inventions and reveal the development of technologies. This dataset contains over 7.9M patents filed through the United States Patent and Trademark Of_x0002_fice (USPTO [4]). A subset of the most relevant patent attributes is carefully selected for analysis, including patent ID, title, application year, assignee name (i.e., the owner of the patent), and cooperative patent classification (CPC, i.e., the category of the patent [1]). The CPC category we used consists of three levels: section, subsection, and group (the lowest level). Private data (e.g., invention disclosures and patents collected by research institutions) are also used.", "data_code": {"sequential": 1, "tables": 1, "temporal": 1}}, "solution": [{"solution_text": "We analyze the data to capture the scientific facts for each research paper and each individual researcher based on a set of carefully defined metrics. Specifically, given a research paper P, the following metrics are designed to help analysts estimate the quality and impact of P in the context of knowledge transfer (Fig. 2(B)),", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Science Inspection View. To facilitate paper-level exploration, the Science Inspection View shows a paper list (E3, Fig. 3(E)) that can be ranked based on the statistical matrices introduced in Section 4.2 as well as a list of histograms showing distributions of paper-level metrics.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Text", "axial_code": [], "componenet_code": ["text"]}, {"solution_text": "Technology Inspection View. This view shows additional context information about patent categories (E3, Fig. 3(D)). In addition to patent categories displayed in the Interplay Graph, the experts wanted more details on the assignee distribution and patent topics to aid in decision-making (e.g., identifying potential partners and directions for invention commercialization). Thus, this view includes a two-level sunburst showing the proportion of assignees and a word cloud in the center showing the keywords of the selected patents.", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "dxf", "index_original": 416, "paper_title": "Action-Evaluator: A Visualization Approach for Player Action Evaluation in Soccer", "pub_year": 2024, "domain": "Sports", "requirement": {"requirement_text": "R2: Identify similar players on decision styles for comparison. Experts also demand to obtain references on player action improvement from comparing to other players with better performance. Particularly, experts would focus on those players with similar decision styles because their action choices are practical to learn from. It is necessary to support the identification of such players.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "player similarity", "data_code": {"sequential": 1, "geometry": 1, "temporal": 1, "tables": 1, "fields": 1}}, "solution": [{"solution_text": "The two-dimensional locations of the glyphs in the scatterplot are projected by the t-SNE algorithm [56] that takes the learned high-dimensional player embedding vectors as input.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The player ranking list presents players in a sortable list to assist the navigation of the target player (R1) (Fig. 4(A1)). Each row of the list indicates a player, which consists of the name label to denote the player and the indicators to represent the player\u2019s importance, including the total action number and the succeeded action number for each action type (Fig. 4(A3)). The indicators in the same action type are encoded together by a stacked bar chart for effective sorting and comparing, where the dark bar represents the number of succeeded actions, and the whole bar indicates that of all actions.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Scatter;Circle", "axial_code": [], "componenet_code": ["scatter", "circle"]}]}, {"author": "dxf", "index_original": 418, "paper_title": "Action-Evaluator: A Visualization Approach for Player Action Evaluation in Soccer", "pub_year": 2024, "domain": "Sports", "requirement": {"requirement_text": "R4: Investigate player action scores of different action choices. Under the same match situation, players would perform variously with different action choices because of the dynamic nature of soccer matches. Providing action scores of different action choices on risk and reward can help evaluate if the choices are successful under such a match situation and find the actions that need to be improved. The system should illustrate these detailed scores.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "action score", "data_code": {"sequential": 1, "geometry": 1, "temporal": 1, "tables": 1, "fields": 1}}, "solution": [{"solution_text": "the scores of different action choices under the selected match situation are presented in the action score list (R4). The action choice diagram is derived from the match situation diagram, containing a pitch-based action representation, and a pressure bar identical to that in the match situation diagram (R4). The action score list is a sortable list of all action choices conducted under the selected match situation (R4) (Fig. 4(B2)). Each row of the sortable list represents an action choice and consists of an action choice diagram and three bar charts that encode the same three metrics as those in the match situation list.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Heatmap;Bar", "axial_code": [], "componenet_code": ["heatmap", "bar"]}]}, {"author": "dxf", "index_original": 421, "paper_title": "QEVIS: Multi-grained Visualization of Distributed Query Execution", "pub_year": 2024, "domain": "distributed query execution", "requirement": {"requirement_text": "R1: Understand query execution. Query execution can be understood at two scales: job scale and task scale. As an overview, the timing information of the jobs and their dependencies can provide a big picture of query execution. In this view, the analysts can know the general time usage of each high-level component and identify execution bot_x0002_tlenecks by analyzing the job dependencies. To gain a more accurate and comprehensive understanding of the query execution process, a micro-level view is necessary. This involves visualizing the execution progress and data dependencies of the atomic tasks. Such detailed visualization enables analysts to identify the required optimizations and explore potential solutions for addressing the bottlenecks.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "When users run a query, the data collection module collects and processes incoming query execution logs with the stream log en_x0002_gine. Specifically, it persists the basic information and passes logs to the database until execution status indicators (i.e., Pause, Terminate,Complete, Timeout) are detected. To monitor query execution at run time, the stream log engine redirects processed logs to the frontend for visualization. In the meantime, the logs are stored in a database to enable in-depth analysis of completed queries. The data analysis module reads data from the database and calculates anomaly scores. The visualization and interaction module displays the query execution process and provides rich interactions to support interactive exploration.", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "temporal": 1}}, "solution": [{"solution_text": "To effectively visualize the progress of job execution, it is essential to display the timing information, including start time, end time, and time usage, as well as the dependencies among jobs in the query logical plan (T1.1.1 and T1.1.3). This data can be modeled as a DAG enhanced with temporal information (TDAG). To tackle the two challenges, we propose a novel TDAG layout method illustrated in Figure 5. It includes the following steps: \u2022 Step 1. We first simplify the job DAG, see Figure 5(a), to a job dependency tree in Figure 5(b). In particular, the output of a job can be the input of many jobs (e.g., \u2265 2 jobs), and we only preserve the edge from a job to its out-neighbor job with the earliest start time. For example, the output of job 1 serves as input for jobs 4 and 6 but the start time of job 4 is earlier than job 6. Thus, we only keep the edge from job 1 to job 4 in the simplified job tree in Figure 5(b). TDAG simplification is conducted to (i) preserve the starting time order of the jobs, and (ii) keep dependent jobs close to each other to prevent visual clutter. \u2022 Step 2. We then plot a rectangle for each job by using the length of the rectangle to indicate job duration and sort the jobs by their start time, as shown in Figure 5(c). This follows human reading habit\n(i.e., reading from top to bottom) and plots the job starting earlier in a higher position. \u2022 Step 3. We next adjust the layout of jobs by utilizing the job dependencies in the simplified tree. In particular, we check the jobs the top to bottom. For each job, if its out-neighbor job could be placed in the same row with it (i.e., non-overlapping), we move the out-neighbor to the same row with it, and add an edge between these two jobs. For instance, the time duration of job 1 does not overlap with the time duration of its out-neighbor job 4, thus we plot jobs 1 and 4 in the first row, see Figure 5(c). \u2022 Step 4. Last, we refine TDAG layout by (i) adding the other edges in the job DAG, for example, jobs 1 and 6 in Figure 5(e), and (ii) reducing the space by combining the rows that do not overlap, for\nexample, job 3 and job 5 are plotted in the same row in Figure 5(e). We visualize the TDAG of TPC-DS query 5 and query 54 by our TDAG layout algorithm in Figure 4(c) and Figure 4(d), respectively. When compared with the visualizations produced by Tez UI Figure 4(a) and Figure 4(b), the visualization generated by our approach offers two advantages: (1) our visualization takes into account the topological structure of the execution plan, resulting in reduced visual clutter as there are fewer crossings among the links and rectangles; (2) our proposed visualization optimizes screen space utilization by considering both dependencies and time duration, allowing for a more efficient visualization.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "To effectively visualize the progress of job execution, it is essential to display the timing information, including start time, end time, and time usage, as well as the dependencies among jobs in the query logical plan (T1.1.1 and T1.1.3). This data can be modeled as a DAG enhanced with temporal information (TDAG). To tackle the two challenges, we propose a novel TDAG layout method illustrated in Figure 5. It includes the following steps: \u2022 Step 1. We first simplify the job DAG, see Figure 5(a), to a job dependency tree in Figure 5(b). In particular, the output of a job can be the input of many jobs (e.g., \u2265 2 jobs), and we only preserve the edge from a job to its out-neighbor job with the earliest start time. For example, the output of job 1 serves as input for jobs 4 and 6 but the start time of job 4 is earlier than job 6. Thus, we only keep the edge from job 1 to job 4 in the simplified job tree in Figure 5(b). TDAG simplification is conducted to (i) preserve the starting time order of the jobs, and (ii) keep dependent jobs close to each other to prevent visual clutter. \u2022 Step 2. We then plot a rectangle for each job by using the length of the rectangle to indicate job duration and sort the jobs by their start time, as shown in Figure 5(c). This follows human reading habit\n(i.e., reading from top to bottom) and plots the job starting earlier in a higher position. \u2022 Step 3. We next adjust the layout of jobs by utilizing the job dependencies in the simplified tree. In particular, we check the jobs the top to bottom. For each job, if its out-neighbor job could be placed in the same row with it (i.e., non-overlapping), we move the out-neighbor to the same row with it, and add an edge between these two jobs. For instance, the time duration of job 1 does not overlap with the time duration of its out-neighbor job 4, thus we plot jobs 1 and 4 in the first row, see Figure 5(c). \u2022 Step 4. Last, we refine TDAG layout by (i) adding the other edges in the job DAG, for example, jobs 1 and 6 in Figure 5(e), and (ii) reducing the space by combining the rows that do not overlap, for\nexample, job 3 and job 5 are plotted in the same row in Figure 5(e). We visualize the TDAG of TPC-DS query 5 and query 54 by our TDAG layout algorithm in Figure 4(c) and Figure 4(d), respectively. When compared with the visualizations produced by Tez UI Figure 4(a) and Figure 4(b), the visualization generated by our approach offers two advantages: (1) our visualization takes into account the topological structure of the execution plan, resulting in reduced visual clutter as there are fewer crossings among the links and rectangles; (2) our proposed visualization optimizes screen space utilization by considering both dependencies and time duration, allowing for a more efficient visualization.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Tree;Bar", "axial_code": [], "componenet_code": ["tree", "bar"]}]}, {"author": "dxf", "index_original": 422, "paper_title": "QEVIS: Multi-grained Visualization of Distributed Query Execution", "pub_year": 2024, "domain": "distributed query execution", "requirement": {"requirement_text": "R1: Understand query execution. Query execution can be understood at two scales: job scale and task scale. As an overview, the timing information of the jobs and their dependencies can provide a big picture of query execution. In this view, the analysts can know the general time usage of each high-level component and identify execution bot_x0002_tlenecks by analyzing the job dependencies. To gain a more accurate and comprehensive understanding of the query execution process, a micro-level view is necessary. This involves visualizing the execution progress and data dependencies of the atomic tasks. Such detailed visualization enables analysts to identify the required optimizations and explore potential solutions for addressing the bottlenecks.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "When users run a query, the data collection module collects and processes incoming query execution logs with the stream log en_x0002_gine. Specifically, it persists the basic information and passes logs to the database until execution status indicators (i.e., Pause, Terminate,Complete, Timeout) are detected. To monitor query execution at run time, the stream log engine redirects processed logs to the frontend for visualization. In the meantime, the logs are stored in a database to enable in-depth analysis of completed queries. The data analysis module reads data from the database and calculates anomaly scores. The visualization and interaction module displays the query execution process and provides rich interactions to support interactive exploration.", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "temporal": 1}}, "solution": [{"solution_text": "We enhance the job view with two visual designs (i.e., parallelism and phase mode) to show more information for each job (T1.1.2). Parallelism mode: we embed the task parallelism (i.e., number of running tasks) into the job rectangle. Specifically, we use a 1D-heatmap to visualize the number of active tasks over time, as shown in Figure 1(b). A gradient color from white to dark blue encodes the number from one to the maximum number of active tasks for the job. We color the number zero with a special color (e.g., gray) as zero indicates that the execution of the job is suspended and users should pay attention. Phase mode: we visualize the percentage of time used by the three phases of each job, as shown in Figure 1(b1), and use three colors to indicate the three phases. This allows users to better understand the jobs, e.g., classifying them into I/O-bounded or compute-bounded jobs.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Heatmap;Bar", "axial_code": [], "componenet_code": ["heatmap", "bar"]}]}, {"author": "dxf", "index_original": 423, "paper_title": "QEVIS: Multi-grained Visualization of Distributed Query Execution", "pub_year": 2024, "domain": "distributed query execution", "requirement": {"requirement_text": "R1: Understand query execution. Query execution can be understood at two scales: job scale and task scale. As an overview, the timing information of the jobs and their dependencies can provide a big picture of query execution. In this view, the analysts can know the general time usage of each high-level component and identify execution bot_x0002_tlenecks by analyzing the job dependencies. To gain a more accurate and comprehensive understanding of the query execution process, a micro-level view is necessary. This involves visualizing the execution progress and data dependencies of the atomic tasks. Such detailed visualization enables analysts to identify the required optimizations and explore potential solutions for addressing the bottlenecks.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "When users run a query, the data collection module collects and processes incoming query execution logs with the stream log en_x0002_gine. Specifically, it persists the basic information and passes logs to the database until execution status indicators (i.e., Pause, Terminate,Complete, Timeout) are detected. To monitor query execution at run time, the stream log engine redirects processed logs to the frontend for visualization. In the meantime, the logs are stored in a database to enable in-depth analysis of completed queries. The data analysis module reads data from the database and calculates anomaly scores. The visualization and interaction module displays the query execution process and provides rich interactions to support interactive exploration.", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "temporal": 1}}, "solution": [{"solution_text": "To effectively visualize the progress of job execution, it is essential to display the timing information, including start time, end time, and time usage, as well as the dependencies among jobs in the query logical plan (T1.1.1 and T1.1.3). This data can be modeled as a DAG enhanced with temporal information (TDAG). To tackle the two challenges, we propose a novel TDAG layout method illustrated in Figure 5. It includes the following steps: \u2022 Step 1. We first simplify the job DAG, see Figure 5(a), to a job dependency tree in Figure 5(b). In particular, the output of a job can be the input of many jobs (e.g., \u2265 2 jobs), and we only preserve the edge from a job to its out-neighbor job with the earliest start time. For example, the output of job 1 serves as input for jobs 4 and 6 but the start time of job 4 is earlier than job 6. Thus, we only keep the edge from job 1 to job 4 in the simplified job tree in Figure 5(b). TDAG simplification is conducted to (i) preserve the starting time order of the jobs, and (ii) keep dependent jobs close to each other to prevent visual clutter. \u2022 Step 2. We then plot a rectangle for each job by using the length of the rectangle to indicate job duration and sort the jobs by their start time, as shown in Figure 5(c). This follows human reading habit\n(i.e., reading from top to bottom) and plots the job starting earlier in a higher position. \u2022 Step 3. We next adjust the layout of jobs by utilizing the job dependencies in the simplified tree. In particular, we check the jobs the top to bottom. For each job, if its out-neighbor job could be placed in the same row with it (i.e., non-overlapping), we move the out-neighbor to the same row with it, and add an edge between these two jobs. For instance, the time duration of job 1 does not overlap with the time duration of its out-neighbor job 4, thus we plot jobs 1 and 4 in the first row, see Figure 5(c). \u2022 Step 4. Last, we refine TDAG layout by (i) adding the other edges in the job DAG, for example, jobs 1 and 6 in Figure 5(e), and (ii) reducing the space by combining the rows that do not overlap, for\nexample, job 3 and job 5 are plotted in the same row in Figure 5(e). We visualize the TDAG of TPC-DS query 5 and query 54 by our TDAG layout algorithm in Figure 4(c) and Figure 4(d), respectively. When compared with the visualizations produced by Tez UI Figure 4(a) and Figure 4(b), the visualization generated by our approach offers two advantages: (1) our visualization takes into account the topological structure of the execution plan, resulting in reduced visual clutter as there are fewer crossings among the links and rectangles; (2) our proposed visualization optimizes screen space utilization by considering both dependencies and time duration, allowing for a more efficient visualization.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "To effectively visualize the progress of job execution, it is essential to display the timing information, including start time, end time, and time usage, as well as the dependencies among jobs in the query logical plan (T1.1.1 and T1.1.3). This data can be modeled as a DAG enhanced with temporal information (TDAG). To tackle the two challenges, we propose a novel TDAG layout method illustrated in Figure 5. It includes the following steps: \u2022 Step 1. We first simplify the job DAG, see Figure 5(a), to a job dependency tree in Figure 5(b). In particular, the output of a job can be the input of many jobs (e.g., \u2265 2 jobs), and we only preserve the edge from a job to its out-neighbor job with the earliest start time. For example, the output of job 1 serves as input for jobs 4 and 6 but the start time of job 4 is earlier than job 6. Thus, we only keep the edge from job 1 to job 4 in the simplified job tree in Figure 5(b). TDAG simplification is conducted to (i) preserve the starting time order of the jobs, and (ii) keep dependent jobs close to each other to prevent visual clutter. \u2022 Step 2. We then plot a rectangle for each job by using the length of the rectangle to indicate job duration and sort the jobs by their start time, as shown in Figure 5(c). This follows human reading habit\n(i.e., reading from top to bottom) and plots the job starting earlier in a higher position. \u2022 Step 3. We next adjust the layout of jobs by utilizing the job dependencies in the simplified tree. In particular, we check the jobs the top to bottom. For each job, if its out-neighbor job could be placed in the same row with it (i.e., non-overlapping), we move the out-neighbor to the same row with it, and add an edge between these two jobs. For instance, the time duration of job 1 does not overlap with the time duration of its out-neighbor job 4, thus we plot jobs 1 and 4 in the first row, see Figure 5(c). \u2022 Step 4. Last, we refine TDAG layout by (i) adding the other edges in the job DAG, for example, jobs 1 and 6 in Figure 5(e), and (ii) reducing the space by combining the rows that do not overlap, for\nexample, job 3 and job 5 are plotted in the same row in Figure 5(e). We visualize the TDAG of TPC-DS query 5 and query 54 by our TDAG layout algorithm in Figure 4(c) and Figure 4(d), respectively. When compared with the visualizations produced by Tez UI Figure 4(a) and Figure 4(b), the visualization generated by our approach offers two advantages: (1) our visualization takes into account the topological structure of the execution plan, resulting in reduced visual clutter as there are fewer crossings among the links and rectangles; (2) our pro_x0002_posed visualization optimizes screen space utilization by considering both dependencies and time duration, allowing for a more efficient visualization.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Tree;Bar", "axial_code": [], "componenet_code": ["tree", "bar"]}]}, {"author": "dxf", "index_original": 424, "paper_title": "QEVIS: Multi-grained Visualization of Distributed Query Execution", "pub_year": 2024, "domain": "distributed query execution", "requirement": {"requirement_text": "R1: Understand query execution. Query execution can be understood at two scales: job scale and task scale. As an overview, the timing information of the jobs and their dependencies can provide a big picture of query execution. In this view, the analysts can know the general time usage of each high-level component and identify execution bot_x0002_tlenecks by analyzing the job dependencies. To gain a more accurate and comprehensive understanding of the query execution process, a micro-level view is necessary. This involves visualizing the execution progress and data dependencies of the atomic tasks. Such detailed visualization enables analysts to identify the required optimizations and explore potential solutions for addressing the bottlenecks.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "When users run a query, the data collection module collects and processes incoming query execution logs with the stream log en_x0002_gine. Specifically, it persists the basic information and passes logs to the database until execution status indicators (i.e., Pause, Terminate,Complete, Timeout) are detected. To monitor query execution at run time, the stream log engine redirects processed logs to the frontend for visualization. In the meantime, the logs are stored in a database to enable in-depth analysis of completed queries. The data analysis module reads data from the database and calculates anomaly scores. The visualization and interaction module displays the query execution process and provides rich interactions to support interactive exploration.", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "temporal": 1}}, "solution": [{"solution_text": "As introduced in Section 2, a query is converted to a DAG of Map/Reducer jobs during query execution. We denote the DAG as G(J,R), where J is the node set, and each node is a Map or Reducer job in the logic execution plan; R is the edge set, which models the depen_x0002_dencies among the jobs. For each atomic task, we divide its execution process into three phases : (1) input, (2) processing, and (3) output, for fine-grained analysis. We use texe = {(tis,tie),(tsp,tep),(tso,teo)} to denote the start time and end time of each phase in task t. Thus, the data model of task t is a tuple \u27e8ts,te,tj,tm,texe\u27e9, which are the task\u2019s start time, end time, job identifier, machine identifier and the execution phases, respec_x0002_tively. The data model of each job is a tuple \u27e8js, je\u27e9, which are the job\u2019s start time and end time, respectively. A job has many tasks, for job x, its start time js is defined as the earliest start time of all its tasks, i.e., js = min{ts|tj = x}, and its end time je is the latest end time of all its tasks, i.e., je = max{te|tj = x}. With the data model above, the original job DAG is augmented to a temporal DAG (TDAG), which contains start/end time for each job and is key to query execution analysis.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "The task view allows users to analyze the tasks in detail (T1.1.3) and consists of multiple panels . The panel at the top of task view shows all tasks of the query, and the remaining panels display the tasks executed on each machine. Each panel displays the machine id (e.g., dbg19) at the top and uses a scatter plot-based visualization to plot the tasks as points in a square view, as shown in Figure 7(a).", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Tree;Bar", "axial_code": [], "componenet_code": ["tree", "bar"]}]}, {"author": "dxf", "index_original": 425, "paper_title": "QEVIS: Multi-grained Visualization of Distributed Query Execution", "pub_year": 2024, "domain": "distributed query execution", "requirement": {"requirement_text": "R2:  Identify the component worth paying attention to. Identifying key components is crucial for analysts to effectively navigate among the multiple levels of granularity. To facilitate this process, it is necessary to provide sufficient information that enables analysts to pinpoint the essential jobs, machines, and tasks that should be explored.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "When users run a query, the data collection module collects and processes incoming query execution logs with the stream log en_x0002_gine. Specifically, it persists the basic information and passes logs to the database until execution status indicators (i.e., Pause, Terminate,Complete, Timeout) are detected. To monitor query execution at run time, the stream log engine redirects processed logs to the frontend for visualization. In the meantime, the logs are stored in a database to enable in-depth analysis of completed queries. The data analysis module reads data from the database and calculates anomaly scores. The visualization and interaction module displays the query execution process and provides rich interactions to support interactive exploration.", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "temporal": 1}}, "solution": [{"solution_text": "To effectively visualize the progress of job execution, it is essential to display the timing information, including start time, end time, and time usage, as well as the dependencies among jobs in the query logical plan (T1.1.1 and T1.1.3). This data can be modeled as a DAG enhanced with temporal information (TDAG). To tackle the two challenges, we propose a novel TDAG layout method illustrated in Figure 5. It includes the following steps: \u2022 Step 1. We first simplify the job DAG, see Figure 5(a), to a job dependency tree in Figure 5(b). In particular, the output of a job can be the input of many jobs (e.g., \u2265 2 jobs), and we only preserve the edge from a job to its out-neighbor job with the earliest start time. For example, the output of job 1 serves as input for jobs 4 and 6 but the start time of job 4 is earlier than job 6. Thus, we only keep the edge from job 1 to job 4 in the simplified job tree in Figure 5(b). TDAG simplification is conducted to (i) preserve the starting time order of the jobs, and (ii) keep dependent jobs close to each other to prevent visual clutter. \u2022 Step 2. We then plot a rectangle for each job by using the length of the rectangle to indicate job duration and sort the jobs by their start time, as shown in Figure 5(c). This follows human reading habit\n(i.e., reading from top to bottom) and plots the job starting earlier in a higher position. \u2022 Step 3. We next adjust the layout of jobs by utilizing the job dependencies in the simplified tree. In particular, we check the jobs the top to bottom. For each job, if its out-neighbor job could be placed in the same row with it (i.e., non-overlapping), we move the out-neighbor to the same row with it, and add an edge between these two jobs. For instance, the time duration of job 1 does not overlap with the time duration of its out-neighbor job 4, thus we plot jobs 1 and 4 in the first row, see Figure 5(c). \u2022 Step 4. Last, we refine TDAG layout by (i) adding the other edges in the job DAG, for example, jobs 1 and 6 in Figure 5(e), and (ii) reducing the space by combining the rows that do not overlap, for\nexample, job 3 and job 5 are plotted in the same row in Figure 5(e). We visualize the TDAG of TPC-DS query 5 and query 54 by our TDAG layout algorithm in Figure 4(c) and Figure 4(d), respectively. When compared with the visualizations produced by Tez UI Figure 4(a) and Figure 4(b), the visualization generated by our approach offers two advantages: (1) our visualization takes into account the topological structure of the execution plan, resulting in reduced visual clutter as there are fewer crossings among the links and rectangles; (2) our proposed visualization optimizes screen space utilization by considering both dependencies and time duration, allowing for a more efficient visualization.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "dxf", "index_original": 426, "paper_title": "QEVIS: Multi-grained Visualization of Distributed Query Execution", "pub_year": 2024, "domain": "distributed query execution", "requirement": {"requirement_text": "R2:  Identify the component worth paying attention to. Identifying key components is crucial for analysts to effectively navigate among the multiple levels of granularity. To facilitate this process, it is necessary to provide sufficient information that enables analysts to pinpoint the essential jobs, machines, and tasks that should be explored.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "When users run a query, the data collection module collects and processes incoming query execution logs with the stream log en_x0002_gine. Specifically, it persists the basic information and passes logs to the database until execution status indicators (i.e., Pause, Terminate,Complete, Timeout) are detected. To monitor query execution at run time, the stream log engine redirects processed logs to the frontend for visualization. In the meantime, the logs are stored in a database to enable in-depth analysis of completed queries. The data analysis module reads data from the database and calculates anomaly scores. The visualization and interaction module displays the query execution process and provides rich interactions to support interactive exploration.", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "temporal": 1}}, "solution": [{"solution_text": "As introduced in Section 2, a query is converted to a DAG of Map/Reducer jobs during query execution. We denote the DAG as G(J,R), where J is the node set, and each node is a Map or Reducer job in the logic execution plan; R is the edge set, which models the depen_x0002_dencies among the jobs. For each atomic task, we divide its execution process into three phases : (1) input, (2) processing, and (3) output, for fine-grained analysis. We use texe = {(tis,tie),(tsp,tep),(tso,teo)} to denote the start time and end time of each phase in task t. Thus, the data model of task t is a tuple \u27e8ts,te,tj,tm,texe\u27e9, which are the task\u2019s start time, end time, job identifier, machine identifier and the execution phases, respec_x0002_tively. The data model of each job is a tuple \u27e8js, je\u27e9, which are the job\u2019s start time and end time, respectively. A job has many tasks, for job x, its start time js is defined as the earliest start time of all its tasks, i.e., js = min{ts|tj = x}, and its end time je is the latest end time of all its tasks, i.e., je = max{te|tj = x}. With the data model above, the original job DAG is augmented to a temporal DAG (TDAG), which contains start/end time for each job and is key to query execution analysis.", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}, {"solution_text": "The task view allows users to analyze the tasks in detail (T1.1.3) and consists of multiple panels . The panel at the top of task view shows all tasks of the query, and the remaining panels display the tasks executed on each machine. Each panel displays the machine id (e.g., dbg19) at the top and uses a scatter plot-based visualization to plot the tasks as points in a square view, as shown in Figure 7(a).", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Tree;Bar", "axial_code": [], "componenet_code": ["tree", "bar"]}]}, {"author": "dxf", "index_original": 427, "paper_title": "QEVIS: Multi-grained Visualization of Distributed Query Execution", "pub_year": 2024, "domain": "distributed query execution", "requirement": {"requirement_text": "R2:  Identify the component worth paying attention to. Identifying key components is crucial for analysts to effectively navigate among the multiple levels of granularity. To facilitate this process, it is necessary to provide sufficient information that enables analysts to pinpoint the essential jobs, machines, and tasks that should be explored.", "requirement_code": {"identify_main_cause_item": 1}}, "data": {"data_text": "When users run a query, the data collection module collects and processes incoming query execution logs with the stream log en_x0002_gine. Specifically, it persists the basic information and passes logs to the database until execution status indicators (i.e., Pause, Terminate,Complete, Timeout) are detected. To monitor query execution at run time, the stream log engine redirects processed logs to the frontend for visualization. In the meantime, the logs are stored in a database to enable in-depth analysis of completed queries. The data analysis module reads data from the database and calculates anomaly scores. The visualization and interaction module displays the query execution process and provides rich interactions to support interactive exploration.", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "temporal": 1}}, "solution": [{"solution_text": "Abnormal parallelism score (APS). APS measures how well the tasks of a job are parallelized, how many jobs share the similar start time and end time. Abnormal duration score (ADS). It is observed that the time usage of the tasks in a job is usually tightly clustered and unexpectedly long query execution time is usually caused by a few long-running tasks [47], which causes a long tail in the time usage distribution. For instance, Figure 6(d) and Figure 6(e) illustrate the task time usage distribution of two jobs, and the distribution in Figure 6(e) has a long tail, which results in a long job time usage.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "dxf", "index_original": 430, "paper_title": "QEVIS: Multi-grained Visualization of Distributed Query Execution", "pub_year": 2024, "domain": "distributed query execution", "requirement": {"requirement_text": "R3:  Reason about specific execution patterns. After identifying the suspicious jobs or tasks, the next step is to find the reasons that cause their abnormal behavior. To accomplish this, analysts should analyze the execution of jobs and tasks, along with relevant information such as machine profiling. Moreover, all of the information should be appropriately visualized and coordinated to enable effective explorations.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "When users run a query, the data collection module collects and processes incoming query execution logs with the stream log en_x0002_gine. Specifically, it persists the basic information and passes logs to the database until execution status indicators (i.e., Pause, Terminate,Complete, Timeout) are detected. To monitor query execution at run time, the stream log engine redirects processed logs to the frontend for visualization. In the meantime, the logs are stored in a database to enable in-depth analysis of completed queries. The data analysis module reads data from the database and calculates anomaly scores. The visualization and interaction module displays the query execution process and provides rich interactions to support interactive exploration.", "data_code": {"sequential": 1, "clusters_and_sets_and_lists": 1, "temporal": 1}}, "solution": [{"solution_text": "QEVIS supports flexible interactions and cross-view linkage to facilitate multi-view exploration. For example, when the user hovers the mouse on a job in the job view, shown as the purple boundary in Figure 1(b), all tasks of this job will be highlighted in the task view, as purple points in Figure 1(d). Conversely, if the user hovers the mouse on a task point in the task view, the corresponding job in the job view and entity list will be highlighted. Moreover, when the user clicks on points in the task view, the entity list will be expanded to show the corresponding tasks and execution machines, shown as the purple items in Figure 1(e). When the user puts the mouse on an element for more than three seconds, a widget showing detailed information about the element will be displayed. Shown as Figure 1(d2), when we put the mouse on the purple task, the task information such as data read, records processed, and time costs of each phase are displayed.", "solution_category": "interaction", "solution_axial": "Selecting;Connect/Relate", "solution_compoent": "", "axial_code": ["Selecting", "Connect/Relate"], "componenet_code": ["selecting", "connect_relate"]}]}, {"author": "dxf", "index_original": 432, "paper_title": "TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations", "pub_year": 2024, "domain": "time-series", "requirement": {"requirement_text": "T1.2: Identify associations among variables. In multivariate timeseries forecasting, it is challenging to isolate a single variable and explain predictions independently. Therefore, users must first comprehend the associations between input variables to understand how these variables jointly influence the predictions.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "time-series data", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Variable Inspector View (Figure 4 (B)) is designed to support the exploration of relationships between a feature space of continuous or discrete input variables and the target variable (T1.2, T2.2). Inspired by [44], we provide a partition-based correlation matrix that partitions a variable into regions using uniform partitioning, and uses Mosaic plots to overview local relationships of pair-wise variables/representations. The correlation matrix consists of multiple plots, each containing several rectangular regions. The regions are colored based on the local relation metrics, with dark colors indicating a stronger correlation while no color for no correlation. The view presents different information for univariate and multivariate time-series data.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Matrix;Heatmap", "axial_code": [], "componenet_code": ["heatmap", "matrix"]}]}, {"author": "dxf", "index_original": 433, "paper_title": "TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations", "pub_year": 2024, "domain": "time-series", "requirement": {"requirement_text": "T2.1: Compare different representations. It is necessary to provide users with an overview of all the available representations to enable efficient comparison, identify problematic representations, and select appropriate ones. To facilitate a deeper understanding of the varying quality of different representations, it is desirable to allow users to zoom in to details of each representation at the finest level, namely specific windows with salient values, which may be the root cause of unexpected model performance.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "time-series data", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "To provide actionable guidance to end-users in a user-friendly way. TimeTuner supports exploratory analysis of the impact of transforma_x0002_tion methods on predictions by combining the counterfactual explana_x0002_tion metrics with a set of interactions to enable users to customize the generation of counterfactual explanation to find more insights and help their decision-making process. In this section, we introduce the two metrics leveraged for the counterfactual explainer", "solution_category": "data_manipulation", "solution_axial": "Rectification;AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation", "Rectification"], "componenet_code": ["algorithmic_calculation", "rectification"]}, {"solution_text": "The view allows users to upload a time-series dataset for exploration and presents an overview of available representations (T2.1). Statistics, including a number of variables, stationarity, and periodicity of the chosen dataset, are presented. Users can also select one or multiple transformation methods and determine their parameters. For example, in Figure 4 (A), the user selects two commonly adopted transformation methods of smoothing and sampling, and chooses time intervals of one month (1), one season (3), half a year (6), and meaningful periods (13) for sunspots advised by SILSO1. The derived representations are presented in a table with sortable bars for users to have an overview of all representations and quickly identify representations of interest. The table presents each representation as a row and shows its training error, validation error, and ACF value encoded by bar lengths. Clicking on a row of interest will highlight the corresponding representation in Representation View (Figure 4 (D)). Users can sort the table according to different metrics. In Figure 4(A), the table shows that the representation \u2018MA-3/Sk-1\u2019 (which stands for Moving Average with m = 3 and skips with step size 1) has a large training error, indicating an abnormal representation for sunspot forecasting tasks. Upon selection, the corresponding representation is highlighted in Representation View (Figure 4 (D)).", "solution_category": "interaction", "solution_axial": "Selecting;Participation/Collaboration", "solution_compoent": "", "axial_code": ["Selecting", "Participation/Collaboration"], "componenet_code": ["selecting", "participation_collaboration"]}]}, {"author": "dxf", "index_original": 434, "paper_title": "TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations", "pub_year": 2024, "domain": "time-series", "requirement": {"requirement_text": "T2.1: Compare different representations. It is necessary to provide users with an overview of all the available representations to enable efficient comparison, identify problematic representations, and select appropriate ones. To facilitate a deeper understanding of the varying quality of different representations, it is desirable to allow users to zoom in to details of each representation at the finest level, namely specific windows with salient values, which may be the root cause of unexpected model performance.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "time-series data, representations and predictions", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We adopt a simplified LSTM model as illustrated in Figure 3, imple_x0002_mented using the Keras framework [3]. The input matrix is a 3D array of shape (samples \u00d7 time steps \u00d7 features), where samples are the number of sliding windows; time steps correspond to the length of the training data within each window; features demote the number of input variables. We first apply the convolutional layer to detect and extract features from the input data. The generated feature maps are then fed into the LSTM layer, which learns to identify important patterns and make predictions based on them. Next, a dense layer and an output layer are used to make predictions using the learned features. To focus on the impact of different transformation methods rather than other experimental settings, we adopt consistent experimental settings for all transformations. Specifically, we choose commonly used setups of 80:20 train-test data split ratio, and 100 epochs that reach converging state for the sunspots and air pollutants cases, to train a model for each time-series representation. We choose a commonly used evaluation metric, RMSE [62], that can be applied to evaluate the performance of both a time-series representation and individual SLICS.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "To provide actionable guidance to end-users in a user-friendly way. TimeTuner supports exploratory analysis of the impact of transforma_x0002_tion methods on predictions by combining the counterfactual explana_x0002_tion metrics with a set of interactions to enable users to customize the\ngeneration of counterfactual explanation to find more insights and help their decision-making process. In this section, we introduce the two metrics leveraged for the counterfactual explainer", "solution_category": "data_manipulation", "solution_axial": "Explainability", "solution_compoent": "", "axial_code": ["Explainability"], "componenet_code": ["explainability"]}, {"solution_text": "Representation View (Figure 4 (D)) is used to compare different representations (T2.1) meanwhile depicting associations between representations and predictions simultaneously (T2.3). A key challenge here is to be scalable to too many time-series representations by different transformation methods and different parameters, each coupled with bivariate representation- and prediction-level metrics. To address the challenge, we propose a novel design of juxtaposed bivariate stripes, with each time-series representation depicted as a strip and multiple representations juxtaposed from top to bottom. All stripes share the same timeline with that in Temporal View. For instance, based on the skip length given in Profile View, the view in Figure 4 (D) provides a total of 28 representations (seven smoothing alternatives \u00d7 four sam_x0002_pling alternatives for the sunspot data). For each slicing window in a representation, we compute its explanation metric (CORR. / SHAP value) as the first dimension and its performance metric (RMSE) as the second dimension. The two-dimensional values of each window are encoded using a VSUP [22]. As Figure 4 (E1) shows, VSUP is in a wedge shape. We divide the values of the first dimension into eight ranges, while the values of the second dimension are divided into four ranges. The use of VSUP encourages users to conduct more careful inspections of error-high slices and compare them. We also support the exploration of changes in a single-dimension metric, by selecting the wanted metric through the drop-down box. The VSUP palette will be changed to a sequential colormap correspondingly.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Matrix;Heatmap", "axial_code": [], "componenet_code": ["heatmap", "matrix"]}, {"solution_text": "We also support the exploration of changes in a single-dimension metric, by selecting the wanted metric through the drop-down box. The VSUP palette will be changed to a sequential colormap correspondingly.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "dxf", "index_original": 435, "paper_title": "TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations", "pub_year": 2024, "domain": "time-series", "requirement": {"requirement_text": "T2.1: Compare different representations. It is necessary to provide users with an overview of all the available representations to enable efficient comparison, identify problematic representations, and select appropriate ones. To facilitate a deeper understanding of the varying quality of different representations, it is desirable to allow users to zoom in to details of each representation at the finest level, namely specific windows with salient values, which may be the root cause of unexpected model performance.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "time-series multivariate data", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Temporal View (Figure 4 (C)) is designed to visualize the details of different time-series representations for univariate data (T2.1) and variables for multivariate data (T1.1). It consists of two parts: horizon graphs for the context information and a line chart for the details. ? For the context information, each variable of the input time-series data is visualized as a 4-layer horizon graph [28] to help users get a picture of data distribution. The horizon graph is a combination of area graph and histogram, allowing for more efficient use of screen real estate. As an example, it can be found that the number of sunspots dramatically increases in the 1960s (dark blue parts). ? For the details, we use a multiple-line chart with colors to encode different representations/variables selected by users. The selected time period is highlighted in the corresponding horizon graph. The line chart allows users to identify and analyze abnormal representations/variables by examining the details.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Bar;Line", "axial_code": [], "componenet_code": ["line", "bar"]}]}, {"author": "dxf", "index_original": 436, "paper_title": "TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations", "pub_year": 2024, "domain": "time-series", "requirement": {"requirement_text": "T2.2: Analyze the association between representations and variables. Users need to examine the relationship between variables and representations, particularly in multivariate time-series forecasting. Understanding how each variable is correlated with, emphasized, or neglected by certain representations can help explain why certain representations lead to specific results.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "time-series data", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Variable Inspector View (Figure 4 (B)) is designed to support the exploration of relationships between a feature space of continuous or discrete input variables and the target variable (T1.2, T2.2). Inspired by [44], we provide a partition-based correlation matrix that partitions a variable into regions using uniform partitioning, and uses Mosaic plots to overview local relationships of pair-wise variables/representations. The correlation matrix consists of multiple plots, each containing several rectangular regions. The regions are colored based on the local relation metrics, with dark colors indicating a stronger correlation while no color for no correlation. The view presents different information for univariate and multivariate time-series data.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Matrix;Heatmap", "axial_code": [], "componenet_code": ["heatmap", "matrix"]}]}, {"author": "dxf", "index_original": 437, "paper_title": "TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations", "pub_year": 2024, "domain": "time-series", "requirement": {"requirement_text": "T2.3: Analyze the association between representations and predictions. One key task for users seeking to explain models in terms of representations is to understand the effects of different representations on predictions. This involves analyzing the correlation between representations and predictions, focusing on localized correlations that can provide counterfactual explanations.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "time-series data, representations and predictions", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We adopt a simplified LSTM model as illustrated in Figure 3, imple_x0002_mented using the Keras framework [3]. The input matrix is a 3D array of shape (samples \u00d7 time steps \u00d7 features), where samples are the number of sliding windows; time steps correspond to the length of the training data within each window; features demote the number of input variables. We first apply the convolutional layer to detect and extract features from the input data. The generated feature maps are then fed into the LSTM layer, which learns to identify important patterns and make predictions based on them. Next, a dense layer and an output layer are used to make predictions using the learned features. To focus on the impact of different transformation methods rather than other experimental settings, we adopt consistent experimental settings for all transformations. Specifically, we choose commonly used setups of 80:20 train-test data split ratio, and 100 epochs that reach converging state for the sunspots and air pollutants cases, to train a model for each time-series representation. We choose a commonly used evaluation metric, RMSE [62], that can be applied to evaluate the performance of both a time-series representation and individual SLICS.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "To provide actionable guidance to end-users in a user-friendly way. TimeTuner supports exploratory analysis of the impact of transforma_x0002_tion methods on predictions by combining the counterfactual explana_x0002_tion metrics with a set of interactions to enable users to customize the\ngeneration of counterfactual explanation to find more insights and help their decision-making process. In this section, we introduce the two metrics leveraged for the counterfactual explainer", "solution_category": "data_manipulation", "solution_axial": "Explainability", "solution_compoent": "", "axial_code": ["Explainability"], "componenet_code": ["explainability"]}, {"solution_text": "Representation View (Figure 4 (D)) is used to compare different representations (T2.1) meanwhile depicting associations between representations and predictions simultaneously (T2.3). A key challenge here is to be scalable to too many time-series representations by different transformation methods and different parameters, each coupled with bivariate representation- and prediction-level metrics. To address the challenge, we propose a novel design of juxtaposed bivariate stripes, with each time-series representation depicted as a strip and multiple representations juxtaposed from top to bottom. All stripes share the same timeline with that in Temporal View. For instance, based on the skip length given in Profile View, the view in Figure 4 (D) provides a total of 28 representations (seven smoothing alternatives \u00d7 four sam_x0002_pling alternatives for the sunspot data). For each slicing window in a representation, we compute its explanation metric (CORR. / SHAP value) as the first dimension and its performance metric (RMSE) as the second dimension. The two-dimensional values of each window are encoded using a VSUP [22]. As Figure 4 (E1) shows, VSUP is in a wedge shape. We divide the values of the first dimension into eight ranges, while the values of the second dimension are divided into four ranges. The use of VSUP encourages users to conduct more careful inspections of error-high slices and compare them. We also support the exploration of changes in a single-dimension metric, by selecting the wanted metric through the drop-down box. The VSUP palette will be changed to a sequential colormap correspondingly.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Matrix;Heatmap", "axial_code": [], "componenet_code": ["heatmap", "matrix"]}, {"solution_text": "We also support the exploration of changes in a single-dimension metric, by selecting the wanted metric through the drop-down box. The VSUP palette will be changed to a sequential colormap correspondingly.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "dxf", "index_original": 438, "paper_title": "TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations", "pub_year": 2024, "domain": "time-series", "requirement": {"requirement_text": "T2.3: Analyze the association between representations and predictions. One key task for users seeking to explain models in terms of representations is to understand the effects of different representations on predictions. This involves analyzing the correlation between representations and predictions, focusing on localized correlations that can provide counterfactual explanations.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "time-series data, explanation metrics and performance metrics", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "We adopt a simplified LSTM model as illustrated in Figure 3, imple_x0002_mented using the Keras framework [3]. The input matrix is a 3D array of shape (samples \u00d7 time steps \u00d7 features), where samples are the number of sliding windows; time steps correspond to the length of the training data within each window; features demote the number of input variables. We first apply the convolutional layer to detect and extract features from the input data. The generated feature maps are then fed into the LSTM layer, which learns to identify important patterns and make predictions based on them. Next, a dense layer and an output layer are used to make predictions using the learned features. To focus on the impact of different transformation methods rather than other experimental settings, we adopt consistent experimental settings for all transformations. Specifically, we choose commonly used setups of 80:20 train-test data split ratio, and 100 epochs that reach converging state for the sunspots and air pollutants cases, to train a model for each time-series representation. We choose a commonly used evaluation metric, RMSE [62], that can be applied to evaluate the performance of both a time-series representation and individual SLICS.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "To provide actionable guidance to end-users in a user-friendly way. TimeTuner supports exploratory analysis of the impact of transforma_x0002_tion methods on predictions by combining the counterfactual explana_x0002_tion metrics with a set of interactions to enable users to customize the\ngeneration of counterfactual explanation to find more insights and help their decision-making process. In this section, we introduce the two metrics leveraged for the counterfactual explainer", "solution_category": "data_manipulation", "solution_axial": "Explainability", "solution_compoent": "", "axial_code": ["Explainability"], "componenet_code": ["explainability"]}, {"solution_text": "This view is designed to reveal the relationship between explanation metrics and performance metrics (T2.3) and subsequently analyze prediction outliers (T3.1). To support the investigation of each prediction, we calculate its corresponding correlation coefficient, prediction error, and SHAP value. In this way, each prediction can be represented as a point in the scatterplot, as shown in Figure 4 (E3). Each point is encoded based on two-dimensional values schemed using the same VSUP with that in Representation View. Due to the large amount of predictions, displaying all of them would cause visual clutter. Therefore, we use random sampling to reduce the overdrawing problem, along with lasso selection allowing users to see more local details by interaction. The random sampling method can faithfully capture the density information of the whole distribution with the number of prediction points in each region proportional to local data density, which is important for the observation of distribution patterns and outliers. To see more details, users can select an interesting region using the lasso selection tool, highlighting all points in the region. In addition, to show a more accurate prediction-level metric relationship, we provide a table (Figure 4 (E2)) adjacent to the scatterplot showing prediction errors (RMSE) along with correlation coefficients. Users can sort the table according to different metrics for multi-faceted ex_x0002_ploration. The data filtering operation in the scatterplot and the table is\ncoordinated, with lasso selection linked to row filtering of the table.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "Users can sort the table according to different metrics for multi-faceted ex_x0002_ploration. The data filtering operation in the scatterplot and the table is coordinated, with lasso selection linked to row filtering of the table.", "solution_category": "interaction", "solution_axial": "Filtering;Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure", "Filtering"], "componenet_code": ["reconfigure", "filtering"]}]}, {"author": "dxf", "index_original": 439, "paper_title": "TimeTuner: Diagnosing Time Representations for Time-Series Forecasting with Counterfactual Explanations", "pub_year": 2024, "domain": "time-series", "requirement": {"requirement_text": "T3.1: Overview the overall patterns of predictions. Overviewing the status of the predictions, including changes of the predictions over time, and corresponding evaluation metrics, is a prerequisite for downstream tasks such as outlier detection.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "time-series data, explanation metrics and performance metrics", "data_code": {"tables": 1, "clusters_and_sets_and_lists": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "This view is designed to reveal the relationship between explanation metrics and performance metrics (T2.3) and subsequently analyze prediction outliers (T3.1). To support the investigation of each prediction, we calculate its corresponding correlation coefficient, prediction error, and SHAP value. In this way, each prediction can be represented as a point in the scatterplot, as shown in Figure 4 (E3). Each point is encoded based on two-dimensional values schemed using the same VSUP with that in Representation View.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Scatter", "axial_code": [], "componenet_code": ["scatter"]}]}, {"author": "dxf", "index_original": 440, "paper_title": "Causality-Based Visual Analysis of Questionnaire Responses", "pub_year": 2024, "domain": "Questionnaire", "requirement": {"requirement_text": "N1: For the sake of efficiency, it is critical for our system to perform association mining algorithm automatically to dig question combinations with potential causality (N1) instead of enumeration.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "question combinations", "data_code": {"tables": 1, "textual": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "we introduce the formal definition of association rules [5] (Sec.4.1) and propose a new computing method to recommend explainable question combinations by association aggregation(N1, N2). To address these challenges, we used a two-step approach to aggregate association rules. In the first step, we aggregated rules of different directions. In the second step, we aggregated options: 1. Merge association rules [5] X ? Y, if the option set of X \u222aY is equal. Use the results of X \u222aY to represent them. For example, we will merge (Q1 : yes,Q2 : no ? Q3 : yes) and (Q3 : yes,Q2 : no ? Q1 : yes). And (Q1 : yes,Q2 : no,Q3 : yes) will be used to represent the above mentioned two association rules. 2. Merge the results of the previous step if their option sets correspond to the same set of questions. Use the questions to represent them. For example, we get two option sets: (Q1 : yes,Q2 : no,Q3 : yes) and (Q1 : yes,Q2 : yes,Q3 : no). We can combine them and use (Q1,Q2,Q3) to represent them.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "dxf", "index_original": 444, "paper_title": "Causality-Based Visual Analysis of Questionnaire Responses", "pub_year": 2024, "domain": "Questionnaire", "requirement": {"requirement_text": "N3: Therefore, these combinations (associations)  can be orderly explored via the help of quantitative indicators (N3).", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "question combinations", "data_code": {"tables": 1, "textual": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "According to the design needs and the workflow, this view should provide users with details of each question combination (N2, N3) and how individual questions are related to each combination (N6). Besides, the respondent groups should also be displayed to help analysts compare the number of major respondents between different question combinations (N2). Question combinations are the core concept in our analysis workflow. This view uses a matrix-based method from UpSet to visualize question combinations (Fig. 2B). Each row represents a question, while each column represents a combination. For each question, we label its id on the left side. And analysts can view the question description by hovering the mouse on the id. For combinations, each of them has two properties: respondent classification (N2) and question relevance (N3). Respondent classification. For a combination, we classify respondents by their answers to the questions in the combination. In our implementation, we take the top n (n is calculated in Sec. 4.2) groups of question options in each combination, representing the answers of the majority of respondents to those specific questions. In other words, the majority group for a question combination includes all the respondents whose answers fall into the top option groups. The more concentrated the responses are, the more easily this combination can be interpreted. We adopt bar charts to reveal this information. The height of each bar is used to encode the number of the majority group, which can effectively show how concentrated or diverse the majority of answers to a question combination are. For example, if there are a great number of respon_x0002_dents choosing the same options for the questions, it is considered an abnormal pattern that is worth further investigation. Question relevance. A question combination may contain two or more questions. We use black and gray colors to indicate whether a question is contained by a combination or not. And we use a black line to connect all included questions following the design of UpSet. Beyond the inner questions, the experts want to look further at how important a question combination is, and how relevant a non_x0002_included question is to the hyperedge. Thus they can be more targeted in some questions and perform further analysis. Therefore, for each question combination, we use the width of connection lines to encode the number of occurrences of the current combinations among all solved combinations. For example, the solved question combinations are: [Q1, Q2], [Q1, Q3], [Q2,Q3], [Q1, Q2, Q3]. Then the width of connection line encoding [Q1, Q2] (which appears in both [Q1, Q2] and [Q1, Q2, Q3]) is wider than [Q1, Q2, Q3] (which only appears in [Q1, Q2, Q3]). This encoding allows users to simply find important combinations which occur most often in the large space of all combinations. For the questions that are not included in that particular combination, we will analyze their correlation with the combination. For the most relevant question, we use darker gray color to encode it in this view. And the darkness is linearly correlated to the calculated value. This question must be relevant to the corresponding question combination and can hint about the direction for further exploration. With these visual encodings, users can more intuitively discover the pattern of questions and combinations in the dataset.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "According to the design needs and the workflow, this view should provide users with details of each question combination (N2, N3) and how individual questions are related to each combination (N6). Besides, the respondent groups should also be displayed to help analysts compare the number of major respondents between different question combinations (N2). Question combinations are the core concept in our analysis workflow. This view uses a matrix-based method from UpSet to visualize question combinations (Fig. 2B). Each row represents a question, while each column represents a combination. For each question, we label its id on the left side. And analysts can view the question description by hovering the mouse on the id. For combinations, each of them has two properties: respondent classification (N2) and question relevance (N3). Respondent classification. For a combination, we classify respondents by their answers to the questions in the combination. In our implementation, we take the top n (n is calculated in Sec. 4.2) groups of question options in each combination, representing the answers of the majority of respondents to those specific questions. In other words, the majority group for a question combination includes all the respondents whose answers fall into the top option groups. The more concentrated the responses are, the more easily this combination can be interpreted. We adopt bar charts to reveal this information. The height of each bar is used to encode the number of the majority group, which can effectively show how concentrated or diverse the majority of answers to a question combination are. For example, if there are a great number of respon_x0002_dents choosing the same options for the questions, it is considered an abnormal pattern that is worth further investigation. Question relevance. A question combination may contain two or more questions. We use black and gray colors to indicate whether a question is contained by a combination or not. And we use a black line to connect all included questions following the design of UpSet. Beyond the inner questions, the experts want to look further at how important a question combination is, and how relevant a non_x0002_included question is to the hyperedge. Thus they can be more targeted in some questions and perform further analysis. Therefore, for each question combination, we use the width of connection lines to encode the number of occurrences of the current combinations among all solved combinations. For example, the solved question combinations are: [Q1, Q2], [Q1, Q3], [Q2,Q3], [Q1, Q2, Q3]. Then the width of connection line encoding [Q1, Q2] (which appears in both [Q1, Q2] and [Q1, Q2, Q3]) is wider than [Q1, Q2, Q3] (which only appears in [Q1, Q2, Q3]). This encoding allows users to simply find important combinations which occur most often in the large space of all combinations. For the questions that are not included in that particular combination, we will analyze their correlation with the combination. For the most relevant question, we use darker gray color to encode it in this view. And the darkness is linearly correlated to the calculated value. This question must be relevant to the corresponding question combination and can hint about the direction for further exploration. With these visual encodings, users can more intuitively discover the pattern of questions and combinations in the dataset.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Bar;Circle;Line", "axial_code": [], "componenet_code": ["line", "bar", "circle"]}, {"solution_text": "For each question, we label its id on the left side. And analysts can view the question description by hovering the mouse on the id. For combinations, each of them has two properties: respondent classification (N2) and question relevance (N3).", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "dxf", "index_original": 447, "paper_title": "Causality-Based Visual Analysis of Questionnaire Responses", "pub_year": 2024, "domain": "Questionnaire", "requirement": {"requirement_text": "N5: Models need to be adjusted manually based on users\u2019 prior knowledge. To be specific, the system requires interactive provision (N5) and visualization of the causal relationship between selected questions.", "requirement_code": {"explain_differences": 1}}, "data": {"data_text": "question combinations", "data_code": {"tables": 1, "textual": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "In our system, we directly adopted the Fast Greedy Equivalence Search (FGES) algorithm [35] to construct causal relationships between questions and used a DAG design [33] to visualize the final structure.", "solution_category": "data_manipulation", "solution_axial": "Retrieval", "solution_compoent": "", "axial_code": ["Retrieval"], "componenet_code": ["retrieval"]}, {"solution_text": "a causal view for showing causality and linear regression among questions in a question combination (N5). Refer to Causality Explorer [44], every question (node) is represented by a pie chart (Fig. 2C), where each sector encodes the proportion of an answer. This can help analysts learn the distribution of answers for each question and provide guidance for exploration. For instance, the analyst can quickly identify questions that most respondents share the same answer. The outer ring of nodes reveals how well a question is fitted by the linear regression of its cause questions (Fig. 2C). The better the fit, the greater the angle As one of the most important variables when analyzing, we encode uncertainty using link thickness, which is considered the most effective visual channel [22] for lines. Thicker edges represent lower uncertainties. Linear regression coefficients, on the other hand, not only indicate the strength of the influence, but also show whether the influence is positive or negative. They are another important property that analysts are concerned about, so we just place the precise values of coefficients right next to the edges. We adopt the DAG layout proposed by Xiao et al. [44], where cell positions are determined by their casual relationships. For example, if Q1 causes Q2, the vertical position of Q1 is lower than Q2, which helps analysts quickly understand the directions of causal relationships.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "Pie;Tree", "axial_code": [], "componenet_code": ["tree", "pie"]}]}, {"author": "dxf", "index_original": 448, "paper_title": "Causality-Based Visual Analysis of Questionnaire Responses", "pub_year": 2024, "domain": "Questionnaire", "requirement": {"requirement_text": "N6: At this stage, question combinations showing high relevance to others are likely to pique analysts\u2019 interest. Hence, it is necessary to visualize the relationships between different question combinations (N6), as the stronger the relationship between a combination of questions, the more likely it is to be at the center of the causality.", "requirement_code": {"discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "question combinations", "data_code": {"tables": 1, "textual": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "To address the issue of sparsity and help analysts understand the different aspects of the questions, we adopted a clustering algorithm based on weighted sets [42] to cluster the questions automatically. We consider the number of majority respondents of a question combination as weights. This method uses a greedy algorithm to divide the questions automatically and does not require the user to specify the number of clusters in advance. The visualization of each cluster is based on UpSet as described above and is vertically laid out in the question combination view. Furthermore, we provide users with a button to toggle whether to display questions by cluster or not. Users can choose it according to their preferences and the features of the dataset.After the exploration of question combinations, analysts will further select the combination of interest via information from different encod_x0002_ings. The information in a question combination can be divided into the causal structure between questions (N4) and the relationship between respondents and questions (N2). These two types of information can be checked respectively in the causal view and respondent view.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "According to the design needs and the workflow, this view should provide users with details of each question combination (N2, N3) and how individual questions are related to each combination (N6). Besides, the respondent groups should also be displayed to help analysts compare the number of major respondents between different question combinations (N2). Question combinations are the core concept in our analysis workflow. This view uses a matrix-based method from UpSet to visualize question combinations (Fig. 2B). Each row represents a question, while each column represents a combination. For each question, we label its id on the left side. And analysts can view the question description by hovering the mouse on the id. For combinations, each of them has two properties: respondent classification (N2) and question relevance (N3). Respondent classification. For a combination, we classify respondents by their answers to the questions in the combination. In our implementation, we take the top n (n is calculated in Sec. 4.2) groups of question options in each combination, representing the answers of the majority of respondents to those specific questions. In other words, the majority group for a question combination includes all the respondents whose answers fall into the top option groups. The more concentrated the responses are, the more easily this combination can be interpreted. We adopt bar charts to reveal this information. The height of each bar is used to encode the number of the majority group, which can effectively show how concentrated or diverse the majority of answers to a question combination are. For example, if there are a great number of respon_x0002_dents choosing the same options for the questions, it is considered an abnormal pattern that is worth further investigation. Question relevance. A question combination may contain two or more questions. We use black and gray colors to indicate whether a question is contained by a combination or not. And we use a black line to connect all included questions following the design of UpSet. Beyond the inner questions, the experts want to look further at how important a question combination is, and how relevant a non_x0002_included question is to the hyperedge. Thus they can be more targeted in some questions and perform further analysis. Therefore, for each question combination, we use the width of connection lines to encode the number of occurrences of the current combinations among all solved combinations. For example, the solved question combinations are: [Q1, Q2], [Q1, Q3], [Q2,Q3], [Q1, Q2, Q3]. Then the width of connection line encoding [Q1, Q2] (which appears in both [Q1, Q2] and [Q1, Q2, Q3]) is wider than [Q1, Q2, Q3] (which only appears in [Q1, Q2, Q3]). This encoding allows users to simply find important combinations which occur most often in the large space of all combinations. For the questions that are not included in that particular combination, we will analyze their correlation with the combination. For the most relevant question, we use darker gray color to encode it in this view. And the darkness is linearly correlated to the calculated value. This question must be relevant to the corresponding question combination and can hint about the direction for further exploration. With these visual encodings, users can more intuitively discover the pattern of questions and combinations in the dataset.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Bar;Circle;Line", "axial_code": [], "componenet_code": ["line", "bar", "circle"]}]}, {"author": "dxf", "index_original": 452, "paper_title": "MolSieve: A Progressive Visual Analytics System for Molecular Dynamics Simulations", "pub_year": 2024, "domain": "physio-chemical", "requirement": {"requirement_text": "R2: Automatic calculation of analyst-defined properties. The trajectory should be populated with automatically calculated properties that can be defined by an analyst. Time should only be spent computing properties for regions that are potentially interesting. The results should be stored in a data-base for future use.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "Molecular dynamics data, the results of analyst-defined Python scripts", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "To support a wide range of simulations, MolSieve automatically executes, stores, and renders the results of analyst-defined Python scripts (R2, R6). This feature enables analysts to specify properties that indicate a region of interest for the simulation they are studying.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "dxf", "index_original": 453, "paper_title": "MolSieve: A Progressive Visual Analytics System for Molecular Dynamics Simulations", "pub_year": 2024, "domain": "physio-chemical", "requirement": {"requirement_text": "R3: Highlight potentially interesting sub-regions. Once the expertdefined properties are rendered, the analyst should be guided towards sub-regions within transition regions that potentially express a change in the system\u2019s behavior. While R2 focuses on calculating properties, guided visual exploration is another crucial aspect that accelerates the discovery process.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "Molecular dynamics data", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "We used Perron Cluster Cluster Analysis (PCCA) [13] as implemented by pyGPCCA [39] as the basis for MolSieve\u2019s simplification scheme. PCCA has been proven to accurately simplify MD trajectories by clustering together groups of kinetically linked states [20, 21]. PCCA can be applied to simulations where transitions are modeled as a Markov chain. MolSieve simplifies the trajectory by dividing it into tentative transi_x0002_tion regions and super-states. This is achieved by first running PCCA on the trajectory, which divides it into N meta-stable sets of states, referred to as clusters. PCCA assigns a vector of N cluster membership probabilities to each individual state which describes how strongly it belongs to each cluster (Figure 3.2). Then, each individual state\u2019s mem_x0002_bership probability is compared to a threshold set by the analyst (Figure 3.3); if its maximum membership probability is above the threshold,\nit is considered part of a super-state; otherwise, it is considered to be part of a possible transition region (i.e., it occurs in regions where the trajectory moves between clusters). If the simplification threshold is set to its maximum value of 1.0, no portion of the trajectory will be simplified, and every state will be considered a transition region.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Control Charts Each control chart displays the moving average of a roperty inside a transition region and it is colored based on the distance of the moving average from the mean. If the moving average moves one standard deviation above the mean, it is colored blue. If the moving average moves one standard deviation below the mean, it is colored orange, and if it stays within the control limits, it is colored light gray. This coloring scheme, inspired by ColorBrewer [18], draws attention to sequences within the transition region where a change is occurring, and allows analysts to quickly determine what sub-regions are of special interest, fulfilling R3 and R4. Hovering over a control chart displays a tooltip with the current value of the property and associated time-step. ", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "Control Charts Each control chart displays the moving average of a roperty inside a transition region and it is colored based on the distance of the moving average from the mean. If the moving average moves one standard deviation above the mean, it is colored blue. If the moving average moves one standard deviation below the mean, it is colored orange, and if it stays within the control limits, it is colored light gray. This coloring scheme, inspired by ColorBrewer [18], draws attention to sequences within the transition region where a change is occurring, and allows analysts to quickly determine what sub-regions are of special interest, fulfilling R3 and R4. Hovering over a control chart displays a tooltip with the current value of the property and associated time-step. ", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 454, "paper_title": "MolSieve: A Progressive Visual Analytics System for Molecular Dynamics Simulations", "pub_year": 2024, "domain": "physio-chemical", "requirement": {"requirement_text": "R4: Select, compare, and inspect regions of interest in detail. Integrating R1\u20133 should enable the analyst to effectively select and refine regions of interest in a responsive manner, as well as allowing them to inspect a set of customized properties through expressive visualizations.", "requirement_code": {"describe_observation_item": 1, "compare_entities": 1}}, "data": {"data_text": "Molecular dynamics data", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "We used Perron Cluster Cluster Analysis (PCCA) [13] as implemented by pyGPCCA [39] as the basis for MolSieve\u2019s simplification scheme. PCCA has been proven to accurately simplify MD trajectories by clustering together groups of kinetically linked states [20, 21]. PCCA can be applied to simulations where transitions are modeled as a Markov chain. MolSieve simplifies the trajectory by dividing it into tentative transi_x0002_tion regions and super-states. This is achieved by first running PCCA on the trajectory, which divides it into N meta-stable sets of states, referred to as clusters. PCCA assigns a vector of N cluster membership probabilities to each individual state which describes how strongly it belongs to each cluster (Figure 3.2). Then, each individual state\u2019s mem_x0002_bership probability is compared to a threshold set by the analyst (Figure 3.3); if its maximum membership probability is above the threshold,\nit is considered part of a super-state; otherwise, it is considered to be part of a possible transition region (i.e., it occurs in regions where the trajectory moves between clusters). If the simplification threshold is set to its maximum value of 1.0, no portion of the trajectory will be simplified, and every state will be considered a transition region.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Control Charts Each control chart displays the moving average of a roperty inside a transition region and it is colored based on the distance of the moving average from the mean. If the moving average moves one standard deviation above the mean, it is colored blue. If the moving average moves one standard deviation below the mean, it is colored orange, and if it stays within the control limits, it is colored light gray. This coloring scheme, inspired by ColorBrewer [18], draws attention to sequences within the transition region where a change is occurring, and allows analysts to quickly determine what sub-regions are of special interest, fulfilling R3 and R4. Hovering over a control chart displays a tooltip with the current value of the property and associated time-step. ", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Bar", "axial_code": [], "componenet_code": ["bar"]}, {"solution_text": "Control Charts Each control chart displays the moving average of a roperty inside a transition region and it is colored based on the distance of the moving average from the mean. If the moving average moves one standard deviation above the mean, it is colored blue. If the moving average moves one standard deviation below the mean, it is colored orange, and if it stays within the control limits, it is colored light gray. This coloring scheme, inspired by ColorBrewer [18], draws attention to sequences within the transition region where a change is occurring, and allows analysts to quickly determine what sub-regions are of special interest, fulfilling R3 and R4. Hovering over a control chart displays a tooltip with the current value of the property and associated time-step. ", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "dxf", "index_original": 455, "paper_title": "MolSieve: A Progressive Visual Analytics System for Molecular Dynamics Simulations", "pub_year": 2024, "domain": "physio-chemical", "requirement": {"requirement_text": "R4: Select, compare, and inspect regions of interest in detail. Integrating R1\u20133 should enable the analyst to effectively select and refine regions of interest in a responsive manner, as well as allowing them to inspect a set of customized properties through expressive visualizations.", "requirement_code": {"describe_observation_item": 1, "compare_entities": 1}}, "data": {"data_text": "Molecular dynamics data", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "Upon hovering over a Transition Region View, a toolbar with multiple controls is displayed. The Select sub-region button toggles a brush to select sub-regions of interest; completing the selection generates a corresponding SubSequence component (Section 4.2; R4; Figure 6a) To facilitate making fine-grained selections within a region, the Zoom into region button enlarges the transition region so it occupies the entirety of the screen.", "solution_category": "interaction", "solution_axial": "OverviewandExplore", "solution_compoent": "", "axial_code": ["OverviewandExplore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "dxf", "index_original": 456, "paper_title": "MolSieve: A Progressive Visual Analytics System for Molecular Dynamics Simulations", "pub_year": 2024, "domain": "physio-chemical", "requirement": {"requirement_text": "R4: Select, compare, and inspect regions of interest in detail. Integrating R1\u20133 should enable the analyst to effectively select and refine regions of interest in a responsive manner, as well as allowing them to inspect a set of customized properties through expressive visualizations.", "requirement_code": {"describe_observation_item": 1, "compare_entities": 1}}, "data": {"data_text": "Molecular dynamics data", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "Sub-Sequence Components (Figure 2.T2) are added to the bottom of the screen once an analyst completes a selection in a Transition Region View using the Select sub-region button (Figure 6a). They are designed to fulfill R4 and R5, as they allow experts to glean additional insight from regions that they deem to be interesting, and correspond to the abstract/elaborate interaction category in Yi et al. [53].", "solution_category": "interaction", "solution_axial": "Abstract/Elaborate", "solution_compoent": "", "axial_code": ["Abstract/Elaborate"], "componenet_code": ["abstract_elaborate"]}]}, {"author": "dxf", "index_original": 457, "paper_title": "MolSieve: A Progressive Visual Analytics System for Molecular Dynamics Simulations", "pub_year": 2024, "domain": "physio-chemical", "requirement": {"requirement_text": "R5: On-demand calculation of detailed analyses. The selection process detailed in R4 generates sub-regions that may include states that express behaviors of interest. Understanding their behavior requires physically grounded analyses which can be computationally expensive. The analyst should be able to request these analyses on demand and be able to continue exploring the trajectory.", "requirement_code": {"collect_evidence": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "Molecular dynamics data", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "These multi-variate charts (Figures 1.4a, 1.4b) are dynamically added to each Transition Region View, allowing the analyst to combine various properties to generate more powerful control charts that highlight synchronized movements across property values that are difficult to detect using single variable charts, fulfilling R5.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "dxf", "index_original": 458, "paper_title": "MolSieve: A Progressive Visual Analytics System for Molecular Dynamics Simulations", "pub_year": 2024, "domain": "physio-chemical", "requirement": {"requirement_text": "R5: On-demand calculation of detailed analyses. The selection process detailed in R4 generates sub-regions that may include states that express behaviors of interest. Understanding their behavior requires physically grounded analyses which can be computationally expensive. The analyst should be able to request these analyses on demand and be able to continue exploring the trajectory.", "requirement_code": {"collect_evidence": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "Molecular dynamics data", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "Sub-Sequence Components (Figure 2.T2) are added to the bottom of the screen once an analyst completes a selection in a Transition Region View using the Select sub-region button (Figure 6a). They are designed to fulfill R4 and R5, as they allow experts to glean additional insight from regions that they deem to be interesting, and correspond to the abstract/elaborate interaction category in Yi et al. [53].", "solution_category": "interaction", "solution_axial": "Abstract/Elaborate", "solution_compoent": "", "axial_code": ["Abstract/Elaborate"], "componenet_code": ["abstract_elaborate"]}]}, {"author": "dxf", "index_original": 459, "paper_title": "MolSieve: A Progressive Visual Analytics System for Molecular Dynamics Simulations", "pub_year": 2024, "domain": "physio-chemical", "requirement": {"requirement_text": "R5: On-demand calculation of detailed analyses. The selection process detailed in R4 generates sub-regions that may include states that express behaviors of interest. Understanding their behavior requires physically grounded analyses which can be computationally expensive. The analyst should be able to request these analyses on demand and be able to continue exploring the trajectory.", "requirement_code": {"collect_evidence": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "Molecular dynamics data, raditional state ID, time-step", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "At the bottom of each Sub-Sequence Component is a traditional state ID vs. time-step plot of the selection\u2019s constituent states (Figure 5 top). The Sub-Sequence Component also supports running the Nudged Elastic Band calculation [22] using theRun NEB button. Clicking the button (Figure 2.T2) opens a modal window that allows analysts to adjust the parameters of the calculation and make a selection on the sub-sequence that will be used in the calculation. The results from the calculation are used to generate a potential energy graph which shows the minimum energy pathways for the selection they made, fulfilling R5.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "dxf", "index_original": 460, "paper_title": "MolSieve: A Progressive Visual Analytics System for Molecular Dynamics Simulations", "pub_year": 2024, "domain": "physio-chemical", "requirement": {"requirement_text": "R6: Extensibility. An intuitive extension of R2 is the ability to define new properties. The solution should accommodate a broad spectrum of simulation types, enabling analysts to provide customized scripts for calculating system-specific properties. By providing this amount of flexibility, analysts can define properties which typically denote changes in a system. They can then use the visualizations and interactions provided by the solution to quickly identify regions of interest based on these properties.", "requirement_code": {"discover_observation": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "Molecular dynamics data, the results of analyst-defined Python scripts", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "To support a wide range of simulations, MolSieve automatically executes, stores, and renders the results of analyst-defined Python scripts (R2, R6). This feature enables analysts to specify properties that indicate a region of interest for the simulation they are studying.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}]}, {"author": "dxf", "index_original": 461, "paper_title": "MolSieve: A Progressive Visual Analytics System for Molecular Dynamics Simulations", "pub_year": 2024, "domain": "physio-chemical", "requirement": {"requirement_text": "R6: Extensibility. An intuitive extension of R2 is the ability to define new properties. The solution should accommodate a broad spectrum of simulation types, enabling analysts to provide customized scripts for calculating system-specific properties. By providing this amount of flexibility, analysts can define properties which typically denote changes in a system. They can then use the visualizations and interactions provided by the solution to quickly identify regions of interest based on these properties.", "requirement_code": {"discover_observation": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "Molecular dynamics data", "data_code": {"tables": 1, "geometry": 1}}, "solution": [{"solution_text": "The Modify 3D render button in the trajectory toolbar allows analysts to change the way states are rendered in 3D throughout the interface by Python scripts inside the vis_scripts folder in the source code (R6).", "solution_category": "interaction", "solution_axial": "Encode", "solution_compoent": "", "axial_code": ["Encode"], "componenet_code": ["encode"]}]}, {"author": "dxf", "index_original": 464, "paper_title": "InVADo: Interactive Visual Analysis of Molecular Docking Data", "pub_year": 2023, "domain": "Molecular Docking", "requirement": {"requirement_text": "R1: Provide a structured visual and textual access to the docking results, giving an overview that enables browsing and filtering as an entry point for further analysis.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "molecular docking data", "data_code": {"media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "We used Perron Cluster Cluster Analysis (PCCA) [13] as implemented by pyGPCCA [39] as the basis for MolSieve\u2019s simplification scheme. PCCA has been proven to accurately simplify MD trajectories by clustering together groups of kinetically linked states [20, 21]. PCCA can be applied to simulations where transitions are modeled as a Markov chain. MolSieve simplifies the trajectory by dividing it into tentative transi_x0002_tion regions and super-states. This is achieved by first running PCCA on the trajectory, which divides it into N meta-stable sets of states, referred to as clusters. PCCA assigns a vector of N cluster membership probabilities to each individual state which describes how strongly it belongs to each cluster (Figure 3.2). Then, each individual state\u2019s mem_x0002_bership probability is compared to a threshold set by the analyst (Figure 3.3); if its maximum membership probability is above the threshold,\nit is considered part of a super-state; otherwise, it is considered to be part of a possible transition region (i.e., it occurs in regions where the trajectory moves between clusters). If the simplification threshold is set to its maximum value of 1.0, no portion of the trajectory will be simplified, and every state will be considered a transition region.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "As an alternative overview to the stacked bar chart and the box plot, we included a segmented heatmap (Figure 3) that can be reached by switching to the HEATMAP tab. The heatmap not only gives an overview of the docking results and the clustering (R1) but also specifically addresses requirement R4, as it allows the users to easily identify overall good binders. The columns of the heatmap are the clusters or binding sites, the rows are the ligands. We use a segmented heatmap, that is, if a ligand has multiple poses that belong to the same cluster, the corresponding rectangle of the heatmap is divided horizontally into multiple segments. That is, each segment represents a pose of a ligand. The segments are colored according to the docking score of the respective pose using the established inferno color map ( ), with black, mapped to the maximum value (poor binding; binding free energy/score closer to zero) and light yellow mapped to the minimum value (best binding; free energy/score strongly negative). Since the number of ligands usually exceeds the vertical screen space, the plot is scrollable. A tooltip shows the docking score, the ZINC name of the ligand, and the IDs of cluster, ligand, and pose. Similar to the bar charts and the box plot, the users can choose if only the clusters are shown or if the noise is displayed in an additional column of the heatmap.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Heatmap", "axial_code": [], "componenet_code": ["heatmap"]}]}, {"author": "dxf", "index_original": 466, "paper_title": "InVADo: Interactive Visual Analysis of Molecular Docking Data", "pub_year": 2023, "domain": "Molecular Docking", "requirement": {"requirement_text": "R1: Provide a structured visual and textual access to the docking results, giving an overview that enables browsing and filtering as an entry point for further analysis.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "molecular docking data", "data_code": {"media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "If a menu item is selected, three further subcircles appear to the left of the rank (see Figure 8). Clicking on them allows the user to control which 3D models of that ligand are rendered (R1): by default, only the currently selected ligand pose is rendered (see Figure 1 5?).", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "dxf", "index_original": 467, "paper_title": "InVADo: Interactive Visual Analysis of Molecular Docking Data", "pub_year": 2023, "domain": "Molecular Docking", "requirement": {"requirement_text": "R1: Provide a structured visual and textual access to the docking results, giving an overview that enables browsing and filtering as an entry point for further analysis.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "molecular docking data", "data_code": {"media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Besides the aforementioned surface coloring by interaction type or the number of interactions, we added further options to allow users to explore spatial aspects of the docking data. To visualize the estimated binding sites based on the clusters, we offer a transparent rendering mode where the whole protein surface that is not close to the currently selected cluster is rendered transparently. This reduces clutter and helps to focus on the binding site (R1/R2). To achieve this, the binding site patches have to be derived from the ligands in the clusters. This is done by a neighbor search for each atom of all ligands using a distance criterion of 1.0  ?A to find nearby protein atoms. Based on this, the parts of the surface corresponding to these found protein atoms can be determined (gray surface parts in Figure 7 2).", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "dxf", "index_original": 468, "paper_title": "InVADo: Interactive Visual Analysis of Molecular Docking Data", "pub_year": 2023, "domain": "Molecular Docking", "requirement": {"requirement_text": "R1: Provide a structured visual and textual access to the docking results, giving an overview that enables browsing and filtering as an entry point for further analysis.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "molecular docking data", "data_code": {"media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "A Interaction Type Bar Chart can be displayed for each cluster/binding site and summarizes the interactions. The presented and aggregated information provides the user with an additional cluster-specific overview (R1), helps to compare clusters/binding sites, and allows for the identification of binding sites that have an interesting composition or profile of interactions (R2). The glyph-like visualization shown in Figure 7 2consists of two bar charts.", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "dxf", "index_original": 469, "paper_title": "InVADo: Interactive Visual Analysis of Molecular Docking Data", "pub_year": 2023, "domain": "Molecular Docking", "requirement": {"requirement_text": "R2: Identification of potential binding sites and other interesting hot-spots with high binding affinity.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "molecular docking data", "data_code": {"media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "The first step is to identify locations of high binding affinity(R2), i.e., clusters of docked ligand binding poses. The goal is to identify hot-spots with a high density of docked poses while discarding the remaining binding poses that are not within one of these areas. Additionally, users can set a\ndocking score threshold to discard poorly-scored binding poses (R3/R4). Since the number of clusters is not known a priori, we opted for Density-Based Spatial Clustering(DBSCAN) [50]. The input parameters of DBSCAN are the maximum search radius and the minimum number of samples in a cluster. We use the centroids of the poses for clustering. We refer to poses that were discarded either by the score threshold or by not being in one of the DBSCAN clusters as noise. The clusters indicate possible binding sites. Thus, we extract the part of the molecular surface that is close to a cluster and mark it as a potential binding site.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "dxf", "index_original": 470, "paper_title": "InVADo: Interactive Visual Analysis of Molecular Docking Data", "pub_year": 2023, "domain": "Molecular Docking", "requirement": {"requirement_text": "R2: Identification of potential binding sites and other interesting hot-spots with high binding affinity.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "molecular docking data", "data_code": {"media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "The first step is to identify locations of high binding affinity(R2), i.e., clusters of docked ligand binding poses. The goal is to identify hot-spots with a high density of docked poses while discarding the remaining binding poses that are not within one of these areas. Additionally, users can set a\ndocking score threshold to discard poorly-scored binding poses (R3/R4). Since the number of clusters is not known a priori, we opted for Density-Based Spatial Clustering(DBSCAN) [50]. The input parameters of DBSCAN are the maximum search radius and the minimum number of samples in a cluster. We use the centroids of the poses for clustering. We refer to poses that were discarded either by the score threshold or by not being in one of the DBSCAN clusters as noise. The clusters indicate possible binding sites. Thus, we extract the part of the molecular surface that is close to a cluster and mark it as a potential binding site.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The heatmap directly shows whether the binding poses of a specific ligand are docked in many different binding sites or mainly one, and which poses reached high scores\u2014shown by a \u201chot\u201d color on the inferno color map\u2014in any of the binding sites (R3). It also allows users to assess the identified binding sites (R2), e.g., the binding site specificity by checking if a binding site has only a few but good binders or many low-scoring ones.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Heatmap", "axial_code": [], "componenet_code": ["heatmap"]}]}, {"author": "dxf", "index_original": 472, "paper_title": "InVADo: Interactive Visual Analysis of Molecular Docking Data", "pub_year": 2023, "domain": "Molecular Docking", "requirement": {"requirement_text": "R2: Identification of potential binding sites and other interesting hot-spots with high binding affinity.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "molecular docking data", "data_code": {"media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Besides the aforementioned surface coloring by interaction type or the number of interactions, we added further options to allow users to explore spatial aspects of the docking data. To visualize the estimated binding sites based on the clusters, we offer a transparent rendering mode where the whole protein surface that is not close to the currently selected cluster is rendered transparently. This reduces clutter and helps to focus on the binding site (R1/R2). To achieve this, the binding site patches have to be derived from the ligands in the clusters. This is done by a neighbor search for each atom of all ligands using a distance criterion of 1.0  ?A to find nearby protein atoms. Based on this, the parts of the surface corresponding to these found protein atoms can be determined (gray surface parts in Figure 7 2).", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "dxf", "index_original": 473, "paper_title": "InVADo: Interactive Visual Analysis of Molecular Docking Data", "pub_year": 2023, "domain": "Molecular Docking", "requirement": {"requirement_text": "R3: Hit identification by facilitating the determination of the most frequently docked and top-scoring ligands and binding poses.", "requirement_code": {"identify_main_cause_aggregate": 1, "discover_observation": 1}}, "data": {"data_text": "molecular docking data", "data_code": {"media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Additionally, users can set a docking score threshold to discard poorly-scored binding poses (R3/R4).", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "dxf", "index_original": 474, "paper_title": "InVADo: Interactive Visual Analysis of Molecular Docking Data", "pub_year": 2023, "domain": "Molecular Docking", "requirement": {"requirement_text": "R3: Hit identification by facilitating the determination of the most frequently docked and top-scoring ligands and binding poses.", "requirement_code": {"identify_main_cause_aggregate": 1, "discover_observation": 1}}, "data": {"data_text": "molecular docking data", "data_code": {"media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "The first step is to identify locations of high binding affinity(R2), i.e., clusters of docked ligand binding poses. The goal is to identify hot-spots with a high density of docked poses while discarding the remaining binding poses that are not within one of these areas. Additionally, users can set a\ndocking score threshold to discard poorly-scored binding poses (R3/R4). Since the number of clusters is not known a priori, we opted for Density-Based Spatial Clustering(DBSCAN) [50]. The input parameters of DBSCAN are the maximum search radius and the minimum number of samples in a cluster. We use the centroids of the poses for clustering. We refer to poses that were discarded either by the score threshold or by not being in one of the DBSCAN clusters as noise. The clusters indicate possible binding sites. Thus, we extract the part of the molecular surface that is close to a cluster and mark it as a potential binding site.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Each cluster is represented by one vertically aligned bar and box plot, respectively. This combination allows the users to get an impression of how many ligands are within each cluster (height of the bars) and the distribution of docking scores within this cluster (box plot). The latter also contributes to fulfilling R3, as the plots can be used to identify extremes", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "Bar;Box", "axial_code": [], "componenet_code": ["boxplot", "bar"]}]}, {"author": "dxf", "index_original": 475, "paper_title": "InVADo: Interactive Visual Analysis of Molecular Docking Data", "pub_year": 2023, "domain": "Molecular Docking", "requirement": {"requirement_text": "R3: Hit identification by facilitating the determination of the most frequently docked and top-scoring ligands and binding poses.", "requirement_code": {"identify_main_cause_aggregate": 1, "discover_observation": 1}}, "data": {"data_text": "molecular docking data", "data_code": {"media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "The heatmap directly shows whether the binding poses of a specific ligand are docked in many different binding sites or mainly one, and which poses reached high scores\u2014shown by a \u201chot\u201d color on the inferno color map\u2014in any of the binding sites (R3). It also allows users to assess the identified binding sites (R2), e.g., the binding site specificity by checking if a binding site has only a few but good binders or many low-scoring ones.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Heatmap", "axial_code": [], "componenet_code": ["heatmap"]}]}, {"author": "dxf", "index_original": 477, "paper_title": "InVADo: Interactive Visual Analysis of Molecular Docking Data", "pub_year": 2023, "domain": "Molecular Docking", "requirement": {"requirement_text": "R3: Hit identification by facilitating the determination of the most frequently docked and top-scoring ligands and binding poses.", "requirement_code": {"identify_main_cause_aggregate": 1, "discover_observation": 1}}, "data": {"data_text": "molecular docking data", "data_code": {"media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "Additionally, users can set a docking score threshold to discard poorly-scored binding poses (R3/R4).", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "dxf", "index_original": 478, "paper_title": "InVADo: Interactive Visual Analysis of Molecular Docking Data", "pub_year": 2023, "domain": "Molecular Docking", "requirement": {"requirement_text": "R4: Support the identification of overall good binders and ligand types (not only top-scoring ones) for a comprehensive overview and specificity analysis.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "molecular docking data", "data_code": {"media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "The first step is to identify locations of high binding affinity(R2), i.e., clusters of docked ligand binding poses. The goal is to identify hot-spots with a high density of docked poses while discarding the remaining binding poses that are not within one of these areas. Additionally, users can set a\ndocking score threshold to discard poorly-scored binding poses (R3/R4). Since the number of clusters is not known a priori, we opted for Density-Based Spatial Clustering(DBSCAN) [50]. The input parameters of DBSCAN are the maximum search radius and the minimum number of samples in a cluster. We use the centroids of the poses for clustering. We refer to poses that were discarded either by the score threshold or by not being in one of the DBSCAN clusters as noise. The clusters indicate possible binding sites. Thus, we extract the part of the molecular surface that is close to a cluster and mark it as a potential binding site.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The heatmap not only gives an overview of the docking results and the clustering (R1) but also specifically addresses requirement R4, as it allows the users to easily identify overall good binders. Each row can be seen as a docking profile that helps to identify overall good binders (R4).", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Heatmap", "axial_code": [], "componenet_code": ["heatmap"]}]}, {"author": "dxf", "index_original": 481, "paper_title": "InVADo: Interactive Visual Analysis of Molecular Docking Data", "pub_year": 2023, "domain": "Molecular Docking", "requirement": {"requirement_text": "R5: Visualization of docked ligands, interactions, and chemical properties to enable a detailed visual analysis.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "molecular docking data", "data_code": {"media": 1, "geometry": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "This 3D visualization allows for a detailed spatial assessment of the docking pose and interactions with respect to the protein (R5).", "solution_category": "interaction", "solution_axial": "Extractionoffeatures", "solution_compoent": "", "axial_code": ["Extractionoffeatures"], "componenet_code": ["extraction_of_features"]}]}, {"author": "gsh", "index_original": 0, "paper_title": "Motion Browser: Visualizing and Understanding Complex Upper Limb Movement Under Obstetrical Brachial Plexus Injuries", "pub_year": 2020, "domain": "Medical", "requirement": {"requirement_text": "R1 Align heterogeneous data sources. Physicians need to analyze information from muscle and motion sensors and video recordings to compare the muscles within a patient's limb or between his affected and unaffected limb (T.1). To speed up the inspection and exploration, the display should align the information by time, patient, and muscles.", "requirement_code": {"describe_observation_aggregate": 1, "discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "Muscle Activity Signals. The EMG signals play a central role in help-\ning physicians clarify the coordination of muscles throughout patients\u2019\nmotions. While the signals capture the muscle activity with their am-\nplitudes, they are hard to read and compare in the raw format. To first\nfacilitate the inspection of EMG signals, they are usually transformed\nwith root-mean-square (RMS) envelope, which is calculated using a\ntime-windowed RMS function:\nThe RMS value represents the power of the signal, which correlates\nwith the degree of muscle activity and is always positive. As the raw\nEMG signals are oscillating and produce more clutters for visualization,\nturning them into RMS values produces a polarized waveform that is\nmore easily analyzable.\nHowever, there are two challenges from the EMG signals that cannot\nbe solved solely by automatic computation. Firstly, the EMG signals\nare only normalized within the same person. It means that the amplitude\nof muscle signals can only be compared between a patient\u2019s left and\nright limb but not among different patients. Physicians thus use them\n\ufb01rst to \ufb01nd out which muscles in the affected limb are stronger than the\nunaffected limb to deduce the compensatory muscles, then compare the\npresence of stronger muscles among different patients. We address the\ncomparisons of patients by the comparisons within their limbs as the\nscalability challenges of our visual analysis. Secondly, we have to deal\nwith the noisiness of muscle signals. As the EMG signals are collected\nfrom the skin, there might be an ambiguity of the resulting power. We,\ntherefore, have to ensure human-in-the-loop throughout the analysis.", "data_code": {"quantitative": 1}}, "solution": [{"solution_text": "Time Series View Users can inspect raw muscle EMG signals in\nthe time series view (Fig. 1 B\u00a9). They can query with different \ufb01le\ntypes to select the motion of a patient\u2019s limb. Also, the scales across\ndifferent time series views can be aligned so that different time series\npanels can be compared with each other. This view acts as a time series\neditor when users encounter an erroneous motion (i.e. unreasonably\nlong recordings or abnormal muscle signals). Users can visually re\ufb01ne\nthe timeline and muscle selection (R1) and import the re\ufb01ned results\nto the bundle comparison view.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar+line+stacked_area", "axial_code": [], "componenet_code": ["bar", "area", "line"]}]}, {"author": "gsh", "index_original": 1, "paper_title": "Motion Browser: Visualizing and Understanding Complex Upper Limb Movement Under Obstetrical Brachial Plexus Injuries", "pub_year": 2020, "domain": "Medical", "requirement": {"requirement_text": "R1 Align heterogeneous data sources. Physicians need to analyze information from muscle and motion sensors and video recordings to compare the muscles within a patient's limb or between his affected and unaffected limb (T.1). To speed up the inspection and exploration, the display should align the information by time, patient, and muscles.", "requirement_code": {"describe_observation_aggregate": 1, "discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "Muscle Activity Signals. The EMG signals play a central role in help-\ning physicians clarify the coordination of muscles throughout patients\u2019\nmotions. While the signals capture the muscle activity with their am-\nplitudes, they are hard to read and compare in the raw format. To first\nfacilitate the inspection of EMG signals, they are usually transformed\nwith root-mean-square (RMS) envelope, which is calculated using a\ntime-windowed RMS function:\nThe RMS value represents the power of the signal, which correlates\nwith the degree of muscle activity and is always positive. As the raw\nEMG signals are oscillating and produce more clutters for visualization,\nturning them into RMS values produces a polarized waveform that is\nmore easily analyzable.\nHowever, there are two challenges from the EMG signals that cannot\nbe solved solely by automatic computation. Firstly, the EMG signals\nare only normalized within the same person. It means that the amplitude\nof muscle signals can only be compared between a patient\u2019s left and\nright limb but not among different patients. Physicians thus use them\n\ufb01rst to \ufb01nd out which muscles in the affected limb are stronger than the\nunaffected limb to deduce the compensatory muscles, then compare the\npresence of stronger muscles among different patients. We address the\ncomparisons of patients by the comparisons within their limbs as the\nscalability challenges of our visual analysis. Secondly, we have to deal\nwith the noisiness of muscle signals. As the EMG signals are collected\nfrom the skin, there might be an ambiguity of the resulting power. We,\ntherefore, have to ensure human-in-the-loop throughout the analysis.\nData from motion tracking. The 3-dimensional motion analysis\ntracks the patient\u2019s limbs\u2019 coordinates in x, y and z directions. These at-\ntributes enable the derivation of speed and acceleration of limb motions,\nwhich allow physicians to locate a \ufb01ner scope of motion and compare\nthe overall performance without inspecting the video.\nVideo Monitoring of Motions. The video monitoring of the whole mo-\ntion provides a full picture of the patient\u2019s performance. Each cut scene\nlets physicians verify their \ufb01ndings from the analysis of muscle signals.", "data_code": {"geometry": 1, "media": 1, "quantitative": 1}}, "solution": [{"solution_text": "The separation of all data in the view aims at helping the physicians\nexamine all sources of quantitative data in a consistent and aligned\ntime scale (R1).", "solution_category": "data_manipulation", "solution_axial": "Rectification", "solution_compoent": "seqaration", "axial_code": ["Rectification"], "componenet_code": ["rectification"]}]}, {"author": "gsh", "index_original": 2, "paper_title": "Motion Browser: Visualizing and Understanding Complex Upper Limb Movement Under Obstetrical Brachial Plexus Injuries", "pub_year": 2020, "domain": "Medical", "requirement": {"requirement_text": "R2 Display and analyze individual patients' performance in one view grouped by each patient. Our experts indicate that it is a clinical practice that they need to analyze patient's data one by one before summarizing their behavior as a whole. Therefore, the system should display information and analysis of each patient in an individual view, while showing all information in the same window, so that physicians can explore, scroll, and analyze the patients effectively (T.2).", "requirement_code": {"describe_observation_aggregate": 1, "discover_observation": 1, "describe_observation_item": 1}}, "data": {"data_text": "Muscle Activity Signals. The EMG signals play a central role in help-\ning physicians clarify the coordination of muscles throughout patients\u2019\nmotions. While the signals capture the muscle activity with their am-\nplitudes, they are hard to read and compare in the raw format. To first\nfacilitate the inspection of EMG signals, they are usually transformed\nwith root-mean-square (RMS) envelope, which is calculated using a\ntime-windowed RMS function:\nThe RMS value represents the power of the signal, which correlates\nwith the degree of muscle activity and is always positive. As the raw\nEMG signals are oscillating and produce more clutters for visualization,\nturning them into RMS values produces a polarized waveform that is\nmore easily analyzable.\nHowever, there are two challenges from the EMG signals that cannot\nbe solved solely by automatic computation. Firstly, the EMG signals\nare only normalized within the same person. It means that the amplitude\nof muscle signals can only be compared between a patient\u2019s left and\nright limb but not among different patients. Physicians thus use them\n\ufb01rst to \ufb01nd out which muscles in the affected limb are stronger than the\nunaffected limb to deduce the compensatory muscles, then compare the\npresence of stronger muscles among different patients. We address the\ncomparisons of patients by the comparisons within their limbs as the\nscalability challenges of our visual analysis. Secondly, we have to deal\nwith the noisiness of muscle signals. As the EMG signals are collected\nfrom the skin, there might be an ambiguity of the resulting power. We,\ntherefore, have to ensure human-in-the-loop throughout the analysis.", "data_code": {"quantitative": 1}}, "solution": [{"solution_text": "To begin with, we \ufb01rst address the requirement of displaying all sensors\nof each patient in one view (R2). A bundle comparison chart displays\nthe motion assessment information of patient\u2019s both limbs (Fig. 1 A\u00a9,\nFig. 4) side by side", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "stacked_area+area+bar+line+pie", "axial_code": [], "componenet_code": ["pie", "line", "area", "bar", "area"]}]}, {"author": "gsh", "index_original": 3, "paper_title": "Motion Browser: Visualizing and Understanding Complex Upper Limb Movement Under Obstetrical Brachial Plexus Injuries", "pub_year": 2020, "domain": "Medical", "requirement": {"requirement_text": "R3 Enable efficient comparative analysis. It will be time- consuming and cognitively overwhelming to extract stronger mus- cles on each limb by manual selection (T.3). Physicians need an efficient mechanism to compare muscle coordination between each patient's limbs, then use the comparisons to understand the difference among all patients. While the definition of stronger muscles is subtle and depends much on observation, some similar muscle activities, such as similar waveforms and magnitudes, should be filtered away at the beginning.", "requirement_code": {"discover_observation": 1, "compare_entities": 1, "explain_differences": 1}}, "data": {"data_text": "Muscle Activity Signals. The EMG signals play a central role in help-\ning physicians clarify the coordination of muscles throughout patients\u2019\nmotions. While the signals capture the muscle activity with their am-\nplitudes, they are hard to read and compare in the raw format. To first\nfacilitate the inspection of EMG signals, they are usually transformed\nwith root-mean-square (RMS) envelope, which is calculated using a\ntime-windowed RMS function:\nThe RMS value represents the power of the signal, which correlates\nwith the degree of muscle activity and is always positive. As the raw\nEMG signals are oscillating and produce more clutters for visualization,\nturning them into RMS values produces a polarized waveform that is\nmore easily analyzable.\nHowever, there are two challenges from the EMG signals that cannot\nbe solved solely by automatic computation. Firstly, the EMG signals\nare only normalized within the same person. It means that the amplitude\nof muscle signals can only be compared between a patient\u2019s left and\nright limb but not among different patients. Physicians thus use them\n\ufb01rst to \ufb01nd out which muscles in the affected limb are stronger than the\nunaffected limb to deduce the compensatory muscles, then compare the\npresence of stronger muscles among different patients. We address the\ncomparisons of patients by the comparisons within their limbs as the\nscalability challenges of our visual analysis. Secondly, we have to deal\nwith the noisiness of muscle signals. As the EMG signals are collected\nfrom the skin, there might be an ambiguity of the resulting power. We,\ntherefore, have to ensure human-in-the-loop throughout the analysis.", "data_code": {"quantitative": 1}}, "solution": [{"solution_text": "Fig. 4 illustrates the method. First, for each muscle activity, we\nput all the values into a histogram. Then we calculate the difference\nbetween the histograms of the same muscles in both limbs, using\nKullback-Leibler divergence:\nP and Q are the distributions of values in each muscle\u2019s activity. This\nfunction measures how one distribution deviates from another reference\ndistribution and is used as highlights in several situations of visualiza-\ntion [9, 16]. In the example here, when two distributions are similar to\neach other, each log value in the summation will become very close to\nzero, leading to a low divergence. On the other hand, there are great\ndifferences between each bucket, the divergence becomes high. We also\ncalibrate the signals with the skewness [24], such that if the distribution\nis more left-skewed (i.e. more small values) the value will decrease.\nThe histogram is constructed by K Means clustering. For simplicity,\nwe use the elbow method to decide the value of K. In this way, we can\nvisualize a larger number of muscle signals and still quickly identify\nthe significant muscle activities.", "solution_category": "data_manipulation", "solution_axial": "SimilarityCalculation;Clustering&Grouping", "solution_compoent": "data;process;highlight;similarity", "axial_code": ["Clustering&Grouping", "SimilarityCalculation"], "componenet_code": ["clustering_and_grouping", "similarity_calculation"]}]}, {"author": "gsh", "index_original": 4, "paper_title": "Motion Browser: Visualizing and Understanding Complex Upper Limb Movement Under Obstetrical Brachial Plexus Injuries", "pub_year": 2020, "domain": "Medical", "requirement": {"requirement_text": "R4 Export the clinical analysis for presentation. After finishing the data analysis (T.2), experts need to report their findings on the comparison among patients for discussion and verification (T.4). An easily communicated visualization is preferred to showcase their results to other colleagues.", "requirement_code": {"sharing_findings": 1}}, "data": {"data_text": "Muscle Activity Signals. The EMG signals play a central role in help-\ning physicians clarify the coordination of muscles throughout patients\u2019\nmotions. While the signals capture the muscle activity with their am-\nplitudes, they are hard to read and compare in the raw format. To first\nfacilitate the inspection of EMG signals, they are usually transformed\nwith root-mean-square (RMS) envelope, which is calculated using a\ntime-windowed RMS function:\nThe RMS value represents the power of the signal, which correlates\nwith the degree of muscle activity and is always positive. As the raw\nEMG signals are oscillating and produce more clutters for visualization,\nturning them into RMS values produces a polarized waveform that is\nmore easily analyzable.\nHowever, there are two challenges from the EMG signals that cannot\nbe solved solely by automatic computation. Firstly, the EMG signals\nare only normalized within the same person. It means that the amplitude\nof muscle signals can only be compared between a patient\u2019s left and\nright limb but not among different patients. Physicians thus use them\n\ufb01rst to \ufb01nd out which muscles in the affected limb are stronger than the\nunaffected limb to deduce the compensatory muscles, then compare the\npresence of stronger muscles among different patients. We address the\ncomparisons of patients by the comparisons within their limbs as the\nscalability challenges of our visual analysis. Secondly, we have to deal\nwith the noisiness of muscle signals. As the EMG signals are collected\nfrom the skin, there might be an ambiguity of the resulting power. We,\ntherefore, have to ensure human-in-the-loop throughout the analysis.\n\nVideo Monitoring of Motions. The video monitoring of the whole mo-\ntion provides a full picture of the patient\u2019s performance. Each cut scene\nlets physicians verify their \ufb01ndings from the analysis of muscle signals.", "data_code": {"media": 1, "quantitative": 1}}, "solution": [{"solution_text": "A bundle comparison chart displays\nthe motion assessment information of patient\u2019s both limbs (Fig. 1 A\u00a9,\nFig. 4) side by side. The left-hand side shows the data from the affected\nlimb and the right-hand side shows the data from the unaffected limb.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "stacked_area+area+bar+line+pie", "axial_code": [], "componenet_code": ["pie", "line", "area", "bar", "area"]}, {"solution_text": "After users click the play button in the selected time\nintervals (Fig. 1a3), a new window will appear to show the selected\nmuscle activities under the video", "solution_category": "interaction", "solution_axial": "Overview_and_Explore", "solution_compoent": "", "axial_code": ["Overview_and_Explore"], "componenet_code": ["overview_and_explore"]}, {"solution_text": "Video View After users click the play button in the selected time\nintervals (Fig. 1a3), a new window will appear to show the selected\nmuscle activities under the video. All of the information is aligned\nwith a line to synchronize the video time frame and charts (Fig. 1 C\u00a9).\nSuch encoding allows a compact integration of all information and\ninsights obtained aligned with video evidence (R4). Users thus can\nverify their \ufb01ndings and derive reasons between muscle coordination\nand physical outcome", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "video+bar", "axial_code": [], "componenet_code": ["bar", "video"]}]}, {"author": "gsh", "index_original": 5, "paper_title": "VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour Analytics", "pub_year": 2020, "domain": "user behaviour", "requirement": {"requirement_text": "G1 Identify and characterise groups of users with similar roles. A critical mechanism for the evaluation of the activities of a user is to investigate their behaviour in comparison to users with similar roles. To accomplish this, analysts need to identify a group of users that perform similar tasks, understand the characteristics of the kinds of activities performed by the group, and infer the potential roles of the group by investigating the characteristics of their high-level behaviour.", "requirement_code": {"describe_observation_aggregate": 1, "discover_observation": 1, "describe_observation_item": 1}}, "data": {"data_text": "Speci\ufb01c Context and Data. In the context of this paper, we work\nwith a UBA model (designed, built and being used by one of the\nco-author\u2019s organisation) that operates on the logs that comprise of se-\nquences of actions carried out by the users of an administrative interface\nof a login and security server. The log data is split into sessions, each\ncontaining an ordered list of timestamped actions performed by the user\nconducting that session. In each session, particular tasks are conducted\nand actions are performed by the user. Actions in this context are se-\nmantically labelled and functionally relevant activities, e.g., managing\nusers and/or organisations with actions such as \u201cCreateLoginArea\u201d,\n\u201cSearchUsr\u201d, \u201cDisplayOrgaDetails\u201d or performing some intermediate\nactions such as \u201cCloseTab\u201d and \u201cCancel\u201d.\nEach session here is then a data record with the attributes: clock\ntime to indicate the start and end, User ID to indicate the performer,\nother meta information such as browser and operating system, and the\nsequence of actions (labels) and when each action took place within\nthe session. In addition to these, we also get the anomaly score that\nis provided by the underlying UBA with 1 being an unusual and 0\na normal session. The data set that we work with here comprises of\n19,351 sessions performed by 1,670 users within a 15 day time window\nusing 305 unique action types. Although we use this speci\ufb01c data set in\nthis paper, the form of the data is representative of log based solutions\nused for behaviour modelling in many other domains [13, 33].", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "4.1.1 Individual User Pro\ufb01les\nThe user pro\ufb01les combine three levels of features based on the different\ncases of feature availability: inherent features that are explicitly\navailable, derived features that can be directly extracted from the data\nthrough the use of standard statistical measures and latent features\nthat are not directly available for extraction, but would require more\nsophisticated methods to reveal underlying, latent characteristics of\nuser behaviour. Here, we consider the data in our application context\nand list the various features we put into use:\nInherent features. Features that describe the low-level character-\nistics of how users utilise the application. These features are often\nexplicit and provided as meta-data within the session records.\nP Browser used in the session\nP IP Address through which the session is conducted\nP Operating System running on the user\u2019s computer\nDerived Features. Features that are derived through straightfor-\nward statistical computations considering the distribution of actions in\nsessions and the distribution of sessions over users, together with any\nother explicitly derived metric such as model derived scores.\nP Session duration / length as a measure of time taken in the session\nand the count of actions performed\nP Total Sessions as a statistic on how active the user is\nP Total Unique Actions as a measure on how diverse is a user in the\nactions they perform\nLatent Features. Features that provide in-depth understanding\ninto the behaviour of users through the application of a sophisticated\ncomputation, such as modelling and clustering.\nP Anomaly Score for individual sessions as provided by the UBA\nsystem to indicate the normality of a session", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extract;statistical;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": ".3 Clustering Users based on their Tasks\nTo provide a semantic characterisation of users (addressing G1), we\nclassify them based on the tasks they perform. From the output of\ntopic modelling, a session can be modelled as a vector of tasks: si =\n(pi1, p i2, ..., p ik), where p i j is the probability of task j occuring in\nsession s i, with k as the total count of tasks. A user u can then be\nmodelled as a mean of its session vectors: u = 1\nm \u2211m\ni s i, where m is the\nnumber of sessions performed by u.\nHowever, there is a logical issue with adding session vectors. pi j\nindicates how probable task j occurs in session si, but it could be\nmisleading to compare the probability of the same task between two\nsessions. For instance, p11 = 0.5 means 50% of what happens in\nsession 1 is about task 1, and p21 = 0.3 means 30% of what happens in\nsession 2 is about task 1. But it cannot be inferred that session 1 has\nmore activities of task 1 than session 2 due to the difference in their\nnumber of actions. Therefore, we classify the probability based on a\nparticular threshold \u03b8 to indicate whether a task occurs in a session\nor not. Formally, s i = (bi1, b i2, ..., b ik), where bi j = 1 if p i j \u2265 \u03b8 and\nbi j = 0 otherwise. This binary transformation also allows a more\ninterpretable representation of the user vector: u = (t1,t2, ...,t k), where\nti is the proportion of the user\u2019s sessions where task i occurs. Choosing\nthe threshold \u03b8 is a challenging trade-off: small \u03b8 leads to too many\ntasks in one session, whereas large \u03b8 leads to too many sessions not\nassociated with any tasks. We choose \u03b8 = 0.3 for our dataset for two\nreasons: (1) a large number of sessions (94%) will have at least one task,\nand (2) each session will have up to 3 tasks, which is about consistent\nwith reality according to the domain experts.\nWe then apply k-means clustering [28] to cluster users based on\ntheir vector representations. k-means clustering is expected to work\nwell due to the small number of vector dimensions, which is 13 in our\ncase (13 tasks). To decide the optimal k number of topics, we use the\nelbow method [21] to visually select k by assessing the change of a\ncost function with different values of k. Our cost function is the sum of\nsquared distances of all data points to their closest cluster centre. As a\nresult, 11 clusters of users are generated.", "solution_category": "data_manipulation", "solution_axial": "Clustering", "solution_compoent": "model;", "axial_code": ["Clustering"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Group profiles refer to the higher level aggregations of characteristics of users. In order to construct these profiles, users of similar traits are identified through a clustering method explained in Sect. 4.3, and the features of individual users (those described above) are aggregated for each group. Overall, a group profile comprises three key elements: aggregated feature statistics depicting the joint characteristics of the group in terms of the inherent and derived features of individuals, task profiles depicting the most frequent tasks conducted by the group, and representative users chosen as members that are of particular interest for further analysis (using criteria as discussed in Sect. 4.4). Combined together, group and user profiles help address the analysis goals G1 and G2. Moreover, a user profile also characterises how typical sessions look like, allowing comparison at session level (addressing G3).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "data;higer_level_aggregations_of_characteristics_of_users;cluster", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "gsh", "index_original": 6, "paper_title": "VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour Analytics", "pub_year": 2020, "domain": "user behaviour", "requirement": {"requirement_text": "G2 Identify and explore users of interest. Analysts routinely explore large collections of sessions performed by the users with the aim of building an understanding of the common and unusual characteristics of users and identify unusual users for further investigation.", "requirement_code": {"discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "Speci\ufb01c Context and Data. In the context of this paper, we work\nwith a UBA model (designed, built and being used by one of the\nco-author\u2019s organisation) that operates on the logs that comprise of se-\nquences of actions carried out by the users of an administrative interface\nof a login and security server. The log data is split into sessions, each\ncontaining an ordered list of timestamped actions performed by the user\nconducting that session. In each session, particular tasks are conducted\nand actions are performed by the user. Actions in this context are se-\nmantically labelled and functionally relevant activities, e.g., managing\nusers and/or organisations with actions such as \u201cCreateLoginArea\u201d,\n\u201cSearchUsr\u201d, \u201cDisplayOrgaDetails\u201d or performing some intermediate\nactions such as \u201cCloseTab\u201d and \u201cCancel\u201d.\nEach session here is then a data record with the attributes: clock\ntime to indicate the start and end, User ID to indicate the performer,\nother meta information such as browser and operating system, and the\nsequence of actions (labels) and when each action took place within\nthe session. In addition to these, we also get the anomaly score that\nis provided by the underlying UBA with 1 being an unusual and 0\na normal session. The data set that we work with here comprises of\n19,351 sessions performed by 1,670 users within a 15 day time window\nusing 305 unique action types. Although we use this speci\ufb01c data set in\nthis paper, the form of the data is representative of log based solutions\nused for behaviour modelling in many other domains [13, 33].", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "4.1.1 Individual User Pro\ufb01les\nThe user pro\ufb01les combine three levels of features based on the different\ncases of feature availability: inherent features that are explicitly\navailable, derived features that can be directly extracted from the data\nthrough the use of standard statistical measures and latent features\nthat are not directly available for extraction, but would require more\nsophisticated methods to reveal underlying, latent characteristics of\nuser behaviour. Here, we consider the data in our application context\nand list the various features we put into use:\nInherent features. Features that describe the low-level character-\nistics of how users utilise the application. These features are often\nexplicit and provided as meta-data within the session records.\nP Browser used in the session\nP IP Address through which the session is conducted\nP Operating System running on the user\u2019s computer\nDerived Features. Features that are derived through straightfor-\nward statistical computations considering the distribution of actions in\nsessions and the distribution of sessions over users, together with any\nother explicitly derived metric such as model derived scores.\nP Session duration / length as a measure of time taken in the session\nand the count of actions performed\nP Total Sessions as a statistic on how active the user is\nP Total Unique Actions as a measure on how diverse is a user in the\nactions they perform\nLatent Features. Features that provide in-depth understanding\ninto the behaviour of users through the application of a sophisticated\ncomputation, such as modelling and clustering.\nP Anomaly Score for individual sessions as provided by the UBA\nsystem to indicate the normality of a session", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extract;statistical;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": ".3 Clustering Users based on their Tasks\nTo provide a semantic characterisation of users (addressing G1), we\nclassify them based on the tasks they perform. From the output of\ntopic modelling, a session can be modelled as a vector of tasks: si =\n(pi1, p i2, ..., p ik), where p i j is the probability of task j occuring in\nsession s i, with k as the total count of tasks. A user u can then be\nmodelled as a mean of its session vectors: u = 1\nm \u2211m\ni s i, where m is the\nnumber of sessions performed by u.\nHowever, there is a logical issue with adding session vectors. pi j\nindicates how probable task j occurs in session si, but it could be\nmisleading to compare the probability of the same task between two\nsessions. For instance, p11 = 0.5 means 50% of what happens in\nsession 1 is about task 1, and p21 = 0.3 means 30% of what happens in\nsession 2 is about task 1. But it cannot be inferred that session 1 has\nmore activities of task 1 than session 2 due to the difference in their\nnumber of actions. Therefore, we classify the probability based on a\nparticular threshold \u03b8 to indicate whether a task occurs in a session\nor not. Formally, s i = (bi1, b i2, ..., b ik), where bi j = 1 if p i j \u2265 \u03b8 and\nbi j = 0 otherwise. This binary transformation also allows a more\ninterpretable representation of the user vector: u = (t1,t2, ...,t k), where\nti is the proportion of the user\u2019s sessions where task i occurs. Choosing\nthe threshold \u03b8 is a challenging trade-off: small \u03b8 leads to too many\ntasks in one session, whereas large \u03b8 leads to too many sessions not\nassociated with any tasks. We choose \u03b8 = 0.3 for our dataset for two\nreasons: (1) a large number of sessions (94%) will have at least one task,\nand (2) each session will have up to 3 tasks, which is about consistent\nwith reality according to the domain experts.\nWe then apply k-means clustering [28] to cluster users based on\ntheir vector representations. k-means clustering is expected to work\nwell due to the small number of vector dimensions, which is 13 in our\ncase (13 tasks). To decide the optimal k number of topics, we use the\nelbow method [21] to visually select k by assessing the change of a\ncost function with different values of k. Our cost function is the sum of\nsquared distances of all data points to their closest cluster centre. As a\nresult, 11 clusters of users are generated.", "solution_category": "data_manipulation", "solution_axial": "Clustering", "solution_compoent": "model;", "axial_code": ["Clustering"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Group profiles refer to the higher level aggregations of characteristics of users. In order to construct these profiles, users of similar traits are identified through a clustering method explained in Sect. 4.3, and the features of individual users (those described above) are aggregated for each group. Overall, a group profile comprises three key elements: aggregated feature statistics depicting the joint characteristics of the group in terms of the inherent and derived features of individuals, task profiles depicting the most frequent tasks conducted by the group, and representative users chosen as members that are of particular interest for further analysis (using criteria as discussed in Sect. 4.4). Combined together, group and user profiles help address the analysis goals G1 and G2. Moreover, a user profile also characterises how typical sessions look like, allowing comparison at session level (addressing G3).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "data;higer_level_aggregations_of_characteristics_of_users;cluster", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "gsh", "index_original": 7, "paper_title": "VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour Analytics", "pub_year": 2020, "domain": "user behaviour", "requirement": {"requirement_text": "G3 Identify and investigate sessions of interest. Analysts need to identify unusual sessions either visually or through the computational models, and they need to perform in-depth investigations of the suspicious activities and make decisions on whether these are indeed events that need further action.", "requirement_code": {"discover_observation": 1, "collect_evidence": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "Speci\ufb01c Context and Data. In the context of this paper, we work\nwith a UBA model (designed, built and being used by one of the\nco-author\u2019s organisation) that operates on the logs that comprise of se-\nquences of actions carried out by the users of an administrative interface\nof a login and security server. The log data is split into sessions, each\ncontaining an ordered list of timestamped actions performed by the user\nconducting that session. In each session, particular tasks are conducted\nand actions are performed by the user. Actions in this context are se-\nmantically labelled and functionally relevant activities, e.g., managing\nusers and/or organisations with actions such as \u201cCreateLoginArea\u201d,\n\u201cSearchUsr\u201d, \u201cDisplayOrgaDetails\u201d or performing some intermediate\nactions such as \u201cCloseTab\u201d and \u201cCancel\u201d.\nEach session here is then a data record with the attributes: clock\ntime to indicate the start and end, User ID to indicate the performer,\nother meta information such as browser and operating system, and the\nsequence of actions (labels) and when each action took place within\nthe session. In addition to these, we also get the anomaly score that\nis provided by the underlying UBA with 1 being an unusual and 0\na normal session. The data set that we work with here comprises of\n19,351 sessions performed by 1,670 users within a 15 day time window\nusing 305 unique action types. Although we use this speci\ufb01c data set in\nthis paper, the form of the data is representative of log based solutions\nused for behaviour modelling in many other domains [13, 33].", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "4.1.1 Individual User Pro\ufb01les\nThe user pro\ufb01les combine three levels of features based on the different\ncases of feature availability: inherent features that are explicitly\navailable, derived features that can be directly extracted from the data\nthrough the use of standard statistical measures and latent features\nthat are not directly available for extraction, but would require more\nsophisticated methods to reveal underlying, latent characteristics of\nuser behaviour. Here, we consider the data in our application context\nand list the various features we put into use:\nInherent features. Features that describe the low-level character-\nistics of how users utilise the application. These features are often\nexplicit and provided as meta-data within the session records.\nP Browser used in the session\nP IP Address through which the session is conducted\nP Operating System running on the user\u2019s computer\nDerived Features. Features that are derived through straightfor-\nward statistical computations considering the distribution of actions in\nsessions and the distribution of sessions over users, together with any\nother explicitly derived metric such as model derived scores.\nP Session duration / length as a measure of time taken in the session\nand the count of actions performed\nP Total Sessions as a statistic on how active the user is\nP Total Unique Actions as a measure on how diverse is a user in the\nactions they perform\nLatent Features. Features that provide in-depth understanding\ninto the behaviour of users through the application of a sophisticated\ncomputation, such as modelling and clustering.\nP Anomaly Score for individual sessions as provided by the UBA\nsystem to indicate the normality of a session", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extract;statistical;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": ".3 Clustering Users based on their Tasks\nTo provide a semantic characterisation of users (addressing G1), we\nclassify them based on the tasks they perform. From the output of\ntopic modelling, a session can be modelled as a vector of tasks: si =\n(pi1, p i2, ..., p ik), where p i j is the probability of task j occuring in\nsession s i, with k as the total count of tasks. A user u can then be\nmodelled as a mean of its session vectors: u = 1\nm \u2211m\ni s i, where m is the\nnumber of sessions performed by u.\nHowever, there is a logical issue with adding session vectors. pi j\nindicates how probable task j occurs in session si, but it could be\nmisleading to compare the probability of the same task between two\nsessions. For instance, p11 = 0.5 means 50% of what happens in\nsession 1 is about task 1, and p21 = 0.3 means 30% of what happens in\nsession 2 is about task 1. But it cannot be inferred that session 1 has\nmore activities of task 1 than session 2 due to the difference in their\nnumber of actions. Therefore, we classify the probability based on a\nparticular threshold \u03b8 to indicate whether a task occurs in a session\nor not. Formally, s i = (bi1, b i2, ..., b ik), where bi j = 1 if p i j \u2265 \u03b8 and\nbi j = 0 otherwise. This binary transformation also allows a more\ninterpretable representation of the user vector: u = (t1,t2, ...,t k), where\nti is the proportion of the user\u2019s sessions where task i occurs. Choosing\nthe threshold \u03b8 is a challenging trade-off: small \u03b8 leads to too many\ntasks in one session, whereas large \u03b8 leads to too many sessions not\nassociated with any tasks. We choose \u03b8 = 0.3 for our dataset for two\nreasons: (1) a large number of sessions (94%) will have at least one task,\nand (2) each session will have up to 3 tasks, which is about consistent\nwith reality according to the domain experts.\nWe then apply k-means clustering [28] to cluster users based on\ntheir vector representations. k-means clustering is expected to work\nwell due to the small number of vector dimensions, which is 13 in our\ncase (13 tasks). To decide the optimal k number of topics, we use the\nelbow method [21] to visually select k by assessing the change of a\ncost function with different values of k. Our cost function is the sum of\nsquared distances of all data points to their closest cluster centre. As a\nresult, 11 clusters of users are generated.", "solution_category": "data_manipulation", "solution_axial": "Clustering", "solution_compoent": "model;", "axial_code": ["Clustering"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Group profiles refer to the higher level aggregations of characteristics of users. In order to construct these profiles, users of similar traits are identified through a clustering method explained in Sect. 4.3, and the features of individual users (those described above) are aggregated for each group. Overall, a group profile comprises three key elements: aggregated feature statistics depicting the joint characteristics of the group in terms of the inherent and derived features of individuals, task profiles depicting the most frequent tasks conducted by the group, and representative users chosen as members that are of particular interest for further analysis (using criteria as discussed in Sect. 4.4). Combined together, group and user profiles help address the analysis goals G1 and G2. Moreover, a user profile also characterises how typical sessions look like, allowing comparison at session level (addressing G3).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "data;higer_level_aggregations_of_characteristics_of_users;cluster", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "gsh", "index_original": 8, "paper_title": "VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour Analytics", "pub_year": 2020, "domain": "user behaviour", "requirement": {"requirement_text": "G2 Identify and explore users of interest. Analysts routinely explore large collections of sessions performed by the users with the aim of building an understanding of the common and unusual characteristics of users and identify unusual users for further investigation.", "requirement_code": {"discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "Speci\ufb01c Context and Data. In the context of this paper, we work\nwith a UBA model (designed, built and being used by one of the\nco-author\u2019s organisation) that operates on the logs that comprise of se-\nquences of actions carried out by the users of an administrative interface\nof a login and security server. The log data is split into sessions, each\ncontaining an ordered list of timestamped actions performed by the user\nconducting that session. In each session, particular tasks are conducted\nand actions are performed by the user. Actions in this context are se-\nmantically labelled and functionally relevant activities, e.g., managing\nusers and/or organisations with actions such as \u201cCreateLoginArea\u201d,\n\u201cSearchUsr\u201d, \u201cDisplayOrgaDetails\u201d or performing some intermediate\nactions such as \u201cCloseTab\u201d and \u201cCancel\u201d.\nEach session here is then a data record with the attributes: clock\ntime to indicate the start and end, User ID to indicate the performer,\nother meta information such as browser and operating system, and the\nsequence of actions (labels) and when each action took place within\nthe session. In addition to these, we also get the anomaly score that\nis provided by the underlying UBA with 1 being an unusual and 0\na normal session. The data set that we work with here comprises of\n19,351 sessions performed by 1,670 users within a 15 day time window\nusing 305 unique action types. Although we use this speci\ufb01c data set in\nthis paper, the form of the data is representative of log based solutions\nused for behaviour modelling in many other domains [13, 33].", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "4.1.1 Individual User Pro\ufb01les\nThe user pro\ufb01les combine three levels of features based on the different\ncases of feature availability: inherent features that are explicitly\navailable, derived features that can be directly extracted from the data\nthrough the use of standard statistical measures and latent features\nthat are not directly available for extraction, but would require more\nsophisticated methods to reveal underlying, latent characteristics of\nuser behaviour. Here, we consider the data in our application context\nand list the various features we put into use:\nInherent features. Features that describe the low-level character-\nistics of how users utilise the application. These features are often\nexplicit and provided as meta-data within the session records.\nP Browser used in the session\nP IP Address through which the session is conducted\nP Operating System running on the user\u2019s computer\nDerived Features. Features that are derived through straightfor-\nward statistical computations considering the distribution of actions in\nsessions and the distribution of sessions over users, together with any\nother explicitly derived metric such as model derived scores.\nP Session duration / length as a measure of time taken in the session\nand the count of actions performed\nP Total Sessions as a statistic on how active the user is\nP Total Unique Actions as a measure on how diverse is a user in the\nactions they perform\nLatent Features. Features that provide in-depth understanding\ninto the behaviour of users through the application of a sophisticated\ncomputation, such as modelling and clustering.\nP Anomaly Score for individual sessions as provided by the UBA\nsystem to indicate the normality of a session", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extract;statistical;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "4.5 Visualising User Profiles\nThis section discusses the visual design of user profiles to support the\nidentification and exploration of both users and sessions of interest\n(addressing G2, G3).\n4.5.1 Visual Summary and Comparison of User Profiles\nAs discussed earlier, our user profile consists of multiple features that\nmodel different characteristics of user behaviour. Thus, it is essential\nfor analysts to observe many features of user profile concurrently (ad-\ndressing T3). For each feature, we use a small chart to show a summary\nof the user\u2019s sessions and concatenate the charts vertically to produce a\ncompact representation of a user profile. The visual profiles are then stacked together to enable comparison (Fig. 4) (addressing T4, T5) and\nare sorted using the same relevance metric described in Sect. 4.4.\nP Per-profile feature (e.g., the number of sessions). A single hor-\nizontal bar is used, which is unnecessary for one profile but is\nuseful for comparing multiple ones.\nP Per-session quantitative feature (e.g., anomaly score). Initially,\nwe use a histogram to show the distribution of feature values (see\nsupplemental material). However, histograms are sensitive to the\nchoice of the number of bins and are not visually straightforward\nto compare multiple histograms. Therefore, we use kernel den-\nsity estimation (KDE) [38] to avoid rough binning and enhance\ncomparison between multiple features.\nP Per-session nominal feature (e.g., operating system). A simple\nbarchart is used. The same set of nominal values are shared across\nall profiles to enable comparison.\nBoth individual and group profiles have the same visual representa-\ntion (except for the group name and the user name). As seen in Fig. 4,\nthis allows the comparison of multiple characteristics both between\nusers and with their corresponding group G0. Many observations can\nbe made through this figure. All of these users have completed many\nmore sessions and performed many more unique actions than the aver-\nage of their group. The anomaly scores from their sessions are higher\nthan those from their group as well, especially sessions from Blood-\nshed. The behaviour in terms of session length and duration is similar\nbetween users, but very different in terms of starting time.\n4.5.2 Analysis of Sessions in the Context\nIn the investigation of an anomalous session, it is crucial to analyse\nit within the context of the user performing the session so that any\ndeviation from the user\u2019s typical behaviour will be effectively spotted\n(addressing T6). We support this by enabling comparison between the\nsessions of interest and their corresponding user profiles that are built\nfrom all sessions performed by the same users. Sessions of interest\nare superimposed on top of the visual profiles as small orange dots.\nA random noise is added to the vertical position of the dots to avoid\noverplotting. Fig. 4 shows that the distribution of the orange dots is\nroughly the same as the shaded area of the profiles, which indicates that\nthe sessions of interest are similar to what the users typically do. The\nslightly deviated session is the session with highest score from Black\nGoliath, which is manually annotated with a blue circle in Fig. 4.\nWith such a compact representation, it is essential to equip the\nvisualisation with interaction to support further investigation. Mouse\nhovering a session (indicated as red dot) highlights the same session\non other features, enabling to observe different features concurrently.\nFor example, in Fig. 4, hovering the session with the highest score\nfrom Black Goliath reveals that there are no deviations in other features:\nshort length and duration, typical starting time ranges and using the\nsame operating system and browser. It is also possible to brush a\nrange of sessions, possibly from multiple users, to explore and compare\ntheir features (addressing T7). For instance, in Fig. 4, sessions that\nstarted late from Asylum and Bloodshed are selected for exploration.\nOne important next step is to investigate what actually happened in those sessions besides the meta features. To support that, the selected\nsessions are also displayed in the Timeline view (described in Sect. 5.2)\nallowing the examination of the performed actions in temporal order", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar+area+point", "axial_code": [], "componenet_code": ["scatter", "bar", "area"]}]}, {"author": "gsh", "index_original": 9, "paper_title": "VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour Analytics", "pub_year": 2020, "domain": "user behaviour", "requirement": {"requirement_text": "G3 Identify and investigate sessions of interest. Analysts need to identify unusual sessions either visually or through the computational models, and they need to perform in-depth investigations of the suspicious activities and make decisions on whether these are indeed events that need further action.", "requirement_code": {"discover_observation": 1, "collect_evidence": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "Speci\ufb01c Context and Data. In the context of this paper, we work\nwith a UBA model (designed, built and being used by one of the\nco-author\u2019s organisation) that operates on the logs that comprise of se-\nquences of actions carried out by the users of an administrative interface\nof a login and security server. The log data is split into sessions, each\ncontaining an ordered list of timestamped actions performed by the user\nconducting that session. In each session, particular tasks are conducted\nand actions are performed by the user. Actions in this context are se-\nmantically labelled and functionally relevant activities, e.g., managing\nusers and/or organisations with actions such as \u201cCreateLoginArea\u201d,\n\u201cSearchUsr\u201d, \u201cDisplayOrgaDetails\u201d or performing some intermediate\nactions such as \u201cCloseTab\u201d and \u201cCancel\u201d.\nEach session here is then a data record with the attributes: clock\ntime to indicate the start and end, User ID to indicate the performer,\nother meta information such as browser and operating system, and the\nsequence of actions (labels) and when each action took place within\nthe session. In addition to these, we also get the anomaly score that\nis provided by the underlying UBA with 1 being an unusual and 0\na normal session. The data set that we work with here comprises of\n19,351 sessions performed by 1,670 users within a 15 day time window\nusing 305 unique action types. Although we use this speci\ufb01c data set in\nthis paper, the form of the data is representative of log based solutions\nused for behaviour modelling in many other domains [13, 33].", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "4.1.1 Individual User Pro\ufb01les\nThe user pro\ufb01les combine three levels of features based on the different\ncases of feature availability: inherent features that are explicitly\navailable, derived features that can be directly extracted from the data\nthrough the use of standard statistical measures and latent features\nthat are not directly available for extraction, but would require more\nsophisticated methods to reveal underlying, latent characteristics of\nuser behaviour. Here, we consider the data in our application context\nand list the various features we put into use:\nInherent features. Features that describe the low-level character-\nistics of how users utilise the application. These features are often\nexplicit and provided as meta-data within the session records.\nP Browser used in the session\nP IP Address through which the session is conducted\nP Operating System running on the user\u2019s computer\nDerived Features. Features that are derived through straightfor-\nward statistical computations considering the distribution of actions in\nsessions and the distribution of sessions over users, together with any\nother explicitly derived metric such as model derived scores.\nP Session duration / length as a measure of time taken in the session\nand the count of actions performed\nP Total Sessions as a statistic on how active the user is\nP Total Unique Actions as a measure on how diverse is a user in the\nactions they perform\nLatent Features. Features that provide in-depth understanding\ninto the behaviour of users through the application of a sophisticated\ncomputation, such as modelling and clustering.\nP Anomaly Score for individual sessions as provided by the UBA\nsystem to indicate the normality of a session", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extract;statistical;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "4.5 Visualising User Profiles\nThis section discusses the visual design of user profiles to support the\nidentification and exploration of both users and sessions of interest\n(addressing G2, G3).\n4.5.1 Visual Summary and Comparison of User Profiles\nAs discussed earlier, our user profile consists of multiple features that\nmodel different characteristics of user behaviour. Thus, it is essential\nfor analysts to observe many features of user profile concurrently (ad-\ndressing T3). For each feature, we use a small chart to show a summary\nof the user\u2019s sessions and concatenate the charts vertically to produce a\ncompact representation of a user profile. The visual profiles are then stacked together to enable comparison (Fig. 4) (addressing T4, T5) and\nare sorted using the same relevance metric described in Sect. 4.4.\nP Per-profile feature (e.g., the number of sessions). A single hor-\nizontal bar is used, which is unnecessary for one profile but is\nuseful for comparing multiple ones.\nP Per-session quantitative feature (e.g., anomaly score). Initially,\nwe use a histogram to show the distribution of feature values (see\nsupplemental material). However, histograms are sensitive to the\nchoice of the number of bins and are not visually straightforward\nto compare multiple histograms. Therefore, we use kernel den-\nsity estimation (KDE) [38] to avoid rough binning and enhance\ncomparison between multiple features.\nP Per-session nominal feature (e.g., operating system). A simple\nbarchart is used. The same set of nominal values are shared across\nall profiles to enable comparison.\nBoth individual and group profiles have the same visual representa-\ntion (except for the group name and the user name). As seen in Fig. 4,\nthis allows the comparison of multiple characteristics both between\nusers and with their corresponding group G0. Many observations can\nbe made through this figure. All of these users have completed many\nmore sessions and performed many more unique actions than the aver-\nage of their group. The anomaly scores from their sessions are higher\nthan those from their group as well, especially sessions from Blood-\nshed. The behaviour in terms of session length and duration is similar\nbetween users, but very different in terms of starting time.\n4.5.2 Analysis of Sessions in the Context\nIn the investigation of an anomalous session, it is crucial to analyse\nit within the context of the user performing the session so that any\ndeviation from the user\u2019s typical behaviour will be effectively spotted\n(addressing T6). We support this by enabling comparison between the\nsessions of interest and their corresponding user profiles that are built\nfrom all sessions performed by the same users. Sessions of interest\nare superimposed on top of the visual profiles as small orange dots.\nA random noise is added to the vertical position of the dots to avoid\noverplotting. Fig. 4 shows that the distribution of the orange dots is\nroughly the same as the shaded area of the profiles, which indicates that\nthe sessions of interest are similar to what the users typically do. The\nslightly deviated session is the session with highest score from Black\nGoliath, which is manually annotated with a blue circle in Fig. 4.\nWith such a compact representation, it is essential to equip the\nvisualisation with interaction to support further investigation. Mouse\nhovering a session (indicated as red dot) highlights the same session\non other features, enabling to observe different features concurrently.\nFor example, in Fig. 4, hovering the session with the highest score\nfrom Black Goliath reveals that there are no deviations in other features:\nshort length and duration, typical starting time ranges and using the\nsame operating system and browser. It is also possible to brush a\nrange of sessions, possibly from multiple users, to explore and compare\ntheir features (addressing T7). For instance, in Fig. 4, sessions that\nstarted late from Asylum and Bloodshed are selected for exploration.\nOne important next step is to investigate what actually happened in those sessions besides the meta features. To support that, the selected\nsessions are also displayed in the Timeline view (described in Sect. 5.2)\nallowing the examination of the performed actions in temporal order", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar+area+point", "axial_code": [], "componenet_code": ["scatter", "bar", "area"]}]}, {"author": "gsh", "index_original": 10, "paper_title": "VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour Analytics", "pub_year": 2020, "domain": "user behaviour", "requirement": {"requirement_text": "T1 Summarise a user group through the behaviour of its users. In order to understand the high-level characteristics of a group, ana- lysts need to investigate the combined behaviour of its members through an overview of the tasks they perform and the aggre- gate statistics as computed from all the sessions conducted by the group. Examples: investigate whether there are dominant tasks for a group or no clear common tasks; observe the feature aggregates for distinctive activities or working patterns.", "requirement_code": {"describe_observation_aggregate": 1, "discover_observation": 1}}, "data": {"data_text": "Speci\ufb01c Context and Data. In the context of this paper, we work\nwith a UBA model (designed, built and being used by one of the\nco-author\u2019s organisation) that operates on the logs that comprise of se-\nquences of actions carried out by the users of an administrative interface\nof a login and security server. The log data is split into sessions, each\ncontaining an ordered list of timestamped actions performed by the user\nconducting that session. In each session, particular tasks are conducted\nand actions are performed by the user. Actions in this context are se-\nmantically labelled and functionally relevant activities, e.g., managing\nusers and/or organisations with actions such as \u201cCreateLoginArea\u201d,\n\u201cSearchUsr\u201d, \u201cDisplayOrgaDetails\u201d or performing some intermediate\nactions such as \u201cCloseTab\u201d and \u201cCancel\u201d.\nEach session here is then a data record with the attributes: clock\ntime to indicate the start and end, User ID to indicate the performer,\nother meta information such as browser and operating system, and the\nsequence of actions (labels) and when each action took place within\nthe session. In addition to these, we also get the anomaly score that\nis provided by the underlying UBA with 1 being an unusual and 0\na normal session. The data set that we work with here comprises of\n19,351 sessions performed by 1,670 users within a 15 day time window\nusing 305 unique action types. Although we use this speci\ufb01c data set in\nthis paper, the form of the data is representative of log based solutions\nused for behaviour modelling in many other domains [13, 33].", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "4.1.1 Individual User Pro\ufb01les\nThe user pro\ufb01les combine three levels of features based on the different\ncases of feature availability: inherent features that are explicitly\navailable, derived features that can be directly extracted from the data\nthrough the use of standard statistical measures and latent features\nthat are not directly available for extraction, but would require more\nsophisticated methods to reveal underlying, latent characteristics of\nuser behaviour. Here, we consider the data in our application context\nand list the various features we put into use:\nInherent features. Features that describe the low-level character-\nistics of how users utilise the application. These features are often\nexplicit and provided as meta-data within the session records.\nP Browser used in the session\nP IP Address through which the session is conducted\nP Operating System running on the user\u2019s computer\nDerived Features. Features that are derived through straightfor-\nward statistical computations considering the distribution of actions in\nsessions and the distribution of sessions over users, together with any\nother explicitly derived metric such as model derived scores.\nP Session duration / length as a measure of time taken in the session\nand the count of actions performed\nP Total Sessions as a statistic on how active the user is\nP Total Unique Actions as a measure on how diverse is a user in the\nactions they perform\nLatent Features. Features that provide in-depth understanding\ninto the behaviour of users through the application of a sophisticated\ncomputation, such as modelling and clustering.\nP Anomaly Score for individual sessions as provided by the UBA\nsystem to indicate the normality of a session", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extract;statistical;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": ".3 Clustering Users based on their Tasks\nTo provide a semantic characterisation of users (addressing G1), we\nclassify them based on the tasks they perform. From the output of\ntopic modelling, a session can be modelled as a vector of tasks: si =\n(pi1, p i2, ..., p ik), where p i j is the probability of task j occuring in\nsession s i, with k as the total count of tasks. A user u can then be\nmodelled as a mean of its session vectors: u = 1\nm \u2211m\ni s i, where m is the\nnumber of sessions performed by u.\nHowever, there is a logical issue with adding session vectors. pi j\nindicates how probable task j occurs in session si, but it could be\nmisleading to compare the probability of the same task between two\nsessions. For instance, p11 = 0.5 means 50% of what happens in\nsession 1 is about task 1, and p21 = 0.3 means 30% of what happens in\nsession 2 is about task 1. But it cannot be inferred that session 1 has\nmore activities of task 1 than session 2 due to the difference in their\nnumber of actions. Therefore, we classify the probability based on a\nparticular threshold \u03b8 to indicate whether a task occurs in a session\nor not. Formally, s i = (bi1, b i2, ..., b ik), where bi j = 1 if p i j \u2265 \u03b8 and\nbi j = 0 otherwise. This binary transformation also allows a more\ninterpretable representation of the user vector: u = (t1,t2, ...,t k), where\nti is the proportion of the user\u2019s sessions where task i occurs. Choosing\nthe threshold \u03b8 is a challenging trade-off: small \u03b8 leads to too many\ntasks in one session, whereas large \u03b8 leads to too many sessions not\nassociated with any tasks. We choose \u03b8 = 0.3 for our dataset for two\nreasons: (1) a large number of sessions (94%) will have at least one task,\nand (2) each session will have up to 3 tasks, which is about consistent\nwith reality according to the domain experts.\nWe then apply k-means clustering [28] to cluster users based on\ntheir vector representations. k-means clustering is expected to work\nwell due to the small number of vector dimensions, which is 13 in our\ncase (13 tasks). To decide the optimal k number of topics, we use the\nelbow method [21] to visually select k by assessing the change of a\ncost function with different values of k. Our cost function is the sum of\nsquared distances of all data points to their closest cluster centre. As a\nresult, 11 clusters of users are generated.", "solution_category": "data_manipulation", "solution_axial": "Clustering", "solution_compoent": "model;", "axial_code": ["Clustering"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "This section discusses the visual design of user clusters to provide a\nsummary of a cluster (addressing T1) and to support comparison of\nmultiple clusters (addressing T2). We represent a cluster through\nits centroid; i.e., the mean of all user vectors within the cluster:\nc = (p1, p2, ..., p k), where k is the number of all extracted tasks. Seman-\ntically, pi describes how much the users in a cluster works on task i on\naverage. The vector is represented as a sequence of connected circles,\neach for a vector component or task. The circle size corresponds to the\nmagnitude of the vector component. Fig. 2 shows that within 13 tasks,\nusers in group G0 focus more on tasks T0, T1 and T2.\nTo provide a glance into the users in a cluster, we display the number\nof users as text and the 10 most relevant users as rectangular glyphs.\nIn each user glyph, the height corresponds to the number of sessions\nperformed by the user and the lightness proportional to the median\nanomaly score of the user\u2019s sessions. The relevance of user u is com-\nputed as follows: rel(u) = \u221an \u00d7 median{ai}, where n is the number of\nsessions performed by u and ai is the anomaly score of session s i. The\nscaling square root component is to put emphasis on users with higher\nnumber of sessions, since users with both many sessions and higher\nmedian scores are of utmost interest.\nTo enable the comparison of multiple clusters, their visual summaries\nare stacked together (Fig. 3) as in a matrix. This makes it possible to\ncompare the involvement of user groups between different tasks. For\ninstance, as seen in Fig. 3, Task T1 appears in most of user groups;\nwhereas, Task T8 is not common for any groups. The less frequent\ntasks, small circles, can be filtered out to accelerate the observation of\nmore probable tasks. Highlighted horizontal and vertical background\naccompany mouse movement to improve the reading of alignments.\nAfter gaining an understanding of user groups, the most natural next\nstep is to drill down to a particular group or user. The visualisation\nallows triggering this further investigation by clicking on the group\nrepresentation or the rectangular user glyph. We explain how such\nselection affects other views in the following sections.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar+heat_map+circle+matrix", "axial_code": [], "componenet_code": ["bar", "matrix", "heatmap", "circle"]}]}, {"author": "gsh", "index_original": 11, "paper_title": "VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour Analytics", "pub_year": 2020, "domain": "user behaviour", "requirement": {"requirement_text": "T2 Compare multiple user groups. To better understand the groups, analysts need to compare and identify the differentiating charac- teristics within them. Examples: investigate which tasks occur distinctively for a group or shared across several; look for differ- ences in the statistical features, e.g., a group with unusual working hours.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Speci\ufb01c Context and Data. In the context of this paper, we work\nwith a UBA model (designed, built and being used by one of the\nco-author\u2019s organisation) that operates on the logs that comprise of se-\nquences of actions carried out by the users of an administrative interface\nof a login and security server. The log data is split into sessions, each\ncontaining an ordered list of timestamped actions performed by the user\nconducting that session. In each session, particular tasks are conducted\nand actions are performed by the user. Actions in this context are se-\nmantically labelled and functionally relevant activities, e.g., managing\nusers and/or organisations with actions such as \u201cCreateLoginArea\u201d,\n\u201cSearchUsr\u201d, \u201cDisplayOrgaDetails\u201d or performing some intermediate\nactions such as \u201cCloseTab\u201d and \u201cCancel\u201d.\nEach session here is then a data record with the attributes: clock\ntime to indicate the start and end, User ID to indicate the performer,\nother meta information such as browser and operating system, and the\nsequence of actions (labels) and when each action took place within\nthe session. In addition to these, we also get the anomaly score that\nis provided by the underlying UBA with 1 being an unusual and 0\na normal session. The data set that we work with here comprises of\n19,351 sessions performed by 1,670 users within a 15 day time window\nusing 305 unique action types. Although we use this speci\ufb01c data set in\nthis paper, the form of the data is representative of log based solutions\nused for behaviour modelling in many other domains [13, 33].", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "4.1.1 Individual User Pro\ufb01les\nThe user pro\ufb01les combine three levels of features based on the different\ncases of feature availability: inherent features that are explicitly\navailable, derived features that can be directly extracted from the data\nthrough the use of standard statistical measures and latent features\nthat are not directly available for extraction, but would require more\nsophisticated methods to reveal underlying, latent characteristics of\nuser behaviour. Here, we consider the data in our application context\nand list the various features we put into use:\nInherent features. Features that describe the low-level character-\nistics of how users utilise the application. These features are often\nexplicit and provided as meta-data within the session records.\nP Browser used in the session\nP IP Address through which the session is conducted\nP Operating System running on the user\u2019s computer\nDerived Features. Features that are derived through straightfor-\nward statistical computations considering the distribution of actions in\nsessions and the distribution of sessions over users, together with any\nother explicitly derived metric such as model derived scores.\nP Session duration / length as a measure of time taken in the session\nand the count of actions performed\nP Total Sessions as a statistic on how active the user is\nP Total Unique Actions as a measure on how diverse is a user in the\nactions they perform\nLatent Features. Features that provide in-depth understanding\ninto the behaviour of users through the application of a sophisticated\ncomputation, such as modelling and clustering.\nP Anomaly Score for individual sessions as provided by the UBA\nsystem to indicate the normality of a session", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extract;statistical;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": ".3 Clustering Users based on their Tasks\nTo provide a semantic characterisation of users (addressing G1), we\nclassify them based on the tasks they perform. From the output of\ntopic modelling, a session can be modelled as a vector of tasks: si =\n(pi1, p i2, ..., p ik), where p i j is the probability of task j occuring in\nsession s i, with k as the total count of tasks. A user u can then be\nmodelled as a mean of its session vectors: u = 1\nm \u2211m\ni s i, where m is the\nnumber of sessions performed by u.\nHowever, there is a logical issue with adding session vectors. pi j\nindicates how probable task j occurs in session si, but it could be\nmisleading to compare the probability of the same task between two\nsessions. For instance, p11 = 0.5 means 50% of what happens in\nsession 1 is about task 1, and p21 = 0.3 means 30% of what happens in\nsession 2 is about task 1. But it cannot be inferred that session 1 has\nmore activities of task 1 than session 2 due to the difference in their\nnumber of actions. Therefore, we classify the probability based on a\nparticular threshold \u03b8 to indicate whether a task occurs in a session\nor not. Formally, s i = (bi1, b i2, ..., b ik), where bi j = 1 if p i j \u2265 \u03b8 and\nbi j = 0 otherwise. This binary transformation also allows a more\ninterpretable representation of the user vector: u = (t1,t2, ...,t k), where\nti is the proportion of the user\u2019s sessions where task i occurs. Choosing\nthe threshold \u03b8 is a challenging trade-off: small \u03b8 leads to too many\ntasks in one session, whereas large \u03b8 leads to too many sessions not\nassociated with any tasks. We choose \u03b8 = 0.3 for our dataset for two\nreasons: (1) a large number of sessions (94%) will have at least one task,\nand (2) each session will have up to 3 tasks, which is about consistent\nwith reality according to the domain experts.\nWe then apply k-means clustering [28] to cluster users based on\ntheir vector representations. k-means clustering is expected to work\nwell due to the small number of vector dimensions, which is 13 in our\ncase (13 tasks). To decide the optimal k number of topics, we use the\nelbow method [21] to visually select k by assessing the change of a\ncost function with different values of k. Our cost function is the sum of\nsquared distances of all data points to their closest cluster centre. As a\nresult, 11 clusters of users are generated.", "solution_category": "data_manipulation", "solution_axial": "Clustering", "solution_compoent": "model;", "axial_code": ["Clustering"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "This section discusses the visual design of user clusters to provide a\nsummary of a cluster (addressing T1) and to support comparison of\nmultiple clusters (addressing T2). We represent a cluster through\nits centroid; i.e., the mean of all user vectors within the cluster:\nc = (p1, p2, ..., p k), where k is the number of all extracted tasks. Seman-\ntically, pi describes how much the users in a cluster works on task i on\naverage. The vector is represented as a sequence of connected circles,\neach for a vector component or task. The circle size corresponds to the\nmagnitude of the vector component. Fig. 2 shows that within 13 tasks,\nusers in group G0 focus more on tasks T0, T1 and T2.\nTo provide a glance into the users in a cluster, we display the number\nof users as text and the 10 most relevant users as rectangular glyphs.\nIn each user glyph, the height corresponds to the number of sessions\nperformed by the user and the lightness proportional to the median\nanomaly score of the user\u2019s sessions. The relevance of user u is com-\nputed as follows: rel(u) = \u221an \u00d7 median{ai}, where n is the number of\nsessions performed by u and ai is the anomaly score of session s i. The\nscaling square root component is to put emphasis on users with higher\nnumber of sessions, since users with both many sessions and higher\nmedian scores are of utmost interest.\nTo enable the comparison of multiple clusters, their visual summaries\nare stacked together (Fig. 3) as in a matrix. This makes it possible to\ncompare the involvement of user groups between different tasks. For\ninstance, as seen in Fig. 3, Task T1 appears in most of user groups;\nwhereas, Task T8 is not common for any groups. The less frequent\ntasks, small circles, can be filtered out to accelerate the observation of\nmore probable tasks. Highlighted horizontal and vertical background\naccompany mouse movement to improve the reading of alignments.\nAfter gaining an understanding of user groups, the most natural next\nstep is to drill down to a particular group or user. The visualisation\nallows triggering this further investigation by clicking on the group\nrepresentation or the rectangular user glyph. We explain how such\nselection affects other views in the following sections.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar+heat_map+circle+matrix", "axial_code": [], "componenet_code": ["bar", "matrix", "heatmap", "circle"]}]}, {"author": "gsh", "index_original": 12, "paper_title": "VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour Analytics", "pub_year": 2020, "domain": "user behaviour", "requirement": {"requirement_text": "T3 Summarise a user through its characteristic behaviour. Individ- uals have idiosyncratic ways in which they use a system and analysts need to build a multifaceted understanding of user be- haviour to be able to make decisions when they work on individual cases and when they are developing/improving the UBA mod- els. Examples: observe simple behaviour characteristics such as which times of day the system is used; understand which actions are performed in what frequency to complete different tasks", "requirement_code": {"discover_observation": 1, "describe_observation_item": 1}}, "data": {"data_text": "Speci\ufb01c Context and Data. In the context of this paper, we work\nwith a UBA model (designed, built and being used by one of the\nco-author\u2019s organisation) that operates on the logs that comprise of se-\nquences of actions carried out by the users of an administrative interface\nof a login and security server. The log data is split into sessions, each\ncontaining an ordered list of timestamped actions performed by the user\nconducting that session. In each session, particular tasks are conducted\nand actions are performed by the user. Actions in this context are se-\nmantically labelled and functionally relevant activities, e.g., managing\nusers and/or organisations with actions such as \u201cCreateLoginArea\u201d,\n\u201cSearchUsr\u201d, \u201cDisplayOrgaDetails\u201d or performing some intermediate\nactions such as \u201cCloseTab\u201d and \u201cCancel\u201d.\nEach session here is then a data record with the attributes: clock\ntime to indicate the start and end, User ID to indicate the performer,\nother meta information such as browser and operating system, and the\nsequence of actions (labels) and when each action took place within\nthe session. In addition to these, we also get the anomaly score that\nis provided by the underlying UBA with 1 being an unusual and 0\na normal session. The data set that we work with here comprises of\n19,351 sessions performed by 1,670 users within a 15 day time window\nusing 305 unique action types. Although we use this speci\ufb01c data set in\nthis paper, the form of the data is representative of log based solutions\nused for behaviour modelling in many other domains [13, 33].", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "4.1.1 Individual User Pro\ufb01les\nThe user pro\ufb01les combine three levels of features based on the different\ncases of feature availability: inherent features that are explicitly\navailable, derived features that can be directly extracted from the data\nthrough the use of standard statistical measures and latent features\nthat are not directly available for extraction, but would require more\nsophisticated methods to reveal underlying, latent characteristics of\nuser behaviour. Here, we consider the data in our application context\nand list the various features we put into use:\nInherent features. Features that describe the low-level character-\nistics of how users utilise the application. These features are often\nexplicit and provided as meta-data within the session records.\nP Browser used in the session\nP IP Address through which the session is conducted\nP Operating System running on the user\u2019s computer\nDerived Features. Features that are derived through straightfor-\nward statistical computations considering the distribution of actions in\nsessions and the distribution of sessions over users, together with any\nother explicitly derived metric such as model derived scores.\nP Session duration / length as a measure of time taken in the session\nand the count of actions performed\nP Total Sessions as a statistic on how active the user is\nP Total Unique Actions as a measure on how diverse is a user in the\nactions they perform\nLatent Features. Features that provide in-depth understanding\ninto the behaviour of users through the application of a sophisticated\ncomputation, such as modelling and clustering.\nP Anomaly Score for individual sessions as provided by the UBA\nsystem to indicate the normality of a session", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extract;statistical;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": ".3 Clustering Users based on their Tasks\nTo provide a semantic characterisation of users (addressing G1), we\nclassify them based on the tasks they perform. From the output of\ntopic modelling, a session can be modelled as a vector of tasks: si =\n(pi1, p i2, ..., p ik), where p i j is the probability of task j occuring in\nsession s i, with k as the total count of tasks. A user u can then be\nmodelled as a mean of its session vectors: u = 1\nm \u2211m\ni s i, where m is the\nnumber of sessions performed by u.\nHowever, there is a logical issue with adding session vectors. pi j\nindicates how probable task j occurs in session si, but it could be\nmisleading to compare the probability of the same task between two\nsessions. For instance, p11 = 0.5 means 50% of what happens in\nsession 1 is about task 1, and p21 = 0.3 means 30% of what happens in\nsession 2 is about task 1. But it cannot be inferred that session 1 has\nmore activities of task 1 than session 2 due to the difference in their\nnumber of actions. Therefore, we classify the probability based on a\nparticular threshold \u03b8 to indicate whether a task occurs in a session\nor not. Formally, s i = (bi1, b i2, ..., b ik), where bi j = 1 if p i j \u2265 \u03b8 and\nbi j = 0 otherwise. This binary transformation also allows a more\ninterpretable representation of the user vector: u = (t1,t2, ...,t k), where\nti is the proportion of the user\u2019s sessions where task i occurs. Choosing\nthe threshold \u03b8 is a challenging trade-off: small \u03b8 leads to too many\ntasks in one session, whereas large \u03b8 leads to too many sessions not\nassociated with any tasks. We choose \u03b8 = 0.3 for our dataset for two\nreasons: (1) a large number of sessions (94%) will have at least one task,\nand (2) each session will have up to 3 tasks, which is about consistent\nwith reality according to the domain experts.\nWe then apply k-means clustering [28] to cluster users based on\ntheir vector representations. k-means clustering is expected to work\nwell due to the small number of vector dimensions, which is 13 in our\ncase (13 tasks). To decide the optimal k number of topics, we use the\nelbow method [21] to visually select k by assessing the change of a\ncost function with different values of k. Our cost function is the sum of\nsquared distances of all data points to their closest cluster centre. As a\nresult, 11 clusters of users are generated.", "solution_category": "data_manipulation", "solution_axial": "Clustering", "solution_compoent": "model;", "axial_code": ["Clustering"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Group profiles refer to the higher level aggregations of characteristics of users. In order to construct these profiles, users of similar traits are identified through a clustering method explained in Sect. 4.3, and the features of individual users (those described above) are aggregated for each group. Overall, a group profile comprises three key elements: aggregated feature statistics depicting the joint characteristics of the group in terms of the inherent and derived features of individuals, task profiles depicting the most frequent tasks conducted by the group, and representative users chosen as members that are of particular interest for further analysis (using criteria as discussed in Sect. 4.4). Combined together, group and user profiles help address the analysis goals G1 and G2. Moreover, a user profile also characterises how typical sessions look like, allowing comparison at session level (addressing G3).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "data;higer_level_aggregations_of_characteristics_of_users;cluster", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "4.5.1 Visual Summary and Comparison of User Profiles\nAs discussed earlier, our user profile consists of multiple features that\nmodel different characteristics of user behaviour. Thus, it is essential\nfor analysts to observe many features of user profile concurrently (ad-\ndressing T3). For each feature, we use a small chart to show a summary\nof the user\u2019s sessions and concatenate the charts vertically to produce a\ncompact representation of a user profile. The visual profiles are then\nstacked together to enable comparison (Fig. 4) (addressing T4, T5) and\nare sorted using the same relevance metric described in Sect. 4.4.\nP Per-profile feature (e.g., the number of sessions). A single hor-\nizontal bar is used, which is unnecessary for one profile but is\nuseful for comparing multiple ones.\nP Per-session quantitative feature (e.g., anomaly score). Initially,\nwe use a histogram to show the distribution of feature values (see\nsupplemental material). However, histograms are sensitive to the\nchoice of the number of bins and are not visually straightforward\nto compare multiple histograms. Therefore, we use kernel den-\nsity estimation (KDE) [38] to avoid rough binning and enhance\ncomparison between multiple features.\nP Per-session nominal feature (e.g., operating system). A simple\nbarchart is used. The same set of nominal values are shared across\nall profiles to enable comparison.\nBoth individual and group profiles have the same visual representa-\ntion (except for the group name and the user name). As seen in Fig. 4,\nthis allows the comparison of multiple characteristics both between\nusers and with their corresponding group G0. Many observations can\nbe made through this figure. All of these users have completed many\nmore sessions and performed many more unique actions than the aver-\nage of their group. The anomaly scores from their sessions are higher\nthan those from their group as well, especially sessions from Blood-\nshed. The behaviour in terms of session length and duration is similar\nbetween users, but very different in terms of starting time.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar+area+point", "axial_code": [], "componenet_code": ["scatter", "bar", "area"]}]}, {"author": "gsh", "index_original": 13, "paper_title": "VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour Analytics", "pub_year": 2020, "domain": "user behaviour", "requirement": {"requirement_text": "T4 Compare multiple users. Analysts often investigate several users (which could be from the same group or organisation, or any collection of users) simultaneously and need to compare their characteristics to identify users with distinct behaviour or work- ing patterns overall or for a particular time period. Examples: compare the working hours of a user to the other members of the same group; compare the activities of several users in a given temporal frame and check for deviation.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Speci\ufb01c Context and Data. In the context of this paper, we work\nwith a UBA model (designed, built and being used by one of the\nco-author\u2019s organisation) that operates on the logs that comprise of se-\nquences of actions carried out by the users of an administrative interface\nof a login and security server. The log data is split into sessions, each\ncontaining an ordered list of timestamped actions performed by the user\nconducting that session. In each session, particular tasks are conducted\nand actions are performed by the user. Actions in this context are se-\nmantically labelled and functionally relevant activities, e.g., managing\nusers and/or organisations with actions such as \u201cCreateLoginArea\u201d,\n\u201cSearchUsr\u201d, \u201cDisplayOrgaDetails\u201d or performing some intermediate\nactions such as \u201cCloseTab\u201d and \u201cCancel\u201d.\nEach session here is then a data record with the attributes: clock\ntime to indicate the start and end, User ID to indicate the performer,\nother meta information such as browser and operating system, and the\nsequence of actions (labels) and when each action took place within\nthe session. In addition to these, we also get the anomaly score that\nis provided by the underlying UBA with 1 being an unusual and 0\na normal session. The data set that we work with here comprises of\n19,351 sessions performed by 1,670 users within a 15 day time window\nusing 305 unique action types. Although we use this speci\ufb01c data set in\nthis paper, the form of the data is representative of log based solutions\nused for behaviour modelling in many other domains [13, 33].", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "4.1.1 Individual User Pro\ufb01les\nThe user pro\ufb01les combine three levels of features based on the different\ncases of feature availability: inherent features that are explicitly\navailable, derived features that can be directly extracted from the data\nthrough the use of standard statistical measures and latent features\nthat are not directly available for extraction, but would require more\nsophisticated methods to reveal underlying, latent characteristics of\nuser behaviour. Here, we consider the data in our application context\nand list the various features we put into use:\nInherent features. Features that describe the low-level character-\nistics of how users utilise the application. These features are often\nexplicit and provided as meta-data within the session records.\nP Browser used in the session\nP IP Address through which the session is conducted\nP Operating System running on the user\u2019s computer\nDerived Features. Features that are derived through straightfor-\nward statistical computations considering the distribution of actions in\nsessions and the distribution of sessions over users, together with any\nother explicitly derived metric such as model derived scores.\nP Session duration / length as a measure of time taken in the session\nand the count of actions performed\nP Total Sessions as a statistic on how active the user is\nP Total Unique Actions as a measure on how diverse is a user in the\nactions they perform\nLatent Features. Features that provide in-depth understanding\ninto the behaviour of users through the application of a sophisticated\ncomputation, such as modelling and clustering.\nP Anomaly Score for individual sessions as provided by the UBA\nsystem to indicate the normality of a session", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extract;statistical;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": ".3 Clustering Users based on their Tasks\nTo provide a semantic characterisation of users (addressing G1), we\nclassify them based on the tasks they perform. From the output of\ntopic modelling, a session can be modelled as a vector of tasks: si =\n(pi1, p i2, ..., p ik), where p i j is the probability of task j occuring in\nsession s i, with k as the total count of tasks. A user u can then be\nmodelled as a mean of its session vectors: u = 1\nm \u2211m\ni s i, where m is the\nnumber of sessions performed by u.\nHowever, there is a logical issue with adding session vectors. pi j\nindicates how probable task j occurs in session si, but it could be\nmisleading to compare the probability of the same task between two\nsessions. For instance, p11 = 0.5 means 50% of what happens in\nsession 1 is about task 1, and p21 = 0.3 means 30% of what happens in\nsession 2 is about task 1. But it cannot be inferred that session 1 has\nmore activities of task 1 than session 2 due to the difference in their\nnumber of actions. Therefore, we classify the probability based on a\nparticular threshold \u03b8 to indicate whether a task occurs in a session\nor not. Formally, s i = (bi1, b i2, ..., b ik), where bi j = 1 if p i j \u2265 \u03b8 and\nbi j = 0 otherwise. This binary transformation also allows a more\ninterpretable representation of the user vector: u = (t1,t2, ...,t k), where\nti is the proportion of the user\u2019s sessions where task i occurs. Choosing\nthe threshold \u03b8 is a challenging trade-off: small \u03b8 leads to too many\ntasks in one session, whereas large \u03b8 leads to too many sessions not\nassociated with any tasks. We choose \u03b8 = 0.3 for our dataset for two\nreasons: (1) a large number of sessions (94%) will have at least one task,\nand (2) each session will have up to 3 tasks, which is about consistent\nwith reality according to the domain experts.\nWe then apply k-means clustering [28] to cluster users based on\ntheir vector representations. k-means clustering is expected to work\nwell due to the small number of vector dimensions, which is 13 in our\ncase (13 tasks). To decide the optimal k number of topics, we use the\nelbow method [21] to visually select k by assessing the change of a\ncost function with different values of k. Our cost function is the sum of\nsquared distances of all data points to their closest cluster centre. As a\nresult, 11 clusters of users are generated.", "solution_category": "data_manipulation", "solution_axial": "Clustering", "solution_compoent": "model;", "axial_code": ["Clustering"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Group profiles refer to the higher level aggregations of characteristics of users. In order to construct these profiles, users of similar traits are identified through a clustering method explained in Sect. 4.3, and the features of individual users (those described above) are aggregated for each group. Overall, a group profile comprises three key elements: aggregated feature statistics depicting the joint characteristics of the group in terms of the inherent and derived features of individuals, task profiles depicting the most frequent tasks conducted by the group, and representative users chosen as members that are of particular interest for further analysis (using criteria as discussed in Sect. 4.4). Combined together, group and user profiles help address the analysis goals G1 and G2. Moreover, a user profile also characterises how typical sessions look like, allowing comparison at session level (addressing G3).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "data;higer_level_aggregations_of_characteristics_of_users;cluster", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "4.5.1 Visual Summary and Comparison of User Profiles\nAs discussed earlier, our user profile consists of multiple features that\nmodel different characteristics of user behaviour. Thus, it is essential\nfor analysts to observe many features of user profile concurrently (ad-\ndressing T3). For each feature, we use a small chart to show a summary\nof the user\u2019s sessions and concatenate the charts vertically to produce a\ncompact representation of a user profile. The visual profiles are then\nstacked together to enable comparison (Fig. 4) (addressing T4, T5) and\nare sorted using the same relevance metric described in Sect. 4.4.\nP Per-profile feature (e.g., the number of sessions). A single hor-\nizontal bar is used, which is unnecessary for one profile but is\nuseful for comparing multiple ones.\nP Per-session quantitative feature (e.g., anomaly score). Initially,\nwe use a histogram to show the distribution of feature values (see\nsupplemental material). However, histograms are sensitive to the\nchoice of the number of bins and are not visually straightforward\nto compare multiple histograms. Therefore, we use kernel den-\nsity estimation (KDE) [38] to avoid rough binning and enhance\ncomparison between multiple features.\nP Per-session nominal feature (e.g., operating system). A simple\nbarchart is used. The same set of nominal values are shared across\nall profiles to enable comparison.\nBoth individual and group profiles have the same visual representa-\ntion (except for the group name and the user name). As seen in Fig. 4,\nthis allows the comparison of multiple characteristics both between\nusers and with their corresponding group G0. Many observations can\nbe made through this figure. All of these users have completed many\nmore sessions and performed many more unique actions than the aver-\nage of their group. The anomaly scores from their sessions are higher\nthan those from their group as well, especially sessions from Blood-\nshed. The behaviour in terms of session length and duration is similar\nbetween users, but very different in terms of starting time.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar+area+point", "axial_code": [], "componenet_code": ["scatter", "bar", "area"]}]}, {"author": "gsh", "index_original": 14, "paper_title": "VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour Analytics", "pub_year": 2020, "domain": "user behaviour", "requirement": {"requirement_text": "T5 Compare a user with a group. The group level aggregates provide the high-level characteristics of a group of users and analysts need to assess how expected the behaviour of a user is given the characteristics of the group. Examples: for any unusual task for a user given their own historical data, check if that activity is common for their respective groups; assess if a user is different to the overall group characteristics.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "Speci\ufb01c Context and Data. In the context of this paper, we work\nwith a UBA model (designed, built and being used by one of the\nco-author\u2019s organisation) that operates on the logs that comprise of se-\nquences of actions carried out by the users of an administrative interface\nof a login and security server. The log data is split into sessions, each\ncontaining an ordered list of timestamped actions performed by the user\nconducting that session. In each session, particular tasks are conducted\nand actions are performed by the user. Actions in this context are se-\nmantically labelled and functionally relevant activities, e.g., managing\nusers and/or organisations with actions such as \u201cCreateLoginArea\u201d,\n\u201cSearchUsr\u201d, \u201cDisplayOrgaDetails\u201d or performing some intermediate\nactions such as \u201cCloseTab\u201d and \u201cCancel\u201d.\nEach session here is then a data record with the attributes: clock\ntime to indicate the start and end, User ID to indicate the performer,\nother meta information such as browser and operating system, and the\nsequence of actions (labels) and when each action took place within\nthe session. In addition to these, we also get the anomaly score that\nis provided by the underlying UBA with 1 being an unusual and 0\na normal session. The data set that we work with here comprises of\n19,351 sessions performed by 1,670 users within a 15 day time window\nusing 305 unique action types. Although we use this speci\ufb01c data set in\nthis paper, the form of the data is representative of log based solutions\nused for behaviour modelling in many other domains [13, 33].", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "4.1.1 Individual User Pro\ufb01les\nThe user pro\ufb01les combine three levels of features based on the different\ncases of feature availability: inherent features that are explicitly\navailable, derived features that can be directly extracted from the data\nthrough the use of standard statistical measures and latent features\nthat are not directly available for extraction, but would require more\nsophisticated methods to reveal underlying, latent characteristics of\nuser behaviour. Here, we consider the data in our application context\nand list the various features we put into use:\nInherent features. Features that describe the low-level character-\nistics of how users utilise the application. These features are often\nexplicit and provided as meta-data within the session records.\nP Browser used in the session\nP IP Address through which the session is conducted\nP Operating System running on the user\u2019s computer\nDerived Features. Features that are derived through straightfor-\nward statistical computations considering the distribution of actions in\nsessions and the distribution of sessions over users, together with any\nother explicitly derived metric such as model derived scores.\nP Session duration / length as a measure of time taken in the session\nand the count of actions performed\nP Total Sessions as a statistic on how active the user is\nP Total Unique Actions as a measure on how diverse is a user in the\nactions they perform\nLatent Features. Features that provide in-depth understanding\ninto the behaviour of users through the application of a sophisticated\ncomputation, such as modelling and clustering.\nP Anomaly Score for individual sessions as provided by the UBA\nsystem to indicate the normality of a session", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extract;statistical;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": ".3 Clustering Users based on their Tasks\nTo provide a semantic characterisation of users (addressing G1), we\nclassify them based on the tasks they perform. From the output of\ntopic modelling, a session can be modelled as a vector of tasks: si =\n(pi1, p i2, ..., p ik), where p i j is the probability of task j occuring in\nsession s i, with k as the total count of tasks. A user u can then be\nmodelled as a mean of its session vectors: u = 1\nm \u2211m\ni s i, where m is the\nnumber of sessions performed by u.\nHowever, there is a logical issue with adding session vectors. pi j\nindicates how probable task j occurs in session si, but it could be\nmisleading to compare the probability of the same task between two\nsessions. For instance, p11 = 0.5 means 50% of what happens in\nsession 1 is about task 1, and p21 = 0.3 means 30% of what happens in\nsession 2 is about task 1. But it cannot be inferred that session 1 has\nmore activities of task 1 than session 2 due to the difference in their\nnumber of actions. Therefore, we classify the probability based on a\nparticular threshold \u03b8 to indicate whether a task occurs in a session\nor not. Formally, s i = (bi1, b i2, ..., b ik), where bi j = 1 if p i j \u2265 \u03b8 and\nbi j = 0 otherwise. This binary transformation also allows a more\ninterpretable representation of the user vector: u = (t1,t2, ...,t k), where\nti is the proportion of the user\u2019s sessions where task i occurs. Choosing\nthe threshold \u03b8 is a challenging trade-off: small \u03b8 leads to too many\ntasks in one session, whereas large \u03b8 leads to too many sessions not\nassociated with any tasks. We choose \u03b8 = 0.3 for our dataset for two\nreasons: (1) a large number of sessions (94%) will have at least one task,\nand (2) each session will have up to 3 tasks, which is about consistent\nwith reality according to the domain experts.\nWe then apply k-means clustering [28] to cluster users based on\ntheir vector representations. k-means clustering is expected to work\nwell due to the small number of vector dimensions, which is 13 in our\ncase (13 tasks). To decide the optimal k number of topics, we use the\nelbow method [21] to visually select k by assessing the change of a\ncost function with different values of k. Our cost function is the sum of\nsquared distances of all data points to their closest cluster centre. As a\nresult, 11 clusters of users are generated.", "solution_category": "data_manipulation", "solution_axial": "Clustering", "solution_compoent": "model;", "axial_code": ["Clustering"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Group profiles refer to the higher level aggregations of characteristics of users. In order to construct these profiles, users of similar traits are identified through a clustering method explained in Sect. 4.3, and the features of individual users (those described above) are aggregated for each group. Overall, a group profile comprises three key elements: aggregated feature statistics depicting the joint characteristics of the group in terms of the inherent and derived features of individuals, task profiles depicting the most frequent tasks conducted by the group, and representative users chosen as members that are of particular interest for further analysis (using criteria as discussed in Sect. 4.4). Combined together, group and user profiles help address the analysis goals G1 and G2. Moreover, a user profile also characterises how typical sessions look like, allowing comparison at session level (addressing G3).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "data;higer_level_aggregations_of_characteristics_of_users;cluster", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "4.5.1 Visual Summary and Comparison of User Profiles\nAs discussed earlier, our user profile consists of multiple features that\nmodel different characteristics of user behaviour. Thus, it is essential\nfor analysts to observe many features of user profile concurrently (ad-\ndressing T3). For each feature, we use a small chart to show a summary\nof the user\u2019s sessions and concatenate the charts vertically to produce a\ncompact representation of a user profile. The visual profiles are then\nstacked together to enable comparison (Fig. 4) (addressing T4, T5) and\nare sorted using the same relevance metric described in Sect. 4.4.\nP Per-profile feature (e.g., the number of sessions). A single hor-\nizontal bar is used, which is unnecessary for one profile but is\nuseful for comparing multiple ones.\nP Per-session quantitative feature (e.g., anomaly score). Initially,\nwe use a histogram to show the distribution of feature values (see\nsupplemental material). However, histograms are sensitive to the\nchoice of the number of bins and are not visually straightforward\nto compare multiple histograms. Therefore, we use kernel den-\nsity estimation (KDE) [38] to avoid rough binning and enhance\ncomparison between multiple features.\nP Per-session nominal feature (e.g., operating system). A simple\nbarchart is used. The same set of nominal values are shared across\nall profiles to enable comparison.\nBoth individual and group profiles have the same visual representa-\ntion (except for the group name and the user name). As seen in Fig. 4,\nthis allows the comparison of multiple characteristics both between\nusers and with their corresponding group G0. Many observations can\nbe made through this figure. All of these users have completed many\nmore sessions and performed many more unique actions than the aver-\nage of their group. The anomaly scores from their sessions are higher\nthan those from their group as well, especially sessions from Blood-\nshed. The behaviour in terms of session length and duration is similar\nbetween users, but very different in terms of starting time.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar+area+point", "axial_code": [], "componenet_code": ["scatter", "bar", "area"]}]}, {"author": "gsh", "index_original": 15, "paper_title": "VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour Analytics", "pub_year": 2020, "domain": "user behaviour", "requirement": {"requirement_text": "T6 Compare a session with a user's typical behaviour. When a session is flagged as anomalous, a key task is to compare what happens in the session to the overall behaviour of that user and also to the group that the user belongs to. Examples: check if the types of actions conducted in a session fall under the usual tasks of the user; check if the properties of the session, such as duration and action rate, are usual for that user.", "requirement_code": {"compare_entities": 1, "explain_differences": 1}}, "data": {"data_text": "Speci\ufb01c Context and Data. In the context of this paper, we work\nwith a UBA model (designed, built and being used by one of the\nco-author\u2019s organisation) that operates on the logs that comprise of se-\nquences of actions carried out by the users of an administrative interface\nof a login and security server. The log data is split into sessions, each\ncontaining an ordered list of timestamped actions performed by the user\nconducting that session. In each session, particular tasks are conducted\nand actions are performed by the user. Actions in this context are se-\nmantically labelled and functionally relevant activities, e.g., managing\nusers and/or organisations with actions such as \u201cCreateLoginArea\u201d,\n\u201cSearchUsr\u201d, \u201cDisplayOrgaDetails\u201d or performing some intermediate\nactions such as \u201cCloseTab\u201d and \u201cCancel\u201d.\nEach session here is then a data record with the attributes: clock\ntime to indicate the start and end, User ID to indicate the performer,\nother meta information such as browser and operating system, and the\nsequence of actions (labels) and when each action took place within\nthe session. In addition to these, we also get the anomaly score that\nis provided by the underlying UBA with 1 being an unusual and 0\na normal session. The data set that we work with here comprises of\n19,351 sessions performed by 1,670 users within a 15 day time window\nusing 305 unique action types. Although we use this speci\ufb01c data set in\nthis paper, the form of the data is representative of log based solutions\nused for behaviour modelling in many other domains [13, 33].", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "4.1.1 Individual User Pro\ufb01les\nThe user pro\ufb01les combine three levels of features based on the different\ncases of feature availability: inherent features that are explicitly\navailable, derived features that can be directly extracted from the data\nthrough the use of standard statistical measures and latent features\nthat are not directly available for extraction, but would require more\nsophisticated methods to reveal underlying, latent characteristics of\nuser behaviour. Here, we consider the data in our application context\nand list the various features we put into use:\nInherent features. Features that describe the low-level character-\nistics of how users utilise the application. These features are often\nexplicit and provided as meta-data within the session records.\nP Browser used in the session\nP IP Address through which the session is conducted\nP Operating System running on the user\u2019s computer\nDerived Features. Features that are derived through straightfor-\nward statistical computations considering the distribution of actions in\nsessions and the distribution of sessions over users, together with any\nother explicitly derived metric such as model derived scores.\nP Session duration / length as a measure of time taken in the session\nand the count of actions performed\nP Total Sessions as a statistic on how active the user is\nP Total Unique Actions as a measure on how diverse is a user in the\nactions they perform\nLatent Features. Features that provide in-depth understanding\ninto the behaviour of users through the application of a sophisticated\ncomputation, such as modelling and clustering.\nP Anomaly Score for individual sessions as provided by the UBA\nsystem to indicate the normality of a session", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extract;statistical;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": ".3 Clustering Users based on their Tasks\nTo provide a semantic characterisation of users (addressing G1), we\nclassify them based on the tasks they perform. From the output of\ntopic modelling, a session can be modelled as a vector of tasks: si =\n(pi1, p i2, ..., p ik), where p i j is the probability of task j occuring in\nsession s i, with k as the total count of tasks. A user u can then be\nmodelled as a mean of its session vectors: u = 1\nm \u2211m\ni s i, where m is the\nnumber of sessions performed by u.\nHowever, there is a logical issue with adding session vectors. pi j\nindicates how probable task j occurs in session si, but it could be\nmisleading to compare the probability of the same task between two\nsessions. For instance, p11 = 0.5 means 50% of what happens in\nsession 1 is about task 1, and p21 = 0.3 means 30% of what happens in\nsession 2 is about task 1. But it cannot be inferred that session 1 has\nmore activities of task 1 than session 2 due to the difference in their\nnumber of actions. Therefore, we classify the probability based on a\nparticular threshold \u03b8 to indicate whether a task occurs in a session\nor not. Formally, s i = (bi1, b i2, ..., b ik), where bi j = 1 if p i j \u2265 \u03b8 and\nbi j = 0 otherwise. This binary transformation also allows a more\ninterpretable representation of the user vector: u = (t1,t2, ...,t k), where\nti is the proportion of the user\u2019s sessions where task i occurs. Choosing\nthe threshold \u03b8 is a challenging trade-off: small \u03b8 leads to too many\ntasks in one session, whereas large \u03b8 leads to too many sessions not\nassociated with any tasks. We choose \u03b8 = 0.3 for our dataset for two\nreasons: (1) a large number of sessions (94%) will have at least one task,\nand (2) each session will have up to 3 tasks, which is about consistent\nwith reality according to the domain experts.\nWe then apply k-means clustering [28] to cluster users based on\ntheir vector representations. k-means clustering is expected to work\nwell due to the small number of vector dimensions, which is 13 in our\ncase (13 tasks). To decide the optimal k number of topics, we use the\nelbow method [21] to visually select k by assessing the change of a\ncost function with different values of k. Our cost function is the sum of\nsquared distances of all data points to their closest cluster centre. As a\nresult, 11 clusters of users are generated.", "solution_category": "data_manipulation", "solution_axial": "Clustering", "solution_compoent": "model;", "axial_code": ["Clustering"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Group profiles refer to the higher level aggregations of characteristics of users. In order to construct these profiles, users of similar traits are identified through a clustering method explained in Sect. 4.3, and the features of individual users (those described above) are aggregated for each group. Overall, a group profile comprises three key elements: aggregated feature statistics depicting the joint characteristics of the group in terms of the inherent and derived features of individuals, task profiles depicting the most frequent tasks conducted by the group, and representative users chosen as members that are of particular interest for further analysis (using criteria as discussed in Sect. 4.4). Combined together, group and user profiles help address the analysis goals G1 and G2. Moreover, a user profile also characterises how typical sessions look like, allowing comparison at session level (addressing G3).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "data;higer_level_aggregations_of_characteristics_of_users;cluster", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "4.5.2 Analysis of Sessions in the Context\nIn the investigation of an anomalous session, it is crucial to analyse\nit within the context of the user performing the session so that any\ndeviation from the user\u2019s typical behaviour will be effectively spotted\n(addressing T6). We support this by enabling comparison between the\nsessions of interest and their corresponding user profiles that are built\nfrom all sessions performed by the same users. Sessions of interest\nare superimposed on top of the visual profiles as small orange dots.\nA random noise is added to the vertical position of the dots to avoid\noverplotting. Fig. 4 shows that the distribution of the orange dots is\nroughly the same as the shaded area of the profiles, which indicates that\nthe sessions of interest are similar to what the users typically do. The\nslightly deviated session is the session with highest score from Black\nGoliath, which is manually annotated with a blue circle in Fig. 4.\nWith such a compact representation, it is essential to equip the\nvisualisation with interaction to support further investigation. Mouse\nhovering a session (indicated as red dot) highlights the same session\non other features, enabling to observe different features concurrently.\nFor example, in Fig. 4, hovering the session with the highest score\nfrom Black Goliath reveals that there are no deviations in other features:\nshort length and duration, typical starting time ranges and using the\nsame operating system and browser. It is also possible to brush a\nrange of sessions, possibly from multiple users, to explore and compare\ntheir features (addressing T7). For instance, in Fig. 4, sessions that\nstarted late from Asylum and Bloodshed are selected for exploration.\nOne important next step is to investigate what actually happened in\nthose sessions besides the meta features. To support that, the selected\nsessions are also displayed in the Timeline view (described in Sect. 5.2)\nallowing the examination of the performed actions in temporal order.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar+area+point", "axial_code": [], "componenet_code": ["scatter", "bar", "area"]}]}, {"author": "gsh", "index_original": 16, "paper_title": "VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour Analytics", "pub_year": 2020, "domain": "user behaviour", "requirement": {"requirement_text": "T6 Compare a session with a user's typical behaviour. When a session is flagged as anomalous, a key task is to compare what happens in the session to the overall behaviour of that user and also to the group that the user belongs to. Examples: check if the types of actions conducted in a session fall under the usual tasks of the user; check if the properties of the session, such as duration and action rate, are usual for that user.", "requirement_code": {"compare_entities": 1, "explain_differences": 1}}, "data": {"data_text": "Speci\ufb01c Context and Data. In the context of this paper, we work\nwith a UBA model (designed, built and being used by one of the\nco-author\u2019s organisation) that operates on the logs that comprise of se-\nquences of actions carried out by the users of an administrative interface\nof a login and security server. The log data is split into sessions, each\ncontaining an ordered list of timestamped actions performed by the user\nconducting that session. In each session, particular tasks are conducted\nand actions are performed by the user. Actions in this context are se-\nmantically labelled and functionally relevant activities, e.g., managing\nusers and/or organisations with actions such as \u201cCreateLoginArea\u201d,\n\u201cSearchUsr\u201d, \u201cDisplayOrgaDetails\u201d or performing some intermediate\nactions such as \u201cCloseTab\u201d and \u201cCancel\u201d.\nEach session here is then a data record with the attributes: clock\ntime to indicate the start and end, User ID to indicate the performer,\nother meta information such as browser and operating system, and the\nsequence of actions (labels) and when each action took place within\nthe session. In addition to these, we also get the anomaly score that\nis provided by the underlying UBA with 1 being an unusual and 0\na normal session. The data set that we work with here comprises of\n19,351 sessions performed by 1,670 users within a 15 day time window\nusing 305 unique action types. Although we use this speci\ufb01c data set in\nthis paper, the form of the data is representative of log based solutions\nused for behaviour modelling in many other domains [13, 33].", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "4.1.1 Individual User Pro\ufb01les\nThe user pro\ufb01les combine three levels of features based on the different\ncases of feature availability: inherent features that are explicitly\navailable, derived features that can be directly extracted from the data\nthrough the use of standard statistical measures and latent features\nthat are not directly available for extraction, but would require more\nsophisticated methods to reveal underlying, latent characteristics of\nuser behaviour. Here, we consider the data in our application context\nand list the various features we put into use:\nInherent features. Features that describe the low-level character-\nistics of how users utilise the application. These features are often\nexplicit and provided as meta-data within the session records.\nP Browser used in the session\nP IP Address through which the session is conducted\nP Operating System running on the user\u2019s computer\nDerived Features. Features that are derived through straightfor-\nward statistical computations considering the distribution of actions in\nsessions and the distribution of sessions over users, together with any\nother explicitly derived metric such as model derived scores.\nP Session duration / length as a measure of time taken in the session\nand the count of actions performed\nP Total Sessions as a statistic on how active the user is\nP Total Unique Actions as a measure on how diverse is a user in the\nactions they perform\nLatent Features. Features that provide in-depth understanding\ninto the behaviour of users through the application of a sophisticated\ncomputation, such as modelling and clustering.\nP Anomaly Score for individual sessions as provided by the UBA\nsystem to indicate the normality of a session", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extract;statistical;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": ".3 Clustering Users based on their Tasks\nTo provide a semantic characterisation of users (addressing G1), we\nclassify them based on the tasks they perform. From the output of\ntopic modelling, a session can be modelled as a vector of tasks: si =\n(pi1, p i2, ..., p ik), where p i j is the probability of task j occuring in\nsession s i, with k as the total count of tasks. A user u can then be\nmodelled as a mean of its session vectors: u = 1\nm \u2211m\ni s i, where m is the\nnumber of sessions performed by u.\nHowever, there is a logical issue with adding session vectors. pi j\nindicates how probable task j occurs in session si, but it could be\nmisleading to compare the probability of the same task between two\nsessions. For instance, p11 = 0.5 means 50% of what happens in\nsession 1 is about task 1, and p21 = 0.3 means 30% of what happens in\nsession 2 is about task 1. But it cannot be inferred that session 1 has\nmore activities of task 1 than session 2 due to the difference in their\nnumber of actions. Therefore, we classify the probability based on a\nparticular threshold \u03b8 to indicate whether a task occurs in a session\nor not. Formally, s i = (bi1, b i2, ..., b ik), where bi j = 1 if p i j \u2265 \u03b8 and\nbi j = 0 otherwise. This binary transformation also allows a more\ninterpretable representation of the user vector: u = (t1,t2, ...,t k), where\nti is the proportion of the user\u2019s sessions where task i occurs. Choosing\nthe threshold \u03b8 is a challenging trade-off: small \u03b8 leads to too many\ntasks in one session, whereas large \u03b8 leads to too many sessions not\nassociated with any tasks. We choose \u03b8 = 0.3 for our dataset for two\nreasons: (1) a large number of sessions (94%) will have at least one task,\nand (2) each session will have up to 3 tasks, which is about consistent\nwith reality according to the domain experts.\nWe then apply k-means clustering [28] to cluster users based on\ntheir vector representations. k-means clustering is expected to work\nwell due to the small number of vector dimensions, which is 13 in our\ncase (13 tasks). To decide the optimal k number of topics, we use the\nelbow method [21] to visually select k by assessing the change of a\ncost function with different values of k. Our cost function is the sum of\nsquared distances of all data points to their closest cluster centre. As a\nresult, 11 clusters of users are generated.", "solution_category": "data_manipulation", "solution_axial": "Clustering", "solution_compoent": "model;", "axial_code": ["Clustering"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "Group profiles refer to the higher level aggregations of characteristics of users. In order to construct these profiles, users of similar traits are identified through a clustering method explained in Sect. 4.3, and the features of individual users (those described above) are aggregated for each group. Overall, a group profile comprises three key elements: aggregated feature statistics depicting the joint characteristics of the group in terms of the inherent and derived features of individuals, task profiles depicting the most frequent tasks conducted by the group, and representative users chosen as members that are of particular interest for further analysis (using criteria as discussed in Sect. 4.4). Combined together, group and user profiles help address the analysis goals G1 and G2. Moreover, a user profile also characterises how typical sessions look like, allowing comparison at session level (addressing G3).", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "data;higer_level_aggregations_of_characteristics_of_users;cluster", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "4.5.2 Analysis of Sessions in the Context\nIn the investigation of an anomalous session, it is crucial to analyse\nit within the context of the user performing the session so that any\ndeviation from the user\u2019s typical behaviour will be effectively spotted\n(addressing T6). We support this by enabling comparison between the\nsessions of interest and their corresponding user profiles that are built\nfrom all sessions performed by the same users. Sessions of interest\nare superimposed on top of the visual profiles as small orange dots.\nA random noise is added to the vertical position of the dots to avoid\noverplotting. Fig. 4 shows that the distribution of the orange dots is\nroughly the same as the shaded area of the profiles, which indicates that\nthe sessions of interest are similar to what the users typically do. The\nslightly deviated session is the session with highest score from Black\nGoliath, which is manually annotated with a blue circle in Fig. 4.\nWith such a compact representation, it is essential to equip the\nvisualisation with interaction to support further investigation. Mouse\nhovering a session (indicated as red dot) highlights the same session\non other features, enabling to observe different features concurrently.\nFor example, in Fig. 4, hovering the session with the highest score\nfrom Black Goliath reveals that there are no deviations in other features:\nshort length and duration, typical starting time ranges and using the\nsame operating system and browser. It is also possible to brush a\nrange of sessions, possibly from multiple users, to explore and compare\ntheir features (addressing T7). For instance, in Fig. 4, sessions that\nstarted late from Asylum and Bloodshed are selected for exploration.\nOne important next step is to investigate what actually happened in\nthose sessions besides the meta features. To support that, the selected\nsessions are also displayed in the Timeline view (described in Sect. 5.2)\nallowing the examination of the performed actions in temporal order.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar+area+point", "axial_code": [], "componenet_code": ["scatter", "bar", "area"]}, {"solution_text": "4.5.2 Analysis of Sessions in the Context\nIn the investigation of an anomalous session, it is crucial to analyse\nit within the context of the user performing the session so that any\ndeviation from the user\u2019s typical behaviour will be effectively spotted\n(addressing T6). We support this by enabling comparison between the\nsessions of interest and their corresponding user profiles that are built\nfrom all sessions performed by the same users. Sessions of interest\nare superimposed on top of the visual profiles as small orange dots.\nA random noise is added to the vertical position of the dots to avoid\noverplotting. Fig. 4 shows that the distribution of the orange dots is\nroughly the same as the shaded area of the profiles, which indicates that\nthe sessions of interest are similar to what the users typically do. The\nslightly deviated session is the session with highest score from Black\nGoliath, which is manually annotated with a blue circle in Fig. 4.\nWith such a compact representation, it is essential to equip the\nvisualisation with interaction to support further investigation. Mouse\nhovering a session (indicated as red dot) highlights the same session\non other features, enabling to observe different features concurrently.\nFor example, in Fig. 4, hovering the session with the highest score\nfrom Black Goliath reveals that there are no deviations in other features:\nshort length and duration, typical starting time ranges and using the\nsame operating system and browser. It is also possible to brush a\nrange of sessions, possibly from multiple users, to explore and compare\ntheir features (addressing T7). For instance, in Fig. 4, sessions that\nstarted late from Asylum and Bloodshed are selected for exploration.\nOne important next step is to investigate what actually happened in\nthose sessions besides the meta features. To support that, the selected\nsessions are also displayed in the Timeline view (described in Sect. 5.2)\nallowing the examination of the performed actions in temporal order.", "solution_category": "interaction", "solution_axial": "selecting", "solution_compoent": "", "axial_code": ["selecting"], "componenet_code": ["selecting"]}]}, {"author": "gsh", "index_original": 17, "paper_title": "VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour Analytics", "pub_year": 2020, "domain": "user behaviour", "requirement": {"requirement_text": "T7 Compare multiple sessions. Often, multiple sessions from a single user or from several users need to be compared to evaluate the sessions or to be able to deeply understand the activities conducted by the users. Examples: compare the scores of multiple highly- scoring sessions, i.e., suspicious sessions, and check for action types that might help explain the consistent high scores; compare sessions from multiple users (e.g., of the same group) and evaluate a single session in comparison to sessions from other users.", "requirement_code": {"compare_entities": 1, "explain_differences": 1}}, "data": {"data_text": "Speci\ufb01c Context and Data. In the context of this paper, we work\nwith a UBA model (designed, built and being used by one of the\nco-author\u2019s organisation) that operates on the logs that comprise of se-\nquences of actions carried out by the users of an administrative interface\nof a login and security server. The log data is split into sessions, each\ncontaining an ordered list of timestamped actions performed by the user\nconducting that session. In each session, particular tasks are conducted\nand actions are performed by the user. Actions in this context are se-\nmantically labelled and functionally relevant activities, e.g., managing\nusers and/or organisations with actions such as \u201cCreateLoginArea\u201d,\n\u201cSearchUsr\u201d, \u201cDisplayOrgaDetails\u201d or performing some intermediate\nactions such as \u201cCloseTab\u201d and \u201cCancel\u201d.\nEach session here is then a data record with the attributes: clock\ntime to indicate the start and end, User ID to indicate the performer,\nother meta information such as browser and operating system, and the\nsequence of actions (labels) and when each action took place within\nthe session. In addition to these, we also get the anomaly score that\nis provided by the underlying UBA with 1 being an unusual and 0\na normal session. The data set that we work with here comprises of\n19,351 sessions performed by 1,670 users within a 15 day time window\nusing 305 unique action types. Although we use this speci\ufb01c data set in\nthis paper, the form of the data is representative of log based solutions\nused for behaviour modelling in many other domains [13, 33].", "data_code": {"tables": 1, "categorical": 1, "temporal": 1, "sequential": 1, "clusters_and_sets_and_lists": 1}}, "solution": [{"solution_text": "5.2 Session Timeline\nWe reuse the timeline in our previous work [31] (with simplified func-\ntionality) to support the exploration of actions. This Session timeline\nview (Fig. 1 \u2013 bottom) allows exploration of the sessions in detail to\ninvestigate the actions that took place and to gain a closer understanding\nof how user tasks are accomplished. Actions in a session are shown\nalong a time axis according to when they happen. Each action is shown\nas a rectangular glyph, coloured by the action\u2019s group (the generation\nof the colourmap is detailed in Sect. 5.4). Actions can be positioned\nproportionally by their timestamps or sequentially by their temporal\norder to avoid overlapping. By having a compact representation of\na session, it is possible to stack multiple session timelines to enable\ncomparison (addressing T7).", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "stakced_bar+matrix", "axial_code": [], "componenet_code": ["bar", "matrix"]}]}, {"author": "gsh", "index_original": 18, "paper_title": "Visual Analytics for Electromagnetic Situation Awareness in Radio Monitoring and Management", "pub_year": 2020, "domain": "Radio monitoring and management", "requirement": {"requirement_text": "R1: Cluster SigData accurately and efficiently. Signal clustering is the first step of using SigData in RMM. An appropriate method is required to quickly obtain reasonable clustering results from SigData.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "3.1 Data Abstraction\nSpeData formatted in frequency frames are generally used in RMM. As\nshown in Figure 1(a\u2212c), a frequency frame has a group of information\nelements, including sensing time, frequency band, number of frequency\npoints, and frequency spectrum array. For example, a certain frame has a\nfrequency band ranging from 900 MHz to 950 MHz with 1,024 frequency\nsampling points equally distributed in the band. Its frequency spectrum\narray records the signal amplitudes of all the frequency points at a cer-\ntain moment. Relatively high amplitudes indicate that the corresponding\nfrequency points may be occupied by radio signals. As a result, SpeData\neffectively re\ufb02ect the distribution of occupied frequencies in a monitoring\nfrequency band but cannot directly indicate the number of actual radio\nsignals existing in the band.\nSigData are extracted from SpeData, as shown in Figure 1(d). SigData\nare fully structured in the form of multidimensional data records with\ntime information. A SigData record, also known as a signal feature vector\n(SFV), presents the basic signal characteristics of a detected radio signal,\nmainly including detecting time, center frequency, bandwidth, strength,\nand SNR. For instance, a certain SFV indicates that a radio signal with a\ncenter frequency of 950 MHz, a bandwidth of 2,101 MHz, a strength of\n\u221269 dBm, and a SNR of 50 dB was detected at 9:30:05 am. Accordingly,\nSigData characterize the radio signals that may actually exist.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1}}, "solution": [{"solution_text": "5.1 Signal Clustering\nSignal clustering identifies actual radio signals in SigData in accordance\nwith the basic rule that the SFVs of the same signal should have a similar\ncenter frequency and bandwidth. Each cluster represents an actual radio\nsignal. However, traditional clustering methods (e.g., k-means and DB-\nSCAN) experience difficulties in achieving accurate and efficient signal\nclustering as mentioned in C1. We propose a new method for clustering\nsignals in SigData. This section elaborates the five major steps of the\nproposed method.\n5.1.1 Grid Division\nRadio communication often presents an uneven signal distribution. A\nnumber of radio signals may be crowded in a narrow frequency band\nor have a similar bandwidth. In this work, we use a grid division to\ndistinguish the areas with dense and sparse SFV distributions in the data\nspace composed of frequency and bandwidth. In this manner, different\nclustering strategies can be used in the two types of areas to reduce the\nimpact of the uneven signal distribution on the accuracy and efficiency\nof signal clustering. Specifically, we initially divide each dimension of\nthe frequency-bandwidth data space into m equal-sized grids. Then, we\ncalculate the number of SFVs in each grid. Finally, we distinguish dense\nand sparse grids on the basis of a density threshold MinPts. Referring\nto [71], we set the grid number m with \u221aN. Referring to [29], we measure\nthe density threshold MinPts with the average density of non-empty grids.\nThe two settings are expressed aswhere N represents the total number of SFVs, z represents the number of\nnon-empty grids, and densityi denotes the density of a non-empty grid.\n5.1.2 Dense Area Processing\nEmpirically, most SFVs in dense grids are constant signals because of\ncontinuous and long-term communication. As identity characteristics,\nthe center frequency and bandwidth of a constant signal are supposed\nto be constant during communication, although small \ufb02uctuations in the\ntwo characteristics may occasionally occur. Therefore, SFVs with a com-\npact Gaussian convex distribution and a clear boundary in the frequency-\nbandwidth data space normally represent an actual constant signal. This\nphenomenon is in line with the clustering hypothesis proposed by the\nauthors in [56]. They stated that in the fast density-based clustering method\n(FDP), the center data point of a cluster usually has a higher density than\nits surrounding data points and relatively large distances from other cluster\ncenters.\nIn accordance with the FDP method, a basic clustering strategy is\nformed for dense grids. We initially treat the SFVs in all dense grids as a\nwhole area. Then, we distinguish the SFVs that can be regarded as cluster\ncenters. Lastly, we classify the other SFVs into the nearest cluster center\nto complete the clustering. Notably, this process has two key points. The\nfirst point is that the FDP method measures the density of an SFV by\ncalculating the number of SFVs within its cutoff distance dc. Such density\nmeasurement is sensitive to the setting of dc. The second point is that the\nFDP method does not provide a quantitative method for determining the\nnumber of cluster centers.\nFor density measurement, we propose to use the K-nearest neighbor ker-\nnel density [64]. We first find the K-nearest SFVs to the current examined\nSFV i. Then, we calculate their density using a kernel density function.\nThe local density \u03c1 (i) is defined as\nwhere k is usually the value \u221aN, K (i) is the K-nearest neighbors of SFV\ni, and d (i, j) represents the Euclidean distance between SFVs i and j.\nThis density can be accurately estimated because the fixed dc-distance is\ndiscarded, and it can be efficiently calculated because only a few neighbor\nSFVs are involved in the computation.\nTo determine the number of cluster centers automatically, we apply the\nrelative density proposed in [34]. Our basic idea is that if an SFV is a\ncluster center, then no SFV whose density and minimum distance are larger\nthan those of the cluster center SFV exists in its K-nearest neighbors. That\nis, the SFV\u2019s local relative density and local relative minimum distance in\nits K-nearest neighbors should be equal to 1. We define the local relative\ndensity \u03c1\u2217 (i) and relative minimum distance measure \u03b4 \u2217 (i) of SFV i as\n\u03c1\u2217(i) = \u03c1 (i)\nmax j\u03b5K(i)\u222a{i} {\u03c1 ( j)} , \u03b4 \u2217(i) = \u03b4 (i)\nmax j\u03b5K(i)\u222a{i} {\u03b4 ( j)} ,\nwhere \u03b4 (i) = min j:\u03c1( j)>\u03c1(i)d (i, j) .\n(3)\nWe regard SFV i as a cluster center when \u03c1\u2217 (i) = 1 and \u03b4 \u2217 (i) = 1 . After\ntraversing all SFVs, we obtain many SFVs whose number is exactly the\nnumber of cluster centers.\n5.1.3 Sparse Area Processing\nSparse grids are commonly ignored in many grid-based clustering methods,\nbut this is not the case in signal clustering. On one hand, several SFVs in\nsparse grids may be boundary SFVs that belong to the clusters of adjacent\ndense grids. On the other hand, periodic short-term signals may exist\nin sparse grids. These signals appear periodically and transiently. They\nusually contain a certain number of SFVs that are less than a constant\nsignal but more than an instantaneous signal. Therefore, it is possible to\ndistinguish these signals in the signal clustering.\nIn terms of possible cluster boundary SFVs, for each SFV in sparse grids,\nwe calculate the distances from the SFV to all cluster centers in its adjacent\ndense grids. The SFV is assigned to the corresponding cluster when the\ndistance is less than the grid width. Regarding periodic short-term signals,\nfor each sparse grid, we initially use the clustering method of the dense\narea to process the SFVs. Then, we observe the SFVs\u2019 time distribution\nin each obtained cluster. A cluster can be regarded as a periodic short-\nterm signal when the SFVs of this cluster cover most of the monitored\nperiod. Otherwise, all SFVs of this cluster are considered noises. We use\nan empirical value of 80% for this time span ratio.\n5.1.4 Interference Processing\nSeveral SFVs in a cluster may have low strength or SNR values. This\ncondition commonly indicates that a signal, especially a constant one, is\ndisturbed by environmental noises or unknown signals, thereby leading to\npoor communication quality or even temporary interruption. Therefore,\nwe conduct an interference process to re-examine the strength and SNR\nvalues of the SFVs in the obtained clusters. If the strength or SNR of an\nSFV is below the strength threshold or the SNR threshold set by users,\nthen the SFV is treated as a noise. This process can facilitate the detection\nof short-time interruptions or instability of constant signals.\n5.1.5 Authorized Signal Inspection\nWhen users have an authorized signal library that records the major regis-\ntered radio signals allowed for lawful communication, this step must be\nperformed so that the obtained signals match the library signals in terms of\ntime, center frequency, and bandwidth. If an obtained signal already exists\nin the library, then it is an authorized signal. Otherwise, it is unauthorized\nand should be heeded by users for further analysis", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "signal_cluster;method", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}]}, {"author": "gsh", "index_original": 19, "paper_title": "Visual Analytics for Electromagnetic Situation Awareness in Radio Monitoring and Management", "pub_year": 2020, "domain": "Radio monitoring and management", "requirement": {"requirement_text": "R2: Evaluate the electromagnetic situations quantitatively and qualitatively. Numerous factors should be considered when creating measurable and comprehensive situation descriptions. The descriptions can guide the subsequent data analysis.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "3.1 Data Abstraction\nSpeData formatted in frequency frames are generally used in RMM. As\nshown in Figure 1(a\u2212c), a frequency frame has a group of information\nelements, including sensing time, frequency band, number of frequency\npoints, and frequency spectrum array. For example, a certain frame has a\nfrequency band ranging from 900 MHz to 950 MHz with 1,024 frequency\nsampling points equally distributed in the band. Its frequency spectrum\narray records the signal amplitudes of all the frequency points at a cer-\ntain moment. Relatively high amplitudes indicate that the corresponding\nfrequency points may be occupied by radio signals. As a result, SpeData\neffectively re\ufb02ect the distribution of occupied frequencies in a monitoring\nfrequency band but cannot directly indicate the number of actual radio\nsignals existing in the band.\nSigData are extracted from SpeData, as shown in Figure 1(d). SigData\nare fully structured in the form of multidimensional data records with\ntime information. A SigData record, also known as a signal feature vector\n(SFV), presents the basic signal characteristics of a detected radio signal,\nmainly including detecting time, center frequency, bandwidth, strength,\nand SNR. For instance, a certain SFV indicates that a radio signal with a\ncenter frequency of 950 MHz, a bandwidth of 2,101 MHz, a strength of\n\u221269 dBm, and a SNR of 50 dB was detected at 9:30:05 am. Accordingly,\nSigData characterize the radio signals that may actually exist.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1}}, "solution": [{"solution_text": "5.2 Situation Assessment\nElectromagnetic situation awareness is immensely complex and challeng-\ning (C3). In many sophisticated applications introduced in subsection 2.2,\nobjective situation assessment and human-in-loop interactive analysis are\nwell combined to achieve situation awareness. On this basis, we propose\na situation assessment model based on SigData and signal clustering\nresults to provide quantitative and qualitative electromagnetic situation\ndescriptions. This model is expected to reduce users\u2019 cognitive load and\ndomain-experience requirements in situation assessment and facilitate the\nsubsequent situation analysis. The assessment index system and situation-\nlevel generation of the model are described in detail below.\n5.2.1 Assessment Index System\nWe design a multi-index system by considering various influencing factors\nformulated from domain knowledge and expert experience. Each index\nrepresents a situational factor and all indexes are combined to describe an\noverall electromagnetic situation. A time interval and a given frequency\nband are essential for real-time situation monitoring. In this work, we use\na 10s time interval and a frequency unit of 1 MHz by default. In other\nwords, the entire band is equally divided into 1 MHz sub-bands. For each\nsub-band, we design 12 situation indexes that can be classified into indexes\nfor all signals (AL), authorized signals (AU), unauthorized signals (UN),\nand noise (NO) from the perspective of signal types. They can also be\ncategorized into indexes for frequency domain (FRE), time domain (TIM),\nand energy domain (STR) in terms of signal characteristics. These indexes\nform a hierarchical tree structure that conjointly describes the situation of a\n1 MHz sub-band in 10s. Moreover, the index system is an experience-based\ntextual description. The quantitative situation values of these indexes need\nto be calculated by introducing diverse computational analysis methods.\nThe specific calculation methods of the 12 indexes are provided in the\nsupplementary material of this paper.\n5.2.2 Situation-level Generation\nSituation-level generation transforms calculated quantitative index values\ninto readable qualitative situation levels. Referring to [75], we use a\nthree-step method to generate situation levels. (1) We define five levels\nof the electromagnetic situation of a sub-band in a period on the basis\nof the partition method of American National Security Level. The five\nlevels are expressed as dimensionless points from 1 to 5, [1 = low, 2 =\nguarded, 3 = elevated, 4 = high, 5 = severe]. (2) For an individual index, we\ninitially use a trapezoidal fuzzy membership function [10, 75] to transform\na situation value into a fuzzy membership vector belonging to the five\nsituation levels by combining the thresholds provided by users. Then, we\nselect the maximum degree of the fuzzy membership vector as the situation\nlevel of the index. (3) For the assessment index system, we construct a\nfuzzy relation matrix to generate a weighted fuzzy membership vector in\naccordance with the weights of the 12 indexes provided by users. Then,\nwe select the maximum membership degree from the weighted vector as\nthe overall situation level of the index system.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "model;recude_cognitive_load;electromagnetic_situation_awareness", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "gsh", "index_original": 20, "paper_title": "Visual Analytics for Electromagnetic Situation Awareness in Radio Monitoring and Management", "pub_year": 2020, "domain": "Radio monitoring and management", "requirement": {"requirement_text": "R3: Support intuitive situation perception. The calculation results of the situation assessment model should be presented in an intuitive manner. Accordingly, users can easily perceive the current situation, its recent change trend, and high-risk frequency sub-bands.", "requirement_code": {"discover_observation": 1, "describe_observation_item": 1}}, "data": {"data_text": "3.1 Data Abstraction\nSpeData formatted in frequency frames are generally used in RMM. As\nshown in Figure 1(a\u2212c), a frequency frame has a group of information\nelements, including sensing time, frequency band, number of frequency\npoints, and frequency spectrum array. For example, a certain frame has a\nfrequency band ranging from 900 MHz to 950 MHz with 1,024 frequency\nsampling points equally distributed in the band. Its frequency spectrum\narray records the signal amplitudes of all the frequency points at a cer-\ntain moment. Relatively high amplitudes indicate that the corresponding\nfrequency points may be occupied by radio signals. As a result, SpeData\neffectively re\ufb02ect the distribution of occupied frequencies in a monitoring\nfrequency band but cannot directly indicate the number of actual radio\nsignals existing in the band.\nSigData are extracted from SpeData, as shown in Figure 1(d). SigData\nare fully structured in the form of multidimensional data records with\ntime information. A SigData record, also known as a signal feature vector\n(SFV), presents the basic signal characteristics of a detected radio signal,\nmainly including detecting time, center frequency, bandwidth, strength,\nand SNR. For instance, a certain SFV indicates that a radio signal with a\ncenter frequency of 950 MHz, a bandwidth of 2,101 MHz, a strength of\n\u221269 dBm, and a SNR of 50 dB was detected at 9:30:05 am. Accordingly,\nSigData characterize the radio signals that may actually exist.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1}}, "solution": [{"solution_text": "In this work, we use a grid division to\ndistinguish the areas with dense and sparse SFV distributions in the data\nspace composed of frequency and bandwidth. In this manner, different\nclustering strategies can be used in the two types of areas to reduce the\nimpact of the uneven signal distribution on the accuracy and efficiency\nof signal clustering.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "On this basis, we propose\na situation assessment model based on SigData and signal clustering\nresults to provide quantitative and qualitative electromagnetic situation\ndescriptions.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Situation monitoring view adopts the design idea of level-progressive\nheatmap to present an overview of the current electromagnetic situation\nand its recent change trend (Figure 4(b)). The design is manifested from\ntwo aspects, namely, gradual progresses in time and frequency.\nThe current situation is what users are most concerned about. Users\nwish to observe recent situation changes simultaneously (e.g., usually in\nthe last 2\u22123min). We divide the y-axis timeline of the view into two levels,\nnamely, major and auxiliary. The major level occupies a large vertical\nspace at the top of the y-axis to show the current situation. The remaining\ny-axis space is evenly divided into 10 small segments to display the recent\nsituations as the auxiliary level. The default setting of the time interval of a\nsegment is 10s. Hence, the current situation can be easily perceived in the\nmajor level. Continuous small segments in the auxiliary level use a limited\nvertical screen space to reveal the situation\u2019s change trend.\nUsers wish to highlight high-risk frequency sub-bands. We equally\ndivide this view\u2019s x-axis that represents the monitoring band into cells from\nleft to right. A cell represents a frequency sub-band with a default interval\nof 1 MHz. The color of a cell represents the situation level of the relevant\nsub-band generated by situation assessment. Colors are encoded with five\nlevels following the rule that a high risk is presented in a vibrant color.\nWe also use a linear interpolation to handle the color transition between\ncells. This interpolation represents the possible uncertainty of situation\nlevel in the boundary frequencies among adjacent sub-bands. Overall, this\nview provides an intuitive heatmap-based visualization with rich levels of\ndetails on time and frequency.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "heat_map", "axial_code": [], "componenet_code": ["heatmap"]}]}, {"author": "gsh", "index_original": 21, "paper_title": "Visual Analytics for Electromagnetic Situation Awareness in Radio Monitoring and Management", "pub_year": 2020, "domain": "Radio monitoring and management", "requirement": {"requirement_text": "R3: Support intuitive situation perception. The calculation results of the situation assessment model should be presented in an intuitive manner. Accordingly, users can easily perceive the current situation, its recent change trend, and high-risk frequency sub-bands.", "requirement_code": {"discover_observation": 1, "describe_observation_item": 1}}, "data": {"data_text": "3.1 Data Abstraction\nSpeData formatted in frequency frames are generally used in RMM. As\nshown in Figure 1(a\u2212c), a frequency frame has a group of information\nelements, including sensing time, frequency band, number of frequency\npoints, and frequency spectrum array. For example, a certain frame has a\nfrequency band ranging from 900 MHz to 950 MHz with 1,024 frequency\nsampling points equally distributed in the band. Its frequency spectrum\narray records the signal amplitudes of all the frequency points at a cer-\ntain moment. Relatively high amplitudes indicate that the corresponding\nfrequency points may be occupied by radio signals. As a result, SpeData\neffectively re\ufb02ect the distribution of occupied frequencies in a monitoring\nfrequency band but cannot directly indicate the number of actual radio\nsignals existing in the band.\nSigData are extracted from SpeData, as shown in Figure 1(d). SigData\nare fully structured in the form of multidimensional data records with\ntime information. A SigData record, also known as a signal feature vector\n(SFV), presents the basic signal characteristics of a detected radio signal,\nmainly including detecting time, center frequency, bandwidth, strength,\nand SNR. For instance, a certain SFV indicates that a radio signal with a\ncenter frequency of 950 MHz, a bandwidth of 2,101 MHz, a strength of\n\u221269 dBm, and a SNR of 50 dB was detected at 9:30:05 am. Accordingly,\nSigData characterize the radio signals that may actually exist.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1}}, "solution": [{"solution_text": "In this work, we use a grid division to\ndistinguish the areas with dense and sparse SFV distributions in the data\nspace composed of frequency and bandwidth. In this manner, different\nclustering strategies can be used in the two types of areas to reduce the\nimpact of the uneven signal distribution on the accuracy and efficiency\nof signal clustering.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "On this basis, we propose\na situation assessment model based on SigData and signal clustering\nresults to provide quantitative and qualitative electromagnetic situation\ndescriptions.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Signal monitoring view depicts the distribution of all signals in time\nand frequency as well as the time-varying patterns of the signals\u2019 char-\nacteristics. As stated in C2, this view involves two design challenges,\nnamely, (1) how to produce a continuous distribution visualization by\nusing characterized and discretized SigData and (2) how to present the time-\nvarying patterns of a signal\u2019s four basic characteristics simultaneously.\nInspired by a river metaphor [5, 26, 40, 43], we create a new signal time-\nfrequency (STF) diagram to address the two design challenges (Figure 4(c)).\nThe diagram\u2019s x-axis shows the monitored frequencies from left to right,\nand the y-axis represents the time period consistent with the situation view\nfrom top to bottom. This layout is commonly used in the spectrum time-\nfrequency diagram, which our target users are accustomed to. Inside the\ndiagram are ribbon-like rivers, each of which represents a radio signal\nobtained through signal clustering. The number of these ribbon-like rivers\nindicates how many signals are in communication. A river is composed\nof a series of quadrilaterals sequentially arranged along with time. A\nquadrilateral encodes the four means of four basic characteristics of the\nSFVs of a signal in a time span with a default interval of 1s.\nAs shown in Figure 4(d), for a quadrilateral in a river, the x-axis position\nof its center line represents the mean of center frequency. The width\nrepresents the mean of bandwidth. The color of its left sub-area divided\nby the center line represents the mean of signal strength, and the color of\nits right sub-area represents the mean of SNR. For the colors of the two\nsub-areas, the more saturated the color is, the larger the corresponding\nvalue is. Moreover, a time interruption may occur in the corresponding\nriver if a signal does not have sufficient SFVs (five by default) within a time\nspan. The center line of the corresponding river of a signal is highlighted\nin red color when it is a newly found unauthorized signal.\nIn short, the river visualization design of the STF diagram can roughly\ndepict the continuous distribution of all radio signals in time and frequency\nfrom a global view. It can present the time-varying patterns of a signal\u2019s\nfour basic characteristics from a local view. Moreover, it fits users\u2019 mental\nmap well because the river metaphor provides a similar visual perception\nexperience as the spectrum time-frequency diagram commonly used in\nSpeData analysis.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "area", "axial_code": [], "componenet_code": ["area"]}]}, {"author": "gsh", "index_original": 22, "paper_title": "Visual Analytics for Electromagnetic Situation Awareness in Radio Monitoring and Management", "pub_year": 2020, "domain": "Radio monitoring and management", "requirement": {"requirement_text": "R4: Visualize the results of signal clustering. Users require a new diagram to visualize the results of signal clustering so that they can identify the distribution and time-varying patterns of the signals currently existing in the monitoring band.", "requirement_code": {"discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "3.1 Data Abstraction\nSpeData formatted in frequency frames are generally used in RMM. As\nshown in Figure 1(a\u2212c), a frequency frame has a group of information\nelements, including sensing time, frequency band, number of frequency\npoints, and frequency spectrum array. For example, a certain frame has a\nfrequency band ranging from 900 MHz to 950 MHz with 1,024 frequency\nsampling points equally distributed in the band. Its frequency spectrum\narray records the signal amplitudes of all the frequency points at a cer-\ntain moment. Relatively high amplitudes indicate that the corresponding\nfrequency points may be occupied by radio signals. As a result, SpeData\neffectively re\ufb02ect the distribution of occupied frequencies in a monitoring\nfrequency band but cannot directly indicate the number of actual radio\nsignals existing in the band.\nSigData are extracted from SpeData, as shown in Figure 1(d). SigData\nare fully structured in the form of multidimensional data records with\ntime information. A SigData record, also known as a signal feature vector\n(SFV), presents the basic signal characteristics of a detected radio signal,\nmainly including detecting time, center frequency, bandwidth, strength,\nand SNR. For instance, a certain SFV indicates that a radio signal with a\ncenter frequency of 950 MHz, a bandwidth of 2,101 MHz, a strength of\n\u221269 dBm, and a SNR of 50 dB was detected at 9:30:05 am. Accordingly,\nSigData characterize the radio signals that may actually exist.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1}}, "solution": [{"solution_text": "In this work, we use a grid division to\ndistinguish the areas with dense and sparse SFV distributions in the data\nspace composed of frequency and bandwidth. In this manner, different\nclustering strategies can be used in the two types of areas to reduce the\nimpact of the uneven signal distribution on the accuracy and efficiency\nof signal clustering.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "On this basis, we propose\na situation assessment model based on SigData and signal clustering\nresults to provide quantitative and qualitative electromagnetic situation\ndescriptions.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Situation monitoring view adopts the design idea of level-progressive\nheatmap to present an overview of the current electromagnetic situation\nand its recent change trend (Figure 4(b)). The design is manifested from\ntwo aspects, namely, gradual progresses in time and frequency.\nThe current situation is what users are most concerned about. Users\nwish to observe recent situation changes simultaneously (e.g., usually in\nthe last 2\u22123min). We divide the y-axis timeline of the view into two levels,\nnamely, major and auxiliary. The major level occupies a large vertical\nspace at the top of the y-axis to show the current situation. The remaining\ny-axis space is evenly divided into 10 small segments to display the recent\nsituations as the auxiliary level. The default setting of the time interval of a\nsegment is 10s. Hence, the current situation can be easily perceived in the\nmajor level. Continuous small segments in the auxiliary level use a limited\nvertical screen space to reveal the situation\u2019s change trend.\nUsers wish to highlight high-risk frequency sub-bands. We equally\ndivide this view\u2019s x-axis that represents the monitoring band into cells from\nleft to right. A cell represents a frequency sub-band with a default interval\nof 1 MHz. The color of a cell represents the situation level of the relevant\nsub-band generated by situation assessment. Colors are encoded with five\nlevels following the rule that a high risk is presented in a vibrant color.\nWe also use a linear interpolation to handle the color transition between\ncells. This interpolation represents the possible uncertainty of situation\nlevel in the boundary frequencies among adjacent sub-bands. Overall, this\nview provides an intuitive heatmap-based visualization with rich levels of\ndetails on time and frequency.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "heat_map", "axial_code": [], "componenet_code": ["heatmap"]}]}, {"author": "gsh", "index_original": 23, "paper_title": "Visual Analytics for Electromagnetic Situation Awareness in Radio Monitoring and Management", "pub_year": 2020, "domain": "Radio monitoring and management", "requirement": {"requirement_text": "R5: Facilitate deep situation understanding. An interactive data analysis tool is required to involve users in the analysis of contextual information for situation understanding because their domain experience is essential in explaining the causes of anomalies.", "requirement_code": {"identify_main_cause_aggregate": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "3.1 Data Abstraction\nSpeData formatted in frequency frames are generally used in RMM. As\nshown in Figure 1(a\u2212c), a frequency frame has a group of information\nelements, including sensing time, frequency band, number of frequency\npoints, and frequency spectrum array. For example, a certain frame has a\nfrequency band ranging from 900 MHz to 950 MHz with 1,024 frequency\nsampling points equally distributed in the band. Its frequency spectrum\narray records the signal amplitudes of all the frequency points at a cer-\ntain moment. Relatively high amplitudes indicate that the corresponding\nfrequency points may be occupied by radio signals. As a result, SpeData\neffectively re\ufb02ect the distribution of occupied frequencies in a monitoring\nfrequency band but cannot directly indicate the number of actual radio\nsignals existing in the band.\nSigData are extracted from SpeData, as shown in Figure 1(d). SigData\nare fully structured in the form of multidimensional data records with\ntime information. A SigData record, also known as a signal feature vector\n(SFV), presents the basic signal characteristics of a detected radio signal,\nmainly including detecting time, center frequency, bandwidth, strength,\nand SNR. For instance, a certain SFV indicates that a radio signal with a\ncenter frequency of 950 MHz, a bandwidth of 2,101 MHz, a strength of\n\u221269 dBm, and a SNR of 50 dB was detected at 9:30:05 am. Accordingly,\nSigData characterize the radio signals that may actually exist.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1}}, "solution": [{"solution_text": "In this work, we use a grid division to\ndistinguish the areas with dense and sparse SFV distributions in the data\nspace composed of frequency and bandwidth. In this manner, different\nclustering strategies can be used in the two types of areas to reduce the\nimpact of the uneven signal distribution on the accuracy and efficiency\nof signal clustering.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "On this basis, we propose\na situation assessment model based on SigData and signal clustering\nresults to provide quantitative and qualitative electromagnetic situation\ndescriptions.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Situation monitoring view adopts the design idea of level-progressive\nheatmap to present an overview of the current electromagnetic situation\nand its recent change trend (Figure 4(b)). The design is manifested from\ntwo aspects, namely, gradual progresses in time and frequency.\nThe current situation is what users are most concerned about. Users\nwish to observe recent situation changes simultaneously (e.g., usually in\nthe last 2\u22123min). We divide the y-axis timeline of the view into two levels,\nnamely, major and auxiliary. The major level occupies a large vertical\nspace at the top of the y-axis to show the current situation. The remaining\ny-axis space is evenly divided into 10 small segments to display the recent\nsituations as the auxiliary level. The default setting of the time interval of a\nsegment is 10s. Hence, the current situation can be easily perceived in the\nmajor level. Continuous small segments in the auxiliary level use a limited\nvertical screen space to reveal the situation\u2019s change trend.\nUsers wish to highlight high-risk frequency sub-bands. We equally\ndivide this view\u2019s x-axis that represents the monitoring band into cells from\nleft to right. A cell represents a frequency sub-band with a default interval\nof 2 MHz. The color of a cell represents the situation level of the relevant\nsub-band generated by situation assessment. Colors are encoded with five\nlevels following the rule that a high risk is presented in a vibrant color.\nWe also use a linear interpolation to handle the color transition between\ncells. This interpolation represents the possible uncertainty of situation\nlevel in the boundary frequencies among adjacent sub-bands. Overall, this\nview provides an intuitive heatmap-based visualization with rich levels of\ndetails on time and frequency.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "heat_map", "axial_code": [], "componenet_code": ["heatmap"]}]}, {"author": "gsh", "index_original": 24, "paper_title": "Visual Analytics for Electromagnetic Situation Awareness in Radio Monitoring and Management", "pub_year": 2020, "domain": "Radio monitoring and management", "requirement": {"requirement_text": "R5: Facilitate deep situation understanding. An interactive data analysis tool is required to involve users in the analysis of contextual information for situation understanding because their domain experience is essential in explaining the causes of anomalies.", "requirement_code": {"identify_main_cause_aggregate": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "3.1 Data Abstraction\nSpeData formatted in frequency frames are generally used in RMM. As\nshown in Figure 1(a\u2212c), a frequency frame has a group of information\nelements, including sensing time, frequency band, number of frequency\npoints, and frequency spectrum array. For example, a certain frame has a\nfrequency band ranging from 900 MHz to 950 MHz with 1,024 frequency\nsampling points equally distributed in the band. Its frequency spectrum\narray records the signal amplitudes of all the frequency points at a cer-\ntain moment. Relatively high amplitudes indicate that the corresponding\nfrequency points may be occupied by radio signals. As a result, SpeData\neffectively re\ufb02ect the distribution of occupied frequencies in a monitoring\nfrequency band but cannot directly indicate the number of actual radio\nsignals existing in the band.\nSigData are extracted from SpeData, as shown in Figure 1(d). SigData\nare fully structured in the form of multidimensional data records with\ntime information. A SigData record, also known as a signal feature vector\n(SFV), presents the basic signal characteristics of a detected radio signal,\nmainly including detecting time, center frequency, bandwidth, strength,\nand SNR. For instance, a certain SFV indicates that a radio signal with a\ncenter frequency of 950 MHz, a bandwidth of 2,101 MHz, a strength of\n\u221269 dBm, and a SNR of 50 dB was detected at 9:30:05 am. Accordingly,\nSigData characterize the radio signals that may actually exist.", "data_code": {"tables": 1, "quantitative": 1, "temporal": 1}}, "solution": [{"solution_text": "An STF diagram is still used in this view.\nHowever, a new visualization mode (scatter mode) and additional interac-\ntions are provided. The scatter mode visualizes SigData via a scatterplot,\nin which a dot represents an SFV.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "scatter", "axial_code": [], "componenet_code": ["scatter"]}]}, {"author": "gsh", "index_original": 25, "paper_title": "Visual Analytics for Electromagnetic Situation Awareness in Radio Monitoring and Management", "pub_year": 2020, "domain": "Radio monitoring and management", "requirement": {"requirement_text": "R5: Facilitate deep situation understanding. An interactive data analysis tool is required to involve users in the analysis of contextual information for situation understanding because their domain experience is essential in explaining the causes of anomalies.", "requirement_code": {"identify_main_cause_aggregate": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "3.1 Data Abstraction\nSpeData formatted in frequency frames are generally used in RMM. As\nshown in Figure 1(a\u2212c), a frequency frame has a group of information\nelements, including sensing time, frequency band, number of frequency\npoints, and frequency spectrum array. For example, a certain frame has a\nfrequency band ranging from 900 MHz to 950 MHz with 1,024 frequency\nsampling points equally distributed in the band. Its frequency spectrum\narray records the signal amplitudes of all the frequency points at a cer-\ntain moment. Relatively high amplitudes indicate that the corresponding\nfrequency points may be occupied by radio signals. As a result, SpeData\neffectively re\ufb02ect the distribution of occupied frequencies in a monitoring\nfrequency band but cannot directly indicate the number of actual radio\nsignals existing in the band.\nSigData are extracted from SpeData, as shown in Figure 1(d). SigData\nare fully structured in the form of multidimensional data records with\ntime information. A SigData record, also known as a signal feature vector\n(SFV), presents the basic signal characteristics of a detected radio signal,\nmainly including detecting time, center frequency, bandwidth, strength,\nand SNR. For instance, a certain SFV indicates that a radio signal with a\ncenter frequency of 950 MHz, a bandwidth of 2,101 MHz, a strength of\n\u221269 dBm, and a SNR of 50 dB was detected at 9:30:05 am. Accordingly,\nSigData characterize the radio signals that may actually exist.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The time-frequency\nview shows all spectrum frames in SpeData. The y-axis displays the time\n(i.e., the frame number) from top to bottom. The x-axis uses small and\ndense color dots to show the signal amplitudes of all the frequencies in a\nframe from left to right. The brighter the color is, the larger the amplitude\nis.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "heatmap", "axial_code": [], "componenet_code": ["heatmap"]}]}, {"author": "gsh", "index_original": 26, "paper_title": "Visual Analytics for Electromagnetic Situation Awareness in Radio Monitoring and Management", "pub_year": 2020, "domain": "Radio monitoring and management", "requirement": {"requirement_text": "R5: Facilitate deep situation understanding. An interactive data analysis tool is required to involve users in the analysis of contextual information for situation understanding because their domain experience is essential in explaining the causes of anomalies.", "requirement_code": {"identify_main_cause_aggregate": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "3.1 Data Abstraction\nSpeData formatted in frequency frames are generally used in RMM. As\nshown in Figure 1(a\u2212c), a frequency frame has a group of information\nelements, including sensing time, frequency band, number of frequency\npoints, and frequency spectrum array. For example, a certain frame has a\nfrequency band ranging from 900 MHz to 950 MHz with 1,024 frequency\nsampling points equally distributed in the band. Its frequency spectrum\narray records the signal amplitudes of all the frequency points at a cer-\ntain moment. Relatively high amplitudes indicate that the corresponding\nfrequency points may be occupied by radio signals. As a result, SpeData\neffectively re\ufb02ect the distribution of occupied frequencies in a monitoring\nfrequency band but cannot directly indicate the number of actual radio\nsignals existing in the band.\nSigData are extracted from SpeData, as shown in Figure 1(d). SigData\nare fully structured in the form of multidimensional data records with\ntime information. A SigData record, also known as a signal feature vector\n(SFV), presents the basic signal characteristics of a detected radio signal,\nmainly including detecting time, center frequency, bandwidth, strength,\nand SNR. For instance, a certain SFV indicates that a radio signal with a\ncenter frequency of 950 MHz, a bandwidth of 2,101 MHz, a strength of\n\u221269 dBm, and a SNR of 50 dB was detected at 9:30:05 am. Accordingly,\nSigData characterize the radio signals that may actually exist.", "data_code": {"tables": 1, "quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": " The amplitude-frequency view, whose x-axis is the frequency and y-axis\nis the amplitude, uses a line chart to visualize the amplitude variation\nof a frame in frequency, and it also employs a scatterplot to present the\nSFVs in SigData corresponding to the frame. ", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line+scatter", "axial_code": [], "componenet_code": ["scatter", "line"]}]}, {"author": "gsh", "index_original": 27, "paper_title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios", "pub_year": 2020, "domain": "Finance", "requirement": {"requirement_text": "T.1 What is the effectiveness of risk factor model's output over a certain time period? The data we are visualizing comes from a model that is derived statistically. The validity of a risk factor model's output changes over time. This means a risk factor's return might be independent of other factors' at the beginning but strongly correlated after the market conditions evolve. A high correlation between factors returns indicates a weak model output. Updating an ineffective model could prevent investors from taking unnecessary risks and help them optimize the portfolio.", "requirement_code": {"explain_differences": 1, "discover_observation": 1, "evaluate_hypothesis": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "Our visualized data is comprised of the backtesting records for each\nportfolio, the factor exposures of each stock, the sector categories of\neach stock, and the factor returns of the corresponding factors.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Then, we calculated the accumulated factor returns and\nthe correlations among the factor returns.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The design of the upper right part is\nbuilt on a heat map. Inside\neach block, there is a line chart indicating the trends of the correlations\nin greater details.", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "matrix+heat_map+line", "axial_code": [], "componenet_code": ["heatmap", "line", "matrix"]}]}, {"author": "gsh", "index_original": 28, "paper_title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios", "pub_year": 2020, "domain": "Finance", "requirement": {"requirement_text": "T.1 What is the effectiveness of risk factor model's output over a certain time period? The data we are visualizing comes from a model that is derived statistically. The validity of a risk factor model's output changes over time. This means a risk factor's return might be independent of other factors' at the beginning but strongly correlated after the market conditions evolve. A high correlation between factors returns indicates a weak model output. Updating an ineffective model could prevent investors from taking unnecessary risks and help them optimize the portfolio.", "requirement_code": {"explain_differences": 1, "discover_observation": 1, "evaluate_hypothesis": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "Our visualized data is comprised of the backtesting records for each\nportfolio, the factor exposures of each stock, the sector categories of\neach stock, and the factor returns of the corresponding factors.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "After brushing, the portfolio cluster view and the factor\ncorrelation view will be updated accordingly. Users can then proceed\nto the following interaction for detailed analysis ", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "gsh", "index_original": 29, "paper_title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios", "pub_year": 2020, "domain": "Finance", "requirement": {"requirement_text": "T.2 What is the 'crowdedness' of each factor at any given time? When capital flows into a single factor, the expected return on the factor may drop significantly because of the price appreciation. Thus, it is crucial for the investors to speculate hypothesize any future returns on factors before they make an investment and avoid factors that are crowded, even though they might have earned a high return in the past.", "requirement_code": {"discover_observation": 1, "compare_entities": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "Our visualized data is comprised of the backtesting records for each\nportfolio, the factor exposures of each stock, the sector categories of\neach stock, and the factor returns of the corresponding factors.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "This information\nhelps the user to see the effectiveness of a factor model in a specified time period (T1), as well as any investment trends among fund man-\nagers for potential factor crowding (T2)", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "matrix+heat_map+line", "axial_code": [], "componenet_code": ["heatmap", "line", "matrix"]}]}, {"author": "gsh", "index_original": 30, "paper_title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios", "pub_year": 2020, "domain": "Finance", "requirement": {"requirement_text": "T.2 What is the 'crowdedness' of each factor at any given time? When capital flows into a single factor, the expected return on the factor may drop significantly because of the price appreciation. Thus, it is crucial for the investors to speculate hypothesize any future returns on factors before they make an investment and avoid factors that are crowded, even though they might have earned a high return in the past.", "requirement_code": {"discover_observation": 1, "compare_entities": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "Our visualized data is comprised of the backtesting records for each\nportfolio, the factor exposures of each stock, the sector categories of\neach stock, and the factor returns of the corresponding factors.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "After brushing, the portfolio cluster view and the factor\ncorrelation view will be updated accordingly. Users can then proceed\nto the following interaction for detailed analysis ", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "gsh", "index_original": 31, "paper_title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios", "pub_year": 2020, "domain": "Finance", "requirement": {"requirement_text": "T.3 How do various groups of portfolio strategies evaluate risks and industries differently? Portfolio strategies are created by human beings, thus, represent their risk and industry preferences. In- vestors who trust in small companies may take huge risks in the size factor while other investors who favor fast-growing stocks may carry more momentum risk. To understand the rationale behind portfolio strategy groups, investors need to analyze the risks taken and the industry held by the portfolios.", "requirement_code": {"explain_differences": 1, "describe_observation_aggregate": 1, "discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "The portfolio data is multivariate time series data of variant lengths\nbecause the portfolio is constructed within different time periods.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "the first component is an LSTM autoencoder that maps the multivariate\ntime series data of variant lengths into a vector in latent space.", "solution_category": "data_manipulation", "solution_axial": "Wrangling", "solution_compoent": "", "axial_code": ["Wrangling"], "componenet_code": ["wrangling"]}, {"solution_text": ". The second component is a dimension reduction\nalgorithm that reduces the latent vectors to two-dimensional vectors,\nwhich are used as x and y coordinates in the portfolio cluster view.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "There are two parts in the portfolio cluster view, the clustering space\nand the timeline (Fig.1A)", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "Scatter", "axial_code": [], "componenet_code": ["scatter"]}, {"solution_text": "In\nthe portfolio cluster view, users can select the clusters of interest", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "gsh", "index_original": 32, "paper_title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios", "pub_year": 2020, "domain": "Finance", "requirement": {"requirement_text": "T.3 How do various groups of portfolio strategies evaluate risks and industries differently? Portfolio strategies are created by human beings, thus, represent their risk and industry preferences. In- vestors who trust in small companies may take huge risks in the size factor while other investors who favor fast-growing stocks may carry more momentum risk. To understand the rationale behind portfolio strategy groups, investors need to analyze the risks taken and the industry held by the portfolios.", "requirement_code": {"explain_differences": 1, "describe_observation_aggregate": 1, "discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "The portfolio data is multivariate time series data of variant lengths\nbecause the portfolio is constructed within different time periods.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": " line appears in the factor signature\nand is drawn by connecting the points projected to the ten axes by the\nten factor exposure values of the portfolio every trading day. The sector graph is a horizon graph [40] that encodes seven portions\nof a portfolio including cash, the top five sectors of the holding, and\nthe sum of the rest sectors in a top-down manner.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "line+stacked_area", "axial_code": [], "componenet_code": ["area", "line"]}]}, {"author": "gsh", "index_original": 33, "paper_title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios", "pub_year": 2020, "domain": "Finance", "requirement": {"requirement_text": "T.3 How do various groups of portfolio strategies evaluate risks and industries differently? Portfolio strategies are created by human beings, thus, represent their risk and industry preferences. In- vestors who trust in small companies may take huge risks in the size factor while other investors who favor fast-growing stocks may carry more momentum risk. To understand the rationale behind portfolio strategy groups, investors need to analyze the risks taken and the industry held by the portfolios.", "requirement_code": {"explain_differences": 1, "describe_observation_aggregate": 1, "discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "The portfolio data is multivariate time series data of variant lengths\nbecause the portfolio is constructed within different time periods.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "In\nthe portfolio cluster view, users can select the clusters of interest, which\nputs overviews of the various portfolios into the comparison view to\nshow the general patterns of portfolios ", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "gsh", "index_original": 34, "paper_title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios", "pub_year": 2020, "domain": "Finance", "requirement": {"requirement_text": "T.4 How did previous quantitative investors implement the portfolio strategies? As well as understanding the portfolio strategies, the investors also demand that strategies are quickly replicated and deployed to the market. Stock holdings within a timespan give investors hints about implementing portfolio strategies.", "requirement_code": {"discover_observation": 1, "describe_observation_item": 1}}, "data": {"data_text": "The portfolio data is multivariate time series data of variant lengths\nbecause the portfolio is constructed within different time periods.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The comparison view provides overviews of various portfolios, which\nreveals the general patterns in the portfolios, at a glance (T3) and\nenabling users to compare risk and industry preference quickly between\nportfolios (T4)", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "line+stacked_area", "axial_code": [], "componenet_code": ["area", "line"]}]}, {"author": "gsh", "index_original": 35, "paper_title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios", "pub_year": 2020, "domain": "Finance", "requirement": {"requirement_text": "T.4 How did previous quantitative investors implement the portfolio strategies? As well as understanding the portfolio strategies, the investors also demand that strategies are quickly replicated and deployed to the market. Stock holdings within a timespan give investors hints about implementing portfolio strategies.", "requirement_code": {"discover_observation": 1, "describe_observation_item": 1}}, "data": {"data_text": "The portfolio data is multivariate time series data of variant lengths\nbecause the portfolio is constructed within different time periods.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The individual portfolio view displays the management details of a\nportfolio, at the stock holding level, which helps the users further\nexamine and confirm the portfolio\u2019s strategy (T4)", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line+stacked_area", "axial_code": [], "componenet_code": ["area", "line"]}, {"solution_text": "Users can then click on a specific portfolio in the comparison view, to see the portfolio\u2019s strategies in the individual portfolio view, which\nshows the stock holdings of the portfolio.", "solution_category": "interaction", "solution_axial": "Abstract/Elaborate", "solution_compoent": "", "axial_code": ["Abstract/Elaborate"], "componenet_code": ["abstract_elaborate"]}]}, {"author": "gsh", "index_original": 36, "paper_title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios", "pub_year": 2020, "domain": "Finance", "requirement": {"requirement_text": "T.4 How did previous quantitative investors implement the portfolio strategies? As well as understanding the portfolio strategies, the investors also demand that strategies are quickly replicated and deployed to the market. Stock holdings within a timespan give investors hints about implementing portfolio strategies.", "requirement_code": {"discover_observation": 1, "describe_observation_item": 1}}, "data": {"data_text": "The portfolio data is multivariate time series data of variant lengths\nbecause the portfolio is constructed within different time periods.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "In\nthe portfolio cluster view, users can select the clusters of interest, which\nputs overviews of the various portfolios into the comparison view to\nshow the general patterns of portfolios ", "solution_category": "interaction", "solution_axial": "Filtering", "solution_compoent": "", "axial_code": ["Filtering"], "componenet_code": ["filtering"]}]}, {"author": "gsh", "index_original": 37, "paper_title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios", "pub_year": 2020, "domain": "Finance", "requirement": {"requirement_text": "T.5 Which trading style did the portfolios adopt? Fund and/or port- folio managers adopt different trading styles in their practices. Managers may trade in high- or low-frequencies. They may also form a concentrated portfolio or highly-diversified portfolio. Un- derstanding a single portfolios trading strategy gives investors insights into dimensions outside the risk factors.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The portfolio data is multivariate time series data of variant lengths\nbecause the portfolio is constructed within different time periods.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The individual portfolio view displays the management details of a\nportfolio, at the stock holding level, which helps the users further\nexamine and confirm the portfolio\u2019s strategy (T4)", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line+stacked_area", "axial_code": [], "componenet_code": ["area", "line"]}, {"solution_text": "Users can then click on a specific portfolio in the comparison view, to see the portfolio\u2019s strategies in the individual portfolio view, which\nshows the stock holdings of the portfolio.", "solution_category": "interaction", "solution_axial": "Abstract/Elaborate", "solution_compoent": "", "axial_code": ["Abstract/Elaborate"], "componenet_code": ["abstract_elaborate"]}]}, {"author": "gsh", "index_original": 38, "paper_title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios", "pub_year": 2020, "domain": "Finance", "requirement": {"requirement_text": "T.6 How to speculate the future return of a portfolio? In the past, a star portfolio or fund could earn more than 100% annually. Will this trend continue? The portfolios current holdings and the risks it is taking both have a big influence on its future returns.", "requirement_code": {"discover_observation": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "The portfolio data is multivariate time series data of variant lengths\nbecause the portfolio is constructed within different time periods.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "This information\nhelps the user to see the effectiveness of a factor model in a specified time period (T1), as well as any investment trends among fund man-\nagers for potential factor crowding (T2)", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "matrix+heat_map+line", "axial_code": [], "componenet_code": ["heatmap", "line", "matrix"]}]}, {"author": "gsh", "index_original": 39, "paper_title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios", "pub_year": 2020, "domain": "Finance", "requirement": {"requirement_text": "T.6 How to speculate the future return of a portfolio? In the past, a star portfolio or fund could earn more than 100% annually. Will this trend continue? The portfolios current holdings and the risks it is taking both have a big influence on its future returns.", "requirement_code": {"discover_observation": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "The portfolio data is multivariate time series data of variant lengths\nbecause the portfolio is constructed within different time periods.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The comparison view provides overviews of various portfolios, which\nreveals the general patterns in the portfolios, at a glance (T3) and\nenabling users to compare risk and industry preference quickly between\nportfolios (T4)", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "line+stacked_area", "axial_code": [], "componenet_code": ["area", "line"]}]}, {"author": "gsh", "index_original": 40, "paper_title": "Interactive Learning for Identifying Relevant Tweets to Support Real-time Situational Awareness", "pub_year": 2020, "domain": "Social Media", "requirement": {"requirement_text": "DG1 Filter and view relevant data: Filtering data by relevancy re- moves noisy data, allowing the user to more quickly find data that may require immediate attention or contain important information. The ability to view the relevant data itself is equally important for determining the urgency and content of relevant data.", "requirement_code": {"data_filtering": 1, "discover_observation": 1}}, "data": {"data_text": "3.3.3 Corpus for Model Selection and Optimization\nTo experiment with different neural network model types and optimize\nthe selected model, we used a disaster-related corpus annotated on\nthe crowd-sourcing platform, Figure Eight [1]. The dataset contains\n10,876 tweets related to different types of disaster events, such as\nhurricanes and automobile accidents. The data was collected using\nkeywords such as \u201cablaze\u201d or \u201cquarantine\u201d, and therefore, covers a\nwide variety of disaster-related topics. Our main motivation for using\nthis open dataset is its size (as well as topical relevance), enabling the\noptimization of hyperparameters and comparison of various models. In\nthe corpus, each tweet is manually labeled by Figure Eight\u2019s workers as\n\u201cRelevant\u201d, \u201cNot Relevant\u201d, or \u201cCan\u2019t Decide\u201d, and the distribution of\nlabels is unbalanced. Speci\ufb01cally, there are 4,673 \u201cRelevant\u201d instances,\n6,187 \u201cNot Relevant\u201d instances, and 16 \u201cCan\u2019t Decide\u201d instances. This\ndataset has been used in other tweet classi\ufb01cation research projects\n[45]. However, the researchers of that study remove the tweets with the\n\u201cCan\u2019t Decide\u201d label to improve training data quality. As explained in\nthe previous section, we \ufb01nd the \u201cCan\u2019t Decide\u201d option useful for users\nto apply to cases with insuf\ufb01cient context for relevance determination.\nWe randomly shuf\ufb02e the data and divide the dataset into 80% training,\n10% validation, and 10% testing sets.", "data_code": {"tables": 1, "textual": 1, "categorical": 1, "parameter": 1}}, "solution": [{"solution_text": "3.3.2 Design\nAs mentioned before, to enable the use of neural networks for classify-\ning text, we convert the unstructured text (of the tweets) into vectors\nready for consumption by the neural network. When using Word2Vec\nvectors as features for classi\ufb01cation, a common approach is to con-\nvert each word in the sentence to its vector, average the word vectors\nin the sentence, and then use the resulting feature vector for model\ntraining [4, 43]. However, averaging the vectors results in the loss of\nsyntactic information, which can negatively impact classi\ufb01cation re-\nsults [27]. As an example, the two sentences \u201cOnly Mary will attend the\nceremony.\u201d and \u201cMary will only attend the ceremony.\u201d would generate\nidentical averaged sentence vectors since they contain the same set of\nwords, but they differ in meaning. Therefore, to capture both semantic\nand syntactic information, we represent a sentence as a matrix where\neach row i is a 300-dimensional Word2Vec vector corresponding to\nword i in the original sentence.\nThe input to the neural network consists of the matrix represent-\ning the sentence (as described above) and the output consists of the\nclassi\ufb01cation labels for the input sentence (Fig. 2). Speci\ufb01cally, we\nallow a tweet to be (1) Relevant, (2) Not Relevant, or (3) Can\u2019t De-\ncide. The label with the highest probability from the activation function\ncorresponds to the \ufb01nal label given to it. The \u201cCan\u2019t Decide\u201d label\nindicates that the tweet may or may not be relevant depending on the\ncontext. This is useful if the user \ufb01nds a social media post such as\n\u201cRemembering when Hurricane Irma destroyed my home...\u201d that may\nnot directly relate to the current event, but may be semantically relevant,\nand the user does not want to mark such cases as \u201cNot Relevant\u201d. This\ngives the user more \ufb02exibility to accommodate their needs since the\nde\ufb01nition of relevancy will depend on both the user and the situation.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The SMART 2.0 table (Fig. 7(g)\u2013(j)) is extended from SMART in that\nit not only provides a tweet\u2019s creation date and text, but also provides\nthe predicted relevance label (Fig. 7(i)) and the probabilities of a tweet\nbelonging to any of the relevance classes (Fig. 7(j)) (DG1).\nIn particular, the relevance of a tweet can be \u201cRelevant\u201d, \u201cNot Rel-\nevant\u201d, or \u201cCan\u2019t Decide\u201d. The \u201cRelevant\u201d label is colored blue, the\n\u201cNot Relevant\u201d label red, and the \u201cCan\u2019t Decide\u201d label gray to visually\nseparate tweets with different relevance. SMART\u2019s preexisting blue\ncolor scheme motivated us to use the blue, red, and gray diverging\ncoloring for relevancy in order to maintain visual appeal and harmony.\nUsers can directly click on relevance labels to correct the classifier\u2019s\nprediction (DG2). For instance, if a tweet is incorrectly marked \u201cRel-\nevant\u201d, clicking the label will change it to \u201cNot Relevant\u201d or \u201cCan\u2019t\nDecide\u201d, depending on the label the user wishes to assign. Further, a\ndrop down box is included at the top of the relevance label column\n(Fig. 7(h)), which provides the option to filter out data that does not\nhave a specified relevancy (DG1). For example, by selecting \u201cRelevant\u201d\nfrom the drop down box, the table will remove tweets with labels \u201cNot\nRelevant\u201d and \u201cCan\u2019t Decide\u201d from all views and visualizations in\nSMART, including geovisualizations and temporal views.\nThe table also displays the degree (or confidence) of a tweet\u2019s rel-\nevancy. In specific, the probabilities of a tweet being \u201cRelevant\u201d,\n\u201cNot Relevant\u201d, or \u201cCan\u2019t Decide\u201d are represented as a horizontal seg-\nmented bar graph and sized proportional to their respective percentages\n(Fig. 7(j)). In addition, the user can sort tweets based on relevancy\nprobability in ascending or descending order.\nWe provide the relevance probabilities and associated sorting actions\nas a supplementary relevance filtering mechanism (DG1). In particular,\nit is possible for tweets to be classified as \u201cRelevant\u201d by the model, for\nexample, but with low confidence. The probability filtering allows the\nuser to specifically view high-confidence relevant data and therefore\nfurther reduce potentially noisy data.\nThe table provides a performance bar that encodes the estimated\nperformance (F1 score) of the underlying learning model (Fig. 7(g)),\nas well as the number of user-labeled tweets, to inform the user of\nthe model reliability. Since labeled testing data is not available to\nevaluate the model for real-time training (because we assume the user\nmay train on any type of event data and has their own specifications\nfor relevancy), the model\u2019s performance can only be estimated. Based\non our evaluations in Section 3.3.5 with datasets typical of situational\nawareness scenarios (Table 3), the Colorado wildfires dataset generated\nthe F1 score (0.71) closest to the average of the three datasets (0.74).\nTherefore, we use the Colorado wildfires dataset\u2019s logarithmic trendline\ny = 0.09 loge(x) + 0.22 (Fig. 4) to approximate the model\u2019s F1 score as\na function of the number of user-labeled tweets.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "bar_chart+table+text", "axial_code": [], "componenet_code": ["bar", "text", "table"]}, {"solution_text": "The SMART 2.0 table (Fig. 7(g)\u2013(j)) is extended from SMART in that\nit not only provides a tweet\u2019s creation date and text, but also provides\nthe predicted relevance label (Fig. 7(i)) and the probabilities of a tweet\nbelonging to any of the relevance classes (Fig. 7(j)) (DG1).\nIn particular, the relevance of a tweet can be \u201cRelevant\u201d, \u201cNot Rel-\nevant\u201d, or \u201cCan\u2019t Decide\u201d. The \u201cRelevant\u201d label is colored blue, the\n\u201cNot Relevant\u201d label red, and the \u201cCan\u2019t Decide\u201d label gray to visually\nseparate tweets with different relevance. SMART\u2019s preexisting blue\ncolor scheme motivated us to use the blue, red, and gray diverging\ncoloring for relevancy in order to maintain visual appeal and harmony.\nUsers can directly click on relevance labels to correct the classifier\u2019s\nprediction (DG2). For instance, if a tweet is incorrectly marked \u201cRel-\nevant\u201d, clicking the label will change it to \u201cNot Relevant\u201d or \u201cCan\u2019t\nDecide\u201d, depending on the label the user wishes to assign. Further, a\ndrop down box is included at the top of the relevance label column\n(Fig. 7(h)), which provides the option to filter out data that does not\nhave a specified relevancy (DG1). For example, by selecting \u201cRelevant\u201d\nfrom the drop down box, the table will remove tweets with labels \u201cNot\nRelevant\u201d and \u201cCan\u2019t Decide\u201d from all views and visualizations in\nSMART, including geovisualizations and temporal views.\nThe table also displays the degree (or confidence) of a tweet\u2019s rel-\nevancy. In specific, the probabilities of a tweet being \u201cRelevant\u201d,\n\u201cNot Relevant\u201d, or \u201cCan\u2019t Decide\u201d are represented as a horizontal seg-\nmented bar graph and sized proportional to their respective percentages\n(Fig. 7(j)). In addition, the user can sort tweets based on relevancy\nprobability in ascending or descending order.\nWe provide the relevance probabilities and associated sorting actions\nas a supplementary relevance filtering mechanism (DG1). In particular,\nit is possible for tweets to be classified as \u201cRelevant\u201d by the model, for\nexample, but with low confidence. The probability filtering allows the\nuser to specifically view high-confidence relevant data and therefore\nfurther reduce potentially noisy data.\nThe table provides a performance bar that encodes the estimated\nperformance (F1 score) of the underlying learning model (Fig. 7(g)),\nas well as the number of user-labeled tweets, to inform the user of\nthe model reliability. Since labeled testing data is not available to\nevaluate the model for real-time training (because we assume the user\nmay train on any type of event data and has their own specifications\nfor relevancy), the model\u2019s performance can only be estimated. Based\non our evaluations in Section 3.3.5 with datasets typical of situational\nawareness scenarios (Table 3), the Colorado wildfires dataset generated\nthe F1 score (0.71) closest to the average of the three datasets (0.74).\nTherefore, we use the Colorado wildfires dataset\u2019s logarithmic trendline\ny = 0.09 loge(x) + 0.22 (Fig. 4) to approximate the model\u2019s F1 score as\na function of the number of user-labeled tweets.", "solution_category": "interaction", "solution_axial": "filter;Participation/Collaboration;Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure", "filter", "Participation/Collaboration"], "componenet_code": ["reconfigure", "filtering", "participation_collaboration"]}]}, {"author": "gsh", "index_original": 41, "paper_title": "Interactive Learning for Identifying Relevant Tweets to Support Real-time Situational Awareness", "pub_year": 2020, "domain": "Social Media", "requirement": {"requirement_text": "DG1 Filter and view relevant data: Filtering data by relevancy re- moves noisy data, allowing the user to more quickly find data that may require immediate attention or contain important information. The ability to view the relevant data itself is equally important for determining the urgency and content of relevant data.", "requirement_code": {"data_filtering": 1, "discover_observation": 1}}, "data": {"data_text": "3.3.3 Corpus for Model Selection and Optimization\nTo experiment with different neural network model types and optimize\nthe selected model, we used a disaster-related corpus annotated on\nthe crowd-sourcing platform, Figure Eight [1]. The dataset contains\n10,876 tweets related to different types of disaster events, such as\nhurricanes and automobile accidents. The data was collected using\nkeywords such as \u201cablaze\u201d or \u201cquarantine\u201d, and therefore, covers a\nwide variety of disaster-related topics. Our main motivation for using\nthis open dataset is its size (as well as topical relevance), enabling the\noptimization of hyperparameters and comparison of various models. In\nthe corpus, each tweet is manually labeled by Figure Eight\u2019s workers as\n\u201cRelevant\u201d, \u201cNot Relevant\u201d, or \u201cCan\u2019t Decide\u201d, and the distribution of\nlabels is unbalanced. Speci\ufb01cally, there are 4,673 \u201cRelevant\u201d instances,\n6,187 \u201cNot Relevant\u201d instances, and 16 \u201cCan\u2019t Decide\u201d instances. This\ndataset has been used in other tweet classi\ufb01cation research projects\n[45]. However, the researchers of that study remove the tweets with the\n\u201cCan\u2019t Decide\u201d label to improve training data quality. As explained in\nthe previous section, we \ufb01nd the \u201cCan\u2019t Decide\u201d option useful for users\nto apply to cases with insuf\ufb01cient context for relevance determination.\nWe randomly shuf\ufb02e the data and divide the dataset into 80% training,\n10% validation, and 10% testing sets.", "data_code": {"tables": 1, "textual": 1, "categorical": 1, "parameter": 1}}, "solution": [{"solution_text": "3.3.2 Design\nAs mentioned before, to enable the use of neural networks for classify-\ning text, we convert the unstructured text (of the tweets) into vectors\nready for consumption by the neural network. When using Word2Vec\nvectors as features for classi\ufb01cation, a common approach is to con-\nvert each word in the sentence to its vector, average the word vectors\nin the sentence, and then use the resulting feature vector for model\ntraining [4, 43]. However, averaging the vectors results in the loss of\nsyntactic information, which can negatively impact classi\ufb01cation re-\nsults [27]. As an example, the two sentences \u201cOnly Mary will attend the\nceremony.\u201d and \u201cMary will only attend the ceremony.\u201d would generate\nidentical averaged sentence vectors since they contain the same set of\nwords, but they differ in meaning. Therefore, to capture both semantic\nand syntactic information, we represent a sentence as a matrix where\neach row i is a 300-dimensional Word2Vec vector corresponding to\nword i in the original sentence.\nThe input to the neural network consists of the matrix represent-\ning the sentence (as described above) and the output consists of the\nclassi\ufb01cation labels for the input sentence (Fig. 2). Speci\ufb01cally, we\nallow a tweet to be (1) Relevant, (2) Not Relevant, or (3) Can\u2019t De-\ncide. The label with the highest probability from the activation function\ncorresponds to the \ufb01nal label given to it. The \u201cCan\u2019t Decide\u201d label\nindicates that the tweet may or may not be relevant depending on the\ncontext. This is useful if the user \ufb01nds a social media post such as\n\u201cRemembering when Hurricane Irma destroyed my home...\u201d that may\nnot directly relate to the current event, but may be semantically relevant,\nand the user does not want to mark such cases as \u201cNot Relevant\u201d. This\ngives the user more \ufb02exibility to accommodate their needs since the\nde\ufb01nition of relevancy will depend on both the user and the situation.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The SMART 2.0 map is extended from SMART in that it includes a\ntweet\u2019s relevance label (which can be modi\ufb01ed) in addition to its text\nand creation date (Fig. 7(d)(e)). Through the Tweet Tooltip, the user can\ndirectly click on tweet symbols on the map to view their text and asso-\nciated relevancy (DG1). In addition, the user can correct the classi\ufb01ed\nrelevance label (DG2) by clicking on the label itself. Map inspection\ncan allow the user to view and investigate potential geographical rele-\nvancy trends. For example, during crisis events, relevant tweets might\nbe closely grouped on the map, so it may be more bene\ufb01cial for the\nuser to view predicted relevance from the map itself.\nThe interactions between the table and map are synchronized. If\nthe user relabels data on the map, the associated new label will also be\nupdated in the table, and vice versa. In addition, selecting a relevancy\n\ufb01lter from the drop down box in the table \ufb01lters the tweets on the map.", "solution_category": "visualization", "solution_axial": "large_parnel", "solution_compoent": "map;wordcloud;text;", "axial_code": [], "componenet_code": ["wordcloud", "map", "text"]}, {"solution_text": "The SMART 2.0 map is extended from SMART in that it includes a\ntweet\u2019s relevance label (which can be modi\ufb01ed) in addition to its text\nand creation date (Fig. 7(d)(e)). Through the Tweet Tooltip, the user can\ndirectly click on tweet symbols on the map to view their text and asso-\nciated relevancy (DG1). In addition, the user can correct the classi\ufb01ed\nrelevance label (DG2) by clicking on the label itself. Map inspection\ncan allow the user to view and investigate potential geographical rele-\nvancy trends. For example, during crisis events, relevant tweets might\nbe closely grouped on the map, so it may be more bene\ufb01cial for the\nuser to view predicted relevance from the map itself.\nThe interactions between the table and map are synchronized. If\nthe user relabels data on the map, the associated new label will also be\nupdated in the table, and vice versa. In addition, selecting a relevancy\n\ufb01lter from the drop down box in the table \ufb01lters the tweets on the map.", "solution_category": "interaction", "solution_axial": "Overview_and_explore;", "solution_compoent": "", "axial_code": ["Overview_and_explore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "gsh", "index_original": 42, "paper_title": "Interactive Learning for Identifying Relevant Tweets to Support Real-time Situational Awareness", "pub_year": 2020, "domain": "Social Media", "requirement": {"requirement_text": "DG2 Correct incorrect classifications: Since classifiers may provide incorrect results, especially during the early stages of training, it is necessary for the user to be able to correct the label in real- time. This both improves the model's performance and lowers the likelihood that incoming streamed data will be incorrectly classified and missed.", "requirement_code": {"modeling_and_enhancement": 1, "knowledge_injection": 1}}, "data": {"data_text": "3.3.3 Corpus for Model Selection and Optimization\nTo experiment with different neural network model types and optimize\nthe selected model, we used a disaster-related corpus annotated on\nthe crowd-sourcing platform, Figure Eight [1]. The dataset contains\n10,876 tweets related to different types of disaster events, such as\nhurricanes and automobile accidents. The data was collected using\nkeywords such as \u201cablaze\u201d or \u201cquarantine\u201d, and therefore, covers a\nwide variety of disaster-related topics. Our main motivation for using\nthis open dataset is its size (as well as topical relevance), enabling the\noptimization of hyperparameters and comparison of various models. In\nthe corpus, each tweet is manually labeled by Figure Eight\u2019s workers as\n\u201cRelevant\u201d, \u201cNot Relevant\u201d, or \u201cCan\u2019t Decide\u201d, and the distribution of\nlabels is unbalanced. Speci\ufb01cally, there are 4,673 \u201cRelevant\u201d instances,\n6,187 \u201cNot Relevant\u201d instances, and 16 \u201cCan\u2019t Decide\u201d instances. This\ndataset has been used in other tweet classi\ufb01cation research projects\n[45]. However, the researchers of that study remove the tweets with the\n\u201cCan\u2019t Decide\u201d label to improve training data quality. As explained in\nthe previous section, we \ufb01nd the \u201cCan\u2019t Decide\u201d option useful for users\nto apply to cases with insuf\ufb01cient context for relevance determination.\nWe randomly shuf\ufb02e the data and divide the dataset into 80% training,\n10% validation, and 10% testing sets.", "data_code": {"tables": 1, "textual": 1, "categorical": 1, "parameter": 1}}, "solution": [{"solution_text": "3.3.2 Design\nAs mentioned before, to enable the use of neural networks for classify-\ning text, we convert the unstructured text (of the tweets) into vectors\nready for consumption by the neural network. When using Word2Vec\nvectors as features for classi\ufb01cation, a common approach is to con-\nvert each word in the sentence to its vector, average the word vectors\nin the sentence, and then use the resulting feature vector for model\ntraining [4, 43]. However, averaging the vectors results in the loss of\nsyntactic information, which can negatively impact classi\ufb01cation re-\nsults [27]. As an example, the two sentences \u201cOnly Mary will attend the\nceremony.\u201d and \u201cMary will only attend the ceremony.\u201d would generate\nidentical averaged sentence vectors since they contain the same set of\nwords, but they differ in meaning. Therefore, to capture both semantic\nand syntactic information, we represent a sentence as a matrix where\neach row i is a 300-dimensional Word2Vec vector corresponding to\nword i in the original sentence.\nThe input to the neural network consists of the matrix represent-\ning the sentence (as described above) and the output consists of the\nclassi\ufb01cation labels for the input sentence (Fig. 2). Speci\ufb01cally, we\nallow a tweet to be (1) Relevant, (2) Not Relevant, or (3) Can\u2019t De-\ncide. The label with the highest probability from the activation function\ncorresponds to the \ufb01nal label given to it. The \u201cCan\u2019t Decide\u201d label\nindicates that the tweet may or may not be relevant depending on the\ncontext. This is useful if the user \ufb01nds a social media post such as\n\u201cRemembering when Hurricane Irma destroyed my home...\u201d that may\nnot directly relate to the current event, but may be semantically relevant,\nand the user does not want to mark such cases as \u201cNot Relevant\u201d. This\ngives the user more \ufb02exibility to accommodate their needs since the\nde\ufb01nition of relevancy will depend on both the user and the situation.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The SMART 2.0 map is extended from SMART in that it includes a\ntweet\u2019s relevance label (which can be modi\ufb01ed) in addition to its text\nand creation date (Fig. 7(d)(e)). Through the Tweet Tooltip, the user can\ndirectly click on tweet symbols on the map to view their text and asso-\nciated relevancy (DG1). In addition, the user can correct the classi\ufb01ed\nrelevance label (DG2) by clicking on the label itself. Map inspection\ncan allow the user to view and investigate potential geographical rele-\nvancy trends. For example, during crisis events, relevant tweets might\nbe closely grouped on the map, so it may be more bene\ufb01cial for the\nuser to view predicted relevance from the map itself.\nThe interactions between the table and map are synchronized. If\nthe user relabels data on the map, the associated new label will also be\nupdated in the table, and vice versa. In addition, selecting a relevancy\n\ufb01lter from the drop down box in the table \ufb01lters the tweets on the map.", "solution_category": "visualization", "solution_axial": "large_parnel", "solution_compoent": "map;wordcloud;text;", "axial_code": [], "componenet_code": ["wordcloud", "map", "text"]}, {"solution_text": "The SMART 2.0 map is extended from SMART in that it includes a\ntweet\u2019s relevance label (which can be modi\ufb01ed) in addition to its text\nand creation date (Fig. 7(d)(e)). Through the Tweet Tooltip, the user can\ndirectly click on tweet symbols on the map to view their text and asso-\nciated relevancy (DG1). In addition, the user can correct the classi\ufb01ed\nrelevance label (DG2) by clicking on the label itself. Map inspection\ncan allow the user to view and investigate potential geographical rele-\nvancy trends. For example, during crisis events, relevant tweets might\nbe closely grouped on the map, so it may be more bene\ufb01cial for the\nuser to view predicted relevance from the map itself.\nThe interactions between the table and map are synchronized. If\nthe user relabels data on the map, the associated new label will also be\nupdated in the table, and vice versa. In addition, selecting a relevancy\n\ufb01lter from the drop down box in the table \ufb01lters the tweets on the map.", "solution_category": "interaction", "solution_axial": "Overview_and_explore;", "solution_compoent": "", "axial_code": ["Overview_and_explore"], "componenet_code": ["overview_and_explore"]}]}, {"author": "gsh", "index_original": 43, "paper_title": "Interactive Learning for Identifying Relevant Tweets to Support Real-time Situational Awareness", "pub_year": 2020, "domain": "Social Media", "requirement": {"requirement_text": "DG3 Create new classifiers in real-time: The needs of the user can change dramatically over time and vary across users themselves. As an example, one user may wish to train a classifier to find data related to a specific hurricane event to expedite identification of people in desperate need of assistance. However, another user may wish to find data related to safety in general, not just a hurricane. As such, they should each be able to create and train their own classifiers in real-time specific to their needs at the time.", "requirement_code": {"modeling_and_enhancement": 1, "discover_observation": 1}}, "data": {"data_text": "3.3.3 Corpus for Model Selection and Optimization\nTo experiment with different neural network model types and optimize\nthe selected model, we used a disaster-related corpus annotated on\nthe crowd-sourcing platform, Figure Eight [1]. The dataset contains\n10,876 tweets related to different types of disaster events, such as\nhurricanes and automobile accidents. The data was collected using\nkeywords such as \u201cablaze\u201d or \u201cquarantine\u201d, and therefore, covers a\nwide variety of disaster-related topics. Our main motivation for using\nthis open dataset is its size (as well as topical relevance), enabling the\noptimization of hyperparameters and comparison of various models. In\nthe corpus, each tweet is manually labeled by Figure Eight\u2019s workers as\n\u201cRelevant\u201d, \u201cNot Relevant\u201d, or \u201cCan\u2019t Decide\u201d, and the distribution of\nlabels is unbalanced. Speci\ufb01cally, there are 4,673 \u201cRelevant\u201d instances,\n6,187 \u201cNot Relevant\u201d instances, and 16 \u201cCan\u2019t Decide\u201d instances. This\ndataset has been used in other tweet classi\ufb01cation research projects\n[45]. However, the researchers of that study remove the tweets with the\n\u201cCan\u2019t Decide\u201d label to improve training data quality. As explained in\nthe previous section, we \ufb01nd the \u201cCan\u2019t Decide\u201d option useful for users\nto apply to cases with insuf\ufb01cient context for relevance determination.\nWe randomly shuf\ufb02e the data and divide the dataset into 80% training,\n10% validation, and 10% testing sets.", "data_code": {"tables": 1, "textual": 1, "categorical": 1, "parameter": 1}}, "solution": [{"solution_text": "3.3.2 Design\nAs mentioned before, to enable the use of neural networks for classify-\ning text, we convert the unstructured text (of the tweets) into vectors\nready for consumption by the neural network. When using Word2Vec\nvectors as features for classi\ufb01cation, a common approach is to con-\nvert each word in the sentence to its vector, average the word vectors\nin the sentence, and then use the resulting feature vector for model\ntraining [4, 43]. However, averaging the vectors results in the loss of\nsyntactic information, which can negatively impact classi\ufb01cation re-\nsults [27]. As an example, the two sentences \u201cOnly Mary will attend the\nceremony.\u201d and \u201cMary will only attend the ceremony.\u201d would generate\nidentical averaged sentence vectors since they contain the same set of\nwords, but they differ in meaning. Therefore, to capture both semantic\nand syntactic information, we represent a sentence as a matrix where\neach row i is a 300-dimensional Word2Vec vector corresponding to\nword i in the original sentence.\nThe input to the neural network consists of the matrix represent-\ning the sentence (as described above) and the output consists of the\nclassi\ufb01cation labels for the input sentence (Fig. 2). Speci\ufb01cally, we\nallow a tweet to be (1) Relevant, (2) Not Relevant, or (3) Can\u2019t De-\ncide. The label with the highest probability from the activation function\ncorresponds to the \ufb01nal label given to it. The \u201cCan\u2019t Decide\u201d label\nindicates that the tweet may or may not be relevant depending on the\ncontext. This is useful if the user \ufb01nds a social media post such as\n\u201cRemembering when Hurricane Irma destroyed my home...\u201d that may\nnot directly relate to the current event, but may be semantically relevant,\nand the user does not want to mark such cases as \u201cNot Relevant\u201d. This\ngives the user more \ufb02exibility to accommodate their needs since the\nde\ufb01nition of relevancy will depend on both the user and the situation.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "gsh", "index_original": 44, "paper_title": "Interactive Learning for Identifying Relevant Tweets to Support Real-time Situational Awareness", "pub_year": 2020, "domain": "Social Media", "requirement": {"requirement_text": "DG4 Minimize model training time: Although it is important to de-\nsign a high-performing model, time constraints are equally im-\nportant. Speci\ufb01cally, when the model is trained by user feedback,\nthe user should not have to wait for several minutes for the model\nto be retrained and relabel data. Previously streamed data labels\nmay update with retraining, allowing the user to potentially \ufb01nd\nimportant information that they had not seen before. As such, it is\nnecessary to provide these updated results as quickly as possible\nfor real-time situational awareness", "requirement_code": {"modeling_and_enhancement": 1}}, "data": {"data_text": "3.3.3 Corpus for Model Selection and Optimization\nTo experiment with different neural network model types and optimize\nthe selected model, we used a disaster-related corpus annotated on\nthe crowd-sourcing platform, Figure Eight [1]. The dataset contains\n10,876 tweets related to different types of disaster events, such as\nhurricanes and automobile accidents. The data was collected using\nkeywords such as \u201cablaze\u201d or \u201cquarantine\u201d, and therefore, covers a\nwide variety of disaster-related topics. Our main motivation for using\nthis open dataset is its size (as well as topical relevance), enabling the\noptimization of hyperparameters and comparison of various models. In\nthe corpus, each tweet is manually labeled by Figure Eight\u2019s workers as\n\u201cRelevant\u201d, \u201cNot Relevant\u201d, or \u201cCan\u2019t Decide\u201d, and the distribution of\nlabels is unbalanced. Speci\ufb01cally, there are 4,673 \u201cRelevant\u201d instances,\n6,187 \u201cNot Relevant\u201d instances, and 16 \u201cCan\u2019t Decide\u201d instances. This\ndataset has been used in other tweet classi\ufb01cation research projects\n[45]. However, the researchers of that study remove the tweets with the\n\u201cCan\u2019t Decide\u201d label to improve training data quality. As explained in\nthe previous section, we \ufb01nd the \u201cCan\u2019t Decide\u201d option useful for users\nto apply to cases with insuf\ufb01cient context for relevance determination.\nWe randomly shuf\ufb02e the data and divide the dataset into 80% training,\n10% validation, and 10% testing sets.", "data_code": {"tables": 1, "textual": 1, "categorical": 1, "parameter": 1}}, "solution": [{"solution_text": "3.3.2 Design\nAs mentioned before, to enable the use of neural networks for classify-\ning text, we convert the unstructured text (of the tweets) into vectors\nready for consumption by the neural network. When using Word2Vec\nvectors as features for classi\ufb01cation, a common approach is to con-\nvert each word in the sentence to its vector, average the word vectors\nin the sentence, and then use the resulting feature vector for model\ntraining [4, 43]. However, averaging the vectors results in the loss of\nsyntactic information, which can negatively impact classi\ufb01cation re-\nsults [27]. As an example, the two sentences \u201cOnly Mary will attend the\nceremony.\u201d and \u201cMary will only attend the ceremony.\u201d would generate\nidentical averaged sentence vectors since they contain the same set of\nwords, but they differ in meaning. Therefore, to capture both semantic\nand syntactic information, we represent a sentence as a matrix where\neach row i is a 300-dimensional Word2Vec vector corresponding to\nword i in the original sentence.\nThe input to the neural network consists of the matrix represent-\ning the sentence (as described above) and the output consists of the\nclassi\ufb01cation labels for the input sentence (Fig. 2). Speci\ufb01cally, we\nallow a tweet to be (1) Relevant, (2) Not Relevant, or (3) Can\u2019t De-\ncide. The label with the highest probability from the activation function\ncorresponds to the \ufb01nal label given to it. The \u201cCan\u2019t Decide\u201d label\nindicates that the tweet may or may not be relevant depending on the\ncontext. This is useful if the user \ufb01nds a social media post such as\n\u201cRemembering when Hurricane Irma destroyed my home...\u201d that may\nnot directly relate to the current event, but may be semantically relevant,\nand the user does not want to mark such cases as \u201cNot Relevant\u201d. This\ngives the user more \ufb02exibility to accommodate their needs since the\nde\ufb01nition of relevancy will depend on both the user and the situation.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "gsh", "index_original": 45, "paper_title": "Tac-Simur: Tactic-based Simulative Visual Analytics of Table Tennis", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R1 What are the matches and the results of each player? When experts\nselect the data to be analyzed, they will firstly choose a player\nthey are interested in and review the result of the matches he/she\nparticipated in. Usually, they tend to analyze the close matches or\nthe ones revealing the characteristics of the player", "requirement_code": {"discover_observation": 1, "describe_observation_item": 1}}, "data": {"data_text": "The data is manually collected from match videos by professional\ntable tennis players. Both the technical attributes of strokes and the\ncontextual data such as the maker of a stroke, the order of all strokes,\nand the score information are included during collection. The primary\nstroke attributes used for analysis are presented in Table. 1 as follows.", "data_code": {"categorical": 1, "tables": 1}}, "solution": [{"solution_text": "Given the simple topology of the hierarchical structure, we employ a\nnode-link tree rather than space-filling methods such as a treemap to\npresent the hierarchical structure of matches in the player view (G1)", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "tree+point", "axial_code": [], "componenet_code": ["scatter", "tree"]}]}, {"author": "gsh", "index_original": 46, "paper_title": "Tac-Simur: Tactic-based Simulative Visual Analytics of Table Tennis", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R2 What is the playing style of a player? What is the key tactic in the\nmatches? The playing style of a player is intrinsically indicated\nby the type of tactics he/she used most, and the type of tactics\nhe/she scored most. This information can provide navigation and\nreference for analysts while applying adjustments. For example, if\na player is poor at a frequently-used tactic A, then the experts will\nexpect to improve this tactic by adjusting its strokes. Moreover,\nthe information is also necessary to explain the simulation results.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The data is manually collected from match videos by professional\ntable tennis players. Both the technical attributes of strokes and the\ncontextual data such as the maker of a stroke, the order of all strokes,\nand the score information are included during collection. The primary\nstroke attributes used for analysis are presented in Table. 1 as follows.", "data_code": {"categorical": 1, "tables": 1}}, "solution": [{"solution_text": "Tactic list. The list is used for navigating tactics for subsequent\nanalysis. The tactics are all described by one kind of attributes because\n\ufb01ltering with more attributes will severely diminish the adjustable can-\ndidates in the simulation view, thereby limiting exploration. Each row\nof the list contains three components: the tactic (Fig. 5(B4)), the scor-\ning rate (Fig. 5(B5)), and the utilization rate (Fig. 5(B6)). The scoring\nrate of a tactic is symbolized by a donut chart. The utilization rate is\npresented by a bar chart. These two encodings are straightforward for\ncomprehension and ef\ufb01cient for sorting (G2).\ntargeted player in the selected matches. These tactics are presented by\nicons of strokes (G6). They can be sorted based on scoring rates or\nutilization rates (G2)", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "glyph+donut+bar", "axial_code": [], "componenet_code": ["donut", "bar", "glyph"]}]}, {"author": "gsh", "index_original": 47, "paper_title": "Tac-Simur: Tactic-based Simulative Visual Analytics of Table Tennis", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R2 What is the playing style of a player? What is the key tactic in the\nmatches? The playing style of a player is intrinsically indicated\nby the type of tactics he/she used most, and the type of tactics\nhe/she scored most. This information can provide navigation and\nreference for analysts while applying adjustments. For example, if\na player is poor at a frequently-used tactic A, then the experts will\nexpect to improve this tactic by adjusting its strokes. Moreover,\nthe information is also necessary to explain the simulation results.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The data is manually collected from match videos by professional\ntable tennis players. Both the technical attributes of strokes and the\ncontextual data such as the maker of a stroke, the order of all strokes,\nand the score information are included during collection. The primary\nstroke attributes used for analysis are presented in Table. 2 as follows.", "data_code": {"categorical": 1, "tables": 1}}, "solution": [{"solution_text": "The exploration component provides comprehensive options of\nadjustments based on sorting results of each stroke (G2). This view\npresents adjustment options for three consecutive strokes (a tactic) at a\ntime because the experts expect to take tactics into consideration while\nadjusting strokes (Fig. 5(D)). We provide an optional stroke list for\neach stroke (Fig. 5(D3)). Each item in the optional stroke list consists\nof three components, namely, the stroke attributes, the scoring rate, and\nthe adjustability (Fig. 5(D1)) from left to right (R3). The icons (G6) and\nthe donut charts are the same as those in the tactic view. The utilization\nrate is replaced by the adjustability to help assess the feasibility so as\nto expedite the decision-making process. This value is first calculated\nby Equation. 2 and then normalized to one. Similarly, all kinds of\nstrokes can be sorted by scoring rates or adjustability coefficients for\nconvenience (G2). The targeted player\u2019s avatar is also placed at the left\ntop corner of this view with a winning rate. Once an adjustment option\nis selected, the winning rate will change accordingly.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph+donut+bar", "axial_code": [], "componenet_code": ["donut", "bar", "glyph"]}]}, {"author": "gsh", "index_original": 48, "paper_title": "Tac-Simur: Tactic-based Simulative Visual Analytics of Table Tennis", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R2 What is the playing style of a player? What is the key tactic in the\nmatches? The playing style of a player is intrinsically indicated\nby the type of tactics he/she used most, and the type of tactics\nhe/she scored most. This information can provide navigation and\nreference for analysts while applying adjustments. For example, if\na player is poor at a frequently-used tactic A, then the experts will\nexpect to improve this tactic by adjusting its strokes. Moreover,\nthe information is also necessary to explain the simulation results.", "requirement_code": {"describe_observation_aggregate": 1}}, "data": {"data_text": "The data is manually collected from match videos by professional\ntable tennis players. Both the technical attributes of strokes and the\ncontextual data such as the maker of a stroke, the order of all strokes,\nand the score information are included during collection. The primary\nstroke attributes used for analysis are presented in Table. 6 as follows.", "data_code": {"categorical": 1, "tables": 1}}, "solution": [{"solution_text": "hese tactics are presented by\nicons of strokes (G6). The icons (G6) and\nthe donut charts are the same as those in the tactic view. T", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}]}, {"author": "gsh", "index_original": 49, "paper_title": "Tac-Simur: Tactic-based Simulative Visual Analytics of Table Tennis", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R3 What kind of strokes is worth adjusting most? A player could have\nlots of strokes that can be adjusted. However, instead of adjusting\neach stroke to examine the result, experts would like to find a set of\nkey strokes that may be important for improving the performance.\nThis can help them significantly reduce the time for searching\nappropriate adjustment strategies.", "requirement_code": {"identify_main_cause_aggregate": 1}}, "data": {"data_text": "The data is manually collected from match videos by professional\ntable tennis players. Both the technical attributes of strokes and the\ncontextual data such as the maker of a stroke, the order of all strokes,\nand the score information are included during collection. The primary\nstroke attributes used for analysis are presented in Table. 3 as follows.", "data_code": {"categorical": 1, "tables": 1}}, "solution": [{"solution_text": "The exploration component provides comprehensive options of\nadjustments based on sorting results of each stroke (G2). This view\npresents adjustment options for three consecutive strokes (a tactic) at a\ntime because the experts expect to take tactics into consideration while\nadjusting strokes (Fig. 5(D)). We provide an optional stroke list for\neach stroke (Fig. 5(D3)). Each item in the optional stroke list consists\nof three components, namely, the stroke attributes, the scoring rate, and\nthe adjustability (Fig. 5(D1)) from left to right (R3). The icons (G6) and\nthe donut charts are the same as those in the tactic view. The utilization\nrate is replaced by the adjustability to help assess the feasibility so as\nto expedite the decision-making process. This value is first calculated\nby Equation. 2 and then normalized to one. Similarly, all kinds of\nstrokes can be sorted by scoring rates or adjustability coefficients for\nconvenience (G3). The targeted player\u2019s avatar is also placed at the left\ntop corner of this view with a winning rate. Once an adjustment option\nis selected, the winning rate will change accordingly.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "glyph+donut+bar", "axial_code": [], "componenet_code": ["donut", "bar", "glyph"]}]}, {"author": "gsh", "index_original": 50, "paper_title": "Tac-Simur: Tactic-based Simulative Visual Analytics of Table Tennis", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R3 What kind of strokes is worth adjusting most? A player could have\nlots of strokes that can be adjusted. However, instead of adjusting\neach stroke to examine the result, experts would like to find a set of\nkey strokes that may be important for improving the performance.\nThis can help them significantly reduce the time for searching\nappropriate adjustment strategies.", "requirement_code": {"identify_main_cause_aggregate": 1}}, "data": {"data_text": "The data is manually collected from match videos by professional\ntable tennis players. Both the technical attributes of strokes and the\ncontextual data such as the maker of a stroke, the order of all strokes,\nand the score information are included during collection. The primary\nstroke attributes used for analysis are presented in Table. 6 as follows.", "data_code": {"categorical": 1, "tables": 1}}, "solution": [{"solution_text": "hese tactics are presented by\nicons of strokes (G6). The icons (G6) and\nthe donut charts are the same as those in the tactic view. T", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}]}, {"author": "gsh", "index_original": 51, "paper_title": "Tac-Simur: Tactic-based Simulative Visual Analytics of Table Tennis", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R4 What is the effect of an adjustment strategy? What is the feasibility\nof an adjustment strategy? To evaluate an adjustment strategy,\nexperts need to know the effect and feasibility of the adjustment\nstrategy. Specifically, the effect means the increment/decrement\nimposed to final winning rates and the feasibility characterizes the\ndifficulty of utilizing an adjustment strategy in real scenarios", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The data is manually collected from match videos by professional\ntable tennis players. Both the technical attributes of strokes and the\ncontextual data such as the maker of a stroke, the order of all strokes,\nand the score information are included during collection. The primary\nstroke attributes used for analysis are presented in Table. 4 as follows.", "data_code": {"categorical": 1, "tables": 1}}, "solution": [{"solution_text": " The model component ini-\ntializes the hybrid Markov chain model for each player with the data\nfrom the data processing component.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The evaluation component records the tactical adjustments made\nby experts (G4). Each row contains an adjustment strategy and its\neffect and adjustability (Fig. 5(E1)). Given the rally division method\nused in our model, the system only supports adjustments before the\nsixth stroke. The adjustments are recorded by the same icons of the\nselected in the optional stroke list (G6). The effect is computed by\nthe improved high-order Markov chain model de\ufb01ned in Section 4\nand simply shown with the exact value for clarity. The adjustability is\ncalculated by Equation. 2 and demonstrated with a bar chart.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+glyph+pie+bar", "axial_code": [], "componenet_code": ["bar", "pie", "glyph", "table"]}]}, {"author": "gsh", "index_original": 52, "paper_title": "Tac-Simur: Tactic-based Simulative Visual Analytics of Table Tennis", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R4 What is the effect of an adjustment strategy? What is the feasibility\nof an adjustment strategy? To evaluate an adjustment strategy,\nexperts need to know the effect and feasibility of the adjustment\nstrategy. Specifically, the effect means the increment/decrement\nimposed to final winning rates and the feasibility characterizes the\ndifficulty of utilizing an adjustment strategy in real scenarios", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "The data is manually collected from match videos by professional\ntable tennis players. Both the technical attributes of strokes and the\ncontextual data such as the maker of a stroke, the order of all strokes,\nand the score information are included during collection. The primary\nstroke attributes used for analysis are presented in Table. 6 as follows.", "data_code": {"categorical": 1, "tables": 1}}, "solution": [{"solution_text": "hese tactics are presented by\nicons of strokes (G6). The icons (G6) and\nthe donut charts are the same as those in the tactic view. T", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "glyph", "axial_code": [], "componenet_code": ["glyph"]}]}, {"author": "gsh", "index_original": 53, "paper_title": "Tac-Simur: Tactic-based Simulative Visual Analytics of Table Tennis", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R5 How does a tactical adjustment in\ufb02uence the strokes and tactics?\nExperts need to know the reasons for the positive or negative\neffect of a particular adjustment. Generally, the reasons lie in the\nin\ufb02uence of the adjustment to the latter strokes. For example, the\npositive effect of increasing the usage of quick attack, an offensive\nstroke technique at the second stroke is because this adjustment\ncan further raise the usage of other offensive techniques at latter\nstrokes, which can easily enhance the winning rate of the player.", "requirement_code": {"describe_observation_item": 1}}, "data": {"data_text": "The data is manually collected from match videos by professional\ntable tennis players. Both the technical attributes of strokes and the\ncontextual data such as the maker of a stroke, the order of all strokes,\nand the score information are included during collection. The primary\nstroke attributes used for analysis are presented in Table. 5 as follows.", "data_code": {"categorical": 1, "tables": 1}}, "solution": [{"solution_text": " The model component ini-\ntializes the hybrid Markov chain model for each player with the data\nfrom the data processing component.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The explanation component explains the manner in which an ad-\njustment can be achieved and the reason why it performs well/poorly\n(Fig. 6(A)). We simplify the simulation process of the model and present\nit directly for comprehension purposes. The adjustment is placed at the\ncenter of the view (Fig. 6(E)). Only directly related strokes, namely, two\nformer stroke sets(Fig. 6(C)), if they exist, and two latter (Fig. 6(H))\nstroke sets are displayed here to avoid information overload. According\nto the principle of the model, we connect the stroke sets with lines to\nillustrate the correlation between them (G5) (Fig. 6(B)). The dashed\nlines represent the influence from the former stroke sets to the current\nadjustment whereas the solid lines denote the influence from the cur-\nrent adjustment to the latter stroke sets (Fig. 6(F)). The line thickness\nencodes the weight, \u03bb, of the corresponding influence.", "solution_category": "visualization", "solution_axial": "mirror", "solution_compoent": "glyph+heat_map+line", "axial_code": [], "componenet_code": ["heatmap", "line", "glyph"]}]}, {"author": "gsh", "index_original": 54, "paper_title": "Tac-Simur: Tactic-based Simulative Visual Analytics of Table Tennis", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R6 How to conduct a tactical adjustment in practice? Once an adjust-\nment strategy is discovered, experts will expect to figure out how\nto conduct it in real scenarios so that they can communicate it to\nthe players. Specifically, experts need to examine the relationship\nbetween the adjusted stroke and the former strokes to provide a\npractical solution for players (e.g., to increase the usage of quick\nattack at the third stroke, you need to use pendulum to serve).", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The data is manually collected from match videos by professional\ntable tennis players. Both the technical attributes of strokes and the\ncontextual data such as the maker of a stroke, the order of all strokes,\nand the score information are included during collection. The primary\nstroke attributes used for analysis are presented in Table. 5 as follows.", "data_code": {"categorical": 1, "tables": 1}}, "solution": [{"solution_text": " The model component ini-\ntializes the hybrid Markov chain model for each player with the data\nfrom the data processing component.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The explanation component explains the manner in which an ad-\njustment can be achieved and the reason why it performs well/poorly\n(Fig. 6(A)). We simplify the simulation process of the model and present\nit directly for comprehension purposes. The adjustment is placed at the\ncenter of the view (Fig. 6(E)). Only directly related strokes, namely, two\nformer stroke sets(Fig. 6(C)), if they exist, and two latter (Fig. 6(H))\nstroke sets are displayed here to avoid information overload. According\nto the principle of the model, we connect the stroke sets with lines to\nillustrate the correlation between them (G5) (Fig. 6(B)). The dashed\nlines represent the influence from the former stroke sets to the current\nadjustment whereas the solid lines denote the influence from the cur-\nrent adjustment to the latter stroke sets (Fig. 6(F)). The line thickness\nencodes the weight, \u03bb, of the corresponding influence.", "solution_category": "visualization", "solution_axial": "mirror", "solution_compoent": "glyph+heat_map+line", "axial_code": [], "componenet_code": ["heatmap", "line", "glyph"]}]}, {"author": "gsh", "index_original": 55, "paper_title": "GUIRO: User-Guided Matrix Reordering", "pub_year": 2020, "domain": "Matrix representations", "requirement": {"requirement_text": "T1 Compare result usefulness. Matrix reordering algorithms are de- signed to promote certain visual patterns [7]. However, if the dataset simply does not contain the expected topology, these algorithms will likely produce cluttered results. In a typical explo- ration scenario the user will invoke a set of algorithms to compare the distinct outcomes visually.", "requirement_code": {"describe_observation_aggregate": 1, "compare_entities": 1, "describe_observation_item": 1}}, "data": {"data_text": "An adjacency matrix _x0002_= ( n \u00d7n) is a two-dimensional vector with\nn =\u2223N\u2223with N being the vertices in an undirected network", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "The\nlist allows us (a) to experience the algorithms\u2019 visual quality and use-\nfulness, i.e., produced amount of clutter (T1), and (b) demonstrates the\nalgorithm\u2019s pattern variability given the same dataset (T2). ", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix+heat_map", "axial_code": [], "componenet_code": ["heatmap", "matrix"]}, {"solution_text": "In GUIRO, we let the us-\ner choose from 70 ma-\ntrix reordering algorithm-\ns ( +1 identity sort).", "solution_category": "interaction", "solution_axial": "selecting", "solution_compoent": "", "axial_code": ["selecting"], "componenet_code": ["selecting"]}]}, {"author": "gsh", "index_original": 56, "paper_title": "GUIRO: User-Guided Matrix Reordering", "pub_year": 2020, "domain": "Matrix representations", "requirement": {"requirement_text": "T2 Demonstrate patterns variability. The high-level analysis tasks for graph data strongly differ from non-relational data analysis tasks. We will elaborate this point in Sect. 3.1. In an instruc- tional setting, the scholar will raise the awareness that different algorithms will produce distinct dissimilar visual patterns.", "requirement_code": {"discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "An adjacency matrix _x0002_= ( n \u00d7n) is a two-dimensional vector with\nn =\u2223N\u2223with N being the vertices in an undirected network", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "The\nlist allows us (a) to experience the algorithms\u2019 visual quality and use-\nfulness, i.e., produced amount of clutter (T1), and (b) demonstrates the\nalgorithm\u2019s pattern variability given the same dataset (T3).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix+heat_map", "axial_code": [], "componenet_code": ["heatmap", "matrix"]}, {"solution_text": "In GUIRO, we let the us-\ner choose from 70 ma-\ntrix reordering algorithm-\ns ( +2 identity sort).", "solution_category": "interaction", "solution_axial": "selecting", "solution_compoent": "", "axial_code": ["selecting"], "componenet_code": ["selecting"]}]}, {"author": "gsh", "index_original": 57, "paper_title": "GUIRO: User-Guided Matrix Reordering", "pub_year": 2020, "domain": "Matrix representations", "requirement": {"requirement_text": "T3 Reveal the heuristic nature of (many) algorithms. Due to the fac- torial increase of the search space many reordering algorithms are heuristic, sometimes even greedy algorithms. A comprehensive instructional session will illustrate this fact by applying sever- al times the same algorithm on the same dataset, resulting in di\ufb00erent visual results.", "requirement_code": {"discover_observation": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "An adjacency matrix _x0002_= ( n \u00d7n) is a two-dimensional vector with\nn =\u2223N\u2223with N being the vertices in an undirected network", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "The\nlist allows us (a) to experience the algorithms\u2019 visual quality and use-\nfulness, i.e., produced amount of clutter (T1), and (b) demonstrates the\nalgorithm\u2019s pattern variability given the same dataset (T4).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix+heat_map", "axial_code": [], "componenet_code": ["heatmap", "matrix"]}]}, {"author": "gsh", "index_original": 58, "paper_title": "GUIRO: User-Guided Matrix Reordering", "pub_year": 2020, "domain": "Matrix representations", "requirement": {"requirement_text": "T4 Global pattern analysis. An appropriate matrix reordering algo- rithm will reflect the general data topology allowing the analyst to answer canonical questions, regarding connectivity and partition- ing [46]. However, as described in [7] or [80], these algorithms can also fail to produce interpretable results, even if the dataset contains explainable structure. What is needed is a structure- preserving meta visualization that enables network analysts to estimate how much structure a dataset contains.", "requirement_code": {"describe_observation_aggregate": 1, "discover_observation": 1}}, "data": {"data_text": "An adjacency matrix _x0002_= ( n \u00d7n) is a two-dimensional vector with\nn =\u2223N\u2223with N being the vertices in an undirected network", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "n order to examine the typological structure of\n_x0002_(T4), we can allow all projection techniques that find a lower dimen-\nsional representation y =(y1 ,...,yk)T for each vector x =(x1 ,...,xl)T\nwith k \u2264l, such that y preserves the HD row/column similarity", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "They visually represent a mismatch between close projection points,\ni.e., similar column-/row vectors of the matrix, and long connection\nedges, i.e., the sequential placement of dissimilar column-/row vectors\nby the reordering algorithm.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "matrix", "axial_code": [], "componenet_code": ["matrix"]}, {"solution_text": "(1) Invoke an automated local reordering on a selected group of vertices,\nand (T4, T5)", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "gsh", "index_original": 59, "paper_title": "GUIRO: User-Guided Matrix Reordering", "pub_year": 2020, "domain": "Matrix representations", "requirement": {"requirement_text": "T5 Nested pattern analysis. The idealistic view\u2014depicted in Fig. 2\u2014 does not hold for real-world scenarios. To give a practical exam- ple, biological networks often reflect complex and even nested visual pattern relationships [41, 47]. Almost all reordering al- gorithms, however, are designed for global pattern retrieval and interactive visualizations for applying these algorithms in a local context are mostly unexplored.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "An adjacency matrix _x0002_= ( n \u00d7n) is a two-dimensional vector with\nn =\u2223N\u2223with N being the vertices in an undirected network", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "n order to examine the typological structure of\n_x0002_(T4), we can allow all projection techniques that find a lower dimen-\nsional representation y =(y1 ,...,yk)T for each vector x =(x2 ,...,xl)T\nwith k \u2264l, such that y preserves the HD row/column similarity", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "They visually represent a mismatch between close projection points,\ni.e., similar column-/row vectors of the matrix, and long connection\nedges, i.e., the sequential placement of dissimilar column-/row vectors\nby the reordering algorithm.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "matrix", "axial_code": [], "componenet_code": ["matrix"]}, {"solution_text": "(1) Invoke an automated local reordering on a selected group of vertices,\nand (T4, T6)", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "gsh", "index_original": 60, "paper_title": "GUIRO: User-Guided Matrix Reordering", "pub_year": 2020, "domain": "Matrix representations", "requirement": {"requirement_text": "T6 Detect (local) pattern relationships. As mentioned before, visual patterns are not necessarily disjunct but form complex interactions. A network analyst has to distill this information, by retrieving, e.g., who is the central actor connecting two subgroups in a social network. Having found this influencing person the subsequent question would be who is the primary contact point within the groups to distribute the information further.", "requirement_code": {"discover_observation": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "An adjacency matrix _x0002_= ( n \u00d7n) is a two-dimensional vector with\nn =\u2223N\u2223with N being the vertices in an undirected network", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "n order to examine the typological structure of\n_x0002_(T4), we can allow all projection techniques that find a lower dimen-\nsional representation y =(y1 ,...,yk)T for each vector x =(x3 ,...,xl)T\nwith k \u2264l, such that y preserves the HD row/column similarity", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "They visually represent a mismatch between close projection points,\ni.e., similar column-/row vectors of the matrix, and long connection\nedges, i.e., the sequential placement of dissimilar column-/row vectors\nby the reordering algorithm.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "matrix", "axial_code": [], "componenet_code": ["matrix"]}, {"solution_text": "(2) Rearrange vertices or groups by replacing one or more\nexisting edges (T6, T7).", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "gsh", "index_original": 61, "paper_title": "GUIRO: User-Guided Matrix Reordering", "pub_year": 2020, "domain": "Matrix representations", "requirement": {"requirement_text": "T6 Detect (local) pattern relationships. As mentioned before, visual patterns are not necessarily disjunct but form complex interactions. A network analyst has to distill this information, by retrieving, e.g., who is the central actor connecting two subgroups in a social network. Having found this influencing person the subsequent question would be who is the primary contact point within the groups to distribute the information further.", "requirement_code": {"discover_observation": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "An adjacency matrix _x0002_= ( n \u00d7n) is a two-dimensional vector with\nn =\u2223N\u2223with N being the vertices in an undirected network", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "n order to examine the typological structure of\n_x0002_(T4), we can allow all projection techniques that find a lower dimen-\nsional representation y =(y1 ,...,yk)T for each vector x =(x4 ,...,xl)T\nwith k \u2264l, such that y preserves the HD row/column similarity", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "They visually represent a mismatch between close projection points,\ni.e., similar column-/row vectors of the matrix, and long connection\nedges, i.e., the sequential placement of dissimilar column-/row vectors\nby the reordering algorithm.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "matrix", "axial_code": [], "componenet_code": ["matrix"]}, {"solution_text": "The second user intervention is the so-called edge replacement, depict-\ned in Fig. 5. It rearranges a single row/column, respectively group of\nrows/columns relative to each other. This can be useful for detecting\nitem-to-item, item-to-group, or group-to-group relationships (T6).", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "gsh", "index_original": 62, "paper_title": "GUIRO: User-Guided Matrix Reordering", "pub_year": 2020, "domain": "Matrix representations", "requirement": {"requirement_text": "T7 Adapt visual results to emphasize analytic importance. Present- ing the above complex relationships is quintessential for a suc- cessful network analyst. However, based on our experience, these analysts interestingly do not lean necessarily on one automatic matrix reordering result but sacrifice the global reordering on subparts to emphasize a point on other parts.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "An adjacency matrix _x0002_= ( n \u00d7n) is a two-dimensional vector with\nn =\u2223N\u2223with N being the vertices in an undirected network", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "n order to examine the typological structure of\n_x0002_(T4), we can allow all projection techniques that find a lower dimen-\nsional representation y =(y1 ,...,yk)T for each vector x =(x5 ,...,xl)T\nwith k \u2264l, such that y preserves the HD row/column similarity", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "They visually represent a mismatch between close projection points,\ni.e., similar column-/row vectors of the matrix, and long connection\nedges, i.e., the sequential placement of dissimilar column-/row vectors\nby the reordering algorithm.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "matrix", "axial_code": [], "componenet_code": ["matrix"]}, {"solution_text": "The second user intervention is the so-called edge replacement, depict-\ned in Fig. 5. It rearranges a single row/column, respectively group of\nrows/columns relative to each other. This can be useful for detecting\nitem-to-item, item-to-group, or group-to-group relationships (T7).", "solution_category": "interaction", "solution_axial": "Reconfigure", "solution_compoent": "", "axial_code": ["Reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "gsh", "index_original": 63, "paper_title": "GUIRO: User-Guided Matrix Reordering", "pub_year": 2020, "domain": "Matrix representations", "requirement": {"requirement_text": "T8 Inspect Quality Metrics. Algorithm designers are concerned, not only with the visual performance of their algorithms, but also quantitative quality metrics. Comparing a newly designed algorithm against the state-of-the-art is necessary for an objective comparative evaluation.", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "An adjacency matrix _x0002_= ( n \u00d7n) is a two-dimensional vector with\nn =\u2223N\u2223with N being the vertices in an undirected network", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "GUIRO allows algorithm designers to compare 16 quality metrics\n(T8), as well as the row/column index (dis-)similarity. ", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "GUIRO allows algorithm designers to compare 16 quality metrics\n(T8), as well as the row/column index (dis-)similarity. ", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Mtrix+table+PCP", "axial_code": [], "componenet_code": ["parallelcoordinates", "matrix", "table"]}]}, {"author": "gsh", "index_original": 64, "paper_title": "GUIRO: User-Guided Matrix Reordering", "pub_year": 2020, "domain": "Matrix representations", "requirement": {"requirement_text": "T9 Comparing/Weighing up of Design Decisions. Matrix reorder- ing algorithms can be tweaked on two fronts: (a) the process of exploring the permutations space and (b) the notion of similarity between rows/columns. Giving the designer (visual) means to reflect on the impact of algorithm design decisions will lead to better, more stable algorithms for matrix reordering.", "requirement_code": {"compare_entities": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "An adjacency matrix _x0002_= ( n \u00d7n) is a two-dimensional vector with\nn =\u2223N\u2223with N being the vertices in an undirected network", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "GUIRO allows algorithm designers to compare 16 quality metrics\n(T9), as well as the row/column index (dis-)similarity.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "GUIRO allows algorithm designers to compare 16 quality metrics\n(T9), as well as the row/column index (dis-)similarity.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Mtrix+table+PCP", "axial_code": [], "componenet_code": ["parallelcoordinates", "matrix", "table"]}]}, {"author": "gsh", "index_original": 65, "paper_title": "GUIRO: User-Guided Matrix Reordering", "pub_year": 2020, "domain": "Matrix representations", "requirement": {"requirement_text": "T10 What-if Analysis. Black-box algorithms work most of the time. Finding out, however, what went wrong in the error cases is non- trivial. One classic way of doing this kind of \u201cdebugging\u201d is to exchange the input with structurally varying datasets and compare the ordering results.", "requirement_code": {"evaluate_hypothesis": 1}}, "data": {"data_text": "An adjacency matrix _x0002_= ( n \u00d7n) is a two-dimensional vector with\nn =\u2223N\u2223with N being the vertices in an undirected network", "data_code": {"network_and_trees": 1}}, "solution": [{"solution_text": "GUIRO allows algorithm designers to compare 16 quality metrics\n(T10), as well as the row/column index (dis-)similarity.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "GUIRO allows algorithm designers to compare 16 quality metrics\n(T10), as well as the row/column index (dis-)similarity.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "Mtrix+table+PCP", "axial_code": [], "componenet_code": ["parallelcoordinates", "matrix", "table"]}]}, {"author": "gsh", "index_original": 66, "paper_title": "EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos", "pub_year": 2020, "domain": "Emotion", "requirement": {"requirement_text": "T1 To summarize emotion information in a video. It is necessary to summarize emotion information to offer an overview of the en- tire video collection, which helps users identify videos of interest and thereby guide effective exploration. The emotion informa- tion should include the emotion states of each modality and their coherence to represent the overall pattern.", "requirement_code": {"describe_observation_aggregate": 1, "discover_observation": 1}}, "data": {"data_text": "We collect 30 TED Talk videos3 to explore emotion coherence of\npresentation videos. Each video is about 10 minutes long and of high\nquality, with more than one million online reviews.", "data_code": {"media": 1}}, "solution": [{"solution_text": "We conduct a series of data processing steps to extract emotion in-\nformation from face, text, and audio modalities. We first apply well-\nestablished methods to extract information from each modality inde-\npendently. Next, we fuse those data together based on their semantic\nmeanings and align them at different levels of time granularity.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "A design for summarizing the emotion information of the three\nchannels of a video. The line at the top explicitly shows the emotion\ncoherence of the three channels. The bar code chart at the bottom shows\nmore details about the exact emotions of each channel.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "table+line+heap_map", "axial_code": [], "componenet_code": ["heatmap", "line", "table"]}]}, {"author": "gsh", "index_original": 67, "paper_title": "EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos", "pub_year": 2020, "domain": "Emotion", "requirement": {"requirement_text": "T2 To provide video context for the analysis. Our two domain experts suggest that it is still essential to browse original videos for contextualized exploration in addition to summarized information. Due to the complexity of the data, visualizations should support rapid playback and guided navigation of videos in a screen-space- effective and responsive manner.", "requirement_code": {"discover_observation": 1, "describe_observation_aggregate": 1}}, "data": {"data_text": "We collect 30 TED Talk videos3 to explore emotion coherence of\npresentation videos. Each video is about 11 minutes long and of high\nquality, with more than one million online reviews.", "data_code": {"media": 1}}, "solution": [{"solution_text": "fter a video of interest is selected, the original video is presented\nin the bottom part of the video view (Fig. 1a) to allow users to explore\ndetailed information (T2). ", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "video", "axial_code": [], "componenet_code": ["video"]}]}, {"author": "gsh", "index_original": 68, "paper_title": "EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos", "pub_year": 2020, "domain": "Emotion", "requirement": {"requirement_text": "T3 To summarize emotion coherence across different modalities per sentence. Sentences in each transcript segment form a basic semantic unit with the same text emotions in our model. Present- ing their coherence with facial and audio emotions is therefore a vital prerequisite for understanding the emotional expressions in presentations. For instance, do speakers' facial expressions react in conformity with a happy message such as jokes?", "requirement_code": {"discover_observation": 1, "compare_entities": 1}}, "data": {"data_text": "We collect 30 TED Talk videos3 to explore emotion coherence of\npresentation videos. Each video is about 12 minutes long and of high\nquality, with more than one million online reviews.", "data_code": {"media": 1}}, "solution": [{"solution_text": "We conduct a series of data processing steps to extract emotion in-\nformation from face, text, and audio modalities. We first apply well-\nestablished methods to extract information from each modality inde-\npendently. Next, we fuse those data together based on their semantic\nmeanings and align them at different levels of time granularity.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": " An augmented Sankey diagram design for summarizing emo-\ntion coherence from three channels as well as for providing extracted\nfeatures for explanations. Each node represents one type of emotion\nand each link represents a collection of sentences with certain emotions\nshared by two channels, either the face and text channels or the text and\naudio channels. (a) A treemap-based design to show a quick overview\nof representative detected faces in the video. (b) A word cloud design\nto highlight some important words, which gives users some hints about\nthe corresponding context. (c) A histogram design to show the audio\nfeature distribution for different emotions.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "sankey+image+word_cloud+bar+tree_map", "axial_code": [], "componenet_code": ["sankey", "bar", "image", "treemap", "wordcloud"]}]}, {"author": "gsh", "index_original": 69, "paper_title": "EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos", "pub_year": 2020, "domain": "Emotion", "requirement": {"requirement_text": "T4 To support rapid location of sentences of interest. Our experts are interested in examining how certain emotions are expressed, which demands the ability to rapidly locate sentences with emo- tions of interest. In addition, they wish to search for sentences with similar emotion expressions in order to comprehend the effects of such behavior on the overall situation.", "requirement_code": {"discover_observation": 1, "collect_evidence": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "We collect 30 TED Talk videos3 to explore emotion coherence of\npresentation videos. Each video is about 13 minutes long and of high\nquality, with more than one million online reviews.", "data_code": {"media": 1}}, "solution": [{"solution_text": "We conduct a series of data processing steps to extract emotion in-\nformation from face, text, and audio modalities. We first apply well-\nestablished methods to extract information from each modality inde-\npendently. Next, we fuse those data together based on their semantic\nmeanings and align them at different levels of time granularity.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": " An augmented Sankey diagram design for summarizing emo-\ntion coherence from three channels as well as for providing extracted\nfeatures for explanations. Each node represents one type of emotion\nand each link represents a collection of sentences with certain emotions\nshared by two channels, either the face and text channels or the text and\naudio channels. (a) A treemap-based design to show a quick overview\nof representative detected faces in the video. (b) A word cloud design\nto highlight some important words, which gives users some hints about\nthe corresponding context. (c) A histogram design to show the audio\nfeature distribution for different emotions.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "sankey+image+word_cloud+bar+tree_map", "axial_code": [], "componenet_code": ["sankey", "bar", "image", "treemap", "wordcloud"]}]}, {"author": "gsh", "index_original": 70, "paper_title": "EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos", "pub_year": 2020, "domain": "Emotion", "requirement": {"requirement_text": "T5 To display emotion information along with additional fea- tures for explanation. Our experts suggest to offer additional information, such as the face images, keywords, and prosodic features to verify and better understand the emotion expressions. This information should be displayed with the emotion informa- tion to guide the exploration.", "requirement_code": {"collect_evidence": 1, "identify_main_cause_item": 1, "describe_observation_item": 1}}, "data": {"data_text": "We collect 30 TED Talk videos3 to explore emotion coherence of\npresentation videos. Each video is about 14 minutes long and of high\nquality, with more than one million online reviews.", "data_code": {"media": 1}}, "solution": [{"solution_text": "We conduct a series of data processing steps to extract emotion in-\nformation from face, text, and audio modalities. We first apply well-\nestablished methods to extract information from each modality inde-\npendently. Next, we fuse those data together based on their semantic\nmeanings and align them at different levels of time granularity.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": " An augmented Sankey diagram design for summarizing emo-\ntion coherence from three channels as well as for providing extracted\nfeatures for explanations. Each node represents one type of emotion\nand each link represents a collection of sentences with certain emotions\nshared by two channels, either the face and text channels or the text and\naudio channels. (a) A treemap-based design to show a quick overview\nof representative detected faces in the video. (b) A word cloud design\nto highlight some important words, which gives users some hints about\nthe corresponding context. (c) A histogram design to show the audio\nfeature distribution for different emotions.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "sankey+image+word_cloud+bar+tree_map", "axial_code": [], "componenet_code": ["sankey", "bar", "image", "treemap", "wordcloud"]}]}, {"author": "gsh", "index_original": 71, "paper_title": "EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos", "pub_year": 2020, "domain": "Emotion", "requirement": {"requirement_text": "T6 To show the temporal distribution of emotion states and their coherence. The temporal distribution of emotion states and their coherence represents the most detailed and fundamental char- acteristics. This information should be presented in detail and responsively due to its large scale.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We collect 30 TED Talk videos3 to explore emotion coherence of\npresentation videos. Each video is about 15 minutes long and of high\nquality, with more than one million online reviews.", "data_code": {"media": 1}}, "solution": [{"solution_text": "We conduct a series of data processing steps to extract emotion in-\nformation from face, text, and audio modalities. We first apply well-\nestablished methods to extract information from each modality inde-\npendently. Next, we fuse those data together based on their semantic\nmeanings and align them at different levels of time granularity.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The bar code chart at the top shows face emotions at frame\nlevel and text and audio emotions at sentence level, which provides\na more detailed summary than the corresponding bar codes in the\nvideo view (Fig. 1a). . The corresponding\nsentence context will be shown at the bottom part of the detail view.\nSpecifically, the sentence being explored is shown in the middle, and\nthe two previous sentences and two following sentences are also shown\nto provide more context. Three audio features for the selected sentence,\ni.e., pitch, intensity, and amplitude, are explicitly visualized as a line\nchart and a theme river, which reveals temporal changes of audio\nfeatures for the selected sentence", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "line+bar+area+text", "axial_code": [], "componenet_code": ["area", "bar", "text", "line"]}]}, {"author": "gsh", "index_original": 72, "paper_title": "EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos", "pub_year": 2020, "domain": "Emotion", "requirement": {"requirement_text": "T6 To show the temporal distribution of emotion states and their coherence. The temporal distribution of emotion states and their coherence represents the most detailed and fundamental char- acteristics. This information should be presented in detail and responsively due to its large scale.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We collect 30 TED Talk videos3 to explore emotion coherence of\npresentation videos. Each video is about 16 minutes long and of high\nquality, with more than one million online reviews.", "data_code": {"media": 1}}, "solution": [{"solution_text": "We conduct a series of data processing steps to extract emotion in-\nformation from face, text, and audio modalities. We first apply well-\nestablished methods to extract information from each modality inde-\npendently. Next, we fuse those data together based on their semantic\nmeanings and align them at different levels of time granularity.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "we project\nthe emotion information of each sentence as a glyph point on a 2D\nplane by using the t-SNE projection algorithm, where the vector is\nconstructed as Equation 2. Points are linked with curves by following\ntime order.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "we design a pie chart-based glyph. Three equally divided sectors of\na circle are used to encode emotion information of the face, text and\naudio channels. ", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "glyph+point+line", "axial_code": [], "componenet_code": ["scatter", "line", "glyph"]}]}, {"author": "gsh", "index_original": 73, "paper_title": "EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos", "pub_year": 2020, "domain": "Emotion", "requirement": {"requirement_text": "T7 To enable the inspection of details of emotion expressions at the word level. At a more detailed level, the experts want to explore whether the emotion expressions are associated with words. For instance, are certain kinds of words likely to be accompanied by changes in facial expressions?", "requirement_code": {"discover_observation": 1, "describe_observation_item": 1, "explain_differences": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "We collect 30 TED Talk videos3 to explore emotion coherence of\npresentation videos. Each video is about 17 minutes long and of high\nquality, with more than one million online reviews.", "data_code": {"media": 1}}, "solution": [{"solution_text": "5.5 Word View\nOur end users expressed that they would like to further conduct word-\nlevel exploration, especially the frequency of the words used and corre-\nsponding emotions when uttering these words. In this view (Fig. 8c),\nwe provide detailed information for each word used in the video. Three\nattributes are shown, namely word, frequency and face information. For\neach row, the word column directly shows the word used in the video;\nthe frequency column indicates how many times each word is used in\nthe video; and the face information column visualizes the duration of\nsaying this word and the emotion percentage of face emotion by using a\nstacked bar chart. The length of each component in a stacked bar chart\nindicates the duration of the expressed type of emotion. For those faces\ndo not be detected, we use dashed areas to represent them (Fig. 8c). For\nfocusing on detected emotions, users are allowed to hide these dashed\nareas by turning off the switch button. Furthermore, users are allowed\nto sort the word view by speci\ufb01c criteria, such as frequency, as well as\nby using a keyword search.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "table+text+stacked_bar", "axial_code": [], "componenet_code": ["bar", "text", "table"]}]}, {"author": "gsh", "index_original": 74, "paper_title": "EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos", "pub_year": 2020, "domain": "Emotion", "requirement": {"requirement_text": "T7 To enable the inspection of details of emotion expressions at the word level. At a more detailed level, the experts want to explore whether the emotion expressions are associated with words. For instance, are certain kinds of words likely to be accompanied by changes in facial expressions?", "requirement_code": {"discover_observation": 1, "describe_observation_item": 1, "explain_differences": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "We collect 30 TED Talk videos3 to explore emotion coherence of\npresentation videos. Each video is about 18 minutes long and of high\nquality, with more than one million online reviews.", "data_code": {"media": 1}}, "solution": [{"solution_text": "We conduct a series of data processing steps to extract emotion in-\nformation from face, text, and audio modalities. We first apply well-\nestablished methods to extract information from each modality inde-\npendently. Next, we fuse those data together based on their semantic\nmeanings and align them at different levels of time granularity.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "we project\nthe emotion information of each sentence as a glyph point on a 2D\nplane by using the t-SNE projection algorithm, where the vector is\nconstructed as Equation 2. Points are linked with curves by following\ntime order.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "we design a pie chart-based glyph. Three equally divided sectors of\na circle are used to encode emotion information of the face, text and\naudio channels. ", "solution_category": "visualization", "solution_axial": "coordinate", "solution_compoent": "glyph+point+line", "axial_code": [], "componenet_code": ["scatter", "line", "glyph"]}]}, {"author": "gsh", "index_original": 75, "paper_title": "CloudDet: Interactive Visual Analysis of Anomalous Performances in Cloud Computing Systems", "pub_year": 2020, "domain": "Cloud computing; Anomalous Performances", "requirement": {"requirement_text": "T1 Show the overview of anomaly detection results for data query. A large-scale cloud computing system usually contains multiple data centers, with each center containing hundreds of clusters that host tens of thousands of servers. Hence, it is critical to provide visualization techniques that can summarize the cloud computing performances and anomaly detection results to help experts narrow down the search space.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The performance data tracked in cloud computing systems are rep-\nresented by multivariate time series.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Periodicity Detection. As shown in Fig. 3(1), the first step is to\ndetect the structural periodic changes and estimate the periodicity for\neach piece of historical data, which is an important parameter for the\nfollowing steps.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The historical data is further split into three\ncomponents by the seasonal-trend decomposition procedure based on\nloess (STL) [16], which is aimed to extract three temporal components\nfor the anomaly score calculation.", "solution_category": "data_manipulation", "solution_axial": "Wrangling", "solution_compoent": "", "axial_code": ["Wrangling"], "componenet_code": ["wrangling"]}, {"solution_text": "After getting the estimated period value (Tn) for each historical data\nand the numeric value (Sn, Trn and Rn) for the aforementioed three\ncomponents, the respective anomaly scores (ASs) of three patterns are\ncalculated in the anomaly scoring process based on their following\ndetectors (as shown in Fig. 3(2))", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The temporal overview (Fig. 4(2)) aims at revealing an anomaly overview of the variation of all the important tracking metrics (e.g., CPU frequency and memory usage) over time, which provides users with a general impression of the whole data set in a temporal context. We present two types of charts, namely, an area chart and a stacked bar chart, to show the sum of all nodes's anomaly scores in terms of different metrics at every timestamp. The y-axis shows the anomaly score, and a categorical color scheme is used to represent different met- rics. Specifically, the area chart overlaps the different metrics; thus the most anomalous metric at every time point can be highlighted, and the inter-pattern comparison and correlation discovery of multiple metrics can be fulfilled (T5). By contrast, the stacked bar chart emphasizes the total amount of anomaly scores for all the performance metrics at every time point. To assist users in catching the time period of interest more efficiently, we mark the top five points of each performance metric with red dots for reference. Different interaction designs are provided in this view: (1) brushing a time period in the overview for further analysis; (2) switching the view mode, as well as filtering out some metrics or highlighting others (tuning the corresponding color opaque) (Fig. 4(b)); (3) choosing three types of time granularities, namely, minutes, hours and days, as an input parameter to show the performance history at different levels of detail (Fig. 4(2)).", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "bar+area", "axial_code": [], "componenet_code": ["bar", "area"]}, {"solution_text": "Query and Filtering. Users can load different\nsubsets of data by using the query module or direct interaction on\nthe views in Fig. 4(1, 2), as well as focus on a specific metric in the\ntemporal overview by clicking the legend on Fig. 4(b) (T1).", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "gsh", "index_original": 76, "paper_title": "CloudDet: Interactive Visual Analysis of Anomalous Performances in Cloud Computing Systems", "pub_year": 2020, "domain": "Cloud computing; Anomalous Performances", "requirement": {"requirement_text": "T1 Show the overview of anomaly detection results for data query. A large-scale cloud computing system usually contains multiple data centers, with each center containing hundreds of clusters that host tens of thousands of servers. Hence, it is critical to provide visualization techniques that can summarize the cloud computing performances and anomaly detection results to help experts narrow down the search space.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The performance data tracked in cloud computing systems are rep-\nresented by multivariate time series.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Periodicity Detection. As shown in Fig. 3(2), the first step is to\ndetect the structural periodic changes and estimate the periodicity for\neach piece of historical data, which is an important parameter for the\nfollowing steps.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The historical data is further split into three\ncomponents by the seasonal-trend decomposition procedure based on\nloess (STL) [17], which is aimed to extract three temporal components\nfor the anomaly score calculation.", "solution_category": "data_manipulation", "solution_axial": "Wrangling", "solution_compoent": "", "axial_code": ["Wrangling"], "componenet_code": ["wrangling"]}, {"solution_text": "After getting the estimated period value (Tn) for each historical data\nand the numeric value (Sn, Trn and Rn) for the aforementioed three\ncomponents, the respective anomaly scores (ASs) of three patterns are\ncalculated in the anomaly scoring process based on their following\ndetectors (as shown in Fig. 3(3))", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The spatial overview assists users to observe the general anomaly degree\nand query the data of a cloud computing system hierarchically (from\ndata center to sub-level data cluster) through a bubble chart. Firstly,\nas shown in Fig 4(1), each blue outer bubble represents a data center\nand its interior white bubbles represent data clusters belonging to that\ncenter. According to the experts\u2019 feedback, abnormal instances are\nrelatively rare and they hoped the system could directly show them the\nmost likely anomalies. Thus, the spatial overview arranges data centers\naccording to their abnormal scores in descending order, from left to\nright and top to bottom. The bigger a bubble is, the larger its anomaly\nscore is. The calculation of the anomaly score is to sum the scores\nof all nodes in that center. In addition, the inner bubbles will only\nappear when their represented data clusters\u2019 sum of anomaly scores is\nlarger than the human-set threshold. Both the number of data centers\nand the threshold value for data clusters can be set through two sliders\n(Fig. 4(a)). Based on the displayed information, users can query the\ndata from a speci\ufb01c data center and cluster in two ways: (1) clicking\non a white bubble of interest (the tooltip will show the name of the\ncorresponding data center and cluster) to select the tracking records\nfrom its represented data cluster; (2) inputting a speci\ufb01c data center\nand data cluster ID by using the input boxes.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "Bubble", "axial_code": [], "componenet_code": ["bubble"]}, {"solution_text": "Query and Filtering. Users can load different\nsubsets of data by using the query module or direct interaction on\nthe views in Fig. 4(1, 2), as well as focus on a specific metric in the\ntemporal overview by clicking the legend on Fig. 4(b) (T1).", "solution_category": "interaction", "solution_axial": "Selecting", "solution_compoent": "", "axial_code": ["Selecting"], "componenet_code": ["selecting"]}]}, {"author": "gsh", "index_original": 77, "paper_title": "CloudDet: Interactive Visual Analysis of Anomalous Performances in Cloud Computing Systems", "pub_year": 2020, "domain": "Cloud computing; Anomalous Performances", "requirement": {"requirement_text": "T2 Rank the suspicious computing nodes dynamically. To reduce the search effort for suspicious nodes, visualization should be designed to aid the searching and filtering of anomalous perfor- mances by ranking the nodes whilst preserving the time context, e.g., displaying the periodic pattern of the operating activities.", "requirement_code": {"data_filtering": 1}}, "data": {"data_text": "The performance data tracked in cloud computing systems are rep-\nresented by multivariate time series.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Periodicity Detection. As shown in Fig. 3(3), the first step is to\ndetect the structural periodic changes and estimate the periodicity for\neach piece of historical data, which is an important parameter for the\nfollowing steps.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The historical data is further split into three\ncomponents by the seasonal-trend decomposition procedure based on\nloess (STL) [18], which is aimed to extract three temporal components\nfor the anomaly score calculation.", "solution_category": "data_manipulation", "solution_axial": "Wrangling", "solution_compoent": "", "axial_code": ["Wrangling"], "componenet_code": ["wrangling"]}, {"solution_text": "After getting the estimated period value (Tn) for each historical data\nand the numeric value (Sn, Trn and Rn) for the aforementioed three\ncomponents, the respective anomaly scores (ASs) of three patterns are\ncalculated in the anomaly scoring process based on their following\ndetectors (as shown in Fig. 3(4))", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The rank view in Fig. 4(3) shows a list of compute nodes with high anomaly scores within the user-specified time period in the temporal view, which reduces users efforts in searching for suspicious nodes. To rank the nodes, we sum the anomaly scores of different metrics (equation (7)) for each node at every timestamp, and the larger the sum, the more abnormal the node. Each chart in this view represents one compute node and is placed in the increasing order of anomaly ranks, which consists of three components: an information card, an anomaly calendar view and a line connecting the rank view with the performance view. First, the information card employs a bar chart showing the aver- age anomaly degree of different performance metrics. When clicking on this chart, the card will flip and provide textual information about the node, such as the node ID and the node rank. Second, the anomaly calendar view depicts the temporal pattern, especially the potential periodic patterns of a node. Each cell in this view represents one time unit according to the selected time granularity (minute, hour, day). The color, ranging from white to grey, encodes the sum of different metrics' anomaly score on one time unit, for example, it gets darker as the anomaly score increases. Various interaction techniques extend the functionality. Users can modify the arrangement of each calendar via tuning the slider bar for each node in Fig. 4(c). Thus potential period- icity of different nodes may be observed. Finally, the lines between the rank view and the performance view establish a correspondence between the general information and the detailed performance metrics of each node. With the buttons on the right of Fig. 4(d), the user can inspect different nodes according to their rankings (T2).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "bar_chart+heat_map+matrix", "axial_code": [], "componenet_code": ["bar", "heatmap", "matrix"]}]}, {"author": "gsh", "index_original": 78, "paper_title": "CloudDet: Interactive Visual Analysis of Anomalous Performances in Cloud Computing Systems", "pub_year": 2020, "domain": "Cloud computing; Anomalous Performances", "requirement": {"requirement_text": "T3 Browse the data flexibly in multiple ways. Despite the impor- tance of the anomaly scores and ranks, the raw performance data that contain different metrics are of most concern for experts to identify an anomaly case. Therefore, rich interaction techniques should be designed to enable an efficient mechanism to explore the performance data and extract their anomalous patterns.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The performance data tracked in cloud computing systems are rep-\nresented by multivariate time series.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Periodicity Detection. As shown in Fig. 3(3), the first step is to\ndetect the structural periodic changes and estimate the periodicity for\neach piece of historical data, which is an important parameter for the\nfollowing steps.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The historical data is further split into three\ncomponents by the seasonal-trend decomposition procedure based on\nloess (STL) [18], which is aimed to extract three temporal components\nfor the anomaly score calculation.", "solution_category": "data_manipulation", "solution_axial": "Wrangling", "solution_compoent": "", "axial_code": ["Wrangling"], "componenet_code": ["wrangling"]}, {"solution_text": "After getting the estimated period value (Tn) for each historical data\nand the numeric value (Sn, Trn and Rn) for the aforementioed three\ncomponents, the respective anomaly scores (ASs) of three patterns are\ncalculated in the anomaly scoring process based on their following\ndetectors (as shown in Fig. 3(4))", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Horizon Chart Mode. The horizon chart uses a space-efficient method to clearly show the overall trend of each performance metric and improve the comparison among different metrics over time. Each layer represents one metric in this view. Fig 5(1, 2) illustrates the construction of the standard horizon chart, which is extended from a normal line chart that displays the metric value changes over time. Specifically, we offset the negative values such that the zero point for the negative values is at the top of the chart. The metric value is indicated by a dichotomous color scheme ranging from green to white to red (Fig. 5(3)), with green/red encoding a value higher/lower than the average (white). The average value for a given metric is calculated by considering all the nodes in the data cluster. We can interpret the horizon chart with the following logic: (a) use the color to decide whether the data is positive or negative; (b) observe the darkest color to decide the general range of the data; (c) follow the height of the darkest part to understand the temporal trend of the metric value; and (d) read the textual information to learn the real value of the corresponding metric, and the dominant reason (three causes mentioned in Section 4.2) for the anomalies (T3\u20134).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area+stacked_bar+line", "axial_code": [], "componenet_code": ["bar", "line", "area"]}]}, {"author": "gsh", "index_original": 79, "paper_title": "CloudDet: Interactive Visual Analysis of Anomalous Performances in Cloud Computing Systems", "pub_year": 2020, "domain": "Cloud computing; Anomalous Performances", "requirement": {"requirement_text": "T4 Facilitate anomaly detection and interpretation. The visualiza- tion design of the system should consider the combination of the numeric anomaly detection results with the performance metric data. The visualization and interaction should enable users to make comparisons and display correlations over performance metrics at different levels from data summarization to focused context.", "requirement_code": {"discover_observation": 1, "compare_entities": 1, "explain_differences": 1}}, "data": {"data_text": "The performance data tracked in cloud computing systems are rep-\nresented by multivariate time series.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Periodicity Detection. As shown in Fig. 3(4), the first step is to\ndetect the structural periodic changes and estimate the periodicity for\neach piece of historical data, which is an important parameter for the\nfollowing steps.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The historical data is further split into three\ncomponents by the seasonal-trend decomposition procedure based on\nloess (STL) [19], which is aimed to extract three temporal components\nfor the anomaly score calculation.", "solution_category": "data_manipulation", "solution_axial": "Wrangling", "solution_compoent": "", "axial_code": ["Wrangling"], "componenet_code": ["wrangling"]}, {"solution_text": "After getting the estimated period value (Tn) for each historical data\nand the numeric value (Sn, Trn and Rn) for the aforementioed three\ncomponents, the respective anomaly scores (ASs) of three patterns are\ncalculated in the anomaly scoring process based on their following\ndetectors (as shown in Fig. 3(5))", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Horizon Chart Mode. The horizon chart uses a space-efficient method to clearly show the overall trend of each performance metric and improve the comparison among different metrics over time. Each layer represents one metric in this view. Fig 5(1, 2) illustrates the construction of the standard horizon chart, which is extended from a normal line chart that displays the metric value changes over time. Specifically, we offset the negative values such that the zero point for the negative values is at the top of the chart. The metric value is indicated by a dichotomous color scheme ranging from green to white to red (Fig. 5(3)), with green/red encoding a value higher/lower than the average (white). The average value for a given metric is calculated by considering all the nodes in the data cluster. We can interpret the horizon chart with the following logic: (a) use the color to decide whether the data is positive or negative; (b) observe the darkest color to decide the general range of the data; (c) follow the height of the darkest part to understand the temporal trend of the metric value; and (d) read the textual information to learn the real value of the corresponding metric, and the dominant reason (three causes mentioned in Section 4.2) for the anomalies (T3\u20134).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area+stacked_bar+line", "axial_code": [], "componenet_code": ["bar", "line", "area"]}]}, {"author": "gsh", "index_original": 80, "paper_title": "CloudDet: Interactive Visual Analysis of Anomalous Performances in Cloud Computing Systems", "pub_year": 2020, "domain": "Cloud computing; Anomalous Performances", "requirement": {"requirement_text": "T4 Facilitate anomaly detection and interpretation. The visualiza- tion design of the system should consider the combination of the numeric anomaly detection results with the performance metric data. The visualization and interaction should enable users to make comparisons and display correlations over performance metrics at different levels from data summarization to focused context.", "requirement_code": {"discover_observation": 1, "compare_entities": 1, "explain_differences": 1}}, "data": {"data_text": "The performance data tracked in cloud computing systems are rep-\nresented by multivariate time series.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Periodicity Detection. As shown in Fig. 3(4), the first step is to\ndetect the structural periodic changes and estimate the periodicity for\neach piece of historical data, which is an important parameter for the\nfollowing steps.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The historical data is further split into three\ncomponents by the seasonal-trend decomposition procedure based on\nloess (STL) [19], which is aimed to extract three temporal components\nfor the anomaly score calculation.", "solution_category": "data_manipulation", "solution_axial": "Wrangling", "solution_compoent": "", "axial_code": ["Wrangling"], "componenet_code": ["wrangling"]}, {"solution_text": "After getting the estimated period value (Tn) for each historical data\nand the numeric value (Sn, Trn and Rn) for the aforementioed three\ncomponents, the respective anomaly scores (ASs) of three patterns are\ncalculated in the anomaly scoring process based on their following\ndetectors (as shown in Fig. 3(5))", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "Multi-line Mode. A multi-line chart is provided as a more con- ventional and familiar visualization type for users, compared with the horizon chart, to facilitate their understanding of the data. By clicking the \u201cline\u201d button shown in Fig. 4(d), the mode of the performance view will change to the multi-line mode (Fig. 4(6)), where each line presents one metric and the same categorical colors used in our system are applied. Also, each attribute's data is normalized and scaled to the range of [-1,1] because we need to make the units for different attributes consistent with the same y-axis (T4).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area+stacked_bar+line", "axial_code": [], "componenet_code": ["bar", "line", "area"]}]}, {"author": "gsh", "index_original": 81, "paper_title": "CloudDet: Interactive Visual Analysis of Anomalous Performances in Cloud Computing Systems", "pub_year": 2020, "domain": "Cloud computing; Anomalous Performances", "requirement": {"requirement_text": "T4 Facilitate anomaly detection and interpretation. The visualiza- tion design of the system should consider the combination of the numeric anomaly detection results with the performance metric data. The visualization and interaction should enable users to make comparisons and display correlations over performance metrics at different levels from data summarization to focused context.", "requirement_code": {"discover_observation": 1, "compare_entities": 1, "explain_differences": 1}}, "data": {"data_text": "The performance data tracked in cloud computing systems are rep-\nresented by multivariate time series.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Periodicity Detection. As shown in Fig. 3(4), the first step is to\ndetect the structural periodic changes and estimate the periodicity for\neach piece of historical data, which is an important parameter for the\nfollowing steps.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The historical data is further split into three\ncomponents by the seasonal-trend decomposition procedure based on\nloess (STL) [19], which is aimed to extract three temporal components\nfor the anomaly score calculation.", "solution_category": "data_manipulation", "solution_axial": "Wrangling", "solution_compoent": "", "axial_code": ["Wrangling"], "componenet_code": ["wrangling"]}, {"solution_text": "After getting the estimated period value (Tn) for each historical data\nand the numeric value (Sn, Trn and Rn) for the aforementioed three\ncomponents, the respective anomaly scores (ASs) of three patterns are\ncalculated in the anomaly scoring process based on their following\ndetectors (as shown in Fig. 3(5))", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "PCA Mode. The PCA (principal component analysis) view depicts the major trends of a node's performance (Fig. 4(7)). The node's performance data, which is a multivariate time series with several metrics (e.g., CPU frequency and memory usage), are projected to a one-dimensional time series with PCA analysis. Thereby, we can reduce the number of variables and make important trends in the data directly accessible, providing qualitative high-level insights for cloud computing performance and anomaly inspection [3] (T4).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "area+stacked_bar+line", "axial_code": [], "componenet_code": ["bar", "line", "area"]}]}, {"author": "gsh", "index_original": 82, "paper_title": "CloudDet: Interactive Visual Analysis of Anomalous Performances in Cloud Computing Systems", "pub_year": 2020, "domain": "Cloud computing; Anomalous Performances", "requirement": {"requirement_text": "T5 Display the similarity of different computing nodes. In addition to displaying the temporal patterns, another key to understanding the anomalous cloud computing performance is to differentiate anomalous compute nodes from normal ones. To this end, the system should show the clustering of the nodes based on their sim- ilarities, revealing some common features that led to the anomaly.", "requirement_code": {"discover_observation": 1, "compare_entities": 1, "explain_differences": 1}}, "data": {"data_text": "The performance data tracked in cloud computing systems are rep-\nresented by multivariate time series.", "data_code": {"quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Periodicity Detection. As shown in Fig. 3(4), the first step is to\ndetect the structural periodic changes and estimate the periodicity for\neach piece of historical data, which is an important parameter for the\nfollowing steps.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The historical data is further split into three\ncomponents by the seasonal-trend decomposition procedure based on\nloess (STL) [19], which is aimed to extract three temporal components\nfor the anomaly score calculation.", "solution_category": "data_manipulation", "solution_axial": "Wrangling", "solution_compoent": "", "axial_code": ["Wrangling"], "componenet_code": ["wrangling"]}, {"solution_text": "After getting the estimated period value (Tn) for each historical data\nand the numeric value (Sn, Trn and Rn) for the aforementioed three\ncomponents, the respective anomaly scores (ASs) of three patterns are\ncalculated in the anomaly scoring process based on their following\ndetectors (as shown in Fig. 3(5))", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The temporal overview (Fig. 4(2)) aims at revealing an anomaly overview of the variation of all the important tracking metrics (e.g., CPU frequency and memory usage) over time, which provides users with a general impression of the whole data set in a temporal context. We present two types of charts, namely, an area chart and a stacked bar chart, to show the sum of all nodes's anomaly scores in terms of different metrics at every timestamp. The y-axis shows the anomaly score, and a categorical color scheme is used to represent different met- rics. Specifically, the area chart overlaps the different metrics; thus the most anomalous metric at every time point can be highlighted, and the inter-pattern comparison and correlation discovery of multiple metrics can be fulfilled (T5). By contrast, the stacked bar chart emphasizes the total amount of anomaly scores for all the performance metrics at every time point. To assist users in catching the time period of interest more efficiently, we mark the top five points of each performance metric with red dots for reference. Different interaction designs are provided in this view: (1) brushing a time period in the overview for further analysis; (2) switching the view mode, as well as filtering out some metrics or highlighting others (tuning the corresponding color opaque) (Fig. 4(b)); (3) choosing three types of time granularities, namely, minutes, hours and days, as an input parameter to show the performance history at different levels of detail (Fig. 4(2)).", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "bar+area", "axial_code": [], "componenet_code": ["bar", "area"]}]}, {"author": "gsh", "index_original": 83, "paper_title": "PlanningVis: A Visual Analytics Approach to Production Planning in Smart Factories", "pub_year": 2020, "domain": "Production Planning", "requirement": {"requirement_text": "R1 Visualize the optimization process of production planning. The visual design should present the summarized algorithm re- sults of production plans and the difference between two plans. The recorded optimization history can not only help users verify the effect of their manipulation but also provide an overview for planners to choose the best strategy.", "requirement_code": {"describe_observation_aggregate": 1, "discover_observation": 1, "evaluate_hypothesis": 1, "compare_entities": 1}}, "data": {"data_text": "Planners need to make a 30-day\nproduction plan every day based on a hybrid production planning al-\ngorithm [32], which takes the initial inventory of raw materials, the\nproduction capacity of factories [9], the arrangement of holidays,\nand the demand of products as the input. Their work is to assign daily\ntasks to each factory. A typical production task is that Factory A is\nrequired to produce n pieces of Product B on Day C.", "data_code": {"tables": 1, "ordinal": 1}}, "solution": [{"solution_text": "Then, we employ the weighted sum model (WSM) [37] to define\nthe multi-objective production planning problem. The WSM can be\nsolved by integer programming. ", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}]}, {"author": "gsh", "index_original": 84, "paper_title": "PlanningVis: A Visual Analytics Approach to Production Planning in Smart Factories", "pub_year": 2020, "domain": "Production Planning", "requirement": {"requirement_text": "R2 Show the distribution of all the products. Presenting the distri- bution of performance indicators for different products can reveal clusters and anomalies of products, which provides guidance for further exploration.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "distri- bution of performance indicators for different products", "data_code": {"quantitative": 1, "tables": 1}}, "solution": [{"solution_text": "We first compute the daily performance indicators of each product,\nincluding the order delay rate, the production cost, the inventory cost,\nand the smoothing rate of production capacity use. These performance\nindicators are used to evaluate the production plan and identify products\nwith production problems. Also, we calculate the summarized statistics\nsuch as the mean and the variance of a product over 30 days.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The product view contains two components: a segmented parallel\ncoordinates plot to reveal potential clusters and anomalies of products,\nand the product glyph to give a detailed description of \ufb01ltered products.\nThe view can display the distribution of all the products (R2) and\nsupport \ufb01ltering and selecting for further exploration (R3). It provides\nmesoscopic information on the product level for comparative analysis\nof two plans (R8), and gives support to the improvement and simulation\nof production planning (R6, R7).\nThe segmented parallel coordinates plot. Since we use a special\nnegative value to represent the product with no demand (Section 4.2),\ntraditional parallel coordinates plots will create a large gap between the\nnormal values and the abnormal ones, and thus compress normal values\ninto a small area. To resolve this issue, we design a segmented parallel\ncoordinates plot. As illustrated in Fig. 1c1 , we extend the axis in the\ntraditional parallel coordinates plot to two rectangles. The four pairs\nof rectangles represent the four performance indicators, respectively.\nWithin each pair of rectangles, the upper part displays normal values\nwhile the lower part displays abnormal values. Each line in the plot\nrepresents a product. When the line passes through a rectangle, a red-\nand-blue color encoding of this line segment refers to the difference\nbetween the last plan and the current plan. The red color indicates an\nincrease and blue indicates a decrease. A gray triangle on the right side\nof the bar shows that the performance indicator of the current product\nhas an abnormal value in the last plan. Additionally, a brush on the\nrectangle will highlight the selected products and their product glyphs\nwill be displayed for further exploration.\nOn the left of the parallel coordinates plot, we also provide sliders\nwhich show the ranges of differences for users to \ufb01lter products. The\nuser can also search for products of interest.\nProduct glyph. In Fig. 3a, the product glyph depicts the selected\nproducts in detail. It has a circular shape and is tangentially partitioned\ninto four regions to present different performance indicators. The radius\nof the innermost sector encodes the value of the performance indicators,\nand the black line on it points out the average value of all products.\nThe angle of the arc in the middle shows the variance of the daily\nperformance indicator during the 30-day production planning period.\nWe also present the differences in the performance indicators between\nthe last plan and the current plan in the outer arc. The arc starts from\nthe center of each region, and the angle shows the difference. An arc\nextending clockwise with the same color scheme to the performance\nindicator expresses an increase, while one extending counterclockwise\nwith only a black and bold border expresses a decrease. A special case is\nthe data with prede\ufb01ned negative values (Section 4.2), which are shown\nin gray in all the visual elements. Additionally, we use a gray triangle\nat the outermost part of the glyph to indicate that the corresponding\nperformance indicator of the last plan is a special negative number. The\nuser can hover over the visual cues to see the numerical values and click\non the product glyph to explore the production details of this product.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "bar+PCP+glyph+arc+heat_map", "axial_code": [], "componenet_code": ["glyph", "parallelcoordinates", "heatmap", "bar", "arc"]}]}, {"author": "gsh", "index_original": 85, "paper_title": "PlanningVis: A Visual Analytics Approach to Production Planning in Smart Factories", "pub_year": 2020, "domain": "Production Planning", "requirement": {"requirement_text": "R3 Support filtering and selecting products of interest. The visu- alization system should support interactions to filter and select products with specific performance indicator values and present detailed information about the selected products.", "requirement_code": {"data_filtering": 1}}, "data": {"data_text": "distri- bution of performance indicators for different products", "data_code": {"quantitative": 1, "tables": 1}}, "solution": [{"solution_text": "We first compute the daily performance indicators of each product,\nincluding the order delay rate, the production cost, the inventory cost,\nand the smoothing rate of production capacity use. These performance\nindicators are used to evaluate the production plan and identify products\nwith production problems. Also, we calculate the summarized statistics\nsuch as the mean and the variance of a product over 30 days.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The product view contains two components: a segmented parallel\ncoordinates plot to reveal potential clusters and anomalies of products,\nand the product glyph to give a detailed description of \ufb01ltered products.\nThe view can display the distribution of all the products (R2) and\nsupport \ufb01ltering and selecting for further exploration (R3). It provides\nmesoscopic information on the product level for comparative analysis\nof two plans (R8), and gives support to the improvement and simulation\nof production planning (R6, R7).\nThe segmented parallel coordinates plot. Since we use a special\nnegative value to represent the product with no demand (Section 4.2),\ntraditional parallel coordinates plots will create a large gap between the\nnormal values and the abnormal ones, and thus compress normal values\ninto a small area. To resolve this issue, we design a segmented parallel\ncoordinates plot. As illustrated in Fig. 1c1 , we extend the axis in the\ntraditional parallel coordinates plot to two rectangles. The four pairs\nof rectangles represent the four performance indicators, respectively.\nWithin each pair of rectangles, the upper part displays normal values\nwhile the lower part displays abnormal values. Each line in the plot\nrepresents a product. When the line passes through a rectangle, a red-\nand-blue color encoding of this line segment refers to the difference\nbetween the last plan and the current plan. The red color indicates an\nincrease and blue indicates a decrease. A gray triangle on the right side\nof the bar shows that the performance indicator of the current product\nhas an abnormal value in the last plan. Additionally, a brush on the\nrectangle will highlight the selected products and their product glyphs\nwill be displayed for further exploration.\nOn the left of the parallel coordinates plot, we also provide sliders\nwhich show the ranges of differences for users to \ufb01lter products. The\nuser can also search for products of interest.\nProduct glyph. In Fig. 3a, the product glyph depicts the selected\nproducts in detail. It has a circular shape and is tangentially partitioned\ninto four regions to present different performance indicators. The radius\nof the innermost sector encodes the value of the performance indicators,\nand the black line on it points out the average value of all products.\nThe angle of the arc in the middle shows the variance of the daily\nperformance indicator during the 30-day production planning period.\nWe also present the differences in the performance indicators between\nthe last plan and the current plan in the outer arc. The arc starts from\nthe center of each region, and the angle shows the difference. An arc\nextending clockwise with the same color scheme to the performance\nindicator expresses an increase, while one extending counterclockwise\nwith only a black and bold border expresses a decrease. A special case is\nthe data with prede\ufb01ned negative values (Section 4.2), which are shown\nin gray in all the visual elements. Additionally, we use a gray triangle\nat the outermost part of the glyph to indicate that the corresponding\nperformance indicator of the last plan is a special negative number. The\nuser can hover over the visual cues to see the numerical values and click\non the product glyph to explore the production details of this product.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "bar+PCP+glyph+arc+heat_map", "axial_code": [], "componenet_code": ["glyph", "parallelcoordinates", "heatmap", "bar", "arc"]}, {"solution_text": "The view can display the distribution of all the products (R2) and\nsupport filtering and selecting for further exploration (R3)", "solution_category": "interaction", "solution_axial": "filtering+selecting", "solution_compoent": "", "axial_code": ["filtering", "selecting"], "componenet_code": ["filtering", "selecting"]}]}, {"author": "gsh", "index_original": 86, "paper_title": "PlanningVis: A Visual Analytics Approach to Production Planning in Smart Factories", "pub_year": 2020, "domain": "Production Planning", "requirement": {"requirement_text": "R4 Visualize the dependency among products. Due to the depen- dency among products in the BOM tree, the production of a parent product may be limited by the lack of child components. Showing the dependency relationship among products, along with their temporal supply/demand distribution allows problem diagnosis and production planning optimization.", "requirement_code": {"discover_observation": 1, "identify_main_cause_item": 1}}, "data": {"data_text": "distri- bution of performance indicators for different products", "data_code": {"quantitative": 1, "tables": 1}}, "solution": [{"solution_text": "The left side is a dependency tree visualization, which reveals the de-\npendency between products (R4).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "tree+point+heat_map", "axial_code": [], "componenet_code": ["scatter", "tree", "heatmap"]}]}, {"author": "gsh", "index_original": 87, "paper_title": "PlanningVis: A Visual Analytics Approach to Production Planning in Smart Factories", "pub_year": 2020, "domain": "Production Planning", "requirement": {"requirement_text": "R5 Present the production detail of a product in different facto- ries. The system should display the daily production output and production capacity use of each factory. The analysis of detailed production can disclose the workload of each plant and the reason for insufficient production.", "requirement_code": {"discover_observation": 1, "collect_evidence": 1, "identify_main_cause_item": 1, "describe_observation_item": 1}}, "data": {"data_text": "daily production information of the selected product\nin related factories", "data_code": {"ordinal": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The right side is extended bar charts,\nwhich visualize the daily production information of the selected product\nin related factories (R5). ", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "bar+heat_map+line", "axial_code": [], "componenet_code": ["bar", "heatmap", "line"]}]}, {"author": "gsh", "index_original": 88, "paper_title": "PlanningVis: A Visual Analytics Approach to Production Planning in Smart Factories", "pub_year": 2020, "domain": "Production Planning", "requirement": {"requirement_text": "R6 Enable interactive optimization of the production plan. The domain experts are eager for the support of visual interactions to improve production planning. To this end, our design should combine the automated algorithm and domain knowledge. The visual encoding should provide guidance for the manipulation and rapid feedback is needed to verify the effect.", "requirement_code": {"interactivity": 1}}, "data": {"data_text": "two types of configuration data: the daily demand of each\nproduct, and the available resources in each factory, including the\ninitial inventory of raw materials, the capacity sets and the holiday\narrangement.", "data_code": {"ordinal": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The control panel (Fig. 1a) aims at supporting visual manipulation on\nthe con\ufb01guration data of production planning to generate new plans.\nIt displays two types of con\ufb01guration data: the daily demand of each\nproduct, and the available resources in each factory, including the\ninitial inventory of raw materials, the capacity sets and the holiday\narrangement. Based on the interactions provided by the control panel,\nusers can leverage their knowledge to improve a production plan (R6)\nand simulate an unanticipated incident so that they can take measures\nin time to reduce the in\ufb02uence (R7).\nThe visual encoding of the con\ufb01guration data. The initial order\ndemand and the resource con\ufb01guration of the selected plant will be\nshown when users specify the start and end dates of production planning.\nThe order demand for a product is displayed by a line chart overlaid\nwith an area chart, where the horizontal axis represents the time and\nthe vertical axis encodes the value. Additionally, the small circles on\nthe line chart can be dragged to change the value. For the production\nresources in a plant, we encode the initial inventory of the raw material\nby a draggable bar chart and illustrate the capacity sets in a similar\nmanner to the order demand for products. Furthermore, the holiday\nschedule is represented by small triangles, where blue \ufb01lled triangles\nindicate holidays. Users can click an un\ufb01lled triangle to arrange a\nholiday or click a blue \ufb01lled one to cancel the holiday.\nAfter modifying the con\ufb01guration data, users can click the \u201cRun\u201d\nbutton to invoke the production planning algorithm, and the returned\nresult will be added to the plan overview.", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "Line+Bar", "axial_code": [], "componenet_code": ["line", "bar"]}, {"solution_text": "Based on the interactions provided by the control panel,\nusers can leverage their knowledge to improve a production plan (R6)", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "gsh", "index_original": 89, "paper_title": "PlanningVis: A Visual Analytics Approach to Production Planning in Smart Factories", "pub_year": 2020, "domain": "Production Planning", "requirement": {"requirement_text": "R7 Support a fast response to unanticipated incidents. A sudden change in the market and the plant may have an adverse influence on production planning. Revealing the influence and supporting a quick adjustment are critical to developing an efficient production plan.", "requirement_code": {"identify_main_cause_item": 1, "discover_observation": 1, "collect_evidence": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "two types of configuration data: the daily demand of each\nproduct, and the available resources in each factory, including the\ninitial inventory of raw materials, the capacity sets and the holiday\narrangement.", "data_code": {"ordinal": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "Based on the interactions provided by the control panel,\nusers can leverage their knowledge to improve a production plan (R6)\nand simulate an unanticipated incident so that they can take measures\nin time to reduce the influence (R7).", "solution_category": "visualization", "solution_axial": "basics", "solution_compoent": "Line+Bar", "axial_code": [], "componenet_code": ["line", "bar"]}, {"solution_text": "Based on the interactions provided by the control panel,\nusers can leverage their knowledge to improve a production plan (R6)", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "gsh", "index_original": 90, "paper_title": "PlanningVis: A Visual Analytics Approach to Production Planning in Smart Factories", "pub_year": 2020, "domain": "Production Planning", "requirement": {"requirement_text": "R7 Support a fast response to unanticipated incidents. A sudden change in the market and the plant may have an adverse influence on production planning. Revealing the influence and supporting a quick adjustment are critical to developing an efficient production plan.", "requirement_code": {"identify_main_cause_item": 1, "discover_observation": 1, "collect_evidence": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "Planners need to make a 30-day\nproduction plan every day based on a hybrid production planning al-\ngorithm [32], which takes the initial inventory of raw materials, the\nproduction capacity of factories [9], the arrangement of holidays,\nand the demand of products as the input. Their work is to assign daily\ntasks to each factory. A typical production task is that Factory A is\nrequired to produce n pieces of Product B on Day C.", "data_code": {"tables": 1, "ordinal": 1, "temporal": 1}}, "solution": [{"solution_text": "The plan overview utilizes timeline-based glyphs to present the sum-\nmarized information of various production plans and their differences.\nThe view can reveal the macroscopic impact of con\ufb01guration changes,\nincluding improving the plan and simulating unanticipated incidents in\nthe market or the plant (R6, R7). In addition, it displays the recorded\nplanning history which enables users to progressively optimize the\nplan (R1). The visual design is composed of plan glyphs and the links\nbetween them. For visual comparison [11], we adopt juxtaposition\nbetween plan glyphs and explicit encoding in links (R8).\nPlan glyph. The plan glyph encodes summarized algorithm con\ufb01g-\nuration data and performance indicators, as illustrated in Fig. 2a. The\nupper part is a bar chart that shows the four key performance indicators\nsuggested by the domain experts, including order delay rate (red), the\nproduction cost (blue), the inventory cost (green), and the smoothing\nrate of production capacity use (purple), where the color scheme is\nconsistent with other views. The light gray background in the bar\nchart indicates the maximum value among all the plans. The lower\npart uses light orange circles to represent four kinds of con\ufb01guration\ndata. These circles, from left to right, indicate the order demand, the\ninitial inventory of raw materials, the available production capacity,\nand the number of holidays, respectively. The radii of these circles\ndescribe values in disparate ranges. In practice, users may assume the\nproduction capacity is in\ufb01nite so that they can focus on the analysis\nof other production constraints. For this special case, we visualize the\nin\ufb01nite production capacity as a brown circle (the last plan glyph in Fig.\n1b). The upper part and the lower part are separated by a horizontal line\nto avoid the misunderstanding that a relationship between the vertically\naligned visual primitives exists.\nThe visual encoding of the links between plan glyphs. The links,\nas illustrated in Fig. 1b, connect two plans in the optimization history\nto display the difference between them. The triangles on the upper\npart of the link represent the changes of the four types of con\ufb01guration\ndata, which follow the same order as that of the plan glyph. The size\nof the triangle encodes the value while the orientation of the triangle\ndescribes the increase (up) or the decrease (down) of the value. A\ntriangle with a dashed border means that there is no change in this type\nof con\ufb01guration data. We use four horizontal lines on the lower part to\nshow the change in the performance indicators. The width of the lines\nencodes the value and the order is the same as that in the plan glyph.\nThe same color scheme is used to indicate a decrease while a gray line\nmeans an increase.\nWe enable users to hover over the visual cues to view the detailed\ninformation and delete a plan. Users can also click the plan glyph to\nchoose the last plan and the current plan for exploration and comparison.\nAn additional link will be shown at the bottom when the selected plans\nare not consecutive (Fig. 1b). After selecting the plans, the product\nview will give a detailed description of the products.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "glyph+net+bar+circle", "axial_code": [], "componenet_code": ["bar", "circle", "glyph"]}]}, {"author": "gsh", "index_original": 91, "paper_title": "PlanningVis: A Visual Analytics Approach to Production Planning in Smart Factories", "pub_year": 2020, "domain": "Production Planning", "requirement": {"requirement_text": "R7 Support a fast response to unanticipated incidents. A sudden change in the market and the plant may have an adverse influence on production planning. Revealing the influence and supporting a quick adjustment are critical to developing an efficient production plan.", "requirement_code": {"identify_main_cause_item": 1, "discover_observation": 1, "collect_evidence": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "distri- bution of performance indicators for different products", "data_code": {"quantitative": 1, "tables": 1}}, "solution": [{"solution_text": "The product view contains two components: a segmented parallel\ncoordinates plot to reveal potential clusters and anomalies of products,\nand the product glyph to give a detailed description of \ufb01ltered products.\nThe view can display the distribution of all the products (R2) and\nsupport \ufb01ltering and selecting for further exploration (R3). It provides\nmesoscopic information on the product level for comparative analysis\nof two plans (R8), and gives support to the improvement and simulation\nof production planning (R6, R7).\nThe segmented parallel coordinates plot. Since we use a special\nnegative value to represent the product with no demand (Section 4.2),\ntraditional parallel coordinates plots will create a large gap between the\nnormal values and the abnormal ones, and thus compress normal values\ninto a small area. To resolve this issue, we design a segmented parallel\ncoordinates plot. As illustrated in Fig. 1c1 , we extend the axis in the\ntraditional parallel coordinates plot to two rectangles. The four pairs\nof rectangles represent the four performance indicators, respectively.\nWithin each pair of rectangles, the upper part displays normal values\nwhile the lower part displays abnormal values. Each line in the plot\nrepresents a product. When the line passes through a rectangle, a red-\nand-blue color encoding of this line segment refers to the difference\nbetween the last plan and the current plan. The red color indicates an\nincrease and blue indicates a decrease. A gray triangle on the right side\nof the bar shows that the performance indicator of the current product\nhas an abnormal value in the last plan. Additionally, a brush on the\nrectangle will highlight the selected products and their product glyphs\nwill be displayed for further exploration.\nOn the left of the parallel coordinates plot, we also provide sliders\nwhich show the ranges of differences for users to \ufb01lter products. The\nuser can also search for products of interest.\nProduct glyph. In Fig. 3a, the product glyph depicts the selected\nproducts in detail. It has a circular shape and is tangentially partitioned\ninto four regions to present different performance indicators. The radius\nof the innermost sector encodes the value of the performance indicators,\nand the black line on it points out the average value of all products.\nThe angle of the arc in the middle shows the variance of the daily\nperformance indicator during the 30-day production planning period.\nWe also present the differences in the performance indicators between\nthe last plan and the current plan in the outer arc. The arc starts from\nthe center of each region, and the angle shows the difference. An arc\nextending clockwise with the same color scheme to the performance\nindicator expresses an increase, while one extending counterclockwise\nwith only a black and bold border expresses a decrease. A special case is\nthe data with prede\ufb01ned negative values (Section 4.2), which are shown\nin gray in all the visual elements. Additionally, we use a gray triangle\nat the outermost part of the glyph to indicate that the corresponding\nperformance indicator of the last plan is a special negative number. The\nuser can hover over the visual cues to see the numerical values and click\non the product glyph to explore the production details of this product.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "bar+PCP+glyph+arc+heat_map", "axial_code": [], "componenet_code": ["glyph", "parallelcoordinates", "heatmap", "bar", "arc"]}]}, {"author": "gsh", "index_original": 92, "paper_title": "PlanningVis: A Visual Analytics Approach to Production Planning in Smart Factories", "pub_year": 2020, "domain": "Production Planning", "requirement": {"requirement_text": "R7 Support a fast response to unanticipated incidents. A sudden change in the market and the plant may have an adverse influence on production planning. Revealing the influence and supporting a quick adjustment are critical to developing an efficient production plan.", "requirement_code": {"identify_main_cause_item": 1, "discover_observation": 1, "collect_evidence": 1, "evaluate_hypothesis": 1}}, "data": {"data_text": "distri- bution of performance indicators for different products", "data_code": {"tables": 1, "quantitative": 1, "clusters_and_sets_and_lists": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "The production detail view can be divided into the left and right parts.\nThe left side is a dependency tree visualization, which reveals the de-\npendency between products (R4). The right side is extended bar charts,\nwhich visualize the daily production information of the selected product\nin related factories (R5). The visual design discloses the microscopic\ndifferences between two production planning strategies (R8). It can\nfurther help guide the improvement of the production plan (R6) and\nreveal the impact of an unanticipated incident (R7).\nDependency tree. As illustrated in Fig. 1d1 , the dependency tree\ndescribes production dependency from the parent of the selected prod-\nuct to raw materials. The tree layout is generated by a depth-first search\n(DFS) algorithm [36] starting from the node of the selected product,\nwhere the vertical position of the product is ranked by the access order\nand the horizontal position is arranged by the depth of the node in the\ntree. The parent node of the selected product is placed at the top of the\ntree. The node with children can be folded and unfolded upon clicking,\nthus helping the user explore a large BOM tree. Each unfolded node\nis accompanied by a heatmap vertically aligned at the right side of the\nnode (Fig. 1d2 ). The heatmap contains two rows to reveal the rela-\ntionship between supply and demand. The upper row of the heatmap\nencodes the daily remaining inventory, while the lower row encodes the\ndaily delayed order. The value is represented as the color saturation in\nthe two rows. All the heatmaps are horizontally aligned. In this way, by\nviewing the tree with heatmaps, users can identify the impact caused by\nthe shortage of the child product to the parent one. When a user clicks\non the heatmap, the daily performance indicators and the production in\nrelated plants of the clicked product will be presented.\nThe visual encoding of daily production information. Fig. 1d 3\ndisplays the daily performance indicators, namely, the order delay\nrate, the production cost, the inventory cost, and the smoothing rate of\nproduction capacity use, from top to bottom. We encode the value of\nthe current plan with the height of the bar which follows the same color\nscheme used before. The black rectangular border is used to encode\nthe difference between the current plan and the last plan. The value of\nthe current plan is larger when the border is within the colored bar, and\nsmaller when the border is outside the bar. Bars will be changed to gray\nif the value is a special negative number we set before, which means\nthe raw data is missing (Section 4.2). Note that maybe only a part of\nthe bars representing the order delay rate is gray since the demand is\nzero on these days. There are only four bars for the smoothing rate of\nproduction capacity use because it is computed once a week. A dashed\nline is shown to point out where the value is zero. In Fig. 1d 4 , a line\nchart and a bar chart are utilized to show the daily production in a plant.\nThe clicked product in the dependency tree and the related plants are\nconnected by curves, whose width and color saturation represent the\ntotal production output in that plant. In each plant, the downward bars\nindicate the daily production output of the product and the upward bars\nindicate the use of the corresponding capacity set. The visual design\nis similar to that of the performance indicators. Here, the upward bars\nmay be changed to gray, which implies that the production capacity for\nthis product is infinite. Above the bar chart, a black line encodes the\ncapacity utilization rate of the last plan and a blue one encodes that of\nthe current plan.", "solution_category": "visualization", "solution_axial": "nesting", "solution_compoent": "tree+point+heat_map+bar+line", "axial_code": [], "componenet_code": ["tree", "heatmap", "line", "bar", "scatter"]}]}, {"author": "gsh", "index_original": 93, "paper_title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "pub_year": 2020, "domain": "Social Media", "requirement": {"requirement_text": "R1 Show similarities among accounts. This requirement is essen- tial for enabling users to explore different characteristics to cluster the accounts based on [4, 10, 11].", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The\nbenchmark dataset used in the testing was crawled from Twitter and\nprepared by [10], who published the data for research purposes", "data_code": {"geometry": 1, "tables": 1, "textual": 1, "categorical": 1, "temporal": 1}}, "solution": [{"solution_text": "The first functionality is the extraction of a set of features that\nrepresent Twitter accounts. We built on previous research to check\nthe types of features that are known to be useful in spambot detection\n[14,28,39]. We identified a set of fifty features, e.g. total tweet count,\naverage number of links, followers to following ratio, then generated\nfour representations of these features by aggregating them temporally\n(R2). All extracted features are listed in the supplementary materials.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extraction;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The second functionality performed by the back-end is the genera-\ntion of a two-dimensional representation of the accounts, appropriate\nfor clustering tweets in a 2D display (R1). This reduction is useful to\ncommunicate similarities between accounts to the user. VASSL uses the\nextracted time-independent features for this purpose. We incorporate\nfour different dimensionality reduction (DR) techniques: Kernel Princi-\nple component analysis (K-PCA) [31], Linear Discriminant Analysis\n(LDA) [26], Locally Linear Embedding (LLE) [30], and t-distributed\nStochastic Neighbor Embedding (t-SNE) [24]. The four DR techniques\nare included in order to increase the effectiveness of the dimensionality\nreduction results in different contexts. For example, if the task is to\nlabel a set of unlabeled data without any information about labeled data,\nsupervised DR methods such as LDA may not produce good results,\nunlike PCA or LLE, which are unsupervised solutions. However, if a\nuser has already labeled parts of the data, she may utilize a supervised\nDR method for better class separation performance. Another factor that\nencourages our choice of DR techniques is the assumption of linearity.\nWe give users multiple options for reducing the dimension of feature\nspace using linear and non-linear mapping techniques.\nVASSL supports two transformation methods which can be incor-\nporated with the aforementioned dimensionality reduction methods:\nmin-max normalization and standardization. Min-max normalization\nchanges the range of the features and forces it to the range from 0 to\n1. Standardization transforms the values to Z-scores. Such transforma-\ntions are needed for some of the dimensionality reduction techniques.\nFor example, PCA is known to be sensitive to differences in features\nvariance and thus may performs badly if applied before normalization.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "back-end;two-dimensional;reduction;", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The third functionality of the back-end is topic modeling, which is\nperformed by utilizing the Latent Dirichlet Allocation (LDA) model\n[3]. We use the generated topics as a way to cluster accounts (R1,\nR4), as explained in Section 5.4. VASSL employs multiple natural\nlanguage processing techniques, such as lemmatization and stemming\npreprocessing, to transform the set of tweets for each account to tokens\nin a form suitable for LDA. The system then applies LDA to the set of\ntokens and generates a set of topics that best represent accounts\u2019 tweets.\nTo increase the accuracy of the latent topics, the system applies LDA\nto each temporal aggregation level mentioned above.\nOne of the challenges we faced during VASSL development was\nthe data scale. To discover anomalous botnet spammers, analyzing\nlarge numbers of accounts simultaneously is an ideal way to reveal\npatterns. However, the size of the data handled increases exponentially\nwith the number of accounts, because we need to consider multiple\nrepresentations of each account. To overcome this issue, the back-end\nkeeps a communication channel open with connected clients, gradually\nfeeding data. This channel is a querying mechanism between the front-\nend and the back-end, which only sends data that is visible in the\nviews of the front-end and prepares the remaining in the back-end.\nThis reduces the problem of information overload and creates a better\nanalysis experience for the users.\nThe communication channel also facilitates modification of the be-\nhavior of automatic data analysis techniques according to users\u2019 input.\nFor example, users are able to change the parameters of dimensionality\nreduction and topic modeling techniques from the front-end.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "back-end;topic_model", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The timeline view visualizes the distribution of time series features that represent Twitter accounts at three different aggregation levels. Influenced by [41], we choose to use the box plot for its simplicity yet capability of visualizing complex multivariate data, such as our time series. The design of the visualization combines a bubble chart and a box plot to enable the user to select individual accounts, while observing class statistics (R1). Accounts are visualized in this view as points in temporally sorted facets. Each account has a representation in each facet to communicate changes in time. The accounts are grouped in a facet according to assigned labels into genuine, spambot, or unlabeled, which are the three levels of the x-axis of the facet. These groups are color-coded as green, purple and blue respectively. The y-axis of the timeline view represents one or more time series such as the total tweet count and the average number of hashtags in tweets, depending on user selection. The orange boxes on top of each facet are temporal selectors which can be used in temporal zooming interaction as explained below.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Box_plot", "axial_code": [], "componenet_code": ["boxplot"]}, {"solution_text": "The timeline view visualizes the distribution of time series features that represent Twitter accounts at three different aggregation levels. Influenced by [41], we choose to use the box plot for its simplicity yet capability of visualizing complex multivariate data, such as our time series. The design of the visualization combines a bubble chart and a box plot to enable the user to select individual accounts, while observing class statistics (R1). Accounts are visualized in this view as points in temporally sorted facets. Each account has a representation in each facet to communicate changes in time. The accounts are grouped in a facet according to assigned labels into genuine, spambot, or unlabeled, which are the three levels of the x-axis of the facet. These groups are color-coded as green, purple and blue respectively. The y-axis of the timeline view represents one or more time series such as the total tweet count and the average number of hashtags in tweets, depending on user selection. The orange boxes on top of each facet are temporal selectors which can be used in temporal zooming interaction as explained below.", "solution_category": "data_manipulation", "solution_axial": "Rectification;ParameterTuning", "solution_compoent": "", "axial_code": ["ParameterTuning", "Rectification"], "componenet_code": ["parameter_tuning", "rectification"]}, {"solution_text": "The timeline view visualizes the distribution of time series features that represent Twitter accounts at three different aggregation levels. Influenced by [41], we choose to use the box plot for its simplicity yet capability of visualizing complex multivariate data, such as our time series. The design of the visualization combines a bubble chart and a box plot to enable the user to select individual accounts, while observing class statistics (R1). Accounts are visualized in this view as points in temporally sorted facets. Each account has a representation in each facet to communicate changes in time. The accounts are grouped in a facet according to assigned labels into genuine, spambot, or unlabeled, which are the three levels of the x-axis of the facet. These groups are color-coded as green, purple and blue respectively. The y-axis of the timeline view represents one or more time series such as the total tweet count and the average number of hashtags in tweets, depending on user selection. The orange boxes on top of each facet are temporal selectors which can be used in temporal zooming interaction as explained below.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Box_plot", "axial_code": [], "componenet_code": ["boxplot"]}]}, {"author": "gsh", "index_original": 94, "paper_title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "pub_year": 2020, "domain": "Social Media", "requirement": {"requirement_text": "R1 Show similarities among accounts. This requirement is essen- tial for enabling users to explore different characteristics to cluster the accounts based on [4, 10, 11].", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The\nbenchmark dataset used in the testing was crawled from Twitter and\nprepared by [10], who published the data for research purposes", "data_code": {"geometry": 1, "tables": 1, "textual": 1, "categorical": 1, "temporal": 1}}, "solution": [{"solution_text": "The first functionality is the extraction of a set of features that\nrepresent Twitter accounts. We built on previous research to check\nthe types of features that are known to be useful in spambot detection\n[14,28,39]. We identified a set of fifty features, e.g. total tweet count,\naverage number of links, followers to following ratio, then generated\nfour representations of these features by aggregating them temporally\n(R2). All extracted features are listed in the supplementary materials.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extraction;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The second functionality performed by the back-end is the genera-\ntion of a two-dimensional representation of the accounts, appropriate\nfor clustering tweets in a 2D display (R1). This reduction is useful to\ncommunicate similarities between accounts to the user. VASSL uses the\nextracted time-independent features for this purpose. We incorporate\nfour different dimensionality reduction (DR) techniques: Kernel Princi-\nple component analysis (K-PCA) [31], Linear Discriminant Analysis\n(LDA) [26], Locally Linear Embedding (LLE) [30], and t-distributed\nStochastic Neighbor Embedding (t-SNE) [24]. The four DR techniques\nare included in order to increase the effectiveness of the dimensionality\nreduction results in different contexts. For example, if the task is to\nlabel a set of unlabeled data without any information about labeled data,\nsupervised DR methods such as LDA may not produce good results,\nunlike PCA or LLE, which are unsupervised solutions. However, if a\nuser has already labeled parts of the data, she may utilize a supervised\nDR method for better class separation performance. Another factor that\nencourages our choice of DR techniques is the assumption of linearity.\nWe give users multiple options for reducing the dimension of feature\nspace using linear and non-linear mapping techniques.\nVASSL supports two transformation methods which can be incor-\nporated with the aforementioned dimensionality reduction methods:\nmin-max normalization and standardization. Min-max normalization\nchanges the range of the features and forces it to the range from 0 to\n1. Standardization transforms the values to Z-scores. Such transforma-\ntions are needed for some of the dimensionality reduction techniques.\nFor example, PCA is known to be sensitive to differences in features\nvariance and thus may performs badly if applied before normalization.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "back-end;two-dimensional;reduction;", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The third functionality of the back-end is topic modeling, which is\nperformed by utilizing the Latent Dirichlet Allocation (LDA) model\n[3]. We use the generated topics as a way to cluster accounts (R1,\nR4), as explained in Section 5.4. VASSL employs multiple natural\nlanguage processing techniques, such as lemmatization and stemming\npreprocessing, to transform the set of tweets for each account to tokens\nin a form suitable for LDA. The system then applies LDA to the set of\ntokens and generates a set of topics that best represent accounts\u2019 tweets.\nTo increase the accuracy of the latent topics, the system applies LDA\nto each temporal aggregation level mentioned above.\nOne of the challenges we faced during VASSL development was\nthe data scale. To discover anomalous botnet spammers, analyzing\nlarge numbers of accounts simultaneously is an ideal way to reveal\npatterns. However, the size of the data handled increases exponentially\nwith the number of accounts, because we need to consider multiple\nrepresentations of each account. To overcome this issue, the back-end\nkeeps a communication channel open with connected clients, gradually\nfeeding data. This channel is a querying mechanism between the front-\nend and the back-end, which only sends data that is visible in the\nviews of the front-end and prepares the remaining in the back-end.\nThis reduces the problem of information overload and creates a better\nanalysis experience for the users.\nThe communication channel also facilitates modification of the be-\nhavior of automatic data analysis techniques according to users\u2019 input.\nFor example, users are able to change the parameters of dimensionality\nreduction and topic modeling techniques from the front-end.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "back-end;topic_model", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "This topic clustering view uses two visualizations, which help analyze\nthe results of applying Latent Dirichlet Allocation (LDA) to accounts\u2019\ntweets. The \ufb01rst visualization is a bubble chart that represents the\ngenerated latent topics in a two-dimensional space (Fig. 1 (D)). The\naxes of this visualization can be chosen from the control panel to be\neither unique IDs, topics polarity, or topics subjectivity, which are\ncalculated by applying sentiment analysis to the topics\u2019 most probable\nwords. The size of the bubble encodes a score for each topic which\nrepresents the sum of probabilities of posting in that topic by all the\naccounts (see equation 1). The score of a topic Ti is calculated by\nsumming up the probability of that topic in all j documents D n, \u2200n \u2208{1, 2, . . . , j}. Documents in our analysis are accounts represented by\nthe concatenation of all their tweets.\nThe second visualization in the topic clustering view is a word cloud\nvisualization of the most frequent words in generated topics. LDA\ncomputes probabilities that show the distribution of these words in each\ntopic. VASSL utilizes these scores to determine the size of the words\nin the word cloud. When a user hovers over a topic, the word cloud\nchanges the size of the words according to their relevance scores, which\nhelp the user in exploring the semantics of the topics.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "word_cloud+scatter_plot", "axial_code": [], "componenet_code": ["wordcloud", "scatter"]}, {"solution_text": "Selecting a topic results in the selection of all accounts that have\nposted in that topic, with probability more than a threshold selected\nby the user (R1, R4). In other words, we can consider the topics as\nclusters where a single user can belong to more than one cluster (fuzzy\nclustering). Changing the threshold controls the sensitivity of cluster\nmembership. Increasing the threshold narrows the results to accounts\nthat post in a topic frequently, while reducing the threshold includes\naccounts that may rarely post in a topic", "solution_category": "interaction", "solution_axial": "selecting", "solution_compoent": "", "axial_code": ["selecting"], "componenet_code": ["selecting"]}]}, {"author": "gsh", "index_original": 95, "paper_title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "pub_year": 2020, "domain": "Social Media", "requirement": {"requirement_text": "R1 Show similarities among accounts. This requirement is essen- tial for enabling users to explore different characteristics to cluster the accounts based on [4, 10, 11].", "requirement_code": {"compare_entities": 1}}, "data": {"data_text": "The\nbenchmark dataset used in the testing was crawled from Twitter and\nprepared by [10], who published the data for research purposes", "data_code": {"geometry": 1, "tables": 1, "textual": 1, "categorical": 1, "temporal": 1}}, "solution": [{"solution_text": "The first functionality is the extraction of a set of features that\nrepresent Twitter accounts. We built on previous research to check\nthe types of features that are known to be useful in spambot detection\n[14,28,39]. We identified a set of fifty features, e.g. total tweet count,\naverage number of links, followers to following ratio, then generated\nfour representations of these features by aggregating them temporally\n(R2). All extracted features are listed in the supplementary materials.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extraction;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The second functionality performed by the back-end is the genera-\ntion of a two-dimensional representation of the accounts, appropriate\nfor clustering tweets in a 2D display (R1). This reduction is useful to\ncommunicate similarities between accounts to the user. VASSL uses the\nextracted time-independent features for this purpose. We incorporate\nfour different dimensionality reduction (DR) techniques: Kernel Princi-\nple component analysis (K-PCA) [31], Linear Discriminant Analysis\n(LDA) [26], Locally Linear Embedding (LLE) [30], and t-distributed\nStochastic Neighbor Embedding (t-SNE) [24]. The four DR techniques\nare included in order to increase the effectiveness of the dimensionality\nreduction results in different contexts. For example, if the task is to\nlabel a set of unlabeled data without any information about labeled data,\nsupervised DR methods such as LDA may not produce good results,\nunlike PCA or LLE, which are unsupervised solutions. However, if a\nuser has already labeled parts of the data, she may utilize a supervised\nDR method for better class separation performance. Another factor that\nencourages our choice of DR techniques is the assumption of linearity.\nWe give users multiple options for reducing the dimension of feature\nspace using linear and non-linear mapping techniques.\nVASSL supports two transformation methods which can be incor-\nporated with the aforementioned dimensionality reduction methods:\nmin-max normalization and standardization. Min-max normalization\nchanges the range of the features and forces it to the range from 0 to\n1. Standardization transforms the values to Z-scores. Such transforma-\ntions are needed for some of the dimensionality reduction techniques.\nFor example, PCA is known to be sensitive to differences in features\nvariance and thus may performs badly if applied before normalization.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "back-end;two-dimensional;reduction;", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The third functionality of the back-end is topic modeling, which is\nperformed by utilizing the Latent Dirichlet Allocation (LDA) model\n[3]. We use the generated topics as a way to cluster accounts (R1,\nR4), as explained in Section 5.4. VASSL employs multiple natural\nlanguage processing techniques, such as lemmatization and stemming\npreprocessing, to transform the set of tweets for each account to tokens\nin a form suitable for LDA. The system then applies LDA to the set of\ntokens and generates a set of topics that best represent accounts\u2019 tweets.\nTo increase the accuracy of the latent topics, the system applies LDA\nto each temporal aggregation level mentioned above.\nOne of the challenges we faced during VASSL development was\nthe data scale. To discover anomalous botnet spammers, analyzing\nlarge numbers of accounts simultaneously is an ideal way to reveal\npatterns. However, the size of the data handled increases exponentially\nwith the number of accounts, because we need to consider multiple\nrepresentations of each account. To overcome this issue, the back-end\nkeeps a communication channel open with connected clients, gradually\nfeeding data. This channel is a querying mechanism between the front-\nend and the back-end, which only sends data that is visible in the\nviews of the front-end and prepares the remaining in the back-end.\nThis reduces the problem of information overload and creates a better\nanalysis experience for the users.\nThe communication channel also facilitates modification of the be-\nhavior of automatic data analysis techniques according to users\u2019 input.\nFor example, users are able to change the parameters of dimensionality\nreduction and topic modeling techniques from the front-end.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "back-end;topic_model", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The Feature Explorer view visualizes the distribution of accounts in\nselected features using a new design based on a violin plot (see Fig.\n5). We used a violin plot instead of a box plot to enable the user to\nexamine multi-modality in any feature [18], which could indicate a\npotential cluster (R1, R4). Users can select as many features as needed\nin the feature explorer control panel, which contains a list of all features\n(see Fig. 6). The maximum number of features that can be visualized\nsimultaneously depends on screen size and human perception. Selecting\nfeatures divides the available visual space among the features, and thus\ncan reduce the capability of absorbing communicated information.\nThe feature explorer view shows statistical summaries for each\nfeature independently, such as median and quartiles of the overall\naccounts as well as labeled groups. Features are represented as multiple\nhorizontally adjacent facets, having the same Y-axis range in order to\ncorrelate the features. The accounts are represented as points in the\nhorizontal center of each facet. The vertical locations of the accounts in\na facet are determined by the value of the feature for these accounts. To\nreduce the visual clutter of the feature explorer view, the accounts point\nis transparent by default. Hovering over a class distribution increases\nthe opacity of the accounts that belong to the hovered class. The black\nsolid line in the horizontal center of each facet represents the 1st and\n3rd quartile of all accounts regardless of their class while the black\ntick mark represents the median of all accounts. Besides its job of\ncommunicating quartile information of all accounts, the solid black line\ndivides the facet into two areas. The area on the left of the solid line is\nused to indicate spambot and genuine class distributions (purple and\ngreen curves respectively) which are approximated by kernel density\nestimation (KDE) technique. The right area is used to communicate\nthe KDE of the unlabeled accounts distribution as well as selected\naccounts distribution (blue and red curves respectively). These curves\nare visualized in each facet in a similar manner to communicate this\nstatistical information in each feature (R4).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "violin_graph+point", "axial_code": [], "componenet_code": ["scatter", "violet_graph"]}]}, {"author": "gsh", "index_original": 96, "paper_title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "pub_year": 2020, "domain": "Social Media", "requirement": {"requirement_text": "R2 Represent accounts at different aggregation levels. Most of the features we found in the spambot detection literature can be considered time-series features. Examining these features at different temporal aggregation levels reveals different patterns which could help identify spambots [4].", "requirement_code": {"describe_observation_aggregate": 1, "discover_observation": 1}}, "data": {"data_text": "The\nbenchmark dataset used in the testing was crawled from Twitter and\nprepared by [10], who published the data for research purposes", "data_code": {"geometry": 1, "tables": 1, "textual": 1, "categorical": 1, "temporal": 1}}, "solution": [{"solution_text": "The first functionality is the extraction of a set of features that\nrepresent Twitter accounts. We built on previous research to check\nthe types of features that are known to be useful in spambot detection\n[14,28,39]. We identified a set of fifty features, e.g. total tweet count,\naverage number of links, followers to following ratio, then generated\nfour representations of these features by aggregating them temporally\n(R2). All extracted features are listed in the supplementary materials.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extraction;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The second functionality performed by the back-end is the genera-\ntion of a two-dimensional representation of the accounts, appropriate\nfor clustering tweets in a 2D display (R1). This reduction is useful to\ncommunicate similarities between accounts to the user. VASSL uses the\nextracted time-independent features for this purpose. We incorporate\nfour different dimensionality reduction (DR) techniques: Kernel Princi-\nple component analysis (K-PCA) [31], Linear Discriminant Analysis\n(LDA) [26], Locally Linear Embedding (LLE) [30], and t-distributed\nStochastic Neighbor Embedding (t-SNE) [24]. The four DR techniques\nare included in order to increase the effectiveness of the dimensionality\nreduction results in different contexts. For example, if the task is to\nlabel a set of unlabeled data without any information about labeled data,\nsupervised DR methods such as LDA may not produce good results,\nunlike PCA or LLE, which are unsupervised solutions. However, if a\nuser has already labeled parts of the data, she may utilize a supervised\nDR method for better class separation performance. Another factor that\nencourages our choice of DR techniques is the assumption of linearity.\nWe give users multiple options for reducing the dimension of feature\nspace using linear and non-linear mapping techniques.\nVASSL supports two transformation methods which can be incor-\nporated with the aforementioned dimensionality reduction methods:\nmin-max normalization and standardization. Min-max normalization\nchanges the range of the features and forces it to the range from 0 to\n1. Standardization transforms the values to Z-scores. Such transforma-\ntions are needed for some of the dimensionality reduction techniques.\nFor example, PCA is known to be sensitive to differences in features\nvariance and thus may performs badly if applied before normalization.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "back-end;two-dimensional;reduction;", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The third functionality of the back-end is topic modeling, which is\nperformed by utilizing the Latent Dirichlet Allocation (LDA) model\n[3]. We use the generated topics as a way to cluster accounts (R1,\nR4), as explained in Section 5.4. VASSL employs multiple natural\nlanguage processing techniques, such as lemmatization and stemming\npreprocessing, to transform the set of tweets for each account to tokens\nin a form suitable for LDA. The system then applies LDA to the set of\ntokens and generates a set of topics that best represent accounts\u2019 tweets.\nTo increase the accuracy of the latent topics, the system applies LDA\nto each temporal aggregation level mentioned above.\nOne of the challenges we faced during VASSL development was\nthe data scale. To discover anomalous botnet spammers, analyzing\nlarge numbers of accounts simultaneously is an ideal way to reveal\npatterns. However, the size of the data handled increases exponentially\nwith the number of accounts, because we need to consider multiple\nrepresentations of each account. To overcome this issue, the back-end\nkeeps a communication channel open with connected clients, gradually\nfeeding data. This channel is a querying mechanism between the front-\nend and the back-end, which only sends data that is visible in the\nviews of the front-end and prepares the remaining in the back-end.\nThis reduces the problem of information overload and creates a better\nanalysis experience for the users.\nThe communication channel also facilitates modification of the be-\nhavior of automatic data analysis techniques according to users\u2019 input.\nFor example, users are able to change the parameters of dimensionality\nreduction and topic modeling techniques from the front-end.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "back-end;topic_model", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The timeline view visualizes the distribution of time series features that represent Twitter accounts at three different aggregation levels. Influenced by [41], we choose to use the box plot for its simplicity yet capability of visualizing complex multivariate data, such as our time series. The design of the visualization combines a bubble chart and a box plot to enable the user to select individual accounts, while observing class statistics (R1). Accounts are visualized in this view as points in temporally sorted facets. Each account has a representation in each facet to communicate changes in time. The accounts are grouped in a facet according to assigned labels into genuine, spambot, or unlabeled, which are the three levels of the x-axis of the facet. These groups are color-coded as green, purple and blue respectively. The y-axis of the timeline view represents one or more time series such as the total tweet count and the average number of hashtags in tweets, depending on user selection. The orange boxes on top of each facet are temporal selectors which can be used in temporal zooming interaction as explained below.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Box_plot", "axial_code": [], "componenet_code": ["boxplot"]}, {"solution_text": "VASSL supports three main user interactions with the timeline view.\nHovering over the quartile boxes increases their transparency, which\nhelps users examine the underlying distribution of accounts underneath\nthe boxes. Users can also zoom in time by moving the mouse pointer\ninside a facet and scrolling up and down to zoom in and out. Zooming\nfunctionality changes the aggregation level of the time series to year,\nmonth, or day levels (R2, R3).", "solution_category": "interaction", "solution_axial": "Abstract/Elaborate", "solution_compoent": "", "axial_code": ["Abstract/Elaborate"], "componenet_code": ["abstract_elaborate"]}]}, {"author": "gsh", "index_original": 97, "paper_title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "pub_year": 2020, "domain": "Social Media", "requirement": {"requirement_text": "R2 Represent accounts at different aggregation levels. Most of the features we found in the spambot detection literature can be considered time-series features. Examining these features at different temporal aggregation levels reveals different patterns which could help identify spambots [4].", "requirement_code": {"describe_observation_aggregate": 1, "discover_observation": 1}}, "data": {"data_text": "The\nbenchmark dataset used in the testing was crawled from Twitter and\nprepared by [10], who published the data for research purposes", "data_code": {"geometry": 1, "tables": 1, "textual": 1, "categorical": 1, "temporal": 1}}, "solution": [{"solution_text": "The first functionality is the extraction of a set of features that\nrepresent Twitter accounts. We built on previous research to check\nthe types of features that are known to be useful in spambot detection\n[14,28,39]. We identified a set of fifty features, e.g. total tweet count,\naverage number of links, followers to following ratio, then generated\nfour representations of these features by aggregating them temporally\n(R2). All extracted features are listed in the supplementary materials.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extraction;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The second functionality performed by the back-end is the genera-\ntion of a two-dimensional representation of the accounts, appropriate\nfor clustering tweets in a 2D display (R1). This reduction is useful to\ncommunicate similarities between accounts to the user. VASSL uses the\nextracted time-independent features for this purpose. We incorporate\nfour different dimensionality reduction (DR) techniques: Kernel Princi-\nple component analysis (K-PCA) [31], Linear Discriminant Analysis\n(LDA) [26], Locally Linear Embedding (LLE) [30], and t-distributed\nStochastic Neighbor Embedding (t-SNE) [24]. The four DR techniques\nare included in order to increase the effectiveness of the dimensionality\nreduction results in different contexts. For example, if the task is to\nlabel a set of unlabeled data without any information about labeled data,\nsupervised DR methods such as LDA may not produce good results,\nunlike PCA or LLE, which are unsupervised solutions. However, if a\nuser has already labeled parts of the data, she may utilize a supervised\nDR method for better class separation performance. Another factor that\nencourages our choice of DR techniques is the assumption of linearity.\nWe give users multiple options for reducing the dimension of feature\nspace using linear and non-linear mapping techniques.\nVASSL supports two transformation methods which can be incor-\nporated with the aforementioned dimensionality reduction methods:\nmin-max normalization and standardization. Min-max normalization\nchanges the range of the features and forces it to the range from 0 to\n1. Standardization transforms the values to Z-scores. Such transforma-\ntions are needed for some of the dimensionality reduction techniques.\nFor example, PCA is known to be sensitive to differences in features\nvariance and thus may performs badly if applied before normalization.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "back-end;two-dimensional;reduction;", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The third functionality of the back-end is topic modeling, which is\nperformed by utilizing the Latent Dirichlet Allocation (LDA) model\n[3]. We use the generated topics as a way to cluster accounts (R1,\nR4), as explained in Section 5.4. VASSL employs multiple natural\nlanguage processing techniques, such as lemmatization and stemming\npreprocessing, to transform the set of tweets for each account to tokens\nin a form suitable for LDA. The system then applies LDA to the set of\ntokens and generates a set of topics that best represent accounts\u2019 tweets.\nTo increase the accuracy of the latent topics, the system applies LDA\nto each temporal aggregation level mentioned above.\nOne of the challenges we faced during VASSL development was\nthe data scale. To discover anomalous botnet spammers, analyzing\nlarge numbers of accounts simultaneously is an ideal way to reveal\npatterns. However, the size of the data handled increases exponentially\nwith the number of accounts, because we need to consider multiple\nrepresentations of each account. To overcome this issue, the back-end\nkeeps a communication channel open with connected clients, gradually\nfeeding data. This channel is a querying mechanism between the front-\nend and the back-end, which only sends data that is visible in the\nviews of the front-end and prepares the remaining in the back-end.\nThis reduces the problem of information overload and creates a better\nanalysis experience for the users.\nThe communication channel also facilitates modification of the be-\nhavior of automatic data analysis techniques according to users\u2019 input.\nFor example, users are able to change the parameters of dimensionality\nreduction and topic modeling techniques from the front-end.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "back-end;topic_model", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "This topic clustering view uses two visualizations, which help analyze\nthe results of applying Latent Dirichlet Allocation (LDA) to accounts\u2019\ntweets. The \ufb01rst visualization is a bubble chart that represents the\ngenerated latent topics in a two-dimensional space (Fig. 1 (D)). The\naxes of this visualization can be chosen from the control panel to be\neither unique IDs, topics polarity, or topics subjectivity, which are\ncalculated by applying sentiment analysis to the topics\u2019 most probable\nwords. The size of the bubble encodes a score for each topic which\nrepresents the sum of probabilities of posting in that topic by all the\naccounts (see equation 1). The score of a topic Ti is calculated by\nsumming up the probability of that topic in all j documents D n, \u2200n \u2208{1, 2, . . . , j}. Documents in our analysis are accounts represented by\nthe concatenation of all their tweets.\nThe second visualization in the topic clustering view is a word cloud\nvisualization of the most frequent words in generated topics. LDA\ncomputes probabilities that show the distribution of these words in each\ntopic. VASSL utilizes these scores to determine the size of the words\nin the word cloud. When a user hovers over a topic, the word cloud\nchanges the size of the words according to their relevance scores, which\nhelp the user in exploring the semantics of the topics.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "word_cloud+scatter_plot", "axial_code": [], "componenet_code": ["wordcloud", "scatter"]}, {"solution_text": "Topic modeling of tweets is performed with respect to the selected temporal resolution. The topic clustering view is linked to the timeline view. When a user zooms into a particular period using the timeline view, the topics clustering view updates the topics to match the selected period (R2). This enables users to examine the change of tweets topic in time. The trade-off of this flexibility lies in the efficiency of topic modeling; because of the massive size of the documents, the topic modeling procedure is not performed at interaction speed, producing lag times every time the hyperparameters are changed. However, by keeping the hyperparameters constant, the topic views are generated and cached in advance of interaction.", "solution_category": "interaction", "solution_axial": "Connect/Relate", "solution_compoent": "", "axial_code": ["Connect/Relate"], "componenet_code": ["connect_relate"]}]}, {"author": "gsh", "index_original": 98, "paper_title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "pub_year": 2020, "domain": "Social Media", "requirement": {"requirement_text": "R3 Summarize tweets' content and show details on demand. Ac- cording to Shniderman's visual information-seeking mantra [34], it is desirable to visualize the content summary of tweets to the users and show the details of the tweets as interactively requested.", "requirement_code": {"discover_observation": 1, "describe_observation_item": 1}}, "data": {"data_text": "The\nbenchmark dataset used in the testing was crawled from Twitter and\nprepared by [10], who published the data for research purposes", "data_code": {"geometry": 1, "tables": 1, "textual": 1, "categorical": 1, "temporal": 1}}, "solution": [{"solution_text": "The first functionality is the extraction of a set of features that\nrepresent Twitter accounts. We built on previous research to check\nthe types of features that are known to be useful in spambot detection\n[14,28,39]. We identified a set of fifty features, e.g. total tweet count,\naverage number of links, followers to following ratio, then generated\nfour representations of these features by aggregating them temporally\n(R2). All extracted features are listed in the supplementary materials.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extraction;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The second functionality performed by the back-end is the genera-\ntion of a two-dimensional representation of the accounts, appropriate\nfor clustering tweets in a 2D display (R1). This reduction is useful to\ncommunicate similarities between accounts to the user. VASSL uses the\nextracted time-independent features for this purpose. We incorporate\nfour different dimensionality reduction (DR) techniques: Kernel Princi-\nple component analysis (K-PCA) [31], Linear Discriminant Analysis\n(LDA) [26], Locally Linear Embedding (LLE) [30], and t-distributed\nStochastic Neighbor Embedding (t-SNE) [24]. The four DR techniques\nare included in order to increase the effectiveness of the dimensionality\nreduction results in different contexts. For example, if the task is to\nlabel a set of unlabeled data without any information about labeled data,\nsupervised DR methods such as LDA may not produce good results,\nunlike PCA or LLE, which are unsupervised solutions. However, if a\nuser has already labeled parts of the data, she may utilize a supervised\nDR method for better class separation performance. Another factor that\nencourages our choice of DR techniques is the assumption of linearity.\nWe give users multiple options for reducing the dimension of feature\nspace using linear and non-linear mapping techniques.\nVASSL supports two transformation methods which can be incor-\nporated with the aforementioned dimensionality reduction methods:\nmin-max normalization and standardization. Min-max normalization\nchanges the range of the features and forces it to the range from 0 to\n1. Standardization transforms the values to Z-scores. Such transforma-\ntions are needed for some of the dimensionality reduction techniques.\nFor example, PCA is known to be sensitive to differences in features\nvariance and thus may performs badly if applied before normalization.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "back-end;two-dimensional;reduction;", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The third functionality of the back-end is topic modeling, which is\nperformed by utilizing the Latent Dirichlet Allocation (LDA) model\n[3]. We use the generated topics as a way to cluster accounts (R1,\nR4), as explained in Section 5.4. VASSL employs multiple natural\nlanguage processing techniques, such as lemmatization and stemming\npreprocessing, to transform the set of tweets for each account to tokens\nin a form suitable for LDA. The system then applies LDA to the set of\ntokens and generates a set of topics that best represent accounts\u2019 tweets.\nTo increase the accuracy of the latent topics, the system applies LDA\nto each temporal aggregation level mentioned above.\nOne of the challenges we faced during VASSL development was\nthe data scale. To discover anomalous botnet spammers, analyzing\nlarge numbers of accounts simultaneously is an ideal way to reveal\npatterns. However, the size of the data handled increases exponentially\nwith the number of accounts, because we need to consider multiple\nrepresentations of each account. To overcome this issue, the back-end\nkeeps a communication channel open with connected clients, gradually\nfeeding data. This channel is a querying mechanism between the front-\nend and the back-end, which only sends data that is visible in the\nviews of the front-end and prepares the remaining in the back-end.\nThis reduces the problem of information overload and creates a better\nanalysis experience for the users.\nThe communication channel also facilitates modification of the be-\nhavior of automatic data analysis techniques according to users\u2019 input.\nFor example, users are able to change the parameters of dimensionality\nreduction and topic modeling techniques from the front-end.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "back-end;topic_model", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The timeline view visualizes the distribution of time series features that represent Twitter accounts at three different aggregation levels. Influenced by [41], we choose to use the box plot for its simplicity yet capability of visualizing complex multivariate data, such as our time series. The design of the visualization combines a bubble chart and a box plot to enable the user to select individual accounts, while observing class statistics (R1). Accounts are visualized in this view as points in temporally sorted facets. Each account has a representation in each facet to communicate changes in time. The accounts are grouped in a facet according to assigned labels into genuine, spambot, or unlabeled, which are the three levels of the x-axis of the facet. These groups are color-coded as green, purple and blue respectively. The y-axis of the timeline view represents one or more time series such as the total tweet count and the average number of hashtags in tweets, depending on user selection. The orange boxes on top of each facet are temporal selectors which can be used in temporal zooming interaction as explained below.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Box_plot", "axial_code": [], "componenet_code": ["boxplot"]}, {"solution_text": "VASSL supports three main user interactions with the timeline view.\nHovering over the quartile boxes increases their transparency, which\nhelps users examine the underlying distribution of accounts underneath\nthe boxes. Users can also zoom in time by moving the mouse pointer\ninside a facet and scrolling up and down to zoom in and out. Zooming\nfunctionality changes the aggregation level of the time series to year,\nmonth, or day levels (R2, R3).", "solution_category": "interaction", "solution_axial": "Abstract/Elaborate", "solution_compoent": "", "axial_code": ["Abstract/Elaborate"], "componenet_code": ["abstract_elaborate"]}]}, {"author": "gsh", "index_original": 99, "paper_title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "pub_year": 2020, "domain": "Social Media", "requirement": {"requirement_text": "R3 Summarize tweets' content and show details on demand. Ac- cording to Shniderman's visual information-seeking mantra [34], it is desirable to visualize the content summary of tweets to the users and show the details of the tweets as interactively requested.", "requirement_code": {"discover_observation": 1, "describe_observation_item": 1}}, "data": {"data_text": "The\nbenchmark dataset used in the testing was crawled from Twitter and\nprepared by [10], who published the data for research purposes", "data_code": {"geometry": 1, "tables": 1, "textual": 1, "categorical": 1, "temporal": 1}}, "solution": [{"solution_text": "The first functionality is the extraction of a set of features that\nrepresent Twitter accounts. We built on previous research to check\nthe types of features that are known to be useful in spambot detection\n[14,28,39]. We identified a set of fifty features, e.g. total tweet count,\naverage number of links, followers to following ratio, then generated\nfour representations of these features by aggregating them temporally\n(R2). All extracted features are listed in the supplementary materials.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extraction;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The second functionality performed by the back-end is the genera-\ntion of a two-dimensional representation of the accounts, appropriate\nfor clustering tweets in a 2D display (R1). This reduction is useful to\ncommunicate similarities between accounts to the user. VASSL uses the\nextracted time-independent features for this purpose. We incorporate\nfour different dimensionality reduction (DR) techniques: Kernel Princi-\nple component analysis (K-PCA) [31], Linear Discriminant Analysis\n(LDA) [26], Locally Linear Embedding (LLE) [30], and t-distributed\nStochastic Neighbor Embedding (t-SNE) [24]. The four DR techniques\nare included in order to increase the effectiveness of the dimensionality\nreduction results in different contexts. For example, if the task is to\nlabel a set of unlabeled data without any information about labeled data,\nsupervised DR methods such as LDA may not produce good results,\nunlike PCA or LLE, which are unsupervised solutions. However, if a\nuser has already labeled parts of the data, she may utilize a supervised\nDR method for better class separation performance. Another factor that\nencourages our choice of DR techniques is the assumption of linearity.\nWe give users multiple options for reducing the dimension of feature\nspace using linear and non-linear mapping techniques.\nVASSL supports two transformation methods which can be incor-\nporated with the aforementioned dimensionality reduction methods:\nmin-max normalization and standardization. Min-max normalization\nchanges the range of the features and forces it to the range from 0 to\n1. Standardization transforms the values to Z-scores. Such transforma-\ntions are needed for some of the dimensionality reduction techniques.\nFor example, PCA is known to be sensitive to differences in features\nvariance and thus may performs badly if applied before normalization.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "back-end;two-dimensional;reduction;", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The third functionality of the back-end is topic modeling, which is\nperformed by utilizing the Latent Dirichlet Allocation (LDA) model\n[3]. We use the generated topics as a way to cluster accounts (R1,\nR4), as explained in Section 5.4. VASSL employs multiple natural\nlanguage processing techniques, such as lemmatization and stemming\npreprocessing, to transform the set of tweets for each account to tokens\nin a form suitable for LDA. The system then applies LDA to the set of\ntokens and generates a set of topics that best represent accounts\u2019 tweets.\nTo increase the accuracy of the latent topics, the system applies LDA\nto each temporal aggregation level mentioned above.\nOne of the challenges we faced during VASSL development was\nthe data scale. To discover anomalous botnet spammers, analyzing\nlarge numbers of accounts simultaneously is an ideal way to reveal\npatterns. However, the size of the data handled increases exponentially\nwith the number of accounts, because we need to consider multiple\nrepresentations of each account. To overcome this issue, the back-end\nkeeps a communication channel open with connected clients, gradually\nfeeding data. This channel is a querying mechanism between the front-\nend and the back-end, which only sends data that is visible in the\nviews of the front-end and prepares the remaining in the back-end.\nThis reduces the problem of information overload and creates a better\nanalysis experience for the users.\nThe communication channel also facilitates modification of the be-\nhavior of automatic data analysis techniques according to users\u2019 input.\nFor example, users are able to change the parameters of dimensionality\nreduction and topic modeling techniques from the front-end.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "back-end;topic_model", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "Three tabs are used in the details view: accounts\u2019 cards view, tweets\nview, and tweets\u2019 word cloud view. The accounts\u2019 cards view shows\na list of accounts and other useful information, including accounts\u2019\nnames/screen names, pro\ufb01le images, joining date, total tweets, number\nof followers and followees, and the number of likes.\nUsers can select an account by clicking on its card (linked to other\nviews). Once an account or a set of accounts are selected, users can\naccess the tweets view, which shows the selected accounts\u2019 tweets in\nchronological order (R3). Including tweet text during the analysis is\nessential to utilize the human ability to detect automated text generation\n(as explained in Section 6. Instead of accessing the entire tweet text,\nusers can use the word cloud view to visualize selected tweets using the\nword cloud visualization technique [33]. The word cloud visualization\nis helpful for revealing repetitions of wording in selected tweets, which\ncan guide the exploration of tweet text and labeling (R3)", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Table", "axial_code": [], "componenet_code": ["table"]}, {"solution_text": "Three tabs are used in the details view: accounts\u2019 cards view, tweets\nview, and tweets\u2019 word cloud view. The accounts\u2019 cards view shows\na list of accounts and other useful information, including accounts\u2019\nnames/screen names, pro\ufb01le images, joining date, total tweets, number\nof followers and followees, and the number of likes.\nUsers can select an account by clicking on its card (linked to other\nviews). Once an account or a set of accounts are selected, users can\naccess the tweets view, which shows the selected accounts\u2019 tweets in\nchronological order (R3). Including tweet text during the analysis is\nessential to utilize the human ability to detect automated text generation\n(as explained in Section 6. Instead of accessing the entire tweet text,\nusers can use the word cloud view to visualize selected tweets using the\nword cloud visualization technique [33]. The word cloud visualization\nis helpful for revealing repetitions of wording in selected tweets, which\ncan guide the exploration of tweet text and labeling (R3)", "solution_category": "interaction", "solution_axial": "Overview_and_detail", "solution_compoent": "", "axial_code": ["Overview_and_detail"], "componenet_code": ["overview_and_explore"]}]}, {"author": "gsh", "index_original": 100, "paper_title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "pub_year": 2020, "domain": "Social Media", "requirement": {"requirement_text": "R4 Allow the user to highlight and analyze groups of accounts. The system should enable users to highlight and cluster accounts interactively. This interactive clustering is essential to reveal potential group-based spamming activities [10, 11].", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The\nbenchmark dataset used in the testing was crawled from Twitter and\nprepared by [10], who published the data for research purposes", "data_code": {"geometry": 1, "tables": 1, "textual": 1, "categorical": 1, "temporal": 1}}, "solution": [{"solution_text": "The first functionality is the extraction of a set of features that\nrepresent Twitter accounts. We built on previous research to check\nthe types of features that are known to be useful in spambot detection\n[14,28,39]. We identified a set of fifty features, e.g. total tweet count,\naverage number of links, followers to following ratio, then generated\nfour representations of these features by aggregating them temporally\n(R2). All extracted features are listed in the supplementary materials.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extraction;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The second functionality performed by the back-end is the genera-\ntion of a two-dimensional representation of the accounts, appropriate\nfor clustering tweets in a 2D display (R1). This reduction is useful to\ncommunicate similarities between accounts to the user. VASSL uses the\nextracted time-independent features for this purpose. We incorporate\nfour different dimensionality reduction (DR) techniques: Kernel Princi-\nple component analysis (K-PCA) [31], Linear Discriminant Analysis\n(LDA) [26], Locally Linear Embedding (LLE) [30], and t-distributed\nStochastic Neighbor Embedding (t-SNE) [24]. The four DR techniques\nare included in order to increase the effectiveness of the dimensionality\nreduction results in different contexts. For example, if the task is to\nlabel a set of unlabeled data without any information about labeled data,\nsupervised DR methods such as LDA may not produce good results,\nunlike PCA or LLE, which are unsupervised solutions. However, if a\nuser has already labeled parts of the data, she may utilize a supervised\nDR method for better class separation performance. Another factor that\nencourages our choice of DR techniques is the assumption of linearity.\nWe give users multiple options for reducing the dimension of feature\nspace using linear and non-linear mapping techniques.\nVASSL supports two transformation methods which can be incor-\nporated with the aforementioned dimensionality reduction methods:\nmin-max normalization and standardization. Min-max normalization\nchanges the range of the features and forces it to the range from 0 to\n1. Standardization transforms the values to Z-scores. Such transforma-\ntions are needed for some of the dimensionality reduction techniques.\nFor example, PCA is known to be sensitive to differences in features\nvariance and thus may performs badly if applied before normalization.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "back-end;two-dimensional;reduction;", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The third functionality of the back-end is topic modeling, which is\nperformed by utilizing the Latent Dirichlet Allocation (LDA) model\n[3]. We use the generated topics as a way to cluster accounts (R1,\nR4), as explained in Section 5.4. VASSL employs multiple natural\nlanguage processing techniques, such as lemmatization and stemming\npreprocessing, to transform the set of tweets for each account to tokens\nin a form suitable for LDA. The system then applies LDA to the set of\ntokens and generates a set of topics that best represent accounts\u2019 tweets.\nTo increase the accuracy of the latent topics, the system applies LDA\nto each temporal aggregation level mentioned above.\nOne of the challenges we faced during VASSL development was\nthe data scale. To discover anomalous botnet spammers, analyzing\nlarge numbers of accounts simultaneously is an ideal way to reveal\npatterns. However, the size of the data handled increases exponentially\nwith the number of accounts, because we need to consider multiple\nrepresentations of each account. To overcome this issue, the back-end\nkeeps a communication channel open with connected clients, gradually\nfeeding data. This channel is a querying mechanism between the front-\nend and the back-end, which only sends data that is visible in the\nviews of the front-end and prepares the remaining in the back-end.\nThis reduces the problem of information overload and creates a better\nanalysis experience for the users.\nThe communication channel also facilitates modification of the be-\nhavior of automatic data analysis techniques according to users\u2019 input.\nFor example, users are able to change the parameters of dimensionality\nreduction and topic modeling techniques from the front-end.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "back-end;topic_model", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The timeline view visualizes the distribution of time series features that represent Twitter accounts at three different aggregation levels. Influenced by [41], we choose to use the box plot for its simplicity yet capability of visualizing complex multivariate data, such as our time series. The design of the visualization combines a bubble chart and a box plot to enable the user to select individual accounts, while observing class statistics (R1). Accounts are visualized in this view as points in temporally sorted facets. Each account has a representation in each facet to communicate changes in time. The accounts are grouped in a facet according to assigned labels into genuine, spambot, or unlabeled, which are the three levels of the x-axis of the facet. These groups are color-coded as green, purple and blue respectively. The y-axis of the timeline view represents one or more time series such as the total tweet count and the average number of hashtags in tweets, depending on user selection. The orange boxes on top of each facet are temporal selectors which can be used in temporal zooming interaction as explained below.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "Box_plot", "axial_code": [], "componenet_code": ["boxplot"]}, {"solution_text": "The last interaction supported in the timeline view is the account\nselection. Selected accounts are highlighted using red color. Users can\nclick on the points representing the accounts to select and highlight\nthe accounts. Users can also select accounts by brushing on any facet\nto select accounts that overlap with the brush. VASSL highlights\nselected accounts in every time facet as well as in all other views.\nLinking time series allows users to examine trends and anomalies for\nselected accounts over time. Moreover, linking the views allows users\nto examine different information about the selected accounts such as\ntheir tweets, their position in the feature space, etc (R4).", "solution_category": "interaction", "solution_axial": "selecting", "solution_compoent": "", "axial_code": ["selecting"], "componenet_code": ["selecting"]}]}, {"author": "gsh", "index_original": 101, "paper_title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "pub_year": 2020, "domain": "Social Media", "requirement": {"requirement_text": "R4 Allow the user to highlight and analyze groups of accounts. The system should enable users to highlight and cluster accounts interactively. This interactive clustering is essential to reveal potential group-based spamming activities [10, 11].", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The\nbenchmark dataset used in the testing was crawled from Twitter and\nprepared by [10], who published the data for research purposes", "data_code": {"geometry": 1, "tables": 1, "textual": 1, "categorical": 1, "temporal": 1}}, "solution": [{"solution_text": "The first functionality is the extraction of a set of features that\nrepresent Twitter accounts. We built on previous research to check\nthe types of features that are known to be useful in spambot detection\n[14,28,39]. We identified a set of fifty features, e.g. total tweet count,\naverage number of links, followers to following ratio, then generated\nfour representations of these features by aggregating them temporally\n(R2). All extracted features are listed in the supplementary materials.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extraction;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The second functionality performed by the back-end is the genera-\ntion of a two-dimensional representation of the accounts, appropriate\nfor clustering tweets in a 2D display (R1). This reduction is useful to\ncommunicate similarities between accounts to the user. VASSL uses the\nextracted time-independent features for this purpose. We incorporate\nfour different dimensionality reduction (DR) techniques: Kernel Princi-\nple component analysis (K-PCA) [31], Linear Discriminant Analysis\n(LDA) [26], Locally Linear Embedding (LLE) [30], and t-distributed\nStochastic Neighbor Embedding (t-SNE) [24]. The four DR techniques\nare included in order to increase the effectiveness of the dimensionality\nreduction results in different contexts. For example, if the task is to\nlabel a set of unlabeled data without any information about labeled data,\nsupervised DR methods such as LDA may not produce good results,\nunlike PCA or LLE, which are unsupervised solutions. However, if a\nuser has already labeled parts of the data, she may utilize a supervised\nDR method for better class separation performance. Another factor that\nencourages our choice of DR techniques is the assumption of linearity.\nWe give users multiple options for reducing the dimension of feature\nspace using linear and non-linear mapping techniques.\nVASSL supports two transformation methods which can be incor-\nporated with the aforementioned dimensionality reduction methods:\nmin-max normalization and standardization. Min-max normalization\nchanges the range of the features and forces it to the range from 0 to\n1. Standardization transforms the values to Z-scores. Such transforma-\ntions are needed for some of the dimensionality reduction techniques.\nFor example, PCA is known to be sensitive to differences in features\nvariance and thus may performs badly if applied before normalization.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "back-end;two-dimensional;reduction;", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The third functionality of the back-end is topic modeling, which is\nperformed by utilizing the Latent Dirichlet Allocation (LDA) model\n[3]. We use the generated topics as a way to cluster accounts (R1,\nR4), as explained in Section 5.4. VASSL employs multiple natural\nlanguage processing techniques, such as lemmatization and stemming\npreprocessing, to transform the set of tweets for each account to tokens\nin a form suitable for LDA. The system then applies LDA to the set of\ntokens and generates a set of topics that best represent accounts\u2019 tweets.\nTo increase the accuracy of the latent topics, the system applies LDA\nto each temporal aggregation level mentioned above.\nOne of the challenges we faced during VASSL development was\nthe data scale. To discover anomalous botnet spammers, analyzing\nlarge numbers of accounts simultaneously is an ideal way to reveal\npatterns. However, the size of the data handled increases exponentially\nwith the number of accounts, because we need to consider multiple\nrepresentations of each account. To overcome this issue, the back-end\nkeeps a communication channel open with connected clients, gradually\nfeeding data. This channel is a querying mechanism between the front-\nend and the back-end, which only sends data that is visible in the\nviews of the front-end and prepares the remaining in the back-end.\nThis reduces the problem of information overload and creates a better\nanalysis experience for the users.\nThe communication channel also facilitates modification of the be-\nhavior of automatic data analysis techniques according to users\u2019 input.\nFor example, users are able to change the parameters of dimensionality\nreduction and topic modeling techniques from the front-end.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "back-end;topic_model", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "This topic clustering view uses two visualizations, which help analyze\nthe results of applying Latent Dirichlet Allocation (LDA) to accounts\u2019\ntweets. The \ufb01rst visualization is a bubble chart that represents the\ngenerated latent topics in a two-dimensional space (Fig. 1 (D)). The\naxes of this visualization can be chosen from the control panel to be\neither unique IDs, topics polarity, or topics subjectivity, which are\ncalculated by applying sentiment analysis to the topics\u2019 most probable\nwords. The size of the bubble encodes a score for each topic which\nrepresents the sum of probabilities of posting in that topic by all the\naccounts (see equation 1). The score of a topic Ti is calculated by\nsumming up the probability of that topic in all j documents D n, \u2200n \u2208{1, 2, . . . , j}. Documents in our analysis are accounts represented by\nthe concatenation of all their tweets.\nThe second visualization in the topic clustering view is a word cloud\nvisualization of the most frequent words in generated topics. LDA\ncomputes probabilities that show the distribution of these words in each\ntopic. VASSL utilizes these scores to determine the size of the words\nin the word cloud. When a user hovers over a topic, the word cloud\nchanges the size of the words according to their relevance scores, which\nhelp the user in exploring the semantics of the topics.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "word_cloud+scatter_plot", "axial_code": [], "componenet_code": ["wordcloud", "scatter"]}, {"solution_text": "Selecting a topic results in the selection of all accounts that have\nposted in that topic, with probability more than a threshold selected\nby the user (R1, R4). In other words, we can consider the topics as\nclusters where a single user can belong to more than one cluster (fuzzy\nclustering). Changing the threshold controls the sensitivity of cluster\nmembership. Increasing the threshold narrows the results to accounts\nthat post in a topic frequently, while reducing the threshold includes\naccounts that may rarely post in a topic", "solution_category": "interaction", "solution_axial": "selecting", "solution_compoent": "", "axial_code": ["selecting"], "componenet_code": ["selecting"]}]}, {"author": "gsh", "index_original": 102, "paper_title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "pub_year": 2020, "domain": "Social Media", "requirement": {"requirement_text": "R4 Allow the user to highlight and analyze groups of accounts. The system should enable users to highlight and cluster accounts interactively. This interactive clustering is essential to reveal potential group-based spamming activities [10, 11].", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "The\nbenchmark dataset used in the testing was crawled from Twitter and\nprepared by [10], who published the data for research purposes", "data_code": {"geometry": 1, "tables": 1, "textual": 1, "categorical": 1, "temporal": 1}}, "solution": [{"solution_text": "The first functionality is the extraction of a set of features that\nrepresent Twitter accounts. We built on previous research to check\nthe types of features that are known to be useful in spambot detection\n[14,28,39]. We identified a set of fifty features, e.g. total tweet count,\naverage number of links, followers to following ratio, then generated\nfour representations of these features by aggregating them temporally\n(R2). All extracted features are listed in the supplementary materials.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "extraction;", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The second functionality performed by the back-end is the genera-\ntion of a two-dimensional representation of the accounts, appropriate\nfor clustering tweets in a 2D display (R1). This reduction is useful to\ncommunicate similarities between accounts to the user. VASSL uses the\nextracted time-independent features for this purpose. We incorporate\nfour different dimensionality reduction (DR) techniques: Kernel Princi-\nple component analysis (K-PCA) [31], Linear Discriminant Analysis\n(LDA) [26], Locally Linear Embedding (LLE) [30], and t-distributed\nStochastic Neighbor Embedding (t-SNE) [24]. The four DR techniques\nare included in order to increase the effectiveness of the dimensionality\nreduction results in different contexts. For example, if the task is to\nlabel a set of unlabeled data without any information about labeled data,\nsupervised DR methods such as LDA may not produce good results,\nunlike PCA or LLE, which are unsupervised solutions. However, if a\nuser has already labeled parts of the data, she may utilize a supervised\nDR method for better class separation performance. Another factor that\nencourages our choice of DR techniques is the assumption of linearity.\nWe give users multiple options for reducing the dimension of feature\nspace using linear and non-linear mapping techniques.\nVASSL supports two transformation methods which can be incor-\nporated with the aforementioned dimensionality reduction methods:\nmin-max normalization and standardization. Min-max normalization\nchanges the range of the features and forces it to the range from 0 to\n1. Standardization transforms the values to Z-scores. Such transforma-\ntions are needed for some of the dimensionality reduction techniques.\nFor example, PCA is known to be sensitive to differences in features\nvariance and thus may performs badly if applied before normalization.", "solution_category": "data_manipulation", "solution_axial": "DimensionalityReduction", "solution_compoent": "back-end;two-dimensional;reduction;", "axial_code": ["DimensionalityReduction"], "componenet_code": ["dimensionality_reduction"]}, {"solution_text": "The third functionality of the back-end is topic modeling, which is\nperformed by utilizing the Latent Dirichlet Allocation (LDA) model\n[3]. We use the generated topics as a way to cluster accounts (R1,\nR4), as explained in Section 5.4. VASSL employs multiple natural\nlanguage processing techniques, such as lemmatization and stemming\npreprocessing, to transform the set of tweets for each account to tokens\nin a form suitable for LDA. The system then applies LDA to the set of\ntokens and generates a set of topics that best represent accounts\u2019 tweets.\nTo increase the accuracy of the latent topics, the system applies LDA\nto each temporal aggregation level mentioned above.\nOne of the challenges we faced during VASSL development was\nthe data scale. To discover anomalous botnet spammers, analyzing\nlarge numbers of accounts simultaneously is an ideal way to reveal\npatterns. However, the size of the data handled increases exponentially\nwith the number of accounts, because we need to consider multiple\nrepresentations of each account. To overcome this issue, the back-end\nkeeps a communication channel open with connected clients, gradually\nfeeding data. This channel is a querying mechanism between the front-\nend and the back-end, which only sends data that is visible in the\nviews of the front-end and prepares the remaining in the back-end.\nThis reduces the problem of information overload and creates a better\nanalysis experience for the users.\nThe communication channel also facilitates modification of the be-\nhavior of automatic data analysis techniques according to users\u2019 input.\nFor example, users are able to change the parameters of dimensionality\nreduction and topic modeling techniques from the front-end.", "solution_category": "data_manipulation", "solution_axial": "Modeling", "solution_compoent": "back-end;topic_model", "axial_code": ["Modeling"], "componenet_code": ["modeling"]}, {"solution_text": "The Feature Explorer view visualizes the distribution of accounts in\nselected features using a new design based on a violin plot (see Fig.\n5). We used a violin plot instead of a box plot to enable the user to\nexamine multi-modality in any feature [18], which could indicate a\npotential cluster (R1, R4). Users can select as many features as needed\nin the feature explorer control panel, which contains a list of all features\n(see Fig. 6). The maximum number of features that can be visualized\nsimultaneously depends on screen size and human perception. Selecting\nfeatures divides the available visual space among the features, and thus\ncan reduce the capability of absorbing communicated information.\nThe feature explorer view shows statistical summaries for each\nfeature independently, such as median and quartiles of the overall\naccounts as well as labeled groups. Features are represented as multiple\nhorizontally adjacent facets, having the same Y-axis range in order to\ncorrelate the features. The accounts are represented as points in the\nhorizontal center of each facet. The vertical locations of the accounts in\na facet are determined by the value of the feature for these accounts. To\nreduce the visual clutter of the feature explorer view, the accounts point\nis transparent by default. Hovering over a class distribution increases\nthe opacity of the accounts that belong to the hovered class. The black\nsolid line in the horizontal center of each facet represents the 1st and\n3rd quartile of all accounts regardless of their class while the black\ntick mark represents the median of all accounts. Besides its job of\ncommunicating quartile information of all accounts, the solid black line\ndivides the facet into two areas. The area on the left of the solid line is\nused to indicate spambot and genuine class distributions (purple and\ngreen curves respectively) which are approximated by kernel density\nestimation (KDE) technique. The right area is used to communicate\nthe KDE of the unlabeled accounts distribution as well as selected\naccounts distribution (blue and red curves respectively). These curves\nare visualized in each facet in a similar manner to communicate this\nstatistical information in each feature (R4).", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "violin_graph+point", "axial_code": [], "componenet_code": ["scatter", "violet_graph"]}]}, {"author": "gsh", "index_original": 103, "paper_title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "pub_year": 2020, "domain": "Social Media", "requirement": {"requirement_text": "R5 Enhance the efficiency and effectiveness of human workers. The system should enable cost effective annotation and improve the accuracy of detecting spambots as well as reducing the time needed to label groups of accounts by human workers [16].", "requirement_code": {"knowledge_injection": 1}}, "data": {"data_text": "The\nbenchmark dataset used in the testing was crawled from Twitter and\nprepared by [10], who published the data for research purposes", "data_code": {"geometry": 1, "tables": 1, "textual": 1, "categorical": 1, "temporal": 1}}, "solution": [{"solution_text": "Our system utilizes sophisticated techniques and visualizations that require training and expertise. Our goal is to provide new functionalities to more effectively and efficiently identify spambots, to ultimately reduce the time and cost of recruiting expert annotators and the annotation process (R5). The targeted users are human annotators whose terminal goal might be generating labelled datasets, or using the tool to understand and characterize spambot behaviour in a dataset. This guided us through many design choices with a focus on utility. However, for these functionalities to be useful, the users are required to have a certain level of expertise. We made several assumptions about user expertise to operate our system effectively. The most important was users' knowledge of tuning machine learning models; specifically, dimensionality reduction and topic modeling. VASSL is designed to provide experts with interactive control of these techniques which rely heavily on parameters tuning. Familiarity with these tools and experience labeling social spambots will significantly improve users' experience with VASSL.", "solution_category": "interaction", "solution_axial": "Participation/Collaboration", "solution_compoent": "", "axial_code": ["Participation/Collaboration"], "componenet_code": ["participation_collaboration"]}]}, {"author": "gsh", "index_original": 104, "paper_title": "CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R1: Familiar overview first, details later. Following Schneiderman's mantra of overview first, zoom and filter, then details on demand [29], users should start at an overview level that provides familiar statistics as a way to identify points of potential interest. Once they have identified these points, they should be able to drill-down into the detailed spatial data.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We collected spatio-temporal data from two tennis matches; one from a\nprofessional match broadcast on television, the other from an amateur\nmatch using a single consumer-level camera. In both cases, we were\nonly concerned with getting the minimum amount of data needed to\nreasonably represent the 2-D shots in a tennis match, including the\nplayer and ball locations, along with match context data, such as who\nis serving, serve side, serve number, and the match score.\nAlthough we had to collect our data by manually annotating videos\n(taking about 3 hours per hour of video), we recognize the ability\nof state-of-the-art systems to automatically collect ball and player\ntracking information. To ef\ufb01ciently integrate score progression and\nother context information with ball and player locations, we developed\na \ufb01nite state machine, shown in Figure 2, along with keyboard and\nmouse accelerators, to input this data. We also generated a homography\nto translate points from the video image plane into the tennis court plane\nusing Java libraries available from OpenCV (Open Source Computer\nVision Library).", "data_code": {"tables": 1, "categorical": 1, "quantitative": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "4.2 Data Transformation\nThe data, at its simplest level, is a series of play events, where each\nplay event is either a bounce event or a hit event. For each event, we\ninclude the 2-D location data of the ball and the players, along with\nthe score context information and a time stamp (taken directly from\nthe video). We also include summary point information, including who\nwas serving, serve side, whether the point started from a \ufb01rst or second\nserve, the number of shots in the point, and the point outcome (winner,\nunforced error, etc.). To make this data more usable for the visual\nanalytics application we propose in this paper, we perform a series of\ntransformations, including data mirroring, transforming play events\ninto shots, spatial data discretization, and feature generation.\nData mirroring. Each player\u2019s location data is transformed as needed\nto keep them on one half of the court.\nTransforming play events into shots. Players win points by making\ngood shots or forcing their opponent to make bad shots. Using the raw\nspatial data, we convert hit-bounce event pairs (or, less frequently, hit-\nhit event pairs) into shots for each player. Each shot is referenced by a\nsequence number and a reverse sequence number (i.e., number of shots\nuntil the last shot) and indicates the hitting player, the receiving player,\nand whether the shot was a forehand or a backhand. This indexing\nscheme is useful, since the last few shots in a point are likely to be key\ndeterminants of the point outcome. Location data for the players and\nthe ball at the start of the shot, when the ball bounces (if it is not a\nvolley), and at the end of the shot is also included, as is an indication\nof the shot being a serve or a volley.\nSpatial data discretization. In our present and past discussions with\ntennis coaches and teaching professionals, we learned they typically\ndivide the court into three sections horizontally (left, center, and right).\nIn terms of depth, coaches looked for shots that originated from either\nbehind the baseline, inside the baseline, or at the net and that landed\neither short (within the service box area) or deep (past the service box).\nThis results in nine left-to-right one-dimensional shot patterns and six\ndepth-based one-dimensional shot patterns and, when combined, 54\ntwo-dimensional shot patterns. In order to indicate which pattern each\nshot belongs to, the location data is adjusted to the nearest anchor point.\nWe also maintain the original location data, allowing the user to see\nmore accurate shot data if desired.\nFeature generation. In addition to the traditional features that coaches\nare interested in, such as who is serving, serve side, serve number,\nwho won the point and how they won it, we included these additional\nfeatures only possible with the integration of context data with the\nspatio-temporal data:\nT Wide serve. Boolean value that is true if the returner hit the service\nreturn from outside the singles sidelines (i.e., was a wide serve).\nT Serve speed. Indicates the approximate velocity of the serve. Values\nare calibrated separately for each player based on knowledge of their\nabilities. Valid values include slow, medium, and fast.\nT Return of serve stroke side. Indicates if the return was a backhand\nor forehand.\nT Point length. Short points are de\ufb01ned as 0-4 shots, medium points as\n5-8 shots, and long points as 9+ shots. These ranges were validated\nwith multiple tennis coaches and teaching pros in a prior study.\nT Point differential. The number of points within a game separating\nthe players. The valid range for standard games is -3 to +3 and, for\ntie-break games, -6 to +6. Negative values indicate player one is\nbehind and positive values indicate player one is ahead.\nT Game differential. The number of games within the current set\nseparating the two players. The valid range is -5 to +5.\nT Player dominant stroke side. Indicates if a player hit more forehand\nshots than backhand shots in a point. Valid values are forehand,\nbackhand, and none. One stroke side is considered dominant only if\nit accounted for at least 60% of the shots.\nT Player dominant playing depth. Indicates if a player hit more shots\nfrom behind the baseline or inside the baseline. A depth is considered\ndominant only if at least 60% of the shots are made from that depth.\nOtherwise, this feature is set to neutral.\nT Shots from outside the sidelines. Boolean feature indicating if a\nplayer hit any of their shots from outside the singles sidelines.\nT Short shots. Boolean feature indicating if any of a player\u2019s shots\nlanded within their opponent\u2019s service box.\nThese features were generated through interviews with local area\nteaching professionals and coaches and are by no means an exhaustive\nlist of features. Our application can be easily extended to add whatever\nadditional features are desired, as long as those features can each be\nrepresented by a relatively small number of discrete values so they can\nalso be used as \ufb01lters. The raw spatial data, transformed and combined\nwith match context data to generate semantically meaningful features,\nserves as the foundation of our visual analytics system and helps us\nrealize the high-level design requirements enumerated in Section 3.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The Point Selector, depicted in Figure 1 (D), provides a faceted \ufb01ltering\ncapability that allows users to drill-down to speci\ufb01c sets of points based\non feature attribute values. The \ufb01lter selection is then propagated to\nthe Point Analyzer and Shot Analyzer for further analysis. The driving\nrequirements behind the design of the Point Selector are R1 (Familiar\noverview \ufb01rst, details later), R2 (Support multi-faceted, multi-level\nsearch), R3 (Focus on point outcomes), and R5 (Keep the human in\nthe analysis loop)", "solution_category": "interaction", "solution_axial": "Overview_and_Explore", "solution_compoent": "", "axial_code": ["Overview_and_Explore"], "componenet_code": ["overview_and_explore"]}, {"solution_text": "We exemplify R1 by listing the most familiar point features first, including who is serving, which side they are serving from (deuce or ad), and serve number (first or second).", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "bar_chart+text", "axial_code": [], "componenet_code": ["bar", "text"]}]}, {"author": "gsh", "index_original": 105, "paper_title": "CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R1: Familiar overview first, details later. Following Schneiderman's mantra of overview first, zoom and filter, then details on demand [29], users should start at an overview level that provides familiar statistics as a way to identify points of potential interest. Once they have identified these points, they should be able to drill-down into the detailed spatial data.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We collected spatio-temporal data from two tennis matches; one from a\nprofessional match broadcast on television, the other from an amateur\nmatch using a single consumer-level camera. In both cases, we were\nonly concerned with getting the minimum amount of data needed to\nreasonably represent the 2-D shots in a tennis match, including the\nplayer and ball locations, along with match context data, such as who\nis serving, serve side, serve number, and the match score.\nAlthough we had to collect our data by manually annotating videos\n(taking about 3 hours per hour of video), we recognize the ability\nof state-of-the-art systems to automatically collect ball and player\ntracking information. To ef\ufb01ciently integrate score progression and\nother context information with ball and player locations, we developed\na \ufb01nite state machine, shown in Figure 2, along with keyboard and\nmouse accelerators, to input this data. We also generated a homography\nto translate points from the video image plane into the tennis court plane\nusing Java libraries available from OpenCV (Open Source Computer\nVision Library).", "data_code": {"tables": 1, "categorical": 1, "quantitative": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "4.2 Data Transformation\nThe data, at its simplest level, is a series of play events, where each\nplay event is either a bounce event or a hit event. For each event, we\ninclude the 2-D location data of the ball and the players, along with\nthe score context information and a time stamp (taken directly from\nthe video). We also include summary point information, including who\nwas serving, serve side, whether the point started from a \ufb01rst or second\nserve, the number of shots in the point, and the point outcome (winner,\nunforced error, etc.). To make this data more usable for the visual\nanalytics application we propose in this paper, we perform a series of\ntransformations, including data mirroring, transforming play events\ninto shots, spatial data discretization, and feature generation.\nData mirroring. Each player\u2019s location data is transformed as needed\nto keep them on one half of the court.\nTransforming play events into shots. Players win points by making\ngood shots or forcing their opponent to make bad shots. Using the raw\nspatial data, we convert hit-bounce event pairs (or, less frequently, hit-\nhit event pairs) into shots for each player. Each shot is referenced by a\nsequence number and a reverse sequence number (i.e., number of shots\nuntil the last shot) and indicates the hitting player, the receiving player,\nand whether the shot was a forehand or a backhand. This indexing\nscheme is useful, since the last few shots in a point are likely to be key\ndeterminants of the point outcome. Location data for the players and\nthe ball at the start of the shot, when the ball bounces (if it is not a\nvolley), and at the end of the shot is also included, as is an indication\nof the shot being a serve or a volley.\nSpatial data discretization. In our present and past discussions with\ntennis coaches and teaching professionals, we learned they typically\ndivide the court into three sections horizontally (left, center, and right).\nIn terms of depth, coaches looked for shots that originated from either\nbehind the baseline, inside the baseline, or at the net and that landed\neither short (within the service box area) or deep (past the service box).\nThis results in nine left-to-right one-dimensional shot patterns and six\ndepth-based one-dimensional shot patterns and, when combined, 54\ntwo-dimensional shot patterns. In order to indicate which pattern each\nshot belongs to, the location data is adjusted to the nearest anchor point.\nWe also maintain the original location data, allowing the user to see\nmore accurate shot data if desired.\nFeature generation. In addition to the traditional features that coaches\nare interested in, such as who is serving, serve side, serve number,\nwho won the point and how they won it, we included these additional\nfeatures only possible with the integration of context data with the\nspatio-temporal data:\nT Wide serve. Boolean value that is true if the returner hit the service\nreturn from outside the singles sidelines (i.e., was a wide serve).\nT Serve speed. Indicates the approximate velocity of the serve. Values\nare calibrated separately for each player based on knowledge of their\nabilities. Valid values include slow, medium, and fast.\nT Return of serve stroke side. Indicates if the return was a backhand\nor forehand.\nT Point length. Short points are de\ufb01ned as 0-4 shots, medium points as\n5-8 shots, and long points as 9+ shots. These ranges were validated\nwith multiple tennis coaches and teaching pros in a prior study.\nT Point differential. The number of points within a game separating\nthe players. The valid range for standard games is -3 to +3 and, for\ntie-break games, -6 to +6. Negative values indicate player one is\nbehind and positive values indicate player one is ahead.\nT Game differential. The number of games within the current set\nseparating the two players. The valid range is -5 to +5.\nT Player dominant stroke side. Indicates if a player hit more forehand\nshots than backhand shots in a point. Valid values are forehand,\nbackhand, and none. One stroke side is considered dominant only if\nit accounted for at least 60% of the shots.\nT Player dominant playing depth. Indicates if a player hit more shots\nfrom behind the baseline or inside the baseline. A depth is considered\ndominant only if at least 60% of the shots are made from that depth.\nOtherwise, this feature is set to neutral.\nT Shots from outside the sidelines. Boolean feature indicating if a\nplayer hit any of their shots from outside the singles sidelines.\nT Short shots. Boolean feature indicating if any of a player\u2019s shots\nlanded within their opponent\u2019s service box.\nThese features were generated through interviews with local area\nteaching professionals and coaches and are by no means an exhaustive\nlist of features. Our application can be easily extended to add whatever\nadditional features are desired, as long as those features can each be\nrepresented by a relatively small number of discrete values so they can\nalso be used as \ufb01lters. The raw spatial data, transformed and combined\nwith match context data to generate semantically meaningful features,\nserves as the foundation of our visual analytics system and helps us\nrealize the high-level design requirements enumerated in Section 3.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The Point Analyzer is designed to support three distinct analytical perspectives on the data: sequential and similarity-based. The user can switch between perspectives using a drop-down list box. The sequential perspective provides the user with a familiar overview of the entire match in the order the points were played and is in direct support of design requirement R1 (Familiar overview first, details later). Each row contains one game's worth of data and the background color of the row corresponds to who won the game (i.e., blue for player one and red for player two, see 1 (B)). A game where one player is serving but the other player wins is known as a service break and is considered an important event in tennis. To emphasize this event, service breaks are surrounded with a dark border to make them stand out. The small multiples themselves, separated into game-rows, also visually mimic a bar chart, making it easy to distinguish long games, where players were struggling, from short games, easily won by a player.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix+line+point", "axial_code": [], "componenet_code": ["scatter", "line", "matrix"]}]}, {"author": "gsh", "index_original": 106, "paper_title": "CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R2: Support multi-faceted, multi-level search. Coaches and ana- lysts are often familiar with the specific strengths, weaknesses, and strategies of players. They need the capability to approach the data from a variety of viewpoints to look for specific situations or to support or confirm hypotheses. This is consistent with Keim's analyze first - show the important - zoom, filter, and analyze further - details on demand approach [15].", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "We collected spatio-temporal data from two tennis matches; one from a\nprofessional match broadcast on television, the other from an amateur\nmatch using a single consumer-level camera. In both cases, we were\nonly concerned with getting the minimum amount of data needed to\nreasonably represent the 2-D shots in a tennis match, including the\nplayer and ball locations, along with match context data, such as who\nis serving, serve side, serve number, and the match score.\nAlthough we had to collect our data by manually annotating videos\n(taking about 3 hours per hour of video), we recognize the ability\nof state-of-the-art systems to automatically collect ball and player\ntracking information. To ef\ufb01ciently integrate score progression and\nother context information with ball and player locations, we developed\na \ufb01nite state machine, shown in Figure 2, along with keyboard and\nmouse accelerators, to input this data. We also generated a homography\nto translate points from the video image plane into the tennis court plane\nusing Java libraries available from OpenCV (Open Source Computer\nVision Library).", "data_code": {"tables": 1, "categorical": 1, "quantitative": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "4.2 Data Transformation\nThe data, at its simplest level, is a series of play events, where each\nplay event is either a bounce event or a hit event. For each event, we\ninclude the 2-D location data of the ball and the players, along with\nthe score context information and a time stamp (taken directly from\nthe video). We also include summary point information, including who\nwas serving, serve side, whether the point started from a \ufb01rst or second\nserve, the number of shots in the point, and the point outcome (winner,\nunforced error, etc.). To make this data more usable for the visual\nanalytics application we propose in this paper, we perform a series of\ntransformations, including data mirroring, transforming play events\ninto shots, spatial data discretization, and feature generation.\nData mirroring. Each player\u2019s location data is transformed as needed\nto keep them on one half of the court.\nTransforming play events into shots. Players win points by making\ngood shots or forcing their opponent to make bad shots. Using the raw\nspatial data, we convert hit-bounce event pairs (or, less frequently, hit-\nhit event pairs) into shots for each player. Each shot is referenced by a\nsequence number and a reverse sequence number (i.e., number of shots\nuntil the last shot) and indicates the hitting player, the receiving player,\nand whether the shot was a forehand or a backhand. This indexing\nscheme is useful, since the last few shots in a point are likely to be key\ndeterminants of the point outcome. Location data for the players and\nthe ball at the start of the shot, when the ball bounces (if it is not a\nvolley), and at the end of the shot is also included, as is an indication\nof the shot being a serve or a volley.\nSpatial data discretization. In our present and past discussions with\ntennis coaches and teaching professionals, we learned they typically\ndivide the court into three sections horizontally (left, center, and right).\nIn terms of depth, coaches looked for shots that originated from either\nbehind the baseline, inside the baseline, or at the net and that landed\neither short (within the service box area) or deep (past the service box).\nThis results in nine left-to-right one-dimensional shot patterns and six\ndepth-based one-dimensional shot patterns and, when combined, 54\ntwo-dimensional shot patterns. In order to indicate which pattern each\nshot belongs to, the location data is adjusted to the nearest anchor point.\nWe also maintain the original location data, allowing the user to see\nmore accurate shot data if desired.\nFeature generation. In addition to the traditional features that coaches\nare interested in, such as who is serving, serve side, serve number,\nwho won the point and how they won it, we included these additional\nfeatures only possible with the integration of context data with the\nspatio-temporal data:\nT Wide serve. Boolean value that is true if the returner hit the service\nreturn from outside the singles sidelines (i.e., was a wide serve).\nT Serve speed. Indicates the approximate velocity of the serve. Values\nare calibrated separately for each player based on knowledge of their\nabilities. Valid values include slow, medium, and fast.\nT Return of serve stroke side. Indicates if the return was a backhand\nor forehand.\nT Point length. Short points are de\ufb01ned as 0-4 shots, medium points as\n5-8 shots, and long points as 9+ shots. These ranges were validated\nwith multiple tennis coaches and teaching pros in a prior study.\nT Point differential. The number of points within a game separating\nthe players. The valid range for standard games is -3 to +3 and, for\ntie-break games, -6 to +6. Negative values indicate player one is\nbehind and positive values indicate player one is ahead.\nT Game differential. The number of games within the current set\nseparating the two players. The valid range is -5 to +5.\nT Player dominant stroke side. Indicates if a player hit more forehand\nshots than backhand shots in a point. Valid values are forehand,\nbackhand, and none. One stroke side is considered dominant only if\nit accounted for at least 60% of the shots.\nT Player dominant playing depth. Indicates if a player hit more shots\nfrom behind the baseline or inside the baseline. A depth is considered\ndominant only if at least 60% of the shots are made from that depth.\nOtherwise, this feature is set to neutral.\nT Shots from outside the sidelines. Boolean feature indicating if a\nplayer hit any of their shots from outside the singles sidelines.\nT Short shots. Boolean feature indicating if any of a player\u2019s shots\nlanded within their opponent\u2019s service box.\nThese features were generated through interviews with local area\nteaching professionals and coaches and are by no means an exhaustive\nlist of features. Our application can be easily extended to add whatever\nadditional features are desired, as long as those features can each be\nrepresented by a relatively small number of discrete values so they can\nalso be used as \ufb01lters. The raw spatial data, transformed and combined\nwith match context data to generate semantically meaningful features,\nserves as the foundation of our visual analytics system and helps us\nrealize the high-level design requirements enumerated in Section 3.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The Point Selector, depicted in Figure 1 (D), provides a faceted \ufb01ltering\ncapability that allows users to drill-down to speci\ufb01c sets of points based\non feature attribute values. The \ufb01lter selection is then propagated to\nthe Point Analyzer and Shot Analyzer for further analysis. The driving\nrequirements behind the design of the Point Selector are R1 (Familiar\noverview \ufb01rst, details later), R2 (Support multi-faceted, multi-level\nsearch), R3 (Focus on point outcomes), and R5 (Keep the human in\nthe analysis loop)", "solution_category": "interaction", "solution_axial": "Overview_and_Explore", "solution_compoent": "", "axial_code": ["Overview_and_Explore"], "componenet_code": ["overview_and_explore"]}, {"solution_text": "To support R2 (Support multi-faceted, multi-level search) and R5 (Keep the human in the analysis loop), we immediately apply the selected filters to the Point Analyzer and Shot Analyzer components and update the stacked bar charts in this component. We provide complete flexibility to users to iteratively select additional feature attribute values to filter on. For example, a user may select just player one first serves from the deuce side, revealing an unusually large number of points won by player one when player two hits a backhand return.", "solution_category": "interaction", "solution_axial": "filtering", "solution_compoent": "", "axial_code": ["filtering"], "componenet_code": ["filtering"]}]}, {"author": "gsh", "index_original": 107, "paper_title": "CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R2: Support multi-faceted, multi-level search. Coaches and ana- lysts are often familiar with the specific strengths, weaknesses, and strategies of players. They need the capability to approach the data from a variety of viewpoints to look for specific situations or to support or confirm hypotheses. This is consistent with Keim's analyze first - show the important - zoom, filter, and analyze further - details on demand approach [15].", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "We collected spatio-temporal data from two tennis matches; one from a\nprofessional match broadcast on television, the other from an amateur\nmatch using a single consumer-level camera. In both cases, we were\nonly concerned with getting the minimum amount of data needed to\nreasonably represent the 2-D shots in a tennis match, including the\nplayer and ball locations, along with match context data, such as who\nis serving, serve side, serve number, and the match score.\nAlthough we had to collect our data by manually annotating videos\n(taking about 3 hours per hour of video), we recognize the ability\nof state-of-the-art systems to automatically collect ball and player\ntracking information. To ef\ufb01ciently integrate score progression and\nother context information with ball and player locations, we developed\na \ufb01nite state machine, shown in Figure 2, along with keyboard and\nmouse accelerators, to input this data. We also generated a homography\nto translate points from the video image plane into the tennis court plane\nusing Java libraries available from OpenCV (Open Source Computer\nVision Library).", "data_code": {"tables": 1, "categorical": 1, "quantitative": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "4.2 Data Transformation\nThe data, at its simplest level, is a series of play events, where each\nplay event is either a bounce event or a hit event. For each event, we\ninclude the 2-D location data of the ball and the players, along with\nthe score context information and a time stamp (taken directly from\nthe video). We also include summary point information, including who\nwas serving, serve side, whether the point started from a \ufb01rst or second\nserve, the number of shots in the point, and the point outcome (winner,\nunforced error, etc.). To make this data more usable for the visual\nanalytics application we propose in this paper, we perform a series of\ntransformations, including data mirroring, transforming play events\ninto shots, spatial data discretization, and feature generation.\nData mirroring. Each player\u2019s location data is transformed as needed\nto keep them on one half of the court.\nTransforming play events into shots. Players win points by making\ngood shots or forcing their opponent to make bad shots. Using the raw\nspatial data, we convert hit-bounce event pairs (or, less frequently, hit-\nhit event pairs) into shots for each player. Each shot is referenced by a\nsequence number and a reverse sequence number (i.e., number of shots\nuntil the last shot) and indicates the hitting player, the receiving player,\nand whether the shot was a forehand or a backhand. This indexing\nscheme is useful, since the last few shots in a point are likely to be key\ndeterminants of the point outcome. Location data for the players and\nthe ball at the start of the shot, when the ball bounces (if it is not a\nvolley), and at the end of the shot is also included, as is an indication\nof the shot being a serve or a volley.\nSpatial data discretization. In our present and past discussions with\ntennis coaches and teaching professionals, we learned they typically\ndivide the court into three sections horizontally (left, center, and right).\nIn terms of depth, coaches looked for shots that originated from either\nbehind the baseline, inside the baseline, or at the net and that landed\neither short (within the service box area) or deep (past the service box).\nThis results in nine left-to-right one-dimensional shot patterns and six\ndepth-based one-dimensional shot patterns and, when combined, 54\ntwo-dimensional shot patterns. In order to indicate which pattern each\nshot belongs to, the location data is adjusted to the nearest anchor point.\nWe also maintain the original location data, allowing the user to see\nmore accurate shot data if desired.\nFeature generation. In addition to the traditional features that coaches\nare interested in, such as who is serving, serve side, serve number,\nwho won the point and how they won it, we included these additional\nfeatures only possible with the integration of context data with the\nspatio-temporal data:\nT Wide serve. Boolean value that is true if the returner hit the service\nreturn from outside the singles sidelines (i.e., was a wide serve).\nT Serve speed. Indicates the approximate velocity of the serve. Values\nare calibrated separately for each player based on knowledge of their\nabilities. Valid values include slow, medium, and fast.\nT Return of serve stroke side. Indicates if the return was a backhand\nor forehand.\nT Point length. Short points are de\ufb01ned as 0-4 shots, medium points as\n5-8 shots, and long points as 9+ shots. These ranges were validated\nwith multiple tennis coaches and teaching pros in a prior study.\nT Point differential. The number of points within a game separating\nthe players. The valid range for standard games is -3 to +3 and, for\ntie-break games, -6 to +6. Negative values indicate player one is\nbehind and positive values indicate player one is ahead.\nT Game differential. The number of games within the current set\nseparating the two players. The valid range is -5 to +5.\nT Player dominant stroke side. Indicates if a player hit more forehand\nshots than backhand shots in a point. Valid values are forehand,\nbackhand, and none. One stroke side is considered dominant only if\nit accounted for at least 60% of the shots.\nT Player dominant playing depth. Indicates if a player hit more shots\nfrom behind the baseline or inside the baseline. A depth is considered\ndominant only if at least 60% of the shots are made from that depth.\nOtherwise, this feature is set to neutral.\nT Shots from outside the sidelines. Boolean feature indicating if a\nplayer hit any of their shots from outside the singles sidelines.\nT Short shots. Boolean feature indicating if any of a player\u2019s shots\nlanded within their opponent\u2019s service box.\nThese features were generated through interviews with local area\nteaching professionals and coaches and are by no means an exhaustive\nlist of features. Our application can be easily extended to add whatever\nadditional features are desired, as long as those features can each be\nrepresented by a relatively small number of discrete values so they can\nalso be used as \ufb01lters. The raw spatial data, transformed and combined\nwith match context data to generate semantically meaningful features,\nserves as the foundation of our visual analytics system and helps us\nrealize the high-level design requirements enumerated in Section 3.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The interplay between the Point Selector and the Point Analyzer enables\nusers to identify subsets of points they are interested in (the what). The\nShot Analyzer, depicted in Figure 1 (C), helps them see the why by\ndisplaying individual shots for the points selected in the Point Analyzer.\nEach row displays shots from a single point. From left to right, we\ndisplay the serve and the return, followed by the last three shots hit by\neach player. Except for the serves and returns, the shots are always\naligned such that all of the shots in a single column belong to one player,\nmaking it easier to compare shots across points. In our discussions with\ntennis teaching pros and coaches, we veri\ufb01ed that effective serves and\nreturns are key elements of understanding point outcomes. Furthermore,\nif there are any multi-shot patterns associated with winning or losing a\npoint, these are likely to be manifested in the last few shots of a point.\nThe main design requirements underpinning this component are R4\n(Facilitate \ufb01nding patterns) and R5 (Keep the human in the analysis\nloop). There are a number of novel design features that help achieve\nthese requirements, including shot encoding, dimension selection with\nspatial simpli\ufb01cation, shot sequence alignment, and ordering and clus-\ntering facilitated by user-speci\ufb01ed shot similarity features. ", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "matrix+line+point", "axial_code": [], "componenet_code": ["scatter", "line", "matrix"]}, {"solution_text": "When comparing points to determine similarity derived from shot- based features, only shots the points have in common are included in the similarity calculation (using the Gower Metric [9]). Once all of the shot-based features have been selected, the user clicks a button to apply the ordering. This bottom-up approach is particularly useful when combined with the top-down approach of the point filtering capabilities provided by the Point Selector or with point clustering capabilities provided in the Point Analyzer, thus supporting design requirement R2 (Support multi-faceted, multi-level search). For example, users may select to view all winning shots made by a player and then focus in on a subset of these that seem to share a specific combination of shots to get better insights into how to setup winning points.", "solution_category": "interaction", "solution_axial": "reconfigure", "solution_compoent": "", "axial_code": ["reconfigure"], "componenet_code": ["reconfigure"]}]}, {"author": "gsh", "index_original": 108, "paper_title": "CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R3: Focus on point outcomes. Points are the building blocks from which players win matches. Players win points by hitting good shots (winners) or by their opponent making bad shots (errors). Therefore, the common thread running through the visualizations is that each incorporates point outcome.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We collected spatio-temporal data from two tennis matches; one from a\nprofessional match broadcast on television, the other from an amateur\nmatch using a single consumer-level camera. In both cases, we were\nonly concerned with getting the minimum amount of data needed to\nreasonably represent the 2-D shots in a tennis match, including the\nplayer and ball locations, along with match context data, such as who\nis serving, serve side, serve number, and the match score.\nAlthough we had to collect our data by manually annotating videos\n(taking about 3 hours per hour of video), we recognize the ability\nof state-of-the-art systems to automatically collect ball and player\ntracking information. To ef\ufb01ciently integrate score progression and\nother context information with ball and player locations, we developed\na \ufb01nite state machine, shown in Figure 2, along with keyboard and\nmouse accelerators, to input this data. We also generated a homography\nto translate points from the video image plane into the tennis court plane\nusing Java libraries available from OpenCV (Open Source Computer\nVision Library).", "data_code": {"tables": 1, "categorical": 1, "quantitative": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "4.2 Data Transformation\nThe data, at its simplest level, is a series of play events, where each\nplay event is either a bounce event or a hit event. For each event, we\ninclude the 2-D location data of the ball and the players, along with\nthe score context information and a time stamp (taken directly from\nthe video). We also include summary point information, including who\nwas serving, serve side, whether the point started from a \ufb01rst or second\nserve, the number of shots in the point, and the point outcome (winner,\nunforced error, etc.). To make this data more usable for the visual\nanalytics application we propose in this paper, we perform a series of\ntransformations, including data mirroring, transforming play events\ninto shots, spatial data discretization, and feature generation.\nData mirroring. Each player\u2019s location data is transformed as needed\nto keep them on one half of the court.\nTransforming play events into shots. Players win points by making\ngood shots or forcing their opponent to make bad shots. Using the raw\nspatial data, we convert hit-bounce event pairs (or, less frequently, hit-\nhit event pairs) into shots for each player. Each shot is referenced by a\nsequence number and a reverse sequence number (i.e., number of shots\nuntil the last shot) and indicates the hitting player, the receiving player,\nand whether the shot was a forehand or a backhand. This indexing\nscheme is useful, since the last few shots in a point are likely to be key\ndeterminants of the point outcome. Location data for the players and\nthe ball at the start of the shot, when the ball bounces (if it is not a\nvolley), and at the end of the shot is also included, as is an indication\nof the shot being a serve or a volley.\nSpatial data discretization. In our present and past discussions with\ntennis coaches and teaching professionals, we learned they typically\ndivide the court into three sections horizontally (left, center, and right).\nIn terms of depth, coaches looked for shots that originated from either\nbehind the baseline, inside the baseline, or at the net and that landed\neither short (within the service box area) or deep (past the service box).\nThis results in nine left-to-right one-dimensional shot patterns and six\ndepth-based one-dimensional shot patterns and, when combined, 54\ntwo-dimensional shot patterns. In order to indicate which pattern each\nshot belongs to, the location data is adjusted to the nearest anchor point.\nWe also maintain the original location data, allowing the user to see\nmore accurate shot data if desired.\nFeature generation. In addition to the traditional features that coaches\nare interested in, such as who is serving, serve side, serve number,\nwho won the point and how they won it, we included these additional\nfeatures only possible with the integration of context data with the\nspatio-temporal data:\nT Wide serve. Boolean value that is true if the returner hit the service\nreturn from outside the singles sidelines (i.e., was a wide serve).\nT Serve speed. Indicates the approximate velocity of the serve. Values\nare calibrated separately for each player based on knowledge of their\nabilities. Valid values include slow, medium, and fast.\nT Return of serve stroke side. Indicates if the return was a backhand\nor forehand.\nT Point length. Short points are de\ufb01ned as 0-4 shots, medium points as\n5-8 shots, and long points as 9+ shots. These ranges were validated\nwith multiple tennis coaches and teaching pros in a prior study.\nT Point differential. The number of points within a game separating\nthe players. The valid range for standard games is -3 to +3 and, for\ntie-break games, -6 to +6. Negative values indicate player one is\nbehind and positive values indicate player one is ahead.\nT Game differential. The number of games within the current set\nseparating the two players. The valid range is -5 to +5.\nT Player dominant stroke side. Indicates if a player hit more forehand\nshots than backhand shots in a point. Valid values are forehand,\nbackhand, and none. One stroke side is considered dominant only if\nit accounted for at least 60% of the shots.\nT Player dominant playing depth. Indicates if a player hit more shots\nfrom behind the baseline or inside the baseline. A depth is considered\ndominant only if at least 60% of the shots are made from that depth.\nOtherwise, this feature is set to neutral.\nT Shots from outside the sidelines. Boolean feature indicating if a\nplayer hit any of their shots from outside the singles sidelines.\nT Short shots. Boolean feature indicating if any of a player\u2019s shots\nlanded within their opponent\u2019s service box.\nThese features were generated through interviews with local area\nteaching professionals and coaches and are by no means an exhaustive\nlist of features. Our application can be easily extended to add whatever\nadditional features are desired, as long as those features can each be\nrepresented by a relatively small number of discrete values so they can\nalso be used as \ufb01lters. The raw spatial data, transformed and combined\nwith match context data to generate semantically meaningful features,\nserves as the foundation of our visual analytics system and helps us\nrealize the high-level design requirements enumerated in Section 3.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The Point Selector, depicted in Figure 1 (D), provides a faceted \ufb01ltering\ncapability that allows users to drill-down to speci\ufb01c sets of points based\non feature attribute values. The \ufb01lter selection is then propagated to\nthe Point Analyzer and Shot Analyzer for further analysis. The driving\nrequirements behind the design of the Point Selector are R1 (Familiar\noverview \ufb01rst, details later), R2 (Support multi-faceted, multi-level\nsearch), R3 (Focus on point outcomes), and R5 (Keep the human in\nthe analysis loop)", "solution_category": "interaction", "solution_axial": "Overview_and_Explore", "solution_compoent": "", "axial_code": ["Overview_and_Explore"], "componenet_code": ["overview_and_explore"]}, {"solution_text": "We exemplify R3 by displaying stacked bar charts showing the number of points won by each player for each feature attribute value. These are shown as a backdrop to the feature attribute values. Everything is displayed from the perspective of player one, with blue bars indicating points won by player one and red bars indicating points won by player two.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "bar_chart+text", "axial_code": [], "componenet_code": ["bar", "text"]}]}, {"author": "gsh", "index_original": 109, "paper_title": "CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R3: Focus on point outcomes. Points are the building blocks from which players win matches. Players win points by hitting good shots (winners) or by their opponent making bad shots (errors). Therefore, the common thread running through the visualizations is that each incorporates point outcome.", "requirement_code": {"discover_observation": 1}}, "data": {"data_text": "We collected spatio-temporal data from two tennis matches; one from a\nprofessional match broadcast on television, the other from an amateur\nmatch using a single consumer-level camera. In both cases, we were\nonly concerned with getting the minimum amount of data needed to\nreasonably represent the 2-D shots in a tennis match, including the\nplayer and ball locations, along with match context data, such as who\nis serving, serve side, serve number, and the match score.\nAlthough we had to collect our data by manually annotating videos\n(taking about 3 hours per hour of video), we recognize the ability\nof state-of-the-art systems to automatically collect ball and player\ntracking information. To ef\ufb01ciently integrate score progression and\nother context information with ball and player locations, we developed\na \ufb01nite state machine, shown in Figure 2, along with keyboard and\nmouse accelerators, to input this data. We also generated a homography\nto translate points from the video image plane into the tennis court plane\nusing Java libraries available from OpenCV (Open Source Computer\nVision Library).", "data_code": {"tables": 1, "categorical": 1, "quantitative": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "4.2 Data Transformation\nThe data, at its simplest level, is a series of play events, where each\nplay event is either a bounce event or a hit event. For each event, we\ninclude the 2-D location data of the ball and the players, along with\nthe score context information and a time stamp (taken directly from\nthe video). We also include summary point information, including who\nwas serving, serve side, whether the point started from a \ufb01rst or second\nserve, the number of shots in the point, and the point outcome (winner,\nunforced error, etc.). To make this data more usable for the visual\nanalytics application we propose in this paper, we perform a series of\ntransformations, including data mirroring, transforming play events\ninto shots, spatial data discretization, and feature generation.\nData mirroring. Each player\u2019s location data is transformed as needed\nto keep them on one half of the court.\nTransforming play events into shots. Players win points by making\ngood shots or forcing their opponent to make bad shots. Using the raw\nspatial data, we convert hit-bounce event pairs (or, less frequently, hit-\nhit event pairs) into shots for each player. Each shot is referenced by a\nsequence number and a reverse sequence number (i.e., number of shots\nuntil the last shot) and indicates the hitting player, the receiving player,\nand whether the shot was a forehand or a backhand. This indexing\nscheme is useful, since the last few shots in a point are likely to be key\ndeterminants of the point outcome. Location data for the players and\nthe ball at the start of the shot, when the ball bounces (if it is not a\nvolley), and at the end of the shot is also included, as is an indication\nof the shot being a serve or a volley.\nSpatial data discretization. In our present and past discussions with\ntennis coaches and teaching professionals, we learned they typically\ndivide the court into three sections horizontally (left, center, and right).\nIn terms of depth, coaches looked for shots that originated from either\nbehind the baseline, inside the baseline, or at the net and that landed\neither short (within the service box area) or deep (past the service box).\nThis results in nine left-to-right one-dimensional shot patterns and six\ndepth-based one-dimensional shot patterns and, when combined, 54\ntwo-dimensional shot patterns. In order to indicate which pattern each\nshot belongs to, the location data is adjusted to the nearest anchor point.\nWe also maintain the original location data, allowing the user to see\nmore accurate shot data if desired.\nFeature generation. In addition to the traditional features that coaches\nare interested in, such as who is serving, serve side, serve number,\nwho won the point and how they won it, we included these additional\nfeatures only possible with the integration of context data with the\nspatio-temporal data:\nT Wide serve. Boolean value that is true if the returner hit the service\nreturn from outside the singles sidelines (i.e., was a wide serve).\nT Serve speed. Indicates the approximate velocity of the serve. Values\nare calibrated separately for each player based on knowledge of their\nabilities. Valid values include slow, medium, and fast.\nT Return of serve stroke side. Indicates if the return was a backhand\nor forehand.\nT Point length. Short points are de\ufb01ned as 0-4 shots, medium points as\n5-8 shots, and long points as 9+ shots. These ranges were validated\nwith multiple tennis coaches and teaching pros in a prior study.\nT Point differential. The number of points within a game separating\nthe players. The valid range for standard games is -3 to +3 and, for\ntie-break games, -6 to +6. Negative values indicate player one is\nbehind and positive values indicate player one is ahead.\nT Game differential. The number of games within the current set\nseparating the two players. The valid range is -5 to +5.\nT Player dominant stroke side. Indicates if a player hit more forehand\nshots than backhand shots in a point. Valid values are forehand,\nbackhand, and none. One stroke side is considered dominant only if\nit accounted for at least 60% of the shots.\nT Player dominant playing depth. Indicates if a player hit more shots\nfrom behind the baseline or inside the baseline. A depth is considered\ndominant only if at least 60% of the shots are made from that depth.\nOtherwise, this feature is set to neutral.\nT Shots from outside the sidelines. Boolean feature indicating if a\nplayer hit any of their shots from outside the singles sidelines.\nT Short shots. Boolean feature indicating if any of a player\u2019s shots\nlanded within their opponent\u2019s service box.\nThese features were generated through interviews with local area\nteaching professionals and coaches and are by no means an exhaustive\nlist of features. Our application can be easily extended to add whatever\nadditional features are desired, as long as those features can each be\nrepresented by a relatively small number of discrete values so they can\nalso be used as \ufb01lters. The raw spatial data, transformed and combined\nwith match context data to generate semantically meaningful features,\nserves as the foundation of our visual analytics system and helps us\nrealize the high-level design requirements enumerated in Section 3.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The Point Analyzer is designed to support three distinct analytical perspectives on the data: sequential and similarity-based. The user can switch between perspectives using a drop-down list box. The sequential perspective provides the user with a familiar overview of the entire match in the order the points were played and is in direct support of design requirement R1 (Familiar overview first, details later). Each row contains one game's worth of data and the background color of the row corresponds to who won the game (i.e., blue for player one and red for player two, see 1 (B)). A game where one player is serving but the other player wins is known as a service break and is considered an important event in tennis. To emphasize this event, service breaks are surrounded with a dark border to make them stand out. The small multiples themselves, separated into game-rows, also visually mimic a bar chart, making it easy to distinguish long games, where players were struggling, from short games, easily won by a player.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix+line+point", "axial_code": [], "componenet_code": ["scatter", "line", "matrix"]}, {"solution_text": "5.2.1 1-D Space-Time Charts\nThe design of the 1-D space-time charts is driven primarily by re-\nquirement R4 (Facilitate \ufb01nding patterns), but is also in\ufb02uenced by\nrequirement R3 (Focus on point outcomes) and R5 (Keep the human in\nthe analysis loop). To understand how these requirements in\ufb02uence the\ndesign, we describe how the design evolved to its current form. This is\nshown in Figure 3.\nIn our design, we use the tennis court diagram as a backdrop to\nprovide a reference point for the location data. Players are represented\nas colored circles (blue for player one, red for player two). The stroke\nside players hit a shot from (backhand or forehand) is very important,\nas one side tends to be weaker or less reliable than the other. Therefore,\nwe encode forehand shots as solid circles and backhand shots as hollow\ncircles. Ball landing locations are also encoded using colored circles\nthat are noticeably smaller than the player circles. These were initially\nencoded using yellow, maintaining the metaphor of a tennis ball. How-\never, we soon discovered this made it difficult to distinguish which\nplayer hit which shot. We therefore encoded each ball color based on\nwho hit the ball. Following the Gestalt law of continuity, we connected\nthe player locations using blue lines for player one, red lines for player\ntwo, and yellow lines for the ball landing locations.\nFigure 3 (A) depicts an early design, where we kept each player on\ntheir side of the court. In this design, the time dimension starts from\nboth the left and right sides and continues towards the middle of the\ncourt. The main problem with this design is the difficulty in directly\ncomparing player locations at the same time due to the dual time axes.\nIt also requires more space because each player\u2019s location remains on\ntheir half of the court.\nWe then plotted both players as playing from the left side of the\ncourt, as shown in 3 (B). This allows us to use a single time axis,\nrunning from left to right. This type of plot requires flipping both the x-\nand y-coordinates of the player on the right to put them on the left (i.e.,\nso the player\u2019s relative court location is maintained). The main issue\nwith this approach is that, when players are on opposite sides of the\ncourt (i.e., diagonal from one another), they appear co-located on this\nchart. Similarly, when players are on the same side of the court (i.e.,\nnear the same sideline), they are plotted in different locations on this\nchart, which appears counter-intuitive.\nFor the left/right dimension, we therefore settle on the design shown\nin Figure 3 (C). In this design, we only horizontally flip the right-side\nplayer\u2019s location data, so that when players are on opposite sides of the\ncourt, that is also depicted in the chart. We then add additional context\ninformation to this chart to make it more meaningful, including the\nmatch score, a serve indicator, and a point outcome indicator.\nTo visualize the depth-based information, it comes natural to keep\nplayers on their respective side of the tennis court and run the time axis\nfrom top to bottom, with the ball and player locations being staggered\nalong the time axis to indicate the order of shots. The challenge is to\nplot the ball location in a way to effectively differentiate short shots\nfrom deep shots. In our first attempt, shown in Figure 3 (D), we apply\nhorizontal lines connecting the hitting player to the ball bounce location.\nThe main issue with this approach is that the chart is overly dominated\nby the yellow lines and it is dif\ufb01cult to see the \ufb02ow. We overcome this\nproblem in our second attempt, shown in Figure 3 (E), by connecting\nthe ball landing locations together, thus giving a better impression of\nthe back and forth shots. However, this design is dominated by the ball\ntrajectories, posing a challenge to effectively interpret the situation.\nWe \ufb01nally settle on the design shown in Figure 3 (F), where we\nonly connect the ball landing locations on the same side of the court.\nAlthough we lose the back and forth nature of the shots, this approach\ntakes the focus from the speci\ufb01c ball trajectory to the overall situation\nand creates a distinct depth pattern for each player, thus making these\npatterns easier to see (supporting requirement R4). All of the same\ntreatments applied to the left/right chart to include context information\nare incorporated here.\nThe \ufb01nal design and its encoding is explained in Figure 4. The top\nleft is the resulting graphic if we try to display all the player movements\nand shots on one single tennis court. In addition to the problem of\nocclusions, there is no easy way to follow the progression of shots and\nmovements in the point. We therefore split out the left/right component\nand depth component separately and then plot each one over time. The\ntop right image depicts the left/right movements of the players and ball\non the court and the bottom left image depicts the depth movements.\nContext information is also encoded on the graphics. This includes\nscore information (both in the current game and in the overall match),\nand the indication of the serving player using a red or blue vertical line\non the left side of the chart. A solid line indicates the point started\nfrom a \ufb01rst serve, while a dashed line indicates it started from a second\nserve. Similarly, we use a color-coded line on the right side of the chart\nto indicate who won the point. A solid line means they won the point\ndue to a \u201cwinner\u201d shot, while a dashed line means they won the point\ndue to an error by their opponent. This exempli\ufb01es our support for\nrequirement R3.\nMost tennis players have one stroke side (backhand or forehand) that\nis better than the other. One typical strategy is to try make your oppo-\nnent hit shots from their weaker side while you try to make shots from\nyour stronger side. Therefore, we incorporate stroke side information\ninto the chart. A solid red or blue circle indicates a forehand stroke,\nwhile a hollow red or blue circle indicates a backhand stroke. For the\nball landing locations, we use smaller-diameter circles that share the\nsame color as the player that hit the ball. This color coding is essential\nsince users have the option of only plotting ball locations.\nWe recognize that the design choices we have made for the 1-D\nSpace-Time Charts involve trade-offs between temporal alignment and\nspatial alignment, with an emphasis on maintaining temporal alignment\nin order to facilitate the direct, dynamic comparison of player positions\n(i.e., hitting diagonally across the court to one another or down-the-\nline). Of course, spatial \ufb01delity is maintained in the Shot Analyzer,\ndescribed in section 5.3. To help minimize clutter, the user can toggle\noff the player and/or ball visualization components, allowing them to\nfocus of speci\ufb01c aspects.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line+point", "axial_code": [], "componenet_code": ["scatter", "line"]}]}, {"author": "gsh", "index_original": 110, "paper_title": "CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R4: Facilitate finding patterns. Once users have drilled down to a specific set of points, they want to figure out the hows and whys to explain those points. The system should facilitate this process through visual cues and interaction techniques designed to make spatial patterns in the data more salient.", "requirement_code": {"discover_observation": 1, "identify_main_cause_item": 1, "describe_observation_item": 1}}, "data": {"data_text": "We collected spatio-temporal data from two tennis matches; one from a\nprofessional match broadcast on television, the other from an amateur\nmatch using a single consumer-level camera. In both cases, we were\nonly concerned with getting the minimum amount of data needed to\nreasonably represent the 2-D shots in a tennis match, including the\nplayer and ball locations, along with match context data, such as who\nis serving, serve side, serve number, and the match score.\nAlthough we had to collect our data by manually annotating videos\n(taking about 3 hours per hour of video), we recognize the ability\nof state-of-the-art systems to automatically collect ball and player\ntracking information. To ef\ufb01ciently integrate score progression and\nother context information with ball and player locations, we developed\na \ufb01nite state machine, shown in Figure 2, along with keyboard and\nmouse accelerators, to input this data. We also generated a homography\nto translate points from the video image plane into the tennis court plane\nusing Java libraries available from OpenCV (Open Source Computer\nVision Library).", "data_code": {"tables": 1, "categorical": 1, "quantitative": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "4.2 Data Transformation\nThe data, at its simplest level, is a series of play events, where each\nplay event is either a bounce event or a hit event. For each event, we\ninclude the 2-D location data of the ball and the players, along with\nthe score context information and a time stamp (taken directly from\nthe video). We also include summary point information, including who\nwas serving, serve side, whether the point started from a \ufb01rst or second\nserve, the number of shots in the point, and the point outcome (winner,\nunforced error, etc.). To make this data more usable for the visual\nanalytics application we propose in this paper, we perform a series of\ntransformations, including data mirroring, transforming play events\ninto shots, spatial data discretization, and feature generation.\nData mirroring. Each player\u2019s location data is transformed as needed\nto keep them on one half of the court.\nTransforming play events into shots. Players win points by making\ngood shots or forcing their opponent to make bad shots. Using the raw\nspatial data, we convert hit-bounce event pairs (or, less frequently, hit-\nhit event pairs) into shots for each player. Each shot is referenced by a\nsequence number and a reverse sequence number (i.e., number of shots\nuntil the last shot) and indicates the hitting player, the receiving player,\nand whether the shot was a forehand or a backhand. This indexing\nscheme is useful, since the last few shots in a point are likely to be key\ndeterminants of the point outcome. Location data for the players and\nthe ball at the start of the shot, when the ball bounces (if it is not a\nvolley), and at the end of the shot is also included, as is an indication\nof the shot being a serve or a volley.\nSpatial data discretization. In our present and past discussions with\ntennis coaches and teaching professionals, we learned they typically\ndivide the court into three sections horizontally (left, center, and right).\nIn terms of depth, coaches looked for shots that originated from either\nbehind the baseline, inside the baseline, or at the net and that landed\neither short (within the service box area) or deep (past the service box).\nThis results in nine left-to-right one-dimensional shot patterns and six\ndepth-based one-dimensional shot patterns and, when combined, 54\ntwo-dimensional shot patterns. In order to indicate which pattern each\nshot belongs to, the location data is adjusted to the nearest anchor point.\nWe also maintain the original location data, allowing the user to see\nmore accurate shot data if desired.\nFeature generation. In addition to the traditional features that coaches\nare interested in, such as who is serving, serve side, serve number,\nwho won the point and how they won it, we included these additional\nfeatures only possible with the integration of context data with the\nspatio-temporal data:\nT Wide serve. Boolean value that is true if the returner hit the service\nreturn from outside the singles sidelines (i.e., was a wide serve).\nT Serve speed. Indicates the approximate velocity of the serve. Values\nare calibrated separately for each player based on knowledge of their\nabilities. Valid values include slow, medium, and fast.\nT Return of serve stroke side. Indicates if the return was a backhand\nor forehand.\nT Point length. Short points are de\ufb01ned as 0-4 shots, medium points as\n5-8 shots, and long points as 9+ shots. These ranges were validated\nwith multiple tennis coaches and teaching pros in a prior study.\nT Point differential. The number of points within a game separating\nthe players. The valid range for standard games is -3 to +3 and, for\ntie-break games, -6 to +6. Negative values indicate player one is\nbehind and positive values indicate player one is ahead.\nT Game differential. The number of games within the current set\nseparating the two players. The valid range is -5 to +5.\nT Player dominant stroke side. Indicates if a player hit more forehand\nshots than backhand shots in a point. Valid values are forehand,\nbackhand, and none. One stroke side is considered dominant only if\nit accounted for at least 60% of the shots.\nT Player dominant playing depth. Indicates if a player hit more shots\nfrom behind the baseline or inside the baseline. A depth is considered\ndominant only if at least 60% of the shots are made from that depth.\nOtherwise, this feature is set to neutral.\nT Shots from outside the sidelines. Boolean feature indicating if a\nplayer hit any of their shots from outside the singles sidelines.\nT Short shots. Boolean feature indicating if any of a player\u2019s shots\nlanded within their opponent\u2019s service box.\nThese features were generated through interviews with local area\nteaching professionals and coaches and are by no means an exhaustive\nlist of features. Our application can be easily extended to add whatever\nadditional features are desired, as long as those features can each be\nrepresented by a relatively small number of discrete values so they can\nalso be used as \ufb01lters. The raw spatial data, transformed and combined\nwith match context data to generate semantically meaningful features,\nserves as the foundation of our visual analytics system and helps us\nrealize the high-level design requirements enumerated in Section 3.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The Point Analyzer is designed to support three distinct analytical perspectives on the data: sequential and similarity-based. The user can switch between perspectives using a drop-down list box. The sequential perspective provides the user with a familiar overview of the entire match in the order the points were played and is in direct support of design requirement R1 (Familiar overview first, details later). Each row contains one game's worth of data and the background color of the row corresponds to who won the game (i.e., blue for player one and red for player two, see 1 (B)). A game where one player is serving but the other player wins is known as a service break and is considered an important event in tennis. To emphasize this event, service breaks are surrounded with a dark border to make them stand out. The small multiples themselves, separated into game-rows, also visually mimic a bar chart, making it easy to distinguish long games, where players were struggling, from short games, easily won by a player.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix+line+point", "axial_code": [], "componenet_code": ["scatter", "line", "matrix"]}, {"solution_text": "5.2.1 1-D Space-Time Charts\nThe design of the 1-D space-time charts is driven primarily by re-\nquirement R4 (Facilitate \ufb01nding patterns), but is also in\ufb02uenced by\nrequirement R3 (Focus on point outcomes) and R5 (Keep the human in\nthe analysis loop). To understand how these requirements in\ufb02uence the\ndesign, we describe how the design evolved to its current form. This is\nshown in Figure 3.\nIn our design, we use the tennis court diagram as a backdrop to\nprovide a reference point for the location data. Players are represented\nas colored circles (blue for player one, red for player two). The stroke\nside players hit a shot from (backhand or forehand) is very important,\nas one side tends to be weaker or less reliable than the other. Therefore,\nwe encode forehand shots as solid circles and backhand shots as hollow\ncircles. Ball landing locations are also encoded using colored circles\nthat are noticeably smaller than the player circles. These were initially\nencoded using yellow, maintaining the metaphor of a tennis ball. How-\never, we soon discovered this made it difficult to distinguish which\nplayer hit which shot. We therefore encoded each ball color based on\nwho hit the ball. Following the Gestalt law of continuity, we connected\nthe player locations using blue lines for player one, red lines for player\ntwo, and yellow lines for the ball landing locations.\nFigure 3 (A) depicts an early design, where we kept each player on\ntheir side of the court. In this design, the time dimension starts from\nboth the left and right sides and continues towards the middle of the\ncourt. The main problem with this design is the difficulty in directly\ncomparing player locations at the same time due to the dual time axes.\nIt also requires more space because each player\u2019s location remains on\ntheir half of the court.\nWe then plotted both players as playing from the left side of the\ncourt, as shown in 3 (B). This allows us to use a single time axis,\nrunning from left to right. This type of plot requires flipping both the x-\nand y-coordinates of the player on the right to put them on the left (i.e.,\nso the player\u2019s relative court location is maintained). The main issue\nwith this approach is that, when players are on opposite sides of the\ncourt (i.e., diagonal from one another), they appear co-located on this\nchart. Similarly, when players are on the same side of the court (i.e.,\nnear the same sideline), they are plotted in different locations on this\nchart, which appears counter-intuitive.\nFor the left/right dimension, we therefore settle on the design shown\nin Figure 3 (C). In this design, we only horizontally flip the right-side\nplayer\u2019s location data, so that when players are on opposite sides of the\ncourt, that is also depicted in the chart. We then add additional context\ninformation to this chart to make it more meaningful, including the\nmatch score, a serve indicator, and a point outcome indicator.\nTo visualize the depth-based information, it comes natural to keep\nplayers on their respective side of the tennis court and run the time axis\nfrom top to bottom, with the ball and player locations being staggered\nalong the time axis to indicate the order of shots. The challenge is to\nplot the ball location in a way to effectively differentiate short shots\nfrom deep shots. In our first attempt, shown in Figure 3 (D), we apply\nhorizontal lines connecting the hitting player to the ball bounce location.\nThe main issue with this approach is that the chart is overly dominated\nby the yellow lines and it is dif\ufb01cult to see the \ufb02ow. We overcome this\nproblem in our second attempt, shown in Figure 3 (E), by connecting\nthe ball landing locations together, thus giving a better impression of\nthe back and forth shots. However, this design is dominated by the ball\ntrajectories, posing a challenge to effectively interpret the situation.\nWe \ufb01nally settle on the design shown in Figure 3 (F), where we\nonly connect the ball landing locations on the same side of the court.\nAlthough we lose the back and forth nature of the shots, this approach\ntakes the focus from the speci\ufb01c ball trajectory to the overall situation\nand creates a distinct depth pattern for each player, thus making these\npatterns easier to see (supporting requirement R4). All of the same\ntreatments applied to the left/right chart to include context information\nare incorporated here.\nThe \ufb01nal design and its encoding is explained in Figure 4. The top\nleft is the resulting graphic if we try to display all the player movements\nand shots on one single tennis court. In addition to the problem of\nocclusions, there is no easy way to follow the progression of shots and\nmovements in the point. We therefore split out the left/right component\nand depth component separately and then plot each one over time. The\ntop right image depicts the left/right movements of the players and ball\non the court and the bottom left image depicts the depth movements.\nContext information is also encoded on the graphics. This includes\nscore information (both in the current game and in the overall match),\nand the indication of the serving player using a red or blue vertical line\non the left side of the chart. A solid line indicates the point started\nfrom a \ufb01rst serve, while a dashed line indicates it started from a second\nserve. Similarly, we use a color-coded line on the right side of the chart\nto indicate who won the point. A solid line means they won the point\ndue to a \u201cwinner\u201d shot, while a dashed line means they won the point\ndue to an error by their opponent. This exempli\ufb01es our support for\nrequirement R3.\nMost tennis players have one stroke side (backhand or forehand) that\nis better than the other. One typical strategy is to try make your oppo-\nnent hit shots from their weaker side while you try to make shots from\nyour stronger side. Therefore, we incorporate stroke side information\ninto the chart. A solid red or blue circle indicates a forehand stroke,\nwhile a hollow red or blue circle indicates a backhand stroke. For the\nball landing locations, we use smaller-diameter circles that share the\nsame color as the player that hit the ball. This color coding is essential\nsince users have the option of only plotting ball locations.\nWe recognize that the design choices we have made for the 1-D\nSpace-Time Charts involve trade-offs between temporal alignment and\nspatial alignment, with an emphasis on maintaining temporal alignment\nin order to facilitate the direct, dynamic comparison of player positions\n(i.e., hitting diagonally across the court to one another or down-the-\nline). Of course, spatial \ufb01delity is maintained in the Shot Analyzer,\ndescribed in section 5.3. To help minimize clutter, the user can toggle\noff the player and/or ball visualization components, allowing them to\nfocus of speci\ufb01c aspects.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line+point", "axial_code": [], "componenet_code": ["scatter", "line"]}]}, {"author": "gsh", "index_original": 111, "paper_title": "CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R4: Facilitate finding patterns. Once users have drilled down to a specific set of points, they want to figure out the hows and whys to explain those points. The system should facilitate this process through visual cues and interaction techniques designed to make spatial patterns in the data more salient.", "requirement_code": {"discover_observation": 1, "identify_main_cause_item": 1, "describe_observation_item": 1}}, "data": {"data_text": "We collected spatio-temporal data from two tennis matches; one from a\nprofessional match broadcast on television, the other from an amateur\nmatch using a single consumer-level camera. In both cases, we were\nonly concerned with getting the minimum amount of data needed to\nreasonably represent the 2-D shots in a tennis match, including the\nplayer and ball locations, along with match context data, such as who\nis serving, serve side, serve number, and the match score.\nAlthough we had to collect our data by manually annotating videos\n(taking about 3 hours per hour of video), we recognize the ability\nof state-of-the-art systems to automatically collect ball and player\ntracking information. To ef\ufb01ciently integrate score progression and\nother context information with ball and player locations, we developed\na \ufb01nite state machine, shown in Figure 2, along with keyboard and\nmouse accelerators, to input this data. We also generated a homography\nto translate points from the video image plane into the tennis court plane\nusing Java libraries available from OpenCV (Open Source Computer\nVision Library).", "data_code": {"tables": 1, "categorical": 1, "quantitative": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "4.2 Data Transformation\nThe data, at its simplest level, is a series of play events, where each\nplay event is either a bounce event or a hit event. For each event, we\ninclude the 2-D location data of the ball and the players, along with\nthe score context information and a time stamp (taken directly from\nthe video). We also include summary point information, including who\nwas serving, serve side, whether the point started from a \ufb01rst or second\nserve, the number of shots in the point, and the point outcome (winner,\nunforced error, etc.). To make this data more usable for the visual\nanalytics application we propose in this paper, we perform a series of\ntransformations, including data mirroring, transforming play events\ninto shots, spatial data discretization, and feature generation.\nData mirroring. Each player\u2019s location data is transformed as needed\nto keep them on one half of the court.\nTransforming play events into shots. Players win points by making\ngood shots or forcing their opponent to make bad shots. Using the raw\nspatial data, we convert hit-bounce event pairs (or, less frequently, hit-\nhit event pairs) into shots for each player. Each shot is referenced by a\nsequence number and a reverse sequence number (i.e., number of shots\nuntil the last shot) and indicates the hitting player, the receiving player,\nand whether the shot was a forehand or a backhand. This indexing\nscheme is useful, since the last few shots in a point are likely to be key\ndeterminants of the point outcome. Location data for the players and\nthe ball at the start of the shot, when the ball bounces (if it is not a\nvolley), and at the end of the shot is also included, as is an indication\nof the shot being a serve or a volley.\nSpatial data discretization. In our present and past discussions with\ntennis coaches and teaching professionals, we learned they typically\ndivide the court into three sections horizontally (left, center, and right).\nIn terms of depth, coaches looked for shots that originated from either\nbehind the baseline, inside the baseline, or at the net and that landed\neither short (within the service box area) or deep (past the service box).\nThis results in nine left-to-right one-dimensional shot patterns and six\ndepth-based one-dimensional shot patterns and, when combined, 54\ntwo-dimensional shot patterns. In order to indicate which pattern each\nshot belongs to, the location data is adjusted to the nearest anchor point.\nWe also maintain the original location data, allowing the user to see\nmore accurate shot data if desired.\nFeature generation. In addition to the traditional features that coaches\nare interested in, such as who is serving, serve side, serve number,\nwho won the point and how they won it, we included these additional\nfeatures only possible with the integration of context data with the\nspatio-temporal data:\nT Wide serve. Boolean value that is true if the returner hit the service\nreturn from outside the singles sidelines (i.e., was a wide serve).\nT Serve speed. Indicates the approximate velocity of the serve. Values\nare calibrated separately for each player based on knowledge of their\nabilities. Valid values include slow, medium, and fast.\nT Return of serve stroke side. Indicates if the return was a backhand\nor forehand.\nT Point length. Short points are de\ufb01ned as 0-4 shots, medium points as\n5-8 shots, and long points as 9+ shots. These ranges were validated\nwith multiple tennis coaches and teaching pros in a prior study.\nT Point differential. The number of points within a game separating\nthe players. The valid range for standard games is -3 to +3 and, for\ntie-break games, -6 to +6. Negative values indicate player one is\nbehind and positive values indicate player one is ahead.\nT Game differential. The number of games within the current set\nseparating the two players. The valid range is -5 to +5.\nT Player dominant stroke side. Indicates if a player hit more forehand\nshots than backhand shots in a point. Valid values are forehand,\nbackhand, and none. One stroke side is considered dominant only if\nit accounted for at least 60% of the shots.\nT Player dominant playing depth. Indicates if a player hit more shots\nfrom behind the baseline or inside the baseline. A depth is considered\ndominant only if at least 60% of the shots are made from that depth.\nOtherwise, this feature is set to neutral.\nT Shots from outside the sidelines. Boolean feature indicating if a\nplayer hit any of their shots from outside the singles sidelines.\nT Short shots. Boolean feature indicating if any of a player\u2019s shots\nlanded within their opponent\u2019s service box.\nThese features were generated through interviews with local area\nteaching professionals and coaches and are by no means an exhaustive\nlist of features. Our application can be easily extended to add whatever\nadditional features are desired, as long as those features can each be\nrepresented by a relatively small number of discrete values so they can\nalso be used as \ufb01lters. The raw spatial data, transformed and combined\nwith match context data to generate semantically meaningful features,\nserves as the foundation of our visual analytics system and helps us\nrealize the high-level design requirements enumerated in Section 3.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The similarity-based perspective, driven by design requirement\nR4 (Facilitate \ufb01nding patterns), helps users \ufb01nd patterns in the one-\ndimensional space-time charts by applying simple but effective ordering\nand clustering capabilities.", "solution_category": "data_manipulation", "solution_axial": "Clustering&Grouping", "solution_compoent": "", "axial_code": ["Clustering&Grouping"], "componenet_code": ["clustering_and_grouping"]}, {"solution_text": "The Point Analyzer is designed to support three distinct analytical perspectives on the data: sequential and similarity-based. The user can switch between perspectives using a drop-down list box. The sequential perspective provides the user with a familiar overview of the entire match in the order the points were played and is in direct support of design requirement R1 (Familiar overview first, details later). Each row contains one game's worth of data and the background color of the row corresponds to who won the game (i.e., blue for player one and red for player two, see 1 (B)). A game where one player is serving but the other player wins is known as a service break and is considered an important event in tennis. To emphasize this event, service breaks are surrounded with a dark border to make them stand out. The small multiples themselves, separated into game-rows, also visually mimic a bar chart, making it easy to distinguish long games, where players were struggling, from short games, easily won by a player.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix+line+point", "axial_code": [], "componenet_code": ["scatter", "line", "matrix"]}]}, {"author": "gsh", "index_original": 112, "paper_title": "CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R4: Facilitate finding patterns. Once users have drilled down to a specific set of points, they want to figure out the hows and whys to explain those points. The system should facilitate this process through visual cues and interaction techniques designed to make spatial patterns in the data more salient.", "requirement_code": {"discover_observation": 1, "identify_main_cause_item": 1, "describe_observation_item": 1}}, "data": {"data_text": "We collected spatio-temporal data from two tennis matches; one from a\nprofessional match broadcast on television, the other from an amateur\nmatch using a single consumer-level camera. In both cases, we were\nonly concerned with getting the minimum amount of data needed to\nreasonably represent the 2-D shots in a tennis match, including the\nplayer and ball locations, along with match context data, such as who\nis serving, serve side, serve number, and the match score.\nAlthough we had to collect our data by manually annotating videos\n(taking about 3 hours per hour of video), we recognize the ability\nof state-of-the-art systems to automatically collect ball and player\ntracking information. To ef\ufb01ciently integrate score progression and\nother context information with ball and player locations, we developed\na \ufb01nite state machine, shown in Figure 2, along with keyboard and\nmouse accelerators, to input this data. We also generated a homography\nto translate points from the video image plane into the tennis court plane\nusing Java libraries available from OpenCV (Open Source Computer\nVision Library).", "data_code": {"tables": 1, "categorical": 1, "quantitative": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "4.2 Data Transformation\nThe data, at its simplest level, is a series of play events, where each\nplay event is either a bounce event or a hit event. For each event, we\ninclude the 2-D location data of the ball and the players, along with\nthe score context information and a time stamp (taken directly from\nthe video). We also include summary point information, including who\nwas serving, serve side, whether the point started from a \ufb01rst or second\nserve, the number of shots in the point, and the point outcome (winner,\nunforced error, etc.). To make this data more usable for the visual\nanalytics application we propose in this paper, we perform a series of\ntransformations, including data mirroring, transforming play events\ninto shots, spatial data discretization, and feature generation.\nData mirroring. Each player\u2019s location data is transformed as needed\nto keep them on one half of the court.\nTransforming play events into shots. Players win points by making\ngood shots or forcing their opponent to make bad shots. Using the raw\nspatial data, we convert hit-bounce event pairs (or, less frequently, hit-\nhit event pairs) into shots for each player. Each shot is referenced by a\nsequence number and a reverse sequence number (i.e., number of shots\nuntil the last shot) and indicates the hitting player, the receiving player,\nand whether the shot was a forehand or a backhand. This indexing\nscheme is useful, since the last few shots in a point are likely to be key\ndeterminants of the point outcome. Location data for the players and\nthe ball at the start of the shot, when the ball bounces (if it is not a\nvolley), and at the end of the shot is also included, as is an indication\nof the shot being a serve or a volley.\nSpatial data discretization. In our present and past discussions with\ntennis coaches and teaching professionals, we learned they typically\ndivide the court into three sections horizontally (left, center, and right).\nIn terms of depth, coaches looked for shots that originated from either\nbehind the baseline, inside the baseline, or at the net and that landed\neither short (within the service box area) or deep (past the service box).\nThis results in nine left-to-right one-dimensional shot patterns and six\ndepth-based one-dimensional shot patterns and, when combined, 54\ntwo-dimensional shot patterns. In order to indicate which pattern each\nshot belongs to, the location data is adjusted to the nearest anchor point.\nWe also maintain the original location data, allowing the user to see\nmore accurate shot data if desired.\nFeature generation. In addition to the traditional features that coaches\nare interested in, such as who is serving, serve side, serve number,\nwho won the point and how they won it, we included these additional\nfeatures only possible with the integration of context data with the\nspatio-temporal data:\nT Wide serve. Boolean value that is true if the returner hit the service\nreturn from outside the singles sidelines (i.e., was a wide serve).\nT Serve speed. Indicates the approximate velocity of the serve. Values\nare calibrated separately for each player based on knowledge of their\nabilities. Valid values include slow, medium, and fast.\nT Return of serve stroke side. Indicates if the return was a backhand\nor forehand.\nT Point length. Short points are de\ufb01ned as 0-4 shots, medium points as\n5-8 shots, and long points as 9+ shots. These ranges were validated\nwith multiple tennis coaches and teaching pros in a prior study.\nT Point differential. The number of points within a game separating\nthe players. The valid range for standard games is -3 to +3 and, for\ntie-break games, -6 to +6. Negative values indicate player one is\nbehind and positive values indicate player one is ahead.\nT Game differential. The number of games within the current set\nseparating the two players. The valid range is -5 to +5.\nT Player dominant stroke side. Indicates if a player hit more forehand\nshots than backhand shots in a point. Valid values are forehand,\nbackhand, and none. One stroke side is considered dominant only if\nit accounted for at least 60% of the shots.\nT Player dominant playing depth. Indicates if a player hit more shots\nfrom behind the baseline or inside the baseline. A depth is considered\ndominant only if at least 60% of the shots are made from that depth.\nOtherwise, this feature is set to neutral.\nT Shots from outside the sidelines. Boolean feature indicating if a\nplayer hit any of their shots from outside the singles sidelines.\nT Short shots. Boolean feature indicating if any of a player\u2019s shots\nlanded within their opponent\u2019s service box.\nThese features were generated through interviews with local area\nteaching professionals and coaches and are by no means an exhaustive\nlist of features. Our application can be easily extended to add whatever\nadditional features are desired, as long as those features can each be\nrepresented by a relatively small number of discrete values so they can\nalso be used as \ufb01lters. The raw spatial data, transformed and combined\nwith match context data to generate semantically meaningful features,\nserves as the foundation of our visual analytics system and helps us\nrealize the high-level design requirements enumerated in Section 3.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "5.3 Shot Analyzer\nThe interplay between the Point Selector and the Point Analyzer enables\nusers to identify subsets of points they are interested in (the what). The\nShot Analyzer, depicted in Figure 1 (C), helps them see the why by\ndisplaying individual shots for the points selected in the Point Analyzer.\nEach row displays shots from a single point. From left to right, we\ndisplay the serve and the return, followed by the last three shots hit by\neach player. Except for the serves and returns, the shots are always\naligned such that all of the shots in a single column belong to one player,\nmaking it easier to compare shots across points. In our discussions with\ntennis teaching pros and coaches, we veri\ufb01ed that effective serves and\nreturns are key elements of understanding point outcomes. Furthermore,\nif there are any multi-shot patterns associated with winning or losing a\npoint, these are likely to be manifested in the last few shots of a point.\nThe main design requirements underpinning this component are R4\n(Facilitate \ufb01nding patterns) and R5 (Keep the human in the analysis\nloop). There are a number of novel design features that help achieve\nthese requirements, including shot encoding, dimension selection with\nspatial simpli\ufb01cation, shot sequence alignment, and ordering and clus-\ntering facilitated by user-speci\ufb01ed shot similarity features.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "matrix+line+point", "axial_code": [], "componenet_code": ["scatter", "line", "matrix"]}]}, {"author": "gsh", "index_original": 113, "paper_title": "CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R5: Keep the human in the analysis loop. Users are not just passive consumers of the results provided. They want to be active participants in the knowledge discovery process by introducing their own assumptions and beliefs into the analysis loop to confirm or reject hypotheses.", "requirement_code": {"collect_evidence": 1, "interactivity": 1}}, "data": {"data_text": "We collected spatio-temporal data from two tennis matches; one from a\nprofessional match broadcast on television, the other from an amateur\nmatch using a single consumer-level camera. In both cases, we were\nonly concerned with getting the minimum amount of data needed to\nreasonably represent the 2-D shots in a tennis match, including the\nplayer and ball locations, along with match context data, such as who\nis serving, serve side, serve number, and the match score.\nAlthough we had to collect our data by manually annotating videos\n(taking about 3 hours per hour of video), we recognize the ability\nof state-of-the-art systems to automatically collect ball and player\ntracking information. To ef\ufb01ciently integrate score progression and\nother context information with ball and player locations, we developed\na \ufb01nite state machine, shown in Figure 2, along with keyboard and\nmouse accelerators, to input this data. We also generated a homography\nto translate points from the video image plane into the tennis court plane\nusing Java libraries available from OpenCV (Open Source Computer\nVision Library).", "data_code": {"tables": 1, "categorical": 1, "quantitative": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "4.2 Data Transformation\nThe data, at its simplest level, is a series of play events, where each\nplay event is either a bounce event or a hit event. For each event, we\ninclude the 2-D location data of the ball and the players, along with\nthe score context information and a time stamp (taken directly from\nthe video). We also include summary point information, including who\nwas serving, serve side, whether the point started from a \ufb01rst or second\nserve, the number of shots in the point, and the point outcome (winner,\nunforced error, etc.). To make this data more usable for the visual\nanalytics application we propose in this paper, we perform a series of\ntransformations, including data mirroring, transforming play events\ninto shots, spatial data discretization, and feature generation.\nData mirroring. Each player\u2019s location data is transformed as needed\nto keep them on one half of the court.\nTransforming play events into shots. Players win points by making\ngood shots or forcing their opponent to make bad shots. Using the raw\nspatial data, we convert hit-bounce event pairs (or, less frequently, hit-\nhit event pairs) into shots for each player. Each shot is referenced by a\nsequence number and a reverse sequence number (i.e., number of shots\nuntil the last shot) and indicates the hitting player, the receiving player,\nand whether the shot was a forehand or a backhand. This indexing\nscheme is useful, since the last few shots in a point are likely to be key\ndeterminants of the point outcome. Location data for the players and\nthe ball at the start of the shot, when the ball bounces (if it is not a\nvolley), and at the end of the shot is also included, as is an indication\nof the shot being a serve or a volley.\nSpatial data discretization. In our present and past discussions with\ntennis coaches and teaching professionals, we learned they typically\ndivide the court into three sections horizontally (left, center, and right).\nIn terms of depth, coaches looked for shots that originated from either\nbehind the baseline, inside the baseline, or at the net and that landed\neither short (within the service box area) or deep (past the service box).\nThis results in nine left-to-right one-dimensional shot patterns and six\ndepth-based one-dimensional shot patterns and, when combined, 54\ntwo-dimensional shot patterns. In order to indicate which pattern each\nshot belongs to, the location data is adjusted to the nearest anchor point.\nWe also maintain the original location data, allowing the user to see\nmore accurate shot data if desired.\nFeature generation. In addition to the traditional features that coaches\nare interested in, such as who is serving, serve side, serve number,\nwho won the point and how they won it, we included these additional\nfeatures only possible with the integration of context data with the\nspatio-temporal data:\nT Wide serve. Boolean value that is true if the returner hit the service\nreturn from outside the singles sidelines (i.e., was a wide serve).\nT Serve speed. Indicates the approximate velocity of the serve. Values\nare calibrated separately for each player based on knowledge of their\nabilities. Valid values include slow, medium, and fast.\nT Return of serve stroke side. Indicates if the return was a backhand\nor forehand.\nT Point length. Short points are de\ufb01ned as 0-4 shots, medium points as\n5-8 shots, and long points as 9+ shots. These ranges were validated\nwith multiple tennis coaches and teaching pros in a prior study.\nT Point differential. The number of points within a game separating\nthe players. The valid range for standard games is -3 to +3 and, for\ntie-break games, -6 to +6. Negative values indicate player one is\nbehind and positive values indicate player one is ahead.\nT Game differential. The number of games within the current set\nseparating the two players. The valid range is -5 to +5.\nT Player dominant stroke side. Indicates if a player hit more forehand\nshots than backhand shots in a point. Valid values are forehand,\nbackhand, and none. One stroke side is considered dominant only if\nit accounted for at least 60% of the shots.\nT Player dominant playing depth. Indicates if a player hit more shots\nfrom behind the baseline or inside the baseline. A depth is considered\ndominant only if at least 60% of the shots are made from that depth.\nOtherwise, this feature is set to neutral.\nT Shots from outside the sidelines. Boolean feature indicating if a\nplayer hit any of their shots from outside the singles sidelines.\nT Short shots. Boolean feature indicating if any of a player\u2019s shots\nlanded within their opponent\u2019s service box.\nThese features were generated through interviews with local area\nteaching professionals and coaches and are by no means an exhaustive\nlist of features. Our application can be easily extended to add whatever\nadditional features are desired, as long as those features can each be\nrepresented by a relatively small number of discrete values so they can\nalso be used as \ufb01lters. The raw spatial data, transformed and combined\nwith match context data to generate semantically meaningful features,\nserves as the foundation of our visual analytics system and helps us\nrealize the high-level design requirements enumerated in Section 3.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "5.3 Shot Analyzer\nThe interplay between the Point Selector and the Point Analyzer enables\nusers to identify subsets of points they are interested in (the what). The\nShot Analyzer, depicted in Figure 1 (C), helps them see the why by\ndisplaying individual shots for the points selected in the Point Analyzer.\nEach row displays shots from a single point. From left to right, we\ndisplay the serve and the return, followed by the last three shots hit by\neach player. Except for the serves and returns, the shots are always\naligned such that all of the shots in a single column belong to one player,\nmaking it easier to compare shots across points. In our discussions with\ntennis teaching pros and coaches, we veri\ufb01ed that effective serves and\nreturns are key elements of understanding point outcomes. Furthermore,\nif there are any multi-shot patterns associated with winning or losing a\npoint, these are likely to be manifested in the last few shots of a point.\nThe main design requirements underpinning this component are R4\n(Facilitate \ufb01nding patterns) and R5 (Keep the human in the analysis\nloop). There are a number of novel design features that help achieve\nthese requirements, including shot encoding, dimension selection with\nspatial simpli\ufb01cation, shot sequence alignment, and ordering and clus-\ntering facilitated by user-speci\ufb01ed shot similarity features.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "matrix+line+point", "axial_code": [], "componenet_code": ["scatter", "line", "matrix"]}]}, {"author": "gsh", "index_original": 114, "paper_title": "CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R5: Keep the human in the analysis loop. Users are not just passive consumers of the results provided. They want to be active participants in the knowledge discovery process by introducing their own assumptions and beliefs into the analysis loop to confirm or reject hypotheses.", "requirement_code": {"collect_evidence": 1, "interactivity": 1}}, "data": {"data_text": "We collected spatio-temporal data from two tennis matches; one from a\nprofessional match broadcast on television, the other from an amateur\nmatch using a single consumer-level camera. In both cases, we were\nonly concerned with getting the minimum amount of data needed to\nreasonably represent the 2-D shots in a tennis match, including the\nplayer and ball locations, along with match context data, such as who\nis serving, serve side, serve number, and the match score.\nAlthough we had to collect our data by manually annotating videos\n(taking about 3 hours per hour of video), we recognize the ability\nof state-of-the-art systems to automatically collect ball and player\ntracking information. To ef\ufb01ciently integrate score progression and\nother context information with ball and player locations, we developed\na \ufb01nite state machine, shown in Figure 2, along with keyboard and\nmouse accelerators, to input this data. We also generated a homography\nto translate points from the video image plane into the tennis court plane\nusing Java libraries available from OpenCV (Open Source Computer\nVision Library).", "data_code": {"tables": 1, "categorical": 1, "quantitative": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "4.2 Data Transformation\nThe data, at its simplest level, is a series of play events, where each\nplay event is either a bounce event or a hit event. For each event, we\ninclude the 2-D location data of the ball and the players, along with\nthe score context information and a time stamp (taken directly from\nthe video). We also include summary point information, including who\nwas serving, serve side, whether the point started from a \ufb01rst or second\nserve, the number of shots in the point, and the point outcome (winner,\nunforced error, etc.). To make this data more usable for the visual\nanalytics application we propose in this paper, we perform a series of\ntransformations, including data mirroring, transforming play events\ninto shots, spatial data discretization, and feature generation.\nData mirroring. Each player\u2019s location data is transformed as needed\nto keep them on one half of the court.\nTransforming play events into shots. Players win points by making\ngood shots or forcing their opponent to make bad shots. Using the raw\nspatial data, we convert hit-bounce event pairs (or, less frequently, hit-\nhit event pairs) into shots for each player. Each shot is referenced by a\nsequence number and a reverse sequence number (i.e., number of shots\nuntil the last shot) and indicates the hitting player, the receiving player,\nand whether the shot was a forehand or a backhand. This indexing\nscheme is useful, since the last few shots in a point are likely to be key\ndeterminants of the point outcome. Location data for the players and\nthe ball at the start of the shot, when the ball bounces (if it is not a\nvolley), and at the end of the shot is also included, as is an indication\nof the shot being a serve or a volley.\nSpatial data discretization. In our present and past discussions with\ntennis coaches and teaching professionals, we learned they typically\ndivide the court into three sections horizontally (left, center, and right).\nIn terms of depth, coaches looked for shots that originated from either\nbehind the baseline, inside the baseline, or at the net and that landed\neither short (within the service box area) or deep (past the service box).\nThis results in nine left-to-right one-dimensional shot patterns and six\ndepth-based one-dimensional shot patterns and, when combined, 54\ntwo-dimensional shot patterns. In order to indicate which pattern each\nshot belongs to, the location data is adjusted to the nearest anchor point.\nWe also maintain the original location data, allowing the user to see\nmore accurate shot data if desired.\nFeature generation. In addition to the traditional features that coaches\nare interested in, such as who is serving, serve side, serve number,\nwho won the point and how they won it, we included these additional\nfeatures only possible with the integration of context data with the\nspatio-temporal data:\nT Wide serve. Boolean value that is true if the returner hit the service\nreturn from outside the singles sidelines (i.e., was a wide serve).\nT Serve speed. Indicates the approximate velocity of the serve. Values\nare calibrated separately for each player based on knowledge of their\nabilities. Valid values include slow, medium, and fast.\nT Return of serve stroke side. Indicates if the return was a backhand\nor forehand.\nT Point length. Short points are de\ufb01ned as 0-4 shots, medium points as\n5-8 shots, and long points as 9+ shots. These ranges were validated\nwith multiple tennis coaches and teaching pros in a prior study.\nT Point differential. The number of points within a game separating\nthe players. The valid range for standard games is -3 to +3 and, for\ntie-break games, -6 to +6. Negative values indicate player one is\nbehind and positive values indicate player one is ahead.\nT Game differential. The number of games within the current set\nseparating the two players. The valid range is -5 to +5.\nT Player dominant stroke side. Indicates if a player hit more forehand\nshots than backhand shots in a point. Valid values are forehand,\nbackhand, and none. One stroke side is considered dominant only if\nit accounted for at least 60% of the shots.\nT Player dominant playing depth. Indicates if a player hit more shots\nfrom behind the baseline or inside the baseline. A depth is considered\ndominant only if at least 60% of the shots are made from that depth.\nOtherwise, this feature is set to neutral.\nT Shots from outside the sidelines. Boolean feature indicating if a\nplayer hit any of their shots from outside the singles sidelines.\nT Short shots. Boolean feature indicating if any of a player\u2019s shots\nlanded within their opponent\u2019s service box.\nThese features were generated through interviews with local area\nteaching professionals and coaches and are by no means an exhaustive\nlist of features. Our application can be easily extended to add whatever\nadditional features are desired, as long as those features can each be\nrepresented by a relatively small number of discrete values so they can\nalso be used as \ufb01lters. The raw spatial data, transformed and combined\nwith match context data to generate semantically meaningful features,\nserves as the foundation of our visual analytics system and helps us\nrealize the high-level design requirements enumerated in Section 3.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The Point Analyzer is designed to support three distinct analytical perspectives on the data: sequential and similarity-based. The user can switch between perspectives using a drop-down list box. The sequential perspective provides the user with a familiar overview of the entire match in the order the points were played and is in direct support of design requirement R1 (Familiar overview first, details later). Each row contains one game's worth of data and the background color of the row corresponds to who won the game (i.e., blue for player one and red for player two, see 1 (B)). A game where one player is serving but the other player wins is known as a service break and is considered an important event in tennis. To emphasize this event, service breaks are surrounded with a dark border to make them stand out. The small multiples themselves, separated into game-rows, also visually mimic a bar chart, making it easy to distinguish long games, where players were struggling, from short games, easily won by a player.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix+line+point", "axial_code": [], "componenet_code": ["scatter", "line", "matrix"]}, {"solution_text": "We also support R5 (Keep the human in the\nanalysis loop) by allowing users to specify which features to consider\nwhen calculating similarity measures. These include all of the features\nlisted in Section 4.2.", "solution_category": "interaction", "solution_axial": "Encode", "solution_compoent": "", "axial_code": ["Encode"], "componenet_code": ["encode"]}]}, {"author": "gsh", "index_original": 115, "paper_title": "CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R5: Keep the human in the analysis loop. Users are not just passive consumers of the results provided. They want to be active participants in the knowledge discovery process by introducing their own assumptions and beliefs into the analysis loop to confirm or reject hypotheses.", "requirement_code": {"collect_evidence": 1, "interactivity": 1}}, "data": {"data_text": "We collected spatio-temporal data from two tennis matches; one from a\nprofessional match broadcast on television, the other from an amateur\nmatch using a single consumer-level camera. In both cases, we were\nonly concerned with getting the minimum amount of data needed to\nreasonably represent the 2-D shots in a tennis match, including the\nplayer and ball locations, along with match context data, such as who\nis serving, serve side, serve number, and the match score.\nAlthough we had to collect our data by manually annotating videos\n(taking about 3 hours per hour of video), we recognize the ability\nof state-of-the-art systems to automatically collect ball and player\ntracking information. To ef\ufb01ciently integrate score progression and\nother context information with ball and player locations, we developed\na \ufb01nite state machine, shown in Figure 2, along with keyboard and\nmouse accelerators, to input this data. We also generated a homography\nto translate points from the video image plane into the tennis court plane\nusing Java libraries available from OpenCV (Open Source Computer\nVision Library).", "data_code": {"tables": 1, "categorical": 1, "quantitative": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "4.2 Data Transformation\nThe data, at its simplest level, is a series of play events, where each\nplay event is either a bounce event or a hit event. For each event, we\ninclude the 2-D location data of the ball and the players, along with\nthe score context information and a time stamp (taken directly from\nthe video). We also include summary point information, including who\nwas serving, serve side, whether the point started from a \ufb01rst or second\nserve, the number of shots in the point, and the point outcome (winner,\nunforced error, etc.). To make this data more usable for the visual\nanalytics application we propose in this paper, we perform a series of\ntransformations, including data mirroring, transforming play events\ninto shots, spatial data discretization, and feature generation.\nData mirroring. Each player\u2019s location data is transformed as needed\nto keep them on one half of the court.\nTransforming play events into shots. Players win points by making\ngood shots or forcing their opponent to make bad shots. Using the raw\nspatial data, we convert hit-bounce event pairs (or, less frequently, hit-\nhit event pairs) into shots for each player. Each shot is referenced by a\nsequence number and a reverse sequence number (i.e., number of shots\nuntil the last shot) and indicates the hitting player, the receiving player,\nand whether the shot was a forehand or a backhand. This indexing\nscheme is useful, since the last few shots in a point are likely to be key\ndeterminants of the point outcome. Location data for the players and\nthe ball at the start of the shot, when the ball bounces (if it is not a\nvolley), and at the end of the shot is also included, as is an indication\nof the shot being a serve or a volley.\nSpatial data discretization. In our present and past discussions with\ntennis coaches and teaching professionals, we learned they typically\ndivide the court into three sections horizontally (left, center, and right).\nIn terms of depth, coaches looked for shots that originated from either\nbehind the baseline, inside the baseline, or at the net and that landed\neither short (within the service box area) or deep (past the service box).\nThis results in nine left-to-right one-dimensional shot patterns and six\ndepth-based one-dimensional shot patterns and, when combined, 54\ntwo-dimensional shot patterns. In order to indicate which pattern each\nshot belongs to, the location data is adjusted to the nearest anchor point.\nWe also maintain the original location data, allowing the user to see\nmore accurate shot data if desired.\nFeature generation. In addition to the traditional features that coaches\nare interested in, such as who is serving, serve side, serve number,\nwho won the point and how they won it, we included these additional\nfeatures only possible with the integration of context data with the\nspatio-temporal data:\nT Wide serve. Boolean value that is true if the returner hit the service\nreturn from outside the singles sidelines (i.e., was a wide serve).\nT Serve speed. Indicates the approximate velocity of the serve. Values\nare calibrated separately for each player based on knowledge of their\nabilities. Valid values include slow, medium, and fast.\nT Return of serve stroke side. Indicates if the return was a backhand\nor forehand.\nT Point length. Short points are de\ufb01ned as 0-4 shots, medium points as\n5-8 shots, and long points as 9+ shots. These ranges were validated\nwith multiple tennis coaches and teaching pros in a prior study.\nT Point differential. The number of points within a game separating\nthe players. The valid range for standard games is -3 to +3 and, for\ntie-break games, -6 to +6. Negative values indicate player one is\nbehind and positive values indicate player one is ahead.\nT Game differential. The number of games within the current set\nseparating the two players. The valid range is -5 to +5.\nT Player dominant stroke side. Indicates if a player hit more forehand\nshots than backhand shots in a point. Valid values are forehand,\nbackhand, and none. One stroke side is considered dominant only if\nit accounted for at least 60% of the shots.\nT Player dominant playing depth. Indicates if a player hit more shots\nfrom behind the baseline or inside the baseline. A depth is considered\ndominant only if at least 60% of the shots are made from that depth.\nOtherwise, this feature is set to neutral.\nT Shots from outside the sidelines. Boolean feature indicating if a\nplayer hit any of their shots from outside the singles sidelines.\nT Short shots. Boolean feature indicating if any of a player\u2019s shots\nlanded within their opponent\u2019s service box.\nThese features were generated through interviews with local area\nteaching professionals and coaches and are by no means an exhaustive\nlist of features. Our application can be easily extended to add whatever\nadditional features are desired, as long as those features can each be\nrepresented by a relatively small number of discrete values so they can\nalso be used as \ufb01lters. The raw spatial data, transformed and combined\nwith match context data to generate semantically meaningful features,\nserves as the foundation of our visual analytics system and helps us\nrealize the high-level design requirements enumerated in Section 3.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The Point Analyzer is designed to support three distinct analytical perspectives on the data: sequential and similarity-based. The user can switch between perspectives using a drop-down list box. The sequential perspective provides the user with a familiar overview of the entire match in the order the points were played and is in direct support of design requirement R1 (Familiar overview first, details later). Each row contains one game's worth of data and the background color of the row corresponds to who won the game (i.e., blue for player one and red for player two, see 1 (B)). A game where one player is serving but the other player wins is known as a service break and is considered an important event in tennis. To emphasize this event, service breaks are surrounded with a dark border to make them stand out. The small multiples themselves, separated into game-rows, also visually mimic a bar chart, making it easy to distinguish long games, where players were struggling, from short games, easily won by a player.", "solution_category": "visualization", "solution_axial": "repetition", "solution_compoent": "matrix+line+point", "axial_code": [], "componenet_code": ["scatter", "line", "matrix"]}, {"solution_text": "5.2.1 1-D Space-Time Charts\nThe design of the 1-D space-time charts is driven primarily by re-\nquirement R4 (Facilitate \ufb01nding patterns), but is also in\ufb02uenced by\nrequirement R3 (Focus on point outcomes) and R5 (Keep the human in\nthe analysis loop). To understand how these requirements in\ufb02uence the\ndesign, we describe how the design evolved to its current form. This is\nshown in Figure 3.\nIn our design, we use the tennis court diagram as a backdrop to\nprovide a reference point for the location data. Players are represented\nas colored circles (blue for player one, red for player two). The stroke\nside players hit a shot from (backhand or forehand) is very important,\nas one side tends to be weaker or less reliable than the other. Therefore,\nwe encode forehand shots as solid circles and backhand shots as hollow\ncircles. Ball landing locations are also encoded using colored circles\nthat are noticeably smaller than the player circles. These were initially\nencoded using yellow, maintaining the metaphor of a tennis ball. How-\never, we soon discovered this made it difficult to distinguish which\nplayer hit which shot. We therefore encoded each ball color based on\nwho hit the ball. Following the Gestalt law of continuity, we connected\nthe player locations using blue lines for player one, red lines for player\ntwo, and yellow lines for the ball landing locations.\nFigure 3 (A) depicts an early design, where we kept each player on\ntheir side of the court. In this design, the time dimension starts from\nboth the left and right sides and continues towards the middle of the\ncourt. The main problem with this design is the difficulty in directly\ncomparing player locations at the same time due to the dual time axes.\nIt also requires more space because each player\u2019s location remains on\ntheir half of the court.\nWe then plotted both players as playing from the left side of the\ncourt, as shown in 3 (B). This allows us to use a single time axis,\nrunning from left to right. This type of plot requires flipping both the x-\nand y-coordinates of the player on the right to put them on the left (i.e.,\nso the player\u2019s relative court location is maintained). The main issue\nwith this approach is that, when players are on opposite sides of the\ncourt (i.e., diagonal from one another), they appear co-located on this\nchart. Similarly, when players are on the same side of the court (i.e.,\nnear the same sideline), they are plotted in different locations on this\nchart, which appears counter-intuitive.\nFor the left/right dimension, we therefore settle on the design shown\nin Figure 3 (C). In this design, we only horizontally flip the right-side\nplayer\u2019s location data, so that when players are on opposite sides of the\ncourt, that is also depicted in the chart. We then add additional context\ninformation to this chart to make it more meaningful, including the\nmatch score, a serve indicator, and a point outcome indicator.\nTo visualize the depth-based information, it comes natural to keep\nplayers on their respective side of the tennis court and run the time axis\nfrom top to bottom, with the ball and player locations being staggered\nalong the time axis to indicate the order of shots. The challenge is to\nplot the ball location in a way to effectively differentiate short shots\nfrom deep shots. In our first attempt, shown in Figure 3 (D), we apply\nhorizontal lines connecting the hitting player to the ball bounce location.\nThe main issue with this approach is that the chart is overly dominated\nby the yellow lines and it is dif\ufb01cult to see the \ufb02ow. We overcome this\nproblem in our second attempt, shown in Figure 3 (E), by connecting\nthe ball landing locations together, thus giving a better impression of\nthe back and forth shots. However, this design is dominated by the ball\ntrajectories, posing a challenge to effectively interpret the situation.\nWe \ufb01nally settle on the design shown in Figure 3 (F), where we\nonly connect the ball landing locations on the same side of the court.\nAlthough we lose the back and forth nature of the shots, this approach\ntakes the focus from the speci\ufb01c ball trajectory to the overall situation\nand creates a distinct depth pattern for each player, thus making these\npatterns easier to see (supporting requirement R4). All of the same\ntreatments applied to the left/right chart to include context information\nare incorporated here.\nThe \ufb01nal design and its encoding is explained in Figure 4. The top\nleft is the resulting graphic if we try to display all the player movements\nand shots on one single tennis court. In addition to the problem of\nocclusions, there is no easy way to follow the progression of shots and\nmovements in the point. We therefore split out the left/right component\nand depth component separately and then plot each one over time. The\ntop right image depicts the left/right movements of the players and ball\non the court and the bottom left image depicts the depth movements.\nContext information is also encoded on the graphics. This includes\nscore information (both in the current game and in the overall match),\nand the indication of the serving player using a red or blue vertical line\non the left side of the chart. A solid line indicates the point started\nfrom a \ufb01rst serve, while a dashed line indicates it started from a second\nserve. Similarly, we use a color-coded line on the right side of the chart\nto indicate who won the point. A solid line means they won the point\ndue to a \u201cwinner\u201d shot, while a dashed line means they won the point\ndue to an error by their opponent. This exempli\ufb01es our support for\nrequirement R3.\nMost tennis players have one stroke side (backhand or forehand) that\nis better than the other. One typical strategy is to try make your oppo-\nnent hit shots from their weaker side while you try to make shots from\nyour stronger side. Therefore, we incorporate stroke side information\ninto the chart. A solid red or blue circle indicates a forehand stroke,\nwhile a hollow red or blue circle indicates a backhand stroke. For the\nball landing locations, we use smaller-diameter circles that share the\nsame color as the player that hit the ball. This color coding is essential\nsince users have the option of only plotting ball locations.\nWe recognize that the design choices we have made for the 1-D\nSpace-Time Charts involve trade-offs between temporal alignment and\nspatial alignment, with an emphasis on maintaining temporal alignment\nin order to facilitate the direct, dynamic comparison of player positions\n(i.e., hitting diagonally across the court to one another or down-the-\nline). Of course, spatial \ufb01delity is maintained in the Shot Analyzer,\ndescribed in section 5.3. To help minimize clutter, the user can toggle\noff the player and/or ball visualization components, allowing them to\nfocus of speci\ufb01c aspects.", "solution_category": "visualization", "solution_axial": "co_axis", "solution_compoent": "line+point", "axial_code": [], "componenet_code": ["scatter", "line"]}]}, {"author": "gsh", "index_original": 116, "paper_title": "CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R5: Keep the human in the analysis loop. Users are not just passive consumers of the results provided. They want to be active participants in the knowledge discovery process by introducing their own assumptions and beliefs into the analysis loop to confirm or reject hypotheses.", "requirement_code": {"collect_evidence": 1, "interactivity": 1}}, "data": {"data_text": "We collected spatio-temporal data from two tennis matches; one from a\nprofessional match broadcast on television, the other from an amateur\nmatch using a single consumer-level camera. In both cases, we were\nonly concerned with getting the minimum amount of data needed to\nreasonably represent the 2-D shots in a tennis match, including the\nplayer and ball locations, along with match context data, such as who\nis serving, serve side, serve number, and the match score.\nAlthough we had to collect our data by manually annotating videos\n(taking about 3 hours per hour of video), we recognize the ability\nof state-of-the-art systems to automatically collect ball and player\ntracking information. To ef\ufb01ciently integrate score progression and\nother context information with ball and player locations, we developed\na \ufb01nite state machine, shown in Figure 2, along with keyboard and\nmouse accelerators, to input this data. We also generated a homography\nto translate points from the video image plane into the tennis court plane\nusing Java libraries available from OpenCV (Open Source Computer\nVision Library).", "data_code": {"tables": 1, "categorical": 1, "quantitative": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "4.2 Data Transformation\nThe data, at its simplest level, is a series of play events, where each\nplay event is either a bounce event or a hit event. For each event, we\ninclude the 2-D location data of the ball and the players, along with\nthe score context information and a time stamp (taken directly from\nthe video). We also include summary point information, including who\nwas serving, serve side, whether the point started from a \ufb01rst or second\nserve, the number of shots in the point, and the point outcome (winner,\nunforced error, etc.). To make this data more usable for the visual\nanalytics application we propose in this paper, we perform a series of\ntransformations, including data mirroring, transforming play events\ninto shots, spatial data discretization, and feature generation.\nData mirroring. Each player\u2019s location data is transformed as needed\nto keep them on one half of the court.\nTransforming play events into shots. Players win points by making\ngood shots or forcing their opponent to make bad shots. Using the raw\nspatial data, we convert hit-bounce event pairs (or, less frequently, hit-\nhit event pairs) into shots for each player. Each shot is referenced by a\nsequence number and a reverse sequence number (i.e., number of shots\nuntil the last shot) and indicates the hitting player, the receiving player,\nand whether the shot was a forehand or a backhand. This indexing\nscheme is useful, since the last few shots in a point are likely to be key\ndeterminants of the point outcome. Location data for the players and\nthe ball at the start of the shot, when the ball bounces (if it is not a\nvolley), and at the end of the shot is also included, as is an indication\nof the shot being a serve or a volley.\nSpatial data discretization. In our present and past discussions with\ntennis coaches and teaching professionals, we learned they typically\ndivide the court into three sections horizontally (left, center, and right).\nIn terms of depth, coaches looked for shots that originated from either\nbehind the baseline, inside the baseline, or at the net and that landed\neither short (within the service box area) or deep (past the service box).\nThis results in nine left-to-right one-dimensional shot patterns and six\ndepth-based one-dimensional shot patterns and, when combined, 54\ntwo-dimensional shot patterns. In order to indicate which pattern each\nshot belongs to, the location data is adjusted to the nearest anchor point.\nWe also maintain the original location data, allowing the user to see\nmore accurate shot data if desired.\nFeature generation. In addition to the traditional features that coaches\nare interested in, such as who is serving, serve side, serve number,\nwho won the point and how they won it, we included these additional\nfeatures only possible with the integration of context data with the\nspatio-temporal data:\nT Wide serve. Boolean value that is true if the returner hit the service\nreturn from outside the singles sidelines (i.e., was a wide serve).\nT Serve speed. Indicates the approximate velocity of the serve. Values\nare calibrated separately for each player based on knowledge of their\nabilities. Valid values include slow, medium, and fast.\nT Return of serve stroke side. Indicates if the return was a backhand\nor forehand.\nT Point length. Short points are de\ufb01ned as 0-4 shots, medium points as\n5-8 shots, and long points as 9+ shots. These ranges were validated\nwith multiple tennis coaches and teaching pros in a prior study.\nT Point differential. The number of points within a game separating\nthe players. The valid range for standard games is -3 to +3 and, for\ntie-break games, -6 to +6. Negative values indicate player one is\nbehind and positive values indicate player one is ahead.\nT Game differential. The number of games within the current set\nseparating the two players. The valid range is -5 to +5.\nT Player dominant stroke side. Indicates if a player hit more forehand\nshots than backhand shots in a point. Valid values are forehand,\nbackhand, and none. One stroke side is considered dominant only if\nit accounted for at least 60% of the shots.\nT Player dominant playing depth. Indicates if a player hit more shots\nfrom behind the baseline or inside the baseline. A depth is considered\ndominant only if at least 60% of the shots are made from that depth.\nOtherwise, this feature is set to neutral.\nT Shots from outside the sidelines. Boolean feature indicating if a\nplayer hit any of their shots from outside the singles sidelines.\nT Short shots. Boolean feature indicating if any of a player\u2019s shots\nlanded within their opponent\u2019s service box.\nThese features were generated through interviews with local area\nteaching professionals and coaches and are by no means an exhaustive\nlist of features. Our application can be easily extended to add whatever\nadditional features are desired, as long as those features can each be\nrepresented by a relatively small number of discrete values so they can\nalso be used as \ufb01lters. The raw spatial data, transformed and combined\nwith match context data to generate semantically meaningful features,\nserves as the foundation of our visual analytics system and helps us\nrealize the high-level design requirements enumerated in Section 3.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "The Point Selector, depicted in Figure 1 (D), provides a faceted \ufb01ltering\ncapability that allows users to drill-down to speci\ufb01c sets of points based\non feature attribute values. The \ufb01lter selection is then propagated to\nthe Point Analyzer and Shot Analyzer for further analysis. The driving\nrequirements behind the design of the Point Selector are R1 (Familiar\noverview \ufb01rst, details later), R2 (Support multi-faceted, multi-level\nsearch), R3 (Focus on point outcomes), and R5 (Keep the human in\nthe analysis loop)", "solution_category": "interaction", "solution_axial": "Overview_and_Explore", "solution_compoent": "", "axial_code": ["Overview_and_Explore"], "componenet_code": ["overview_and_explore"]}, {"solution_text": "To support R2 (Support multi-faceted, multi-level search) and R5 (Keep the human in the analysis loop), we immediately apply the selected filters to the Point Analyzer and Shot Analyzer components and update the stacked bar charts in this component. We provide complete flexibility to users to iteratively select additional feature attribute values to filter on. For example, a user may select just player one first serves from the deuce side, revealing an unusually large number of points won by player one when player two hits a backhand return.", "solution_category": "interaction", "solution_axial": "filtering", "solution_compoent": "", "axial_code": ["filtering"], "componenet_code": ["filtering"]}]}, {"author": "gsh", "index_original": 117, "paper_title": "CourtTime: Generating Actionable Insights into Tennis Matches Using Visual Analytics", "pub_year": 2020, "domain": "Sport", "requirement": {"requirement_text": "R6: Provide ready access to the raw data. The various visualizations and interactions can provide substantial evidence to users that can explain how and why various points ended the way they did. However, a picture (or in this case, a video) is worth a thousand convincing colorful visualizations. Coaches, analysts, and the players themselves will want to see the actual video clips in order to really accept the validity of the findings.", "requirement_code": {"collect_evidence": 1}}, "data": {"data_text": "We collected spatio-temporal data from two tennis matches; one from a\nprofessional match broadcast on television, the other from an amateur\nmatch using a single consumer-level camera. In both cases, we were\nonly concerned with getting the minimum amount of data needed to\nreasonably represent the 2-D shots in a tennis match, including the\nplayer and ball locations, along with match context data, such as who\nis serving, serve side, serve number, and the match score.\nAlthough we had to collect our data by manually annotating videos\n(taking about 3 hours per hour of video), we recognize the ability\nof state-of-the-art systems to automatically collect ball and player\ntracking information. To ef\ufb01ciently integrate score progression and\nother context information with ball and player locations, we developed\na \ufb01nite state machine, shown in Figure 2, along with keyboard and\nmouse accelerators, to input this data. We also generated a homography\nto translate points from the video image plane into the tennis court plane\nusing Java libraries available from OpenCV (Open Source Computer\nVision Library).", "data_code": {"tables": 1, "categorical": 1, "quantitative": 1, "media": 1, "geometry": 1, "temporal": 1, "sequential": 1}}, "solution": [{"solution_text": "4.2 Data Transformation\nThe data, at its simplest level, is a series of play events, where each\nplay event is either a bounce event or a hit event. For each event, we\ninclude the 2-D location data of the ball and the players, along with\nthe score context information and a time stamp (taken directly from\nthe video). We also include summary point information, including who\nwas serving, serve side, whether the point started from a \ufb01rst or second\nserve, the number of shots in the point, and the point outcome (winner,\nunforced error, etc.). To make this data more usable for the visual\nanalytics application we propose in this paper, we perform a series of\ntransformations, including data mirroring, transforming play events\ninto shots, spatial data discretization, and feature generation.\nData mirroring. Each player\u2019s location data is transformed as needed\nto keep them on one half of the court.\nTransforming play events into shots. Players win points by making\ngood shots or forcing their opponent to make bad shots. Using the raw\nspatial data, we convert hit-bounce event pairs (or, less frequently, hit-\nhit event pairs) into shots for each player. Each shot is referenced by a\nsequence number and a reverse sequence number (i.e., number of shots\nuntil the last shot) and indicates the hitting player, the receiving player,\nand whether the shot was a forehand or a backhand. This indexing\nscheme is useful, since the last few shots in a point are likely to be key\ndeterminants of the point outcome. Location data for the players and\nthe ball at the start of the shot, when the ball bounces (if it is not a\nvolley), and at the end of the shot is also included, as is an indication\nof the shot being a serve or a volley.\nSpatial data discretization. In our present and past discussions with\ntennis coaches and teaching professionals, we learned they typically\ndivide the court into three sections horizontally (left, center, and right).\nIn terms of depth, coaches looked for shots that originated from either\nbehind the baseline, inside the baseline, or at the net and that landed\neither short (within the service box area) or deep (past the service box).\nThis results in nine left-to-right one-dimensional shot patterns and six\ndepth-based one-dimensional shot patterns and, when combined, 54\ntwo-dimensional shot patterns. In order to indicate which pattern each\nshot belongs to, the location data is adjusted to the nearest anchor point.\nWe also maintain the original location data, allowing the user to see\nmore accurate shot data if desired.\nFeature generation. In addition to the traditional features that coaches\nare interested in, such as who is serving, serve side, serve number,\nwho won the point and how they won it, we included these additional\nfeatures only possible with the integration of context data with the\nspatio-temporal data:\nT Wide serve. Boolean value that is true if the returner hit the service\nreturn from outside the singles sidelines (i.e., was a wide serve).\nT Serve speed. Indicates the approximate velocity of the serve. Values\nare calibrated separately for each player based on knowledge of their\nabilities. Valid values include slow, medium, and fast.\nT Return of serve stroke side. Indicates if the return was a backhand\nor forehand.\nT Point length. Short points are de\ufb01ned as 0-4 shots, medium points as\n5-8 shots, and long points as 9+ shots. These ranges were validated\nwith multiple tennis coaches and teaching pros in a prior study.\nT Point differential. The number of points within a game separating\nthe players. The valid range for standard games is -3 to +3 and, for\ntie-break games, -6 to +6. Negative values indicate player one is\nbehind and positive values indicate player one is ahead.\nT Game differential. The number of games within the current set\nseparating the two players. The valid range is -5 to +5.\nT Player dominant stroke side. Indicates if a player hit more forehand\nshots than backhand shots in a point. Valid values are forehand,\nbackhand, and none. One stroke side is considered dominant only if\nit accounted for at least 60% of the shots.\nT Player dominant playing depth. Indicates if a player hit more shots\nfrom behind the baseline or inside the baseline. A depth is considered\ndominant only if at least 60% of the shots are made from that depth.\nOtherwise, this feature is set to neutral.\nT Shots from outside the sidelines. Boolean feature indicating if a\nplayer hit any of their shots from outside the singles sidelines.\nT Short shots. Boolean feature indicating if any of a player\u2019s shots\nlanded within their opponent\u2019s service box.\nThese features were generated through interviews with local area\nteaching professionals and coaches and are by no means an exhaustive\nlist of features. Our application can be easily extended to add whatever\nadditional features are desired, as long as those features can each be\nrepresented by a relatively small number of discrete values so they can\nalso be used as \ufb01lters. The raw spatial data, transformed and combined\nwith match context data to generate semantically meaningful features,\nserves as the foundation of our visual analytics system and helps us\nrealize the high-level design requirements enumerated in Section 3.", "solution_category": "data_manipulation", "solution_axial": "AlgorithmicCalculation", "solution_compoent": "", "axial_code": ["AlgorithmicCalculation"], "componenet_code": ["algorithmic_calculation"]}, {"solution_text": "5.3 Shot Analyzer\nThe interplay between the Point Selector and the Point Analyzer enables\nusers to identify subsets of points they are interested in (the what). The\nShot Analyzer, depicted in Figure 1 (C), helps them see the why by\ndisplaying individual shots for the points selected in the Point Analyzer.\nEach row displays shots from a single point. From left to right, we\ndisplay the serve and the return, followed by the last three shots hit by\neach player. Except for the serves and returns, the shots are always\naligned such that all of the shots in a single column belong to one player,\nmaking it easier to compare shots across points. In our discussions with\ntennis teaching pros and coaches, we veri\ufb01ed that effective serves and\nreturns are key elements of understanding point outcomes. Furthermore,\nif there are any multi-shot patterns associated with winning or losing a\npoint, these are likely to be manifested in the last few shots of a point.\nThe main design requirements underpinning this component are R4\n(Facilitate \ufb01nding patterns) and R5 (Keep the human in the analysis\nloop). There are a number of novel design features that help achieve\nthese requirements, including shot encoding, dimension selection with\nspatial simpli\ufb01cation, shot sequence alignment, and ordering and clus-\ntering facilitated by user-speci\ufb01ed shot similarity features.", "solution_category": "visualization", "solution_axial": "stack", "solution_compoent": "matrix+line+point", "axial_code": [], "componenet_code": ["scatter", "line", "matrix"]}, {"solution_text": "5.3.1 Shot Encoding\nPoints are won by either making good shots or forcing your opponent\nto make bad shots. To help coaches and analysts understand why one\nplayer or the other won a speci\ufb01c point, we encode player and ball\nlocation data along with other context information for key shots within\neach point (i.e., the serve, return, and last 3 shots by each player). We\nuse the same encoding scheme we employed in the Point Analyzer. For\nshots other than the serve, we encode the location of the players, the\nball bounce location, the shot trajectory, and whether the shot was a\nforehand or backhand. For the serve, we encode a \ufb01rst serve using a\nsolid trajectory line and a second serve using a dashed trajectory line.\nFor shots, we use solid circles to represent forehands and hollow circles\nto represent backhands.\nFurthermore, our interviews with local area teaching pros and\ncoaches shows that they typically distinguish the side-to-side aspect of\nthe game from the depth-based components when discussing strategies.\nTo facilitate this perspective, we provide them the option of viewing\nshot data from a single dimension (left/right or depth) or in both di-\nmensions simultaneously. To emphasize visual shot patterns, we allow\nusers to apply a spatial discretization scheme (described in Section 4.2)\nto the location data. For the left/right dimension, this will result in\nonly nine possible shot trajectories. For the depth dimension, this is\neven simpler with only six possible shot trajectories. Once a particular\npattern has been found, users can toggle back to the raw data view in\norder to display the actual location data, supporting R6 (Provide ready\naccess to the raw data).", "solution_category": "interaction", "solution_axial": "selecting", "solution_compoent": "", "axial_code": ["selecting"], "componenet_code": ["selecting"]}]}]