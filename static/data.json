[
  {
    "author": "zsz",
    "index_original": 2,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T1: Cell Type Discovery and Calling. The task most frequently mentioned by all experts is identifying and analyzing speci\ufb01c types and states of cells based on the intensity and pattern of staining with speci\ufb01c antibodies (O1, O2, P1, P2, CB1-6). Challenges: Challenges lie in processing, displaying, and faceting the large and high-dimensional data, as well as in mutual support of manual and automated analysis. A lack of adequate tools makes this task very time-consuming at present.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "Classification and clustering can be triggered in different views to support the hierarchical discovery of cell types.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 3,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T1: Cell Type Discovery and Calling. The task most frequently mentioned by all experts is identifying and analyzing speci\ufb01c types and states of cells based on the intensity and pattern of staining with speci\ufb01c antibodies (O1, O2, P1, P2, CB1-6). Challenges: Challenges lie in processing, displaying, and faceting the large and high-dimensional data, as well as in mutual support of manual and automated analysis. A lack of adequate tools makes this task very time-consuming at present.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 4,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T2: Overview-Detail Exploration of Multi-Channel Image Data. A crucial task for oncologists and pathologists is rapid navigation and visualization of multi-channel images (O1, O2, P1, P2). Pathologists are accustomed to moving slides back and forth physically on a micro-scope stage and switching between high and low power views. They rely on a seamless visual experience to make a diagnosis. Challenges: Image analysis must not only support seamless pan and zoom, but also switching between groups of channels. Current tools do not scale beyond 4-5 channels and lack on-demand rendering, blending of channels, and means to emphasize (and recall) regions or individual cells of interest.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "Facetto\u2019s image viewer allows users to navigate and explore large multi- channel image data using a scalable multi-resolution visualization approach. The visualization supports interactive and seamless zooming and panning, on-demand rendering of multi-channel information, man- ual selection of cells, and highlighting of classification and clustering results as well as faceting operations. The image viewer also provides details on demand when hovering over individual cells.",
        "solution_category": "interaction",
        "solution_axial": "Filtering,OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["Filtering", "OverviewandExplore"],
        "componenet_code": ["filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 5,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T2: Overview-Detail Exploration of Multi-Channel Image Data. A crucial task for oncologists and pathologists is rapid navigation and visualization of multi-channel images (O1, O2, P1, P2). Pathologists are accustomed to moving slides back and forth physically on a micro-scope stage and switching between high and low power views. They rely on a seamless visual experience to make a diagnosis. Challenges: Image analysis must not only support seamless pan and zoom, but also switching between groups of channels. Current tools do not scale beyond 4-5 channels and lack on-demand rendering, blending of channels, and means to emphasize (and recall) regions or individual cells of interest.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Support dynamic selection of cells and their visual display in image space and dynamically adjust the render mode based on the cur-rent pixel\u2019s cell ID. ",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "image+area",
        "axial_code": ["Coordinate"],
        "componenet_code": ["image", "area"]
      },
      {
        "solution_text": "Facetto supports different render modes, depending on whether the user is currently focusing on the entire dataset, a subset(i.e., a node in the hierarchical phenotype tree), or a user selection (i.e.,based either on manual selection or a clustering/classification result).Facetto can then display individual cells in their original grayscale in-tensity, apply a color and opacity transfer function, or show color overlays for cluster/class membership.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 6,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T3: Data Filtering and (Sub-)Structuring. Another task frequently performed by pathologists is gating, which refers to manual \ufb01ltering of selected image channels based on the channel\u2019s intensity value range (often visualized as a frequency-intensity plot), or speci\ufb01c spatial features or regions of interest (P2, O2, CB2). Challenges: Analysis steps such as gating are often applied in an iterative manner in which the data is hierarchically faceted into subsets. These subsets can then be further analyzed, used in benchmarks, or exported for presentation or reuse with other samples. Thus, tracking the evolution and provenance of gates and gated data is important.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": { "tables": 1, "clusters_and_sets_and_lists": 1, "media": 1 }
    },
    "solution": [
      {
        "solution_text": "Experts usually start with a region of interest (ROI) at high resolution (so thatindividuals cells are visible) that represents a subset of the completemulti-channel image stack and then define spatial and image featuresto create data subsets of interest. The results obtained on this ROI are then applied to the entire specimen",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "tree+circle",
        "axial_code": ["Nesting"],
        "componenet_code": ["circle", "tree"]
      },
      {
        "solution_text": "Experts usually start with a region of interest (ROI) at high resolution (so thatindividuals cells are visible) that represents a subset of the completemulti-channel image stack and then define spatial and image featuresto create data subsets of interest. The results obtained on this ROI are then applied to the entire specimen",
        "solution_category": "interaction",
        "solution_axial": "Extractionoffeatures",
        "solution_compoent": "",
        "axial_code": ["Extractionoffeatures"],
        "componenet_code": ["extraction_of_features"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 7,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T3: Data Filtering and (Sub-)Structuring. Another task frequently performed by pathologists is gating, which refers to manual \ufb01ltering of selected image channels based on the channel\u2019s intensity value range (often visualized as a frequency-intensity plot), or speci\ufb01c spatial features or regions of interest (P2, O2, CB2). Challenges: Analysis steps such as gating are often applied in an iterative manner in which the data is hierarchically faceted into subsets. These subsets can then be further analyzed, used in benchmarks, or exported for presentation or reuse with other samples. Thus, tracking the evolution and provenance of gates and gated data is important.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Visually encode features of interest and subsets of the data. To make spatial distributions and correlations among multiple channels pre-attentively visible, we allow the user to blend the data from different channels into a single image.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "image+area",
        "axial_code": ["Coordinate"],
        "componenet_code": ["image", "area"]
      },
      {
        "solution_text": "Users select the appropriate channels for their current tasks based on domain knowledge. For each channel, users can set and modify a linear color and opacity transfer function by specifying the respective intensity range (i.e., lower and upper bound) as well as the colors of the transfer function.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 8,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T3: Data Filtering and (Sub-)Structuring. Another task frequently performed by pathologists is gating, which refers to manual \ufb01ltering of selected image channels based on the channel\u2019s intensity value range (often visualized as a frequency-intensity plot), or speci\ufb01c spatial features or regions of interest (P2, O2, CB2). Challenges: Analysis steps such as gating are often applied in an iterative manner in which the data is hierarchically faceted into subsets. These subsets can then be further analyzed, used in benchmarks, or exported for presentation or reuse with other samples. Thus, tracking the evolution and provenance of gates and gated data is important.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Support dynamic selection of cells and their visual display in image space and dynamically adjust the render mode based on the cur-rent pixel\u2019s cell ID. ",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "image+area",
        "axial_code": ["Coordinate"],
        "componenet_code": ["image", "area"]
      },
      {
        "solution_text": "Facetto supports different render modes, depending on whether the user is currently focusing on the entire dataset, a subset(i.e., a node in the hierarchical phenotype tree), or a user selection (i.e.,based either on manual selection or a clustering/classification result).Facetto can then display individual cells in their original grayscale in-tensity, apply a color and opacity transfer function, or show color overlays for cluster/class membership.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 9,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T3: Data Filtering and (Sub-)Structuring. Another task frequently performed by pathologists is gating, which refers to manual \ufb01ltering of selected image channels based on the channel\u2019s intensity value range (often visualized as a frequency-intensity plot), or speci\ufb01c spatial features or regions of interest (P2, O2, CB2). Challenges: Analysis steps such as gating are often applied in an iterative manner in which the data is hierarchically faceted into subsets. These subsets can then be further analyzed, used in benchmarks, or exported for presentation or reuse with other samples. Thus, tracking the evolution and provenance of gates and gated data is important.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Each ridge is equipped with range sliders, allowing to filter theunderlying data with visual feedback at the level of the distribution.The selection range is used directly for specifying the color transferfunction applied in the image viewer. To allow the exploration of fea-ture distributions for subsets of the image or individually selected cells,we overlay the parallel x-axes in the ridgeplot with vertical polylines(see orange paths), encoding each cell\u2019s features as a connectedpath, similar to parallel coordinate plots [36]. To reduce clutter, wedecrease opacity as the number of selected cells grows, an approachthat provides an indication of the correlations and distribution rangesof a selection, while focusing less on individual cells. Alternatively, asingle polyline can be used to represent the mean values of a cluster.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "image+parallelcoordinates",
        "axial_code": ["Stack"],
        "componenet_code": ["image", "parallelcoordinates"]
      },
      {
        "solution_text": "Each ridge is equipped with range sliders, allowing to filter theunderlying data with visual feedback at the level of the distribution.The selection range is used directly for specifying the color transferfunction applied in the image viewer. To allow the exploration of fea-ture distributions for subsets of the image or individually selected cells,we overlay the parallel x-axes in the ridgeplot with vertical polylines(see orange paths), encoding each cell\u2019s features as a connectedpath, similar to parallel coordinate plots [36]. To reduce clutter, wedecrease opacity as the number of selected cells grows, an approachthat provides an indication of the correlations and distribution rangesof a selection, while focusing less on individual cells. Alternatively, asingle polyline can be used to represent the mean values of a cluster.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 10,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T4: Proofreading and Analyzing Results in Spatial Context. Many algorithms operate on features computed from images following segmentation; these include mean intensity value per cell and channel. Feature extraction and segmentation from tissues, in which cells of different sizes and shapes are crowded together, are challenging tasks for which software tools are still being developed. As a result, it is essential that the results of feature extraction are checked and corrected prior to downstream data processing (CB1, CB3). This requires effective means to link feature and image space (P1, O1). Challenges: Currently, such linking is only supported by HistoCat [60], and generally requires domain experts to continuously switch between tools (CB2).",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Visually encode features of interest and subsets of the data. To make spatial distributions and correlations among multiple channels pre-attentively visible, we allow the user to blend the data from different channels into a single image.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "image+area",
        "axial_code": ["Coordinate"],
        "componenet_code": ["image", "area"]
      },
      {
        "solution_text": "Users select the appropriate channels for their current tasks based on domain knowledge. For each channel, users can set and modify a linear color and opacity transfer function by specifying the respective intensity range (i.e., lower and upper bound) as well as the colors of the transfer function.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 11,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T4: Proofreading and Analyzing Results in Spatial Context. Many algorithms operate on features computed from images following segmentation; these include mean intensity value per cell and channel. Feature extraction and segmentation from tissues, in which cells of different sizes and shapes are crowded together, are challenging tasks for which software tools are still being developed. As a result, it is essential that the results of feature extraction are checked and corrected prior to downstream data processing (CB1, CB3). This requires effective means to link feature and image space (P1, O1). Challenges: Currently, such linking is only supported by HistoCat [60], and generally requires domain experts to continuously switch between tools (CB2).",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": { "tables": 1, "clusters_and_sets_and_lists": 1, "media": 1 }
    },
    "solution": [
      {
        "solution_text": "Experts can sort, inspect, select,and manipulate individual values for each feature of a cell using theinteractive visual tabular display. The main goal of thetabular view is a) detailed analysis and direct manipulation of individualcells, and b) allowing users access to the original data table. We encodethe extracted intensity values in the tabular view as numbers, as wellas by using small multiples of bars. The tabular view is also colorencoded, with each color indicating a distinct phenotypic class. Allviews in Facetto are connected via brushing and linking so that userscan analyze a selection from different perspectives. In this way, userscan mark (and edit) individual cells with certain features in the tabularview and inspect spatial context in the image view or vice versa.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "bar"]
      },
      {
        "solution_text": "Experts can sort, inspect, select,and manipulate individual values for each feature of a cell using theinteractive visual tabular display. The main goal of thetabular view is a) detailed analysis and direct manipulation of individualcells, and b) allowing users access to the original data table. We encodethe extracted intensity values in the tabular view as numbers, as wellas by using small multiples of bars. The tabular view is also colorencoded, with each color indicating a distinct phenotypic class. Allviews in Facetto are connected via brushing and linking so that userscan analyze a selection from different perspectives. In this way, userscan mark (and edit) individual cells with certain features in the tabularview and inspect spatial context in the image view or vice versa.",
        "solution_category": "interaction",
        "solution_axial": "Selecting,Reconfigure,Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": [
          "Participation/Collaboration",
          "Reconfigure",
          "Selecting"
        ],
        "componenet_code": [
          "participation/collaboration",
          "reconfigure",
          "selecting"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 12,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T5: Deriving Pro\ufb01les for (Sub)regions and Classes. Once a type/region is detected, it is important to identify, annotate, and extract a pro\ufb01le of typical marker distributions within an area of interest (O2, P2). The pro\ufb01le includes statistical measures and distributions of cell features and can be used to present the outcome of an analysis session, diagnosis, or as a starting point for further analysis. Challenges: The variables used to construct pro\ufb01les, and the ways in which these variables are displayed, are not standardized and can only be developed by human-machine interaction.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": { "tables": 1, "clusters_and_sets_and_lists": 1, "media": 1 }
    },
    "solution": [
      {
        "solution_text": "Users can build up a hierarchy of different data subsets, and we automatically display this ongoing analysis in a hierarchical phenotype tree view. This allows users to track their progress, and to maintain an overview of their data faceting and analysis steps they have performed.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "tree+circle",
        "axial_code": ["Nesting"],
        "componenet_code": ["circle", "tree"]
      },
      {
        "solution_text": "Users can build up a hierarchy of different data subsets, and we automatically display this ongoing analysis in a hierarchical phenotype tree view. This allows users to track their progress, and to maintain an overview of their data faceting and analysis steps they have performed.",
        "solution_category": "interaction",
        "solution_axial": "Selecting,Abstract/Elaborate",
        "solution_compoent": "",
        "axial_code": ["Abstract/Elaborate", "Selecting"],
        "componenet_code": ["abstract/elaborate", "selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 13,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T5: Deriving Pro\ufb01les for (Sub)regions and Classes. Once a type/region is detected, it is important to identify, annotate, and extract a pro\ufb01le of typical marker distributions within an area of interest (O2, P2). The pro\ufb01le includes statistical measures and distributions of cell features and can be used to present the outcome of an analysis session, diagnosis, or as a starting point for further analysis. Challenges: The variables used to construct pro\ufb01les, and the ways in which these variables are displayed, are not standardized and can only be developed by human-machine interaction.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "To reveal contextual feature information for anindividual cell or for a selection, users can click on a cell in the im-age viewer and show a visual profile card in which data statistics aresummarized. The card shows a boxplot of the feature space, thephenotype labels, and a short summary that includes any previous userannotation, making it possible for information to be acquired sequen-tially over a number of sessions involving multiple users.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "scatter",
        "axial_code": ["Repetition"],
        "componenet_code": ["scatter"]
      },
      {
        "solution_text": "To reveal contextual feature information for anindividual cell or for a selection, users can click on a cell in the im-age viewer and show a visual profile card in which data statistics aresummarized. The card shows a boxplot of the feature space, thephenotype labels, and a short summary that includes any previous userannotation, making it possible for information to be acquired sequen-tially over a number of sessions involving multiple users.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 14,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T5: Deriving Pro\ufb01les for (Sub)regions and Classes. Once a type/region is detected, it is important to identify, annotate, and extract a pro\ufb01le of typical marker distributions within an area of interest (O2, P2). The pro\ufb01le includes statistical measures and distributions of cell features and can be used to present the outcome of an analysis session, diagnosis, or as a starting point for further analysis. Challenges: The variables used to construct pro\ufb01les, and the ways in which these variables are displayed, are not standardized and can only be developed by human-machine interaction.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "For the exploration of each chan-nel\u2019s distribution, we integrated a ridgeplot that comprisesmultiple areas alongside relevant information about the variablebeing examined. Each chart represents a feature\u2019s value distribu-tion (a ridge). Typically, the distribution of features that are derivedfrom a channel\u2019s intensity values is skewed, having a few distinct peaksand some outliers.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "image+parallelcoordinates",
        "axial_code": ["Stack"],
        "componenet_code": ["image", "parallelcoordinates"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 15,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T5: Deriving Pro\ufb01les for (Sub)regions and Classes. Once a type/region is detected, it is important to identify, annotate, and extract a pro\ufb01le of typical marker distributions within an area of interest (O2, P2). The pro\ufb01le includes statistical measures and distributions of cell features and can be used to present the outcome of an analysis session, diagnosis, or as a starting point for further analysis. Challenges: The variables used to construct pro\ufb01les, and the ways in which these variables are displayed, are not standardized and can only be developed by human-machine interaction.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "We visualize higher-order similarities and differences between data subsets using dimensionality reduction techniques and subsequent displayin a 2D scatterplot. We use UMAP (uniform manifold approximationand projection for dimension reduction), a recently developed ma-chine learning technique, to display features of the current data subset.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "We visualize higher-order similarities and differences between data subsets using dimensionality reduction techniques and subsequent displayin a 2D scatterplot. We use UMAP (uniform manifold approximationand projection for dimension reduction), a recently developed ma-chine learning technique, to display features of the current data subset.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "scatter",
        "axial_code": ["Repetition"],
        "componenet_code": ["scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 18,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Obtain the emotion status of all the people in a video. Given a specific video, users have a great interest in gaining a quick overview of the video content. For example, what is the overall emotion trend as the video progresses? What kind of emotion dominates the video? Compared with checking the original video back and forth, a visual overview would greatly reduce the browsing burden.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "It is important to provide users with an overview of the emotion evolution of individuals (R1). Thus, we design a summary view to provide users with both a static and a dynamic summary of the emotions: the emotion archives (Fig. 3a) to visualize the emotion distribution of individuals (static summary), and the emotion \ufb02ow (Fig. 3b) to show the dynamic evolution of these emotions (dynamic summary).",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling,Modeling",
        "solution_compoent": "Videosampling,categoricalmodels",
        "axial_code": ["Sampling", "Modeling"],
        "componenet_code": ["sampling", "modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 19,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Obtain the emotion status of all the people in a video. Given a specific video, users have a great interest in gaining a quick overview of the video content. For example, what is the overall emotion trend as the video progresses? What kind of emotion dominates the video? Compared with checking the original video back and forth, a visual overview would greatly reduce the browsing burden.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "It is important to provide users with an overview of the emotion evolution of individuals. Thus, we design a summary view to provide users with both a static and a dynamic summary of the emotions: the emotion flow to show the dynamic evolution of these emotions (dynamic summary).",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling,Modeling",
        "solution_compoent": "Videosampling,categoricalmodels",
        "axial_code": ["Sampling", "Modeling"],
        "componenet_code": ["sampling", "modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 20,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Uncover emotion patterns of an individual in a video. After gaining an overview of the given video, users concentrate on an individual of interest. For example, most parents are concerned about their own children, and they are likely to explore individuals in a video. What is the emotion pattern of a selected person in this video? How do his/her emotions evolve over time?",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We use lines to connect the emotion archives to the corresponding flows. Each line represents a person, starting from a person selected in the emotion archives, and then connects to his/her emotion flow. Therefore, users can easily track personal emotion evolution.",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling,Modeling",
        "solution_compoent": "Videosampling,categoricalmodels",
        "axial_code": ["Sampling", "Modeling"],
        "componenet_code": ["sampling", "modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 21,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Uncover emotion patterns of an individual in a video. After gaining an overview of the given video, users concentrate on an individual of interest. For example, most parents are concerned about their own children, and they are likely to explore individuals in a video. What is the emotion pattern of a selected person in this video? How do his/her emotions evolve over time?",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The character view visualizes the emotion status of a selected person with a portrait glyph. We adopt a tailored donut chart in this design. Each annular sector on the outer part represents an emotion. The area of each annular sector illustrates the amount of the corresponding emotion which appears in the video.",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling,Modeling",
        "solution_compoent": "Videosampling,categoricalmodels",
        "axial_code": ["Sampling", "Modeling"],
        "componenet_code": ["sampling", "modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 22,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Compare emotion portraits of different people. Users would like to explore a person of interest, especially to obtain his/her relative status in a video. Further comparisons between different people empower users to identify abnormal patterns. For example, teachers may worry about a special student in the class, and parents are curious about whether their children behave differently compared to others. Therefore, comparing different people\u2019s emotion patterns and measuring their similarity and difference are very valuable for users.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We use lines to connect the emotion archives to the corresponding flows. Each line represents a person, starting from a person selected in the emotion archives, and then connects to his/her emotion flow. Therefore, users can easily compare the emotion evolution of different people.",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling,Modeling",
        "solution_compoent": "Videosampling,categoricalmodels",
        "axial_code": ["Sampling", "Modeling"],
        "componenet_code": ["sampling", "modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 23,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Compare emotion portraits of different people. Users would like to explore a person of interest, especially to obtain his/her relative status in a video. Further comparisons between different people empower users to identify abnormal patterns. For example, teachers may worry about a special student in the class, and parents are curious about whether their children behave differently compared to others. Therefore, comparing different people\u2019s emotion patterns and measuring their similarity and difference are very valuable for users.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Comparison between different emotion portraits enables users to identify and compare the characteristics of different people. We adopt a tailored donut chart in this design. Each annular sector on the outer part represents an emotion. The area of each annular sector illustrates the amount of the corresponding emotion which appears in the video.",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling,Modeling",
        "solution_compoent": "Videosampling,categoricalmodels",
        "axial_code": ["Sampling", "Modeling"],
        "componenet_code": ["sampling", "modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 24,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Compare emotion portraits of different people. Users would like to explore a person of interest, especially to obtain his/her relative status in a video. Further comparisons between different people empower users to identify abnormal patterns. For example, teachers may worry about a special student in the class, and parents are curious about whether their children behave differently compared to others. Therefore, comparing different people\u2019s emotion patterns and measuring their similarity and difference are very valuable for users.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The comparison of the basic emotion information between different people become easy with the screen snapshot function. If users want to explore details, they can click the snapshot of interest for further exploration. Snapshot examples are demonstrated on the left-hand side of the character view.",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling,Modeling",
        "solution_compoent": "Videosampling,categoricalmodels",
        "axial_code": ["Sampling", "Modeling"],
        "componenet_code": ["sampling", "modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 25,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Reveal model uncertainty with influencing factors. Emotion recognition algorithms are not perfect and the accuracy is influenced by multiple factors. Leveraging these factors properly can provide useful cues for inferring underlying patterns. For example, the accuracy of emotion recognition probably decreases, when the algorithm processes a child face image with a small face size in the video or occluded by others. It would also be better to allow users to investigate model accuracy and correct corresponding errors if needed.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The influencing factor bar chart mainly shows the aggregation information of face size and occlusion. We use a bar to encode different influencing factors foreach time span. The height of the bar indicates the face size detected in the video (the higher, the larger face size), whereas the black shading area of the bar represents the occlusion degree.",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling",
        "solution_compoent": "Videosampling",
        "axial_code": ["Sampling"],
        "componenet_code": ["sampling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 26,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Reveal model uncertainty with influencing factors. Emotion recognition algorithms are not perfect and the accuracy is influenced by multiple factors. Leveraging these factors properly can provide useful cues for inferring underlying patterns. For example, the accuracy of emotion recognition probably decreases, when the algorithm processes a child face image with a small face size in the video or occluded by others. It would also be better to allow users to investigate model accuracy and correct corresponding errors if needed.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We can easily observe detailed emotion information and influencing factors for the person of interest with such a tailored donut chart design. ",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling,Modeling",
        "solution_compoent": "Videosampling,categoricalmodels",
        "axial_code": ["Sampling", "Modeling"],
        "componenet_code": ["sampling", "modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 27,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Provide context for video analysis. The visual analytics system is based on complex recognition models and abstract data representation. Users also want to know the original video context, which helps them understand the analytical results and validate assumptions. For example, what kind of scenario leads to a change of emotions? Do their assumptions about these findings make sense?",
      "requirement_code": {
        "describe_observation_item": 1,
        "evaluate_hypothesis": 1
      }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We provide the original video for users to explore in the video view. Users can play the video at slow, normal, or fast speeds. When users pause the video, the corresponding faces in each frame are highlighted. Users can also pick out the parts of interest for further exploration based on their observation from the emotion flow. This view is mainly used for providing evidence for users. When users explore other views and find something interesting, they can link to corresponding frames in the video view. Accurately extracting information from a video is a challenge. Therefore, we provide an interactive way for users to correct this inaccuracy. When users identify a wrongly labeled person, they can click on the person and select the correct label. The face label will be automatically updated in the database. Similarly, users can correct the emotion information. With this method, users are allowed to interactively correct any inaccurate information caused by models.",
        "solution_category": "visualization",
        "solution_axial": "Basic",
        "solution_compoent": "Video",
        "axial_code": ["Basic"],
        "componenet_code": ["Video"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 28,
    "paper_title": "Interactive Correction of Mislabeled Training Data",
    "pub_year": 2019,
    "domain": "label",
    "requirement": {
      "requirement_text": "Handling large-scale data. The machine learning problems that our experts face typically involve very complex feature space and statistical characteristics, which require a large volume of training data for model learning.",
      "requirement_code": { "flexibility_and_scalability": 1 }
    },
    "data": {
      "data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).",
      "data_code": {
        "ordinal": 1,
        "clusters_and_sets_and_lists": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "An item view that utilizes a tSNE-based hierarchical visualization to display the distribution of training items, dis-closes patterns of clusters and outliers, and supports interactive exploration of the details.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification,Sampling,DimensionalityReduction",
        "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE",
        "axial_code": ["DimensionalityReduction", "Sampling", "Rectification"],
        "componenet_code": [
          "dimensionality_reduction",
          "sampling",
          "rectification"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 29,
    "paper_title": "Interactive Correction of Mislabeled Training Data",
    "pub_year": 2019,
    "domain": "label",
    "requirement": {
      "requirement_text": "Handling large-scale data. The machine learning problems that our experts face typically involve very complex feature space and statistical characteristics, which require a large volume of training data for model learning.",
      "requirement_code": { "flexibility_and_scalability": 1 }
    },
    "data": {
      "data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).",
      "data_code": {
        "ordinal": 1,
        "clusters_and_sets_and_lists": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "The itemview supports a hierarchical exploration of item distribution. Users are first presented with the overview, i.e., the top level of the hierarchical structure. The classes of items are encoded by colors. The selected items are emphasized by thick edges. Ideally, items are visually clustered by labels.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification,Sampling,DimensionalityReduction",
        "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE",
        "axial_code": ["DimensionalityReduction", "Sampling", "Rectification"],
        "componenet_code": [
          "dimensionality_reduction",
          "sampling",
          "rectification"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 30,
    "paper_title": "Interactive Correction of Mislabeled Training Data",
    "pub_year": 2019,
    "domain": "label",
    "requirement": {
      "requirement_text": "Handling large-scale data. The machine learning problems that our experts face typically involve very complex feature space and statistical characteristics, which require a large volume of training data for model learning.",
      "requirement_code": { "flexibility_and_scalability": 1 }
    },
    "data": {
      "data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).",
      "data_code": {
        "ordinal": 1,
        "clusters_and_sets_and_lists": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "In the item view, there could be overlaps between data items due to the large scale. Our hierarchical visualization is designed to address this issue (R1). During hierarchical navigation, the user can select a small number of items in a drill down operation to reduce overlaps in the new layout. In addition, we provide a density map to display the distribution of items",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification,Sampling,DimensionalityReduction",
        "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE",
        "axial_code": ["DimensionalityReduction", "Sampling", "Rectification"],
        "componenet_code": [
          "dimensionality_reduction",
          "sampling",
          "rectification"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 31,
    "paper_title": "Interactive Correction of Mislabeled Training Data",
    "pub_year": 2019,
    "domain": "label",
    "requirement": {
      "requirement_text": "Examining unusual distribution for identifying labelingerrors. A cleaning pipeline oftenstarts by identifying local regions with unusual patterns of data or la-bel distribution, upon which mislabeled training items can be largelyidentified and inspected. Thus, the experts wanted to quickly locatesuch suspicious regions. Moreover, in each region, the mislabeledtraining items should always be displayed with priority no matterwhich data filter or sampling strategy is applied.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).",
      "data_code": {
        "ordinal": 1,
        "clusters_and_sets_and_lists": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "An item view that utilizes a tSNE-based hierarchical visualization to display the distribution of training items, discloses patterns of clusters and outliers, and supports interactive exploration of the details.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification,Sampling,DimensionalityReduction",
        "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE",
        "axial_code": ["DimensionalityReduction", "Sampling", "Rectification"],
        "componenet_code": [
          "dimensionality_reduction",
          "sampling",
          "rectification"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 32,
    "paper_title": "Interactive Correction of Mislabeled Training Data",
    "pub_year": 2019,
    "domain": "label",
    "requirement": {
      "requirement_text": "Examining unusual distribution for identifying labelingerrors. A cleaning pipeline oftenstarts by identifying local regions with unusual patterns of data or la-bel distribution, upon which mislabeled training items can be largelyidentified and inspected. Thus, the experts wanted to quickly locatesuch suspicious regions. Moreover, in each region, the mislabeledtraining items should always be displayed with priority no matterwhich data filter or sampling strategy is applied.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).",
      "data_code": {
        "ordinal": 1,
        "clusters_and_sets_and_lists": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "The labeling noises in the training data often result in the mixed color distribution in some regions. These suspicious regions indicate potential mislabeled items and deserve further examination, e.g., zooming into the regions to explore more items.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification,Sampling,DimensionalityReduction",
        "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE",
        "axial_code": ["DimensionalityReduction", "Sampling", "Rectification"],
        "componenet_code": [
          "dimensionality_reduction",
          "sampling",
          "rectification"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 33,
    "paper_title": "Interactive Correction of Mislabeled Training Data",
    "pub_year": 2019,
    "domain": "label",
    "requirement": {
      "requirement_text": "Recommending and verifying trusted items. While manual inspection and correction is a routine for debugging mislabeled items, the experts need automated approaches to improve efficiency. Methods such as propagation of trusted items are the most discussed and recognized. However, without efficient algorithms and interactive tools, selecting and validating a set of trusted items from the cluttered visualization can still become laborious. Two requirements were identified by our collaborators based on their experience. First, automatic recommendation algorithms are desired to locate trusted items quickly. Second, flexible ways to examine and compare trusted items are required for further verification",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).",
      "data_code": {
        "ordinal": 1,
        "media": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "Identifying and selecting trusted items (R3). The item view and other three views cooperate to support the identi\ufb01cation, selection, and correction of trusted items. During the exploration, the user can select a set of items and add them into the selected item view. If s/he wants to reduce the number of selected items, s/he can use the \u201cRecommend trusted items\u201d operation to select a representative subset. Images of the selected items are shown in the selected items view Fig. 1 (c), where the user can look at the images, relate to the distribution in the item view, re\ufb01ne the selection accordingly, and correct the labels if labeling errors are noticed. After the re\ufb01nement, the selected items can be added into trusted items. The trusted item view (Fig. 1 (d)) displays the images and labels of all the trusted items. Further editing can be performed in the trusted item view. Usually, the identi\ufb01cation and selection of trusted items in different regions require different strategies. In a region without labeling outliers, the user usually selects trusted items using system \u201crecommendation\u201d, and verifies the labels by only a glance at the images in the selected item view. In a region with labeling outliers, more careful examinations and operations are required from the user, including investigating the distribution at different levels, selecting class-balanced trusted items, and correcting the labels step-by-step for each class.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "image+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["image", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 34,
    "paper_title": "Interactive Correction of Mislabeled Training Data",
    "pub_year": 2019,
    "domain": "label",
    "requirement": {
      "requirement_text": "Recommending and verifying trusted items. While manual inspection and correction is a routine for debugging mislabeled items, the experts need automated approaches to improve efficiency. Methods such as propagation of trusted items are the most discussed and recognized. However, without efficient algorithms and interactive tools, selecting and validating a set of trusted items from the cluttered visualization can still become laborious. Two requirements were identified by our collaborators based on their experience. First, automatic recommendation algorithms are desired to locate trusted items quickly. Second, flexible ways to examine and compare trusted items are required for further verification",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).",
      "data_code": {
        "ordinal": 1,
        "media": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "The trusted item view displays the images and labels of all the trusted items. Further editing can be performed in the trusted item view. Usually, the identification and selection of trusted items in different regions require different strategies. In a region without labeling outliers, the user usually selects trusted items using system \u201crecommendation\u201d, and verifies the labels by only a glance at the images in the selected item view. In a region with labeling outliers, more careful examinations and operations are required from the user, including investigating the distribution at different levels, selecting class-balanced trusted items, and correcting the labels step-by-step for each class.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "image+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["image", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 35,
    "paper_title": "Interactive Correction of Mislabeled Training Data",
    "pub_year": 2019,
    "domain": "label",
    "requirement": {
      "requirement_text": "Recommending and verifying trusted items. While manual inspection and correction is a routine for debugging mislabeled items, the experts need automated approaches to improve efficiency. Methods such as propagation of trusted items are the most discussed and recognized. However, without efficient algorithms and interactive tools, selecting and validating a set of trusted items from the cluttered visualization can still become laborious. Two requirements were identified by our collaborators based on their experience. First, automatic recommendation algorithms are desired to locate trusted items quickly. Second, flexible ways to examine and compare trusted items are required for further verification",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).",
      "data_code": {
        "ordinal": 1,
        "media": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "Propagating trusted items to improve the quality of the training set (R3). The propagation of trusted items is performed iteratively. In each iteration, newly selected trusted items are added into the trusted item set of the last iteration. They are fed into the correction module, which propagates the trusted items to the entire dataset. After the propagation, users can verify the quality improvements from the updated item distribution in the item view and the action trail. The action trail (Fig. 1 (e)) represents the historical record of correction iterations as a tree. Each iteration is represented by a node containing two bar charts: the chart on the top displays the number of newly added trusted items while the other counts the corrected items by the data correction module. In the case of undesired correction resulting, rolling back to previous iterations allows the user to reselect trusted items to re\ufb01ne the propagation results. If there is a lack of obvious visible quality improvements in sequences of iterations, the user can stop the iteration to \ufb01nish the correction process.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "image+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["image", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 36,
    "paper_title": "Interactive Correction of Mislabeled Training Data",
    "pub_year": 2019,
    "domain": "label",
    "requirement": {
      "requirement_text": "Exploring the details. Exploring the details of training items was considered an essential step to verify the labeling correctness. Specifically, the experts would like to be able to explore the details in the context of a distribution visualization so that they can quickly compare groups of items in local regions.",
      "requirement_code": { "evaluate_hypothesis": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).",
      "data_code": {
        "ordinal": 1,
        "clusters_and_sets_and_lists": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "An item view that utilizes a tSNE-based hierarchical visualization to display the distribution of training items, discloses patterns of clusters and outliers, and supports interactive exploration of the details.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification,Sampling,DimensionalityReduction",
        "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE",
        "axial_code": ["DimensionalityReduction", "Sampling", "Rectification"],
        "componenet_code": [
          "dimensionality_reduction",
          "sampling",
          "rectification"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 37,
    "paper_title": "Interactive Correction of Mislabeled Training Data",
    "pub_year": 2019,
    "domain": "label",
    "requirement": {
      "requirement_text": "Exploring the details. Exploring the details of training items was considered an essential step to verify the labeling correctness. Specifically, the experts would like to be able to explore the details in the context of a distribution visualization so that they can quickly compare groups of items in local regions.",
      "requirement_code": { "evaluate_hypothesis": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).",
      "data_code": {
        "ordinal": 1,
        "clusters_and_sets_and_lists": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "The labeling noises in the training data often result in the mixed color distribution in some regions. These suspicious regions indicate potential mislabeled items and deserve further examination, e.g., zooming into the regions to explore more items.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification,Sampling,DimensionalityReduction",
        "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE",
        "axial_code": ["DimensionalityReduction", "Sampling", "Rectification"],
        "componenet_code": [
          "dimensionality_reduction",
          "sampling",
          "rectification"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 38,
    "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
    "pub_year": 2020,
    "domain": "XAI",
    "requirement": {
      "requirement_text": "Facilitate the choice of hyper-parameters through visual exploration and the use of quality metrics.",
      "requirement_code": { "parameter_setting": 1 }
    },
    "data": {
      "data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "A Grid Search mode initiates a systematic parameter search that computes 500 projections by varying the parameters perplexity, learning rate, and max iterations.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 39,
    "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
    "pub_year": 2020,
    "domain": "XAI",
    "requirement": {
      "requirement_text": "Facilitate the choice of hyper-parameters through visual exploration and the use of quality metrics.",
      "requirement_code": { "parameter_setting": 1 }
    },
    "data": {
      "data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "From this pool of 500 projections, 25 representative examples are singled out and shown to the user\u2014in a matrix of thumbnails depicted in Fig. 2\u2014as suggestions of possible projections of the data. In order to choose the representatives, we partition the pool of 500 projections into 25 clusters (with K-Medoids [55]), using Procrustes distance [56] as the dissimilarity measure. The medoids of the 25 resulting clusters are used as representatives. This whole process is transparent to the user and happens in the backend.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 40,
    "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
    "pub_year": 2020,
    "domain": "XAI",
    "requirement": {
      "requirement_text": "Facilitate the choice of hyper-parameters through visual exploration and the use of quality metrics.",
      "requirement_code": { "parameter_setting": 1 }
    },
    "data": {
      "data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Only the representatives are shown. We give extra support to the user by providing the results of 5 quality measures for each representative projection: neighborhood hit (NH), trustworthiness (T), continuity (C), normalized stress (S), and Shepard diagram correlation (SDC), accompanied by the quality metrics average (QMA). They are shown as a grayscale heatmap under each cell of the thumbnail matrix.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 41,
    "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
    "pub_year": 2020,
    "domain": "XAI",
    "requirement": {
      "requirement_text": "Facilitate the choice of hyper-parameters through visual exploration and the use of quality metrics.",
      "requirement_code": { "parameter_setting": 1 }
    },
    "data": {
      "data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "It is then the task of users\u2014through visual exploration and by matching their own personal preferences\u2014to choose the one that looks more promising. After choosing a projection, users will proceed with the visual analysis using all the functionalities described in the next sections. However, the hyper-parameter exploration does not necessarily stop here. The top 6 representatives (according to a user-selected quality measure) are still shown at the top of the main view, and the projection can be switched at any time if the user is not satisfied with the initial choice.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 42,
    "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
    "pub_year": 2020,
    "domain": "XAI",
    "requirement": {
      "requirement_text": "Provide a quick overview of the accuracy of the projection, to support the decision of either moving forward with the analysis or repeating the process of hyper-parameter exploration. ",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "The main view of the tool presents the t-SNE results as an interactive scatterplot, with specific mappings on the points\u2019 colors and sizes.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 43,
    "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
    "pub_year": 2020,
    "domain": "XAI",
    "requirement": {
      "requirement_text": "Provide a quick overview of the accuracy of the projection, to support the decision of either moving forward with the analysis or repeating the process of hyper-parameter exploration. ",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "There are four Interaction Modes for this view, as described next. The first (and default) mode\u2014t-SNE Points Exploration\u2014activates panning, zooming, and hovering, supporting the user to focus on individual patterns of the projection, and to investigate specific points\u2019 dimensions. The second mode\u2014Group Selection\u2014provides a lasso selection tool that triggers updates in other views, such as the Neighborhood Preservation view and the Adaptive PCP. The third option\u2014Dimension Correlation\u2014provides a tool for the user to check the hypothesis that a visual pattern, as observed, is strongly correlated to a pattern in the high-dimensional space. The final mode\u2014Reset Filters\u2014removes every filter applied with the previously-described interaction modes. ",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 44,
    "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
    "pub_year": 2020,
    "domain": "XAI",
    "requirement": {
      "requirement_text": "Provide a quick overview of the accuracy of the projection, to support the decision of either moving forward with the analysis or repeating the process of hyper-parameter exploration. ",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "To complement the main view, the Overview shows the static t-SNE projection and serves as a contextual anchor that is independent of the interactions and/or filters applied to the main view. Data-specific labels (when those exist) are shown using a categorical colormap, along with simple statistics about the data set.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 45,
    "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
    "pub_year": 2020,
    "domain": "XAI",
    "requirement": {
      "requirement_text": "Provide the means to investigate quality further, differentiating between the trustworthiness of different regions of the projection.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "To avoid this clutter problem and increase the readability of the Shepard Diagram for large data sets, we propose the Shepard Heatmap, which is an aggregated version of the Shepard Diagram, with the information of the number of points in each cell mapped to a single-hue colormap. The main goal of the Shepard Heatmap is to offer a broad, simplified overview of the accuracy of the projection in terms of distance preservation: cells close to the main diagonal of the heatmap indicate that the respective pairs of instances have been represented in the 2-D space with distances that are comparable to their original N-D distances. ",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 46,
    "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
    "pub_year": 2020,
    "domain": "XAI",
    "requirement": {
      "requirement_text": "Provide the means to investigate quality further, differentiating between the trustworthiness of different regions of the projection.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Visual Mapping. The Visual Mapping panel includes controls for mapping Density and Remaining Cost of each point to either color or size in the main view. These correspond to information extracted from the t-SNE algorithm itself, which would otherwise be hidden from the analyst. Their inspection, however, may prove fruitful.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 47,
    "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
    "pub_year": 2020,
    "domain": "XAI",
    "requirement": {
      "requirement_text": "Provide the means to investigate quality further, differentiating between the trustworthiness of different regions of the projection.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "The ability to investigatethe extent to which such neighborhoods are preserved is one important piece of the puzzle that forms a full assessment of the accuracy of a t-SNE projection.We present a Neighborhood Preservation plot that shows an overview of the preservation of neighborhoods of different sizes (k) in both the entire projection and the current selection, based on the Jaccard distance between sets. For each value of k, NPk yields the average preservation of neighborhoods of up to k points, centered at the n selected points (or for the entire projection, if nothing is selected). The default visualization for the Neighborhood Preservation is a bar chart, but users have two more options to visualize the same information using line plots. The black bars are always fixed, showing the average preservation for all points of the projection. The ability to investigatethe extent to which such neighborhoods are preserved is one important piece of the puzzle that forms a full assessment of the accuracy of a t-SNE projection.We present a Neighborhood Preservation plot that shows an overview of the preservation of neighborhoods of different sizes (k) in both the entire projection and the current selection, based on the Jaccard distance between sets. For each value of k, NPk yields the average preservation of neighborhoods of up to k points, centered at the n selected points (or for the entire projection, if nothing is selected). The default visualization for the Neighborhood Preservation is a bar chart, but users have two more options to visualize the same information using line plots. The black bars are always fixed, showing the average preservation for all points of the projection.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 48,
    "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
    "pub_year": 2020,
    "domain": "XAI",
    "requirement": {
      "requirement_text": "Allow the interpretation of different visible patterns of the projection in terms of the original data set\u2019s dimensions.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Adaptive Parallel Coordinates Plot. Our first proposal to support the task of interpreting patterns in a t-SNE projection is an Adaptive PCP. It highlights the dimensions of the points selected with the lasso tool, using a maximum of 8 axes at any time, to avoid clutter. The shown axes (and their order) are, however, not fixed, as is the usual case. Instead, they are adapted to the selection in the following way. First, a Principal Component Analysis is performed using only the selected points, but with all dimensions. That yields two results: (1) a set of eigenvectors that represent a new base that best explains the variance of the selected points, and (2) a set of eigenvalues that represent how much variance is explained by each eigenvector. Simulating a reduction of the dimensions of the selected points to 1-Dimensional space using PCA, we pick the eigenvector with the largest eigenvalue, i.e., the most representative one. This N-D vector can be seen as sequence w of N weights, one per original dimen-sion, where the value of wj indicates the importance of dimension j in explaining the variance of the user-selected subset of the data. Finally, we sort w in descending order, then pick the dimensions that correspond to the first (up to) 8 values of the sorted w. These are the (up to) 8 dimensions shown in the PCP axes, in the same descending order (from left to right).",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 49,
    "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
    "pub_year": 2020,
    "domain": "XAI",
    "requirement": {
      "requirement_text": "Allow the interpretation of different visible patterns of the projection in terms of the original data set\u2019s dimensions.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "The results (i.e., relevances of each dimension) are finally shown in an interactive horizontal bar chart, where the dimensions are sorted from top to bottom according to relevance (with the most relevant on the top). While the relevance is computed using the absolute value of the correlation, we decided to show the original value in the bars (including negative correlations to the left of the central axis) to avoid possibly misleading the analyst. The final component of the Dimension Correlation tool is the ability to explore the different dimensions by clicking on the bars, which will change the colormap of the main view to reflect the values of the points for that specific dimension.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 50,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "T1: Hand-Craft, Merge, and Split Clusters. Biologists apply their domain knowledge to create customized clusters to better understand which factor(s) is causing the ascertainment bias on the dataset that are being used popularly. For example, one of the biologist stated: \u201cGiven the identified SNPs [single-nucleotide polymorphisms] that are associated with common disease and traits, it\u2019s interesting to create a cluster of SNPs.\u201d In addition, biologists apply their domain expertise to merge or split two or more clusters depending on how related they think the clusters are based on given feature(s). For example, one of the biologists mentioned: \u201cDepending on the evolutionary history of the genes, two or more clusters can be really related to each other. If ascertained they are related, we will merge them as one cluster.\u201d Another biologist reported that \u201cIn my new project, we are comparing Africans to non-Africans. In this case I merge Americans, East Asians, and Europeans as one cluster, and compare that to Africans data.\u201d",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Merging and Splitting Clusters (T1). To merge two or more  clusters, users first click on a cluster. They then demonstrate their interest in merging two clusters by drag-anddropping the cluster on top of another cluster. Users can drag point(s) out of the cluster and drop into either i) another cluster or ii) a blank space (on the Cluster View). Drag-and-drop items into blank space is translated as forming a new cluster of the selected items outside the current cluster (see Fig. 2). Demonstration-based cluster customization enables users to interact with the data directly and removes any mid-level instruments such as control panels or menus",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 51,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "T2: Divide Each Cluster to Sub-Clusters. Biologists often investigate sub-clusters within a specific cluster to: 1) understand which other factors can affect the cluster, 2) compare two clusters based on the member data items in each, and 3) see trends and patterns in the sub-clusters, with respect to chosen features, We noticed that the biologists found existing solutions challenging because they had to write lines of scripts to compute and visualize sub-clusters in a given cluster. Furthermore, the existing methods prohibit rapid iteration and visualization of results, which inevitably prolongs the exploratory clustering process to understand their data better.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Sub Clustering (T2). Hovering over a cluster reveals a plus button. Users can click on it to open a subcluster panel on the Cluster View, which shows subgroups of the data items within the selected cluster. In addition, a bar chart shows the distribution of a chosen attribute. Alongside, text description highlights the attributes that were used to compute the subclusters. Given that the users are not experts in data science, we do not present the quality metrics (e.g., silhouette scores, homogeneity score, etc.) Instead, we describe cluster models by showing thumbnail previews of clustering results with text descriptions as Fig. 4 shows.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Clustering&Grouping"],
        "componenet_code": [
          "dimensionality_reduction",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 52,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "T2: Divide Each Cluster to Sub-Clusters. Biologists often investigate sub-clusters within a specific cluster to: 1) understand which other factors can affect the cluster, 2) compare two clusters based on the member data items in each, and 3) see trends and patterns in the sub-clusters, with respect to chosen features, We noticed that the biologists found existing solutions challenging because they had to write lines of scripts to compute and visualize sub-clusters in a given cluster. Furthermore, the existing methods prohibit rapid iteration and visualization of results, which inevitably prolongs the exploratory clustering process to understand their data better.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Sub Clustering. When triggered by users, the system builds a sub-cluster model Msi, for data instances E, member of a selected cluster Ci. Unlike the set of main cluster models M, only a single sub-cluster model is generated per cluster (T2). For sub-clustering we relied on the parameterization of the best-recommended cluster model for the entire data i.e., best-found parameteriza tion of the K-Means cluster model. To avoid further compute times that may impact real-time interactions, we did not construct and test multiple cluster models for sub-clustering. However, clicking on the \u201cadd subcluster\u201d button again for the same selected cluster Ci, the system recomputes the sub-cluster model Msi, by  randomly choosing a new set of a learning algorithm v and hyperparameters f; e.g., it picks a new \u201ck\u201d on the \u201cK-Means\u201d cluster model. This technique allows users to rapidly browse a large set of sub-cluster models.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 53,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "T3: Adjust Feature Contributions. Biologists need to easily see by how much different attributes/features contribute to computing a cluster. Moreover, they often need to adjust the importance of different features used for computing a cluster. Biologists currently have to programmatically adjust the importance of features, execute the code, and visualize the outcome. They often repeat this process multiple times until they achieve a satisfactory result. They need interactive methods to view and refine feature contributions.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Delete Data Items or Clusters (T3). Our discussion with biologists revealed that they sometimes need to \u2018exclude\u2019 data items or clusters from their analysis while testing a hypothesis. Thus, we initially implemented the \u2018delete\u2019 feature by enabling users to select a subset of items or clusters from the main view and click on the delete icon. However, when we showed it to the biologists, they had trouble due to inconsistencies between the button-based interaction and other demonstration-based interaction. Currently in Geono-Cluster users cam drag-drop a selected cluster on the delete icon shown on the top-left of the interface to show their interest in moving the selected cluster out of the layout. Similarly, they can drag-drop individual data items to demonstrate their interests in removing them from the cluster assignment.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Clustering&Grouping"],
        "componenet_code": [
          "dimensionality_reduction",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 54,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "T3: Adjust Feature Contributions. Biologists need to easily see by how much different attributes/features contribute to computing a cluster. Moreover, they often need to adjust the importance of different features used for computing a cluster. Biologists currently have to programmatically adjust the importance of features, execute the code, and visualize the outcome. They often repeat this process multiple times until they achieve a satisfactory result. They need interactive methods to view and refine feature contributions.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "User Driven Feature Selection. A cluster model Mi is driven by a set of features F = fi1, fi2; f i3; f i4 . . . . . . :fik as input to compute the distance function which assigns a set of data items D to individual clusters C. In Geono-Cluster, the set of features F is either computed using feature selection methods e.g., \u201cselect K Best\u201d [43], \u201cPCA\u201d [44] or can be retrieved from users if they specify a set of features and their relative weights (from the Attribute Panel supporting the task T3). When users specify a set of k features Fu = fi1, fi2; f i3; f i4 . . . fik with respective weights for each feature (Wu = wi1, wi2; w i3; w i4 . . . wik, the system updates the distance function in the clustering algorithm.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 55,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "T3: Adjust Feature Contributions. Biologists need to easily see by how much different attributes/features contribute to computing a cluster. Moreover, they often need to adjust the importance of different features used for computing a cluster. Biologists currently have to programmatically adjust the importance of features, execute the code, and visualize the outcome. They often repeat this process multiple times until they achieve a satisfactory result. They need interactive methods to view and refine feature contributions.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Similar Item Selection. Users click on a cell (qj) of a quantitative attribute on the Table View to select a value vj of the data item di. Geono-Cluster finds a set of r data instances, U = da; db; dc . . .dr, each of whose value vj falls within a threshold range, say [+eps, -_x0003_eps]. The parameter eps is set for each quantitative attribute Q by heuristics and can be adjusted. This technique allows users to pick data instances which are similar, based on the selected quantitative attribute qj. Further, users can select another quantitative attribute cell qk. Next, from the set of selected data instances U, the system finds all instances V which fall within a threshold range of the value selected for attribute qk. Here the size of V is less than that of U. This technique allow users to filter and select a subset of data instances V from the Table View. For categorical features X, Geono-Cluster performs exact feature value matching instead of matching data items based on a predefined range. Users can drag-drop these V data items to the Cluster View as a single cluster (C = C1). They can continue selecting another set of data items, then add them to the cluster view as a new cluster (C = C1, C 2). Users complete the data exploration or they can request the system to find a model Mi iteratively (T3).",
        "solution_category": "interaction",
        "solution_axial": "Selecting.Filtering",
        "solution_compoent": "",
        "axial_code": ["Selecting", "Filtering"],
        "componenet_code": ["selecting", "filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 56,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "Shifting the Burden of Specification From the Biologists to the Systems. The existing tools and technologies put the burden of specification on biologists. Instead of requiring biologists to specify the clustering models by programming or going through layers of menus, the tool should provide an environment that enables them to demonstrate how the expected clustering outcomes should look like. By translating the given demonstrations, the system could estimate the biologist\u2019s intention and generate appropriate results. This way we could balance the responsibility between the biologist and the system\u2014biologists provide visual demonstrations, based on this, the system infers potential clustering results and recommends them.",
      "requirement_code": { "knowledge_injection": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Cluster View visualizes the clustered data. For testing their hypotheses, biologists often perform actions at the level of data items (e.g., move data items from one cluster to another). We visually present each cluster and its members on the Cluster View. The colored circles in each group represent members of a cluster; the surrounding hull represents the cluster. Users can hover over a circle, which prompts relevant attribute details of the data. Users can specify the number of clusters using the slider shown on the top-left. Cluster View is an environment similar to a spatial workspace in which users can move data items to structure their information and provide visual demonstrations.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Clustering&Grouping"],
        "componenet_code": [
          "dimensionality_reduction",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 57,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "Enable User Interaction to Drive Recommendations. As analysts explore their data, their interests will evolve. Our initial observations and interviews also showed that biologists need to explore various clustering models rapidly during their data analysis process. One potential approach to support such a rapid data analysis is to recommend potential cluster models that biologists should consider during their data analysis process. Furthermore, the clustering recommendations should be adapted for biologists\u2019 analytic goals. The recommendation engine should steer multiple clustering models based on biologist-specified expected visual outcomes. In addition, biologists can also directly adjust feature contributions to update the clustering results. In aggregate, these interactions create demonstrations which serve as the primary units by which biologists communicate their expected changes to the system.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Cluster View is an environment similar to a spatial workspace in which users can move data items to structure their information and provide visual demonstrations. For example, a biologist might notice a set of data items should not be in a specific cluster. Thus, she can demonstrate that those points belong to a different cluster by dragging them from one cluster to another. The system uses the visual demonstrations provided by the users to steer the underlying recommendation engine.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Clustering&Grouping"],
        "componenet_code": [
          "dimensionality_reduction",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 58,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "Enable User Interaction to Drive Recommendations. As analysts explore their data, their interests will evolve. Our initial observations and interviews also showed that biologists need to explore various clustering models rapidly during their data analysis process. One potential approach to support such a rapid data analysis is to recommend potential cluster models that biologists should consider during their data analysis process. Furthermore, the clustering recommendations should be adapted for biologists\u2019 analytic goals. The recommendation engine should steer multiple clustering models based on biologist-specified expected visual outcomes. In addition, biologists can also directly adjust feature contributions to update the clustering results. In aggregate, these interactions create demonstrations which serve as the primary units by which biologists communicate their expected changes to the system.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Attribute Panel lists the attributes of the loaded data set. Users can turn on and off a set of attributes which directly affects the clustering algorithm. Furthermore, users can also adjust attribute contributions, specify-ing relative importance of the selected attributes to define cluster memberships.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 59,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "Enable User Interaction to Drive Recommendations. As analysts explore their data, their interests will evolve. Our initial observations and interviews also showed that biologists need to explore various clustering models rapidly during their data analysis process. One potential approach to support such a rapid data analysis is to recommend potential cluster models that biologists should consider during their data analysis process. Furthermore, the clustering recommendations should be adapted for biologists\u2019 analytic goals. The recommendation engine should steer multiple clustering models based on biologist-specified expected visual outcomes. In addition, biologists can also directly adjust feature contributions to update the clustering results. In aggregate, these interactions create demonstrations which serve as the primary units by which biologists communicate their expected changes to the system.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Recommendation Technique. Geono-Cluster ranks the models in M by their scores S explained below, and visualizes the best clustering layout in the Cluster View. Further, the system allows the user to inspect top f best cluster models from the ranked models M, through the Recommendation Panel. If a user makes any customization to the shown cluster model Mc (e.g., merge or split clusters), the system automatically updates the recommendations by computing a new set of M cluster models, except the model Mc, which is currently shown in the Cluster View. Per iteration, the system updates S and the ranking of the models M based on user interactions with the data. Next it visualizes the best model in M in the Cluster View and shows thumbnail previews of the top f models in the Recommendation Panel.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Clustering&Grouping"],
        "componenet_code": [
          "dimensionality_reduction",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 60,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "Enhance Interpretability of Recommendations. Biologists reported their interest in seeing more details about different clustering results while skimming through different recommendations. However, not all biologists might be familiar with technical terms used to describe a cluster such as silhouette value. Therefore, recommended clustering results should be presented in a transparent manner so that biologists can extract the most important and understandable information (e.g., contributing features) used for clustering results. One powerful approach to enhance transparency of the recommended clustering options is to use natural language to explain them. This way biologists can learn about the recommended clustering outcomes without having to know about more technical terms describing each clustering outcome.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Recommendation Technique. Geono-Cluster ranks the models in M by their scores S explained below, and visualizes the best clustering layout in the Cluster View. Further, the system allows the user to inspect top f best cluster models from the ranked models M, through the Recommendation Panel. If a user makes any customization to the shown cluster model Mc (e.g., merge or split clusters), the system automatically updates the recommendations by computing a new set of M cluster models, except the model Mc, which is currently shown in the Cluster View. Per iteration, the system updates S and the ranking of the models M based on user interactions with the data. Next it visualizes the best model in M in the Cluster View and shows thumbnail previews of the top f models in the Recommendation Panel.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Clustering&Grouping"],
        "componenet_code": [
          "dimensionality_reduction",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 62,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.1 Extract Bias Patterns. As indicated in the previous section, bias patterns are a kind of statistical abstraction of the historical data in a certain spatiotemporal interval. However, methods that aim to extract the potential bias patterns from the reanalysis data are limited.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": {
        "geometry": 1,
        "fields": 1,
        "sequential": 1,
        "temporal": 1
      }
    },
    "solution": [
      {
        "solution_text": "In the bias pattern extraction module, given a forecast result in a spatiotemporal interval and the corresponding observation data, the forecast-observation probability density function at each grid is initially generated via a kernel density estimation (KDE) (R.1). The distribution \ufb01eld can represent the bias pattern at the grid in a given time range because the probability density distribution of the observation-forecast values can re\ufb02ect the distribution characteristics of the forecast and the observation data in the current time window. Furthermore, the probability density distribution can derive the distribution characteristics of the bias, which is equal to the difference between the observation and the forecast data. We used a bottom-up hierarchical clustering algorithm to aggregate a single grid point into a connected area on the basis of the similarity of the PDF among the grids. The result of the hierarchical clustering can be considered a clustering tree of the connected areas with similar bias patterns, which are subsequently fed into the following visual analysis module for further exploration.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 63,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.2 Visualize the Spatiotemporal Distribution of Similar Bias Patterns. E.2 hoped to have a visualization that can well demonstrate the bias patterns in the spatial and temporal dimensions for an overview, which can help him immediately identify the basic properties and distribution of the bias patterns present in the data.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": {
        "geometry": 1,
        "fields": 1,
        "sequential": 1,
        "temporal": 1
      }
    },
    "solution": [
      {
        "solution_text": "In the visual analysis module, several coordinated views and intuitive interactions are provided to analyze the bias patterns. Specifically, we proposed a relative position and similarity-encoded scatter plot to visualize the spatiotemporal distribution of the bias patterns.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 64,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.2 Visualize the Spatiotemporal Distribution of Similar Bias Patterns. E.2 hoped to have a visualization that can well demonstrate the bias patterns in the spatial and temporal dimensions for an overview, which can help him immediately identify the basic properties and distribution of the bias patterns present in the data.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": {
        "geometry": 1,
        "fields": 1,
        "sequential": 1,
        "temporal": 1
      }
    },
    "solution": [
      {
        "solution_text": "The spatiotemporal bias pattern view (Fig. 4a) provides an overview of the areas with similar bias patterns extracted from the reanalysis data in each time window (R.2). ",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 65,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.2 Visualize the Spatiotemporal Distribution of Similar Bias Patterns. E.2 hoped to have a visualization that can well demonstrate the bias patterns in the spatial and temporal dimensions for an overview, which can help him immediately identify the basic properties and distribution of the bias patterns present in the data.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": { "geometry": 1, "fields": 1 }
    },
    "solution": [
      {
        "solution_text": "The map view was designed to visualize the spatial distribution of the similar bias pattern areas and grid points. It also corresponds to the interaction conducted in the spatiotemporal bias pattern view.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 66,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.2 Visualize the Spatiotemporal Distribution of Similar Bias Patterns. E.2 hoped to have a visualization that can well demonstrate the bias patterns in the spatial and temporal dimensions for an overview, which can help him immediately identify the basic properties and distribution of the bias patterns present in the data.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": { "geometry": 1, "fields": 1 }
    },
    "solution": [
      {
        "solution_text": "The map view was designed to visualize the spatial distribution of the similar bias pattern areas and grid points. It also corresponds to the interaction conducted in the spatiotemporal bias pattern view.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 67,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.3 Reveal the Differences and Correlations Among Different Spatiotemporal Bias Patterns. E.2 commented that the current bias analysis methods only focus on a single location, i.e., calibrating precipitation at a single station instead of analyzing its correlation among its neighbors. Understanding the stability and uniqueness of the bias patterns can help explore and make informed decisions. Thus, E.2 wanted to observe the difference and correlations among different spatiotemporal bias patterns.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": {
        "geometry": 1,
        "fields": 1,
        "sequential": 1,
        "temporal": 1
      }
    },
    "solution": [
      {
        "solution_text": "The spatiotemporal bias pattern view also visualizes the transition of grid points between areas with similar bias patterns in the adjacent time windows.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 68,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.3 Reveal the Differences and Correlations Among Different Spatiotemporal Bias Patterns. E.2 commented that the current bias analysis methods only focus on a single location, i.e., calibrating precipitation at a single station instead of analyzing its correlation among its neighbors. Understanding the stability and uniqueness of the bias patterns can help explore and make informed decisions. Thus, E.2 wanted to observe the difference and correlations among different spatiotemporal bias patterns.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": { "geometry": 1, "fields": 1 }
    },
    "solution": [
      {
        "solution_text": "A contour map is used in the map view to convey the stability of each grid point in the space.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 69,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.4 Select Bias Patterns of Interest. E.1 needed to \ufb01lter out the bias patterns of interest from the data overview rapidly for a detailed analysis. Thus, a visual interactive mechanism and visual cues should be provided to assist him in selecting the bias patterns of interest.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": {
        "geometry": 1,
        "fields": 1,
        "sequential": 1,
        "temporal": 1
      }
    },
    "solution": [
      {
        "solution_text": "The users can further filter similar bias pattern areas to explore the bias patterns through interaction further, thereby observing the spatiotemporal characteristics of the bias patterns in conjunction with their domain expertise and interests. For instance, we can click on a similar bias pattern area, and the corresponding areas in each time window would be highlighted automatically. We also support similarity-based filtering interaction. The users can directly input a similar threshold, and the areas with a similarity higher than the threshold would be filtered.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 70,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.5 Facilitate Parameter Comparison. To understand the similarities and differences among bias patterns, comparing the atmospheric parameters when generating the corresponding bias patterns is essential to understand the underlying causes of the bias patterns and to further support making informed decisions.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": {
        "geometry": 1,
        "fields": 1,
        "sequential": 1,
        "temporal": 1
      }
    },
    "solution": [
      {
        "solution_text": "The forecast-observation PDF view provides detailed information on the bias pattern and assists analysts in drafting the calibration curve on the basis of the bias patterns.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 71,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.5 Facilitate Parameter Comparison. To understand the similarities and differences among bias patterns, comparing the atmospheric parameters when generating the corresponding bias patterns is essential to understand the underlying causes of the bias patterns and to further support making informed decisions.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": {
        "geometry": 1,
        "fields": 1,
        "sequential": 1,
        "temporal": 1
      }
    },
    "solution": [
      {
        "solution_text": "The design of the parameter comparison view focused on the correlation between the distribution of the data dimensions and the bias, thus enabling the analysis and comparison of atmospheric parameters generated by the forecast ensemble. That is, it visualizes the data distribution in the five dimensions in a bias pattern and the correlation between the data distribution and the bias of the accumulated precipitation instead of visualizing the correlation among these dimensions.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 72,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.6 Support Detailed Calibrations. According to E.1, the accuracy of weather forecasts largely depends on the subjective judgment of the forecaster in terms of his/her quali\ufb01cation, status, and working conditions, thereby increasing the unreliability of the calibrated results. Therefore, an interactive calibration mechanism is desired to support the combination of domain experts\u2019 experience and bias patterns extracted from historical data; in this manner, the accuracy of forecast calibration is improved.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": {
        "geometry": 1,
        "fields": 1,
        "sequential": 1,
        "temporal": 1
      }
    },
    "solution": [
      {
        "solution_text": "An interactive semi-automatic calibration curve based on the detailed visualization of a specific bias pattern was designed to assist the forecast calibration.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 73,
    "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics",
    "pub_year": 2021,
    "domain": "Graph mining",
    "requirement": {
      "requirement_text": "D1 Visualize the Instance-level Sensitivity. The system should visualize ranking and auditing results for all instances (T1). The view for summarizing the instance-level sensitivity should include the sensitivity index (T1.1) for all nodes with respect to the node attributes (T1.3).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.",
      "data_code": { "network_and_trees": 1 }
    },
    "solution": [
      {
        "solution_text": "The system visualizes the output in the sortable sensitivity index list, which shows each node\u2019s current ranking and sensitivity indices with respect to the node\u2019s class label(s).",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding",
        "solution_compoent": "",
        "axial_code": ["Excluding"],
        "componenet_code": ["excluding"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 74,
    "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics",
    "pub_year": 2021,
    "domain": "Graph mining",
    "requirement": {
      "requirement_text": "D2 Visualize the Effect of Perturbation. The system should be able to guide analysts to explore the perturbation effect of certain node\u2019s removal and support interactions such as sorting, searching and \ufb01ltering to inspect the auditing results and corresponding perturbation effects (T2, T3). This view should include:D2.1 In\ufb02uence Overview, which summarizes the perturbation\u2019s in\ufb02uence, the degree of ranking changes, and the proportion of nodes whose rankings are increased/decreased, etc. (T1.2, T2.1, T3).",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.",
      "data_code": { "network_and_trees": 1 }
    },
    "solution": [
      {
        "solution_text": "The influence overview provides basic information on changes caused by removing a specific node (D2.1). These changes include 1) the number of influenced nodes which have ranking changes after the perturbation; 2, 3) the number of influenced nodes whose ranking increased/decreased after the perturbation; 4, 5) the max/min of increased/decreased ranking changes; 6, 7) the median of increased/decreased ranking changes; and 8) the degrees of the node. These 8 metrics provide the analyst with a statistical overview of the ranking influence of nodes due to perturbations. The radar is used to provide an overview of the sensitivity metrics with respect to the effects of a perturbation. The radar allows for the further addition of new metrics and can also preserve the overall information of the perturbation when the analyst switches between multiple perturbation diagnoses through the tabs on the top of the view.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["Excluding", "AlgorithmicCalculation"],
        "componenet_code": ["excluding", "algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 75,
    "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics",
    "pub_year": 2021,
    "domain": "Graph mining",
    "requirement": {
      "requirement_text": "D2 Visualize the Effect of Perturbation. The system should be able to guide analysts to explore the perturbation effect of certain node\u2019s removal and support interactions such as sorting, searching and \ufb01ltering to inspect the auditing results and corresponding perturbation effects (T2, T3). This view should include: D2.2 Distribution View, which shows how the ranking position changes caused by the perturbation are distributed for each instance and the ranking distribution for each group of nodes. (T2.2, T1.3)",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.",
      "data_code": { "network_and_trees": 1 }
    },
    "solution": [
      {
        "solution_text": "Ranking Change Distribution View: A bar chart is used to show the ranking change distribution. Each bar is a node. The position of the bar on the x axis denotes the original ranking position for the node. The height of the bar on the y axis denotes the ranking change for the node. Colors represent the node labels. We scale the axes of the bar chart such that a 90-degree clockwise rotation of the bar also allows the analyst to infer the future rank of the node.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding",
        "solution_compoent": "",
        "axial_code": ["Excluding"],
        "componenet_code": ["excluding"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 76,
    "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics",
    "pub_year": 2021,
    "domain": "Graph mining",
    "requirement": {
      "requirement_text": "D2 Visualize the Effect of Perturbation. The system should be able to guide analysts to explore the perturbation effect of certain node\u2019s removal and support interactions such as sorting, searching and \ufb01ltering to inspect the auditing results and corresponding perturbation effects (T2, T3). This view should include: D2.2 Distribution View, which shows how the ranking position changes caused by the perturbation are distributed for each instance and the ranking distribution for each group of nodes. (T2.2, T1.3)",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.",
      "data_code": { "network_and_trees": 1 }
    },
    "solution": [
      {
        "solution_text": "Top-k Proportional Distribution View: When applying graph-based ranking algorithms for search engines, there could be an argument that ranking changes of webpages that are not part of the top-k ranking are less important than those in the top-k. This argument can be extended to any general graph ranking problem, where the analyst can choose a k for which elements below this ranking will not be considered. For example, a hiring manager may not be interested in resumes ranked outside of the top-25, but it important to understand whether certain node attributes are underrepresented in the top-k. In the Top-k Proportional Distribution View, we use two donut charts to represent the proportions of nodes of different categories belonging to ranking 1 to ranking k before and after the perturbation, and k is interactively specified.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding",
        "solution_compoent": "",
        "axial_code": ["Excluding"],
        "componenet_code": ["excluding"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 77,
    "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics",
    "pub_year": 2021,
    "domain": "Graph mining",
    "requirement": {
      "requirement_text": "D2 Visualize the Effect of Perturbation. The system should be able to guide analysts to explore the perturbation effect of certain node\u2019s removal and support interactions such as sorting, searching and \ufb01ltering to inspect the auditing results and corresponding perturbation effects (T2, T3). This view should include: D2.3 Ranking Change Detail View, which lists the in\ufb02uenced nodes for this perturbation. The view should support basic query operations, e.g., sorting, \ufb01ltering and searching, etc.(T2.1, T3)",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.",
      "data_code": { "network_and_trees": 1 }
    },
    "solution": [
      {
        "solution_text": "The view should support basic query operations, e.g., sorting, filtering and searching, etc.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding",
        "solution_compoent": "",
        "axial_code": ["Excluding"],
        "componenet_code": ["excluding"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 78,
    "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics",
    "pub_year": 2021,
    "domain": "Graph mining",
    "requirement": {
      "requirement_text": "D2 Visualize the Effect of Perturbation. The system should be able to guide analysts to explore the perturbation effect of certain node\u2019s removal and support interactions such as sorting, searching and \ufb01ltering to inspect the auditing results and corresponding perturbation effects (T2, T3). This view should include: D2.4 Local In\ufb02uence Graph View, which illustrates the relationship between the ranking changes of nodes and the topological changes caused by the perturbation. (T2.3, T3)",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.",
      "data_code": { "network_and_trees": 1 }
    },
    "solution": [
      {
        "solution_text": "We visualize the influence caused by removing/perturbing a node as a customized radial graph layout. In this customized layout, the removed node is set as the center of the force, and the strength of the charge force for each type of the node (hop-1 node, hop-2 node, etc.) is increased gradually based on the number n of hop-n.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding",
        "solution_compoent": "",
        "axial_code": ["Excluding"],
        "componenet_code": ["excluding"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 81,
    "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
    "pub_year": 2021,
    "domain": "Tax evasion",
    "requirement": {
      "requirement_text": "R1 Enable interactive con\ufb01gurations for suspicious RPTTE groups detection. The automated algorithms signi\ufb01cantly improve the ef\ufb01ciency of the tax inspection procedure when they can successfully identify the most relevant suspicious RPTTE groups. As mentioned by E2, different users may want to explore RPTTE groups that satisfy speci\ufb01c conditions and would like to con\ufb01gure different parameters to extend or narrow down the scope of suspicious groups by \ufb01ltering taxpayers not tightly connected to the related party transactions. For example, different users may have an interest in detecting tax evasion groups from different periods, and exploring tax evasion groups with a speci\ufb01c relevance or complexity",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.",
      "data_code": {
        "tables": 1,
        "categorical": 1,
        "textual": 1,
        "temporal": 1,
        "network_and_trees": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Network Construction. Similar to prior research [35], we also fuse multiple data sources and construct a taxpayer network to facilitate further analysis. We \ufb01rst formulate the taxpayer network with taxpayers and investors, where the nodes can be both taxpayers and investors, and edges represent investment relationships. Then we provide more information about the nodes by fusing their pro\ufb01le information and audit records to the network. A connected component in the resulting directed graph re\ufb02ects that of a related party. We further adjust the taxpayer network according to our design requirements (R1) and domain knowledge.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 82,
    "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
    "pub_year": 2021,
    "domain": "Tax evasion",
    "requirement": {
      "requirement_text": "R1 Enable interactive con\ufb01gurations for suspicious RPTTE groups detection. The automated algorithms signi\ufb01cantly improve the ef\ufb01ciency of the tax inspection procedure when they can successfully identify the most relevant suspicious RPTTE groups. As mentioned by E2, different users may want to explore RPTTE groups that satisfy speci\ufb01c conditions and would like to con\ufb01gure different parameters to extend or narrow down the scope of suspicious groups by \ufb01ltering taxpayers not tightly connected to the related party transactions. For example, different users may have an interest in detecting tax evasion groups from different periods, and exploring tax evasion groups with a speci\ufb01c relevance or complexity",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.",
      "data_code": {
        "tables": 1,
        "categorical": 1,
        "textual": 1,
        "temporal": 1,
        "network_and_trees": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The Control Panel consists of a bar chart and a parameter selector to support network fusion through interactively setting the preferred period or relevant thresholds. The bar chart on the right-hand side of the Control Panel offers a temporal summary of the daily related party transaction amount, which reveals a cyclic pattern where most of the peaks are near the end of the month. Since the profit manipulation should happen after tax evaders know how much taxable income they have, we speculate that this information can work as a visual cue to guide the tax administration officers in conducting their exploration of the suspicious RPTTE groups. Users can select their period of concern by brushing or clicking a bar to automatically select the quarter, which is a typical tax period.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 83,
    "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
    "pub_year": 2021,
    "domain": "Tax evasion",
    "requirement": {
      "requirement_text": "R1 Enable interactive con\ufb01gurations for suspicious RPTTE groups detection. The automated algorithms signi\ufb01cantly improve the ef\ufb01ciency of the tax inspection procedure when they can successfully identify the most relevant suspicious RPTTE groups. As mentioned by E2, different users may want to explore RPTTE groups that satisfy speci\ufb01c conditions and would like to con\ufb01gure different parameters to extend or narrow down the scope of suspicious groups by \ufb01ltering taxpayers not tightly connected to the related party transactions. For example, different users may have an interest in detecting tax evasion groups from different periods, and exploring tax evasion groups with a speci\ufb01c relevance or complexity",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.",
      "data_code": {
        "tables": 1,
        "categorical": 1,
        "textual": 1,
        "temporal": 1,
        "network_and_trees": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The parameter selector allows users to configure the maximum transaction chain length and maximum control chain length for the algorithm to determine the extent of the suspicious RPTTE groups.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 84,
    "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
    "pub_year": 2021,
    "domain": "Tax evasion",
    "requirement": {
      "requirement_text": "R1 Enable interactive con\ufb01gurations for suspicious RPTTE groups detection. The automated algorithms signi\ufb01cantly improve the ef\ufb01ciency of the tax inspection procedure when they can successfully identify the most relevant suspicious RPTTE groups. As mentioned by E2, different users may want to explore RPTTE groups that satisfy speci\ufb01c conditions and would like to con\ufb01gure different parameters to extend or narrow down the scope of suspicious groups by \ufb01ltering taxpayers not tightly connected to the related party transactions. For example, different users may have an interest in detecting tax evasion groups from different periods, and exploring tax evasion groups with a speci\ufb01c relevance or complexity",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.",
      "data_code": {
        "tables": 1,
        "categorical": 1,
        "textual": 1,
        "temporal": 1,
        "network_and_trees": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The nodes are labeled with their IDs for users to track the entity down in the Control Panel. Through the Graph View, users can gain an impression about whether the relevant thresholds have over-pruned or under-pruned the group and require a rerun of the network fusion algorithm with a different parameter setting. By hovering over the related party transactions, the common beneficial owners and the entire ownership chain is highlighted to reveal deceptive cases where tax evaders use a complex ownership structure to hide their identities. Clicking on the related party transaction link propagates the details to the detail view for further analysis.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 85,
    "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
    "pub_year": 2021,
    "domain": "Tax evasion",
    "requirement": {
      "requirement_text": "R2 Rank suspicious tax evasion groups with multiple criteria. Given the vast number of suspicious groups, it is critical to help users quickly locate groups with the highest risk level. The system should support the sorting of the groups based on multiple criteria which re\ufb02ects the suspicion in different dimensions. For example, according to E1\u2019s audit experience, one of the major suspicion criteria is the existence of historical tax evasion records because those groups are likely to commit tax evasion again.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.",
      "data_code": {
        "tables": 1,
        "categorical": 1,
        "textual": 1,
        "temporal": 1,
        "network_and_trees": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The Group Overview shows a list of suspicious RPTTE groups, in which each row represents a suspicious RPTTE group, and consists of an arc diagram based glyph and a bar chart to help users focus on the most suspicious groups. We extend the arc diagram as a glyph to visualize the topology of the related party transactions, which allows users to estimate the complexity of the suspicious RPTTE group quickly.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 86,
    "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
    "pub_year": 2021,
    "domain": "Tax evasion",
    "requirement": {
      "requirement_text": "R2 Rank suspicious tax evasion groups with multiple criteria. Given the vast number of suspicious groups, it is critical to help users quickly locate groups with the highest risk level. The system should support the sorting of the groups based on multiple criteria which re\ufb02ects the suspicion in different dimensions. For example, according to E1\u2019s audit experience, one of the major suspicion criteria is the existence of historical tax evasion records because those groups are likely to commit tax evasion again.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.",
      "data_code": {
        "tables": 1,
        "categorical": 1,
        "textual": 1,
        "temporal": 1,
        "network_and_trees": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The bar chart shows three group features, including the number of taxpayers with tax evasion records, the related party transaction amount, and the number of effective related party transactions. By default, the number of effective related party transactions is selected to rank the groups because the feature is engineered to represent one of the standard settings for transfer pricing. Users can click the sort icons located at the top of the list to sort the groups. Hovering over the nodes in the glyph highlights the corresponding node in the graph view. In addition, clicking the row propagates the details of any suspicious RPTTE group to the graph view for further analysis.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 87,
    "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
    "pub_year": 2021,
    "domain": "Tax evasion",
    "requirement": {
      "requirement_text": "R3 Support the interactive exploration of the common bene\ufb01cial owners of taxpayers who conduct the related party transactions and their attributes. As at least one common bene\ufb01cial owner will bene\ufb01t from RPTTE behaviors, exploring the investment and trading relationships helps users to understand how the tax evasion scheme works among taxpayers. In addition, the attributes of taxpayers such as historical tax evasion records provide the tax administration of\ufb01cers a context for suspicion and risk evaluation. Therefore, the experts require that all taxpayer with a common bene\ufb01cial owner should also be clearly visualized, facilitating the deep exploration and inspection of any highly suspicious related party transactions.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.",
      "data_code": {
        "tables": 1,
        "categorical": 1,
        "textual": 1,
        "temporal": 1,
        "network_and_trees": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The Graph View shows the hierarchical investment relationship and related party transactions within the selected suspicious group. We employ the method proposed by Jnger and Mutzel to conduct the graph layout, as it is more efficient in a layered graph drawing and can effectively reduce the crossed links. Also, we propose several encoding schemes to offer context for group assessment. The color of the borders encodes the entity type of node (investor or taxpayer), and the corresponding period-end profit status for the node is encoded by the fill colors where the diverging color scheme is the same as the node color of the detailed view. For the links, the type of relations uses color encoding. To stay consistent with the color encoding scheme of the node type, blue represents the related party transaction, while orange represents an investment. We emphasize the taxpayers with tax evasion records by drawing an exclamation mark in the circle.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 88,
    "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
    "pub_year": 2021,
    "domain": "Tax evasion",
    "requirement": {
      "requirement_text": "R4 Provide convenient pro\ufb01t analysis of taxpayers that conducted related party transactions. To facilitate the tax inspection process, the users need to know how the taxpayers redistribute their pro\ufb01ts through RPTTE behaviors. It is also important to present the related party transaction as evidence for users to quickly make audit decisions. Both E1 and E2 agree that the evaluation of suspicious related party transactions is challenging because tax evaders try their best to disguise their transactions as legal ones. However, the intent of such transactions must be re\ufb02ected in reported pro\ufb01ts, which leads to a lower tax burden. Therefore, pro\ufb01t analysis can act as the critical context to help users in decision-making. The visualization should display the pro\ufb01t status of the taxpayers, as the analysis of pro\ufb01t variations can reveal whether the related party transaction behaviors affect the overall tax burden.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.",
      "data_code": {
        "tables": 1,
        "categorical": 1,
        "textual": 1,
        "temporal": 1,
        "network_and_trees": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "We proposed using two calendar heatmaps to visualize the profit status of two traders who conducted the selected related party transaction. As shown in figure, we encode the cumulative daily profit with the background color of each visual mark in the calendar heatmap. We proposed two diverging color schemes with the help of ColorBrewer, namely, the brown-blue-green and the red-yellow-green. The former is colorblind-friendly, and the latter is consistent with conventional color usages in the Chinese stock market (i.e., red indicates profit while green represents loss).",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 89,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The top straight line in the DAG of a Git repository generally represents the master branch. However, an overwhelming number of branches and the connected links between them could hinder tracking down the origin of changes even for commits in the master branch. To alleviate this problem, Githru removes the connected links between the branches in a DAG to form a group of stems. A stem is a list of ancestor nodes for a specific commit that includes only one of the parents when there are multiple preceding nodes. It is similar to the first-parent option of the git log command, which removes other parent nodes from a branch.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "network+vector",
        "axial_code": ["Nesting"],
        "componenet_code": ["network", "vector"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 90,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The CSM can drastically reduce the number of stems and commits on the screen by removing implicit stems (Fig. 4d), stems corresponding to merged branches, or merged PRs (R1). LinVis [72] proposed a similar approach that grouped parents into a hierarchical structure and presented the structure in Merge-Tree. However, this prior work focused mainly on analyzing the details of CSM-sources (i.e., parent commits) in the master branch and presenting a visual representation of the hierarchy. In contrast, Githru provides an overview of the entire repository by applying the CSM to every stem. Furthermore, users can decide whether to apply a CSM or not, depending on their task. Users can also, if necessary, explicitly visualize the edges between the CSM-base and CSM-sources.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 91,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In another effort to improve scalability, we adopt a clustering technique to group neighboring commits in each stem. The scope of the grouping is confined to similar commits in each stem to preserve the temporal sequence and topology. We exploit the Simple AdditiveWeighting (SAW) model in calculating similarity since this model is intuitive for users to understand and is known to serve exploration well.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Clusterneighboringcommitsinstemstopreservetemporalsequence,topology.SAWmodelforsimilarity.",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 92,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "If there is still an overwhelming number of nodes even after the above techniques are applied, users can additionally apply Non-Con\ufb02ict Commits Clustering, which can group non-neighbor commits (R1). For instance, suppose that a cluster A is not adjacent to a cluster C, but their similarity is above the threshold (being suf\ufb01ciently similar). If the cluster A has no commonly modi\ufb01ed \ufb01les with cluster B (an inbetween cluster of A and C), changing the order of B and C could further simplify the underlying structure by grouping the clusters A and C as illustrated in Fig. 5. However, we make this process optional because it considers only the con\ufb02ict coming from modi\ufb01ed \ufb01les and not the contextual con\ufb02ict",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Clusternon-neighborcommits,simplifystructureoptionally.",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 93,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We emphasize each block using a black border line when it has a CSM-base and the CSM is enabled; the line turns dashed gray if the CSM is disabled. Regarding visual clutter of borders, adjacent clusters with identical pale colors were hardly distinguishable without any additional visual cues. This issue was raised during the interviews with domain experts, and we eventually included borders. The visibility of the edges between the CSM-base and CSM-sources also changes accordingly. This allows us to reduce the number of visualelements in the horizontal dimension without losing the temporal order of commits across stems.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 94,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Grouped Summary View shows a brief overview of the selected clusters as shown in Fig. 1d (R1). The columns in the view are mapped to individual clusters and the width of each column is proportional to the number of commits. This view enables a visual comparison of the relative size among selected clusters, which was frequently cited as a needed task in the requirement analysis (R4). Each column has a group of horizontal bars that brie\ufb02y show the top two or three values from the clustering criteria (i.e., author, commit types, modi\ufb01ed \ufb01les, and keywords). In addition, there are bars for the list of modi\ufb01ed directories and \ufb01les to offer more context. Furthermore, the length of each bar is proportional to the number of relevant commits that users could visually compare. For instance, users could \ufb01nd the author who has contributed the most to the cluster by \ufb01nding the longest bar. Enabling the Summary by CLOC option changes the width of each column and the length of the \ufb01le criteria bar proportionally to the number of CLOCs (changed LOCs, added LOCs + deleted LOCs).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 95,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "When users select a cluster in Grouped Summary View, Cluster Detail View appears at the bottom. This view provides commit-level details along with a visual summary of the affected \ufb01les and directories on the left (R1). A list of raw commit metadata is presented in a tabular form by date in ascending order (Fig. 1f). In the case of a CSM commit, it shows only the CSM-base at \ufb01rst, but users can expand the row to also see the relevant CSM-source commits. On the left of the table, we prepared a \ufb01le icicle tree [39] (Fig. 1e). Since \ufb01les and directories are organized in a hierarchy, we consider a number of space-\ufb01lling approaches to maximize space utilization [42]. Among Tree-Map [42], SunBurst [64], and the icicle tree, we \ufb01nally choose the last to comply with the task requirements. Tree-Map shows limitations in the structural interpretation task [18] and SunBurst is inadequate to embed long \ufb01le names because of the radial coordinate. On the other hand, the icicle tree explicitly shows a structural hierarchy in a Cartesian coordinate system well suited for displaying a string (e.g., \ufb01le-name) horizontally [39]. Also, as the depth of the modi\ufb01ed \ufb01le structure can vary, we enable users to zoom in or out with a mouse click on the icicle tree (R3).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 96,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Selection Cards represent corresponding selections. The stem information, such as a branch name or PR number, is prominently presented on each card, as they directly represent the characteristics of the cluster. For the same reason, the color of the card is also derived from the color of the stem where the selected cluster(s) is located.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 97,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R2: Visualize a graph while preserving topology. The graph representing the abstracted data should be visualized in an interpretable form. The graph should contain abstracted topological data that include (a) the temporal sequence of each node (i.e., commit) and (b) branch information and merge relation; and (c) the graph should be navigable with minimal interactions (T2).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The DAG representation of a Git repository suffers not only from a large number of nodes (i.e., commits) but also from diverging and converging links at implicit and explicit branches. As the number of commits and branches inevitably increases over time in an ongoing project, scalability is crucial for DAG-based visual analysis. As a remedy, we introduce graph reorganizing techniques tailored to the Git metadata, which could interactively reduce the number of nodes and links during analysis.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 98,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R2: Visualize a graph while preserving topology. The graph representing the abstracted data should be visualized in an interpretable form. The graph should contain abstracted topological data that include (a) the temporal sequence of each node (i.e., commit) and (b) branch information and merge relation; and (c) the graph should be navigable with minimal interactions (T2).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "As a resolution, we propose a Context-preserving Squash Merge (CSM). CSM fuses relevant commits (i.e., second parent commits from the merged branch [22]) into a single node for simplicity (Fig. 4c) and fetches messages from the stems to preserve the merge context (R2). For each merge commit on the main stem (i.e., CSM-base), CSM traverses every parent commit (i.e., the CSM-source) on the other stems. When a commit is a parent of multiple CSM-bases, we select the leftmost commit as a base to avoid redundant merges. CSM gathers contextual information from every CSM-source (e.g., author, commit type, and log message) and appends it to the end of the corresponding \ufb01eld in the CSM-base. For instance, the authors of CSM-sources become coauthors of the corresponding CSM-base. However, the list of changed \ufb01les remains the same since CSM-base encompasses the changes from CSM-sources.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 99,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R2: Visualize a graph while preserving topology. The graph representing the abstracted data should be visualized in an interpretable form. The graph should contain abstracted topological data that include (a) the temporal sequence of each node (i.e., commit) and (b) branch information and merge relation; and (c) the graph should be navigable with minimal interactions (T2).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We emphasize each block using a black border line when it has a CSM-base and the CSM is enabled; the line turns dashed gray if the CSM is disabled. Regarding visual clutter of borders, adjacent clusters with identical pale colors were hardly distinguishable without any additional visual cues. This issue was raised during the interviews with domain experts, and we eventually included borders. The visibility of the edges between the CSM-base and CSM-sources also changes accordingly. This allows us to reduce the number of visualelements in the horizontal dimension without losing the temporal order of commits across stems.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 100,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R2: Visualize a graph while preserving topology. The graph representing the abstracted data should be visualized in an interpretable form. The graph should contain abstracted topological data that include (a) the temporal sequence of each node (i.e., commit) and (b) branch information and merge relation; and (c) the graph should be navigable with minimal interactions (T2).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "One can control the granularity of clustering by setting a ClusteringStep encoded as a vertical slider. The desired level of abstraction can be set by adjusting the maximum difference value (threshold)to be clustered. For instance, if one moves up the slider, the clustering becomes granular. Thus, one can analyze fine-grained clusters by moving up the slider. For the same reason, we also provided a way to set Preference Weights for each similarity criterion. For instance, if one wants to cluster only commits with similar commit types, one can simply set the weight of the commit type to 1 and the rest to 0. Such a capability reveals the underlying policy of clustering, helping users to understand the context.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 101,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "On the left of the table, we prepared a file icicle tree. Since files and directories are organized in a hierarchy, we finally choose the icicle tree to comply with the task requirements. Because the icicle tree explicitly shows a structural hierarchy in a Cartesian coordinate system well suited for displaying a string (e.g., file-name) horizontally]. Also, as the depth of the modified file structure can vary, we enable users to zoom in or out with a mouse click on the icicle tree.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 102,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "One can control the granularity of clustering by setting a ClusteringStep encoded as a vertical slider. The desired level of abstraction can be set by adjusting the maximum difference value (threshold)to be clustered. For instance, if one moves up the slider, the clustering becomes granular. Thus, one can analyze fine-grained clusters by moving up the slider. For the same reason, we also provided a way to set Preference Weights for each similarity criterion. For instance, if one wants to cluster only commits with similar commit types, one can simply set the weight of the commit type to 1 and the rest to 0. Such a capability reveals the underlying policy of clustering, helping users to find the information they want by allowing them to set appropriate clustering schemes for their task.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 103,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In many tasks, understanding what happened over a period of time is important. Therefore, we provided a Global Temporal Filter with ways to filter for a certain time period: Brushing and a Select Box. Githru provides two horizontal bars that can be brushed. The bar at the top includes two areas aligned vertically, which represent the number of commits and LOCs by date respectively. The bar at the bottom is a horizontal list of boxes that encodes each commit ordered by date. Both brushes allow for filtering in a specific range and they are synchronized. This method allows users to effectively select a specific period of dates or commits. Users can also select a specific date or release tag using the Select Box. This solves the problem that occurs in brushing when the user has to choose an exact position, which is difficult to select.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 104,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Stem Type Filter Each stem type in Githru has various characteristics, such as the existence of a name, its relation to PRs, and its PR status. Users may want to focus on a particular stem type depending on their task. For instance, there is no need to see merged or closed branches when looking for recently opened PRs. Thus, we offered options to show or hide each stem type (R3).",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 105,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Search and Highlight If one searches for a certain keyword, Githru scans branch names, tags, commit messages, authors, commit IDs, and modi\ufb01ed \ufb01les. Then, it highlights every block that matches. Multiple keyword highlighting is also allowed (R3).",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 106,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": {
        "temporal": 1,
        "network_and_trees": 1,
        "sequential": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "Diff View shows a two-way comparison between selections for authors, commit types, files, and keywords. Since comparison becomes difficult as the number of objects increases and Grouped Summary View already provides a rough overview of a multi-way comparison, a two-way comparison fits the details-on-demand strategy.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 107,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R4: Support comparison. The system should facilitate comparisons (a) based on the number of commits and LOC. The magnitude can be compared according to (b) overall trends, or (c) within/between user-selections. (d) In particular, the information in the changed \ufb01les should be compared while being organized according to the directory that contains the structure of the source code (T3).",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "network_and_trees": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "This view enables a visual comparison of the relative size among selected clusters, which was frequently cited as a needed task in the requirement analysis.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 108,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R4: Support comparison. The system should facilitate comparisons (a) based on the number of commits and LOC. The magnitude can be compared according to (b) overall trends, or (c) within/between user-selections. (d) In particular, the information in the changed \ufb01les should be compared while being organized according to the directory that contains the structure of the source code (T3).",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": {
        "temporal": 1,
        "network_and_trees": 1,
        "sequential": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "Comparison View provides a detailed comparison of clusters or cluster sets (R4) based on similarity criteria and stem topology. It is designed following the details-on-detail strategy: a rough comparison in Grouped Summary View, and a detailed comparison in Comparison View. We used keywords instead of raw messages for comparison since it was more dif\ufb01cult to use unstructured strings than keywords when visualizing the differences and commonalities between clusters. We used Selection Cards to compare stem information and date and used a Diff View for the other criteria.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 109,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R4: Support comparison. The system should facilitate comparisons (a) based on the number of commits and LOC. The magnitude can be compared according to (b) overall trends, or (c) within/between user-selections. (d) In particular, the information in the changed \ufb01les should be compared while being organized according to the directory that contains the structure of the source code (T3).",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": {
        "temporal": 1,
        "network_and_trees": 1,
        "sequential": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "Diff View shows a two-way comparison between selections for authors, commit types, files, and keywords. Since comparison becomes difficult as the number of objects increases and Grouped Summary View already provides a rough overview of a multi-way comparison, a two-way comparison fits the details-on-demand strategy.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 110,
    "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles",
    "pub_year": 2021,
    "domain": "Functional Safety",
    "requirement": {
      "requirement_text": "DG1: FACILITATE E XPLORATION OF A P ROJECT User tasks such as \u201cFind missing links\u201d, \u201cLookup and analyze a node\u2019s end-to-end traceability\u201d are network exploration tasks. Hence, we modeled Functional Safety data as a network and derived this design goal to support tasks like fetching node details on hover, \ufb01nding adjacent nodes, and \ufb01nding paths from one node to another based on the taxonomies by Lee et al. [34] and Pretorius et al. [40].",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Since the visualization canvas and the tab layout are vertically stacked within a project panel, we positioned each panel side by side. This way, SafetyLens would facilitate exploratory analyses within and comparative analyses across projects ",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "scatter+table",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 111,
    "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles",
    "pub_year": 2021,
    "domain": "Functional Safety",
    "requirement": {
      "requirement_text": "DG2: FACILITATE C OMPARISON AMONG P ROJECTS Teams within functional safety can be system-speci\ufb01c and may not always be aware of the day to day progress made by other teams. This may result in duplicate work (e.g., a team may re-implement an artifact from scratch instead of re-using the one already implemented by another team, or for another vehicle). A core goal for SafetyLens, thus, was to provide a unified interface where users can explore and compare multiple projects to find shared (common) nodes, links, or even subgraphs (combination of nodes and links). This unified interface can foster better collaboration among teams while also saving time and resources for the organization.",
      "requirement_code": { "discover_observation": 1, "compare_entities": 1 }
    },
    "data": {
      "data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "D ASHBOARD V IEW, shown in Figure 4, is the landing page and provides an overview of the Functional Safety ecosystem. It consists of a clustered node visualization with each node representing a project. We complemented the visualization with a table to provide a familiar interface to users who are more comfortable with data in spreadsheets. The table consists of project-level information such as {Name, Department, Project In-Charge, and Location}. SafetyLens supports multiple starting points for users to analyze functional safety data. By providing access to all projects within the organization, SafetyLens facilitates collaboration (DG2) among domain experts with different roles (DG5).",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "scatter+table",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 112,
    "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles",
    "pub_year": 2021,
    "domain": "Functional Safety",
    "requirement": {
      "requirement_text": "DG2: FACILITATE C OMPARISON AMONG P ROJECTS Teams within functional safety can be system-speci\ufb01c and may not always be aware of the day to day progress made by other teams. This may result in duplicate work (e.g., a team may re-implement an artifact from scratch instead of re-using the one already implemented by another team, or for another vehicle). A core goal for SafetyLens, thus, was to provide a unified interface where users can explore and compare multiple projects to find shared (common) nodes, links, or even subgraphs (combination of nodes and links). This unified interface can foster better collaboration among teams while also saving time and resources for the organization.",
      "requirement_code": { "discover_observation": 1, "compare_entities": 1 }
    },
    "data": {
      "data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Since the visualization canvas and the tab layout are vertically stacked within a project panel, we positioned each panel side by side. This way, SafetyLens would facilitate exploratory analyses within and comparative analyses across projects ",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "scatter+table",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 113,
    "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles",
    "pub_year": 2021,
    "domain": "Functional Safety",
    "requirement": {
      "requirement_text": "DG2: FACILITATE C OMPARISON AMONG P ROJECTS Teams within functional safety can be system-speci\ufb01c and may not always be aware of the day to day progress made by other teams. This may result in duplicate work (e.g., a team may re-implement an artifact from scratch instead of re-using the one already implemented by another team, or for another vehicle). A core goal for SafetyLens, thus, was to provide a unified interface where users can explore and compare multiple projects to find shared (common) nodes, links, or even subgraphs (combination of nodes and links). This unified interface can foster better collaboration among teams while also saving time and resources for the organization.",
      "requirement_code": { "discover_observation": 1, "compare_entities": 1 }
    },
    "data": {
      "data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "S HARED V IEW shows the nodes and links that are common to / shared by all loaded projects (DG2). These are computed by performing a set intersection operation across project graphs based on unique node identi\ufb01ers. We show it in a separate view instead of overlaying or highlighting in the same view to make it easier for domain experts to begin their analysis.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "scatter+table",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 114,
    "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles",
    "pub_year": 2021,
    "domain": "Functional Safety",
    "requirement": {
      "requirement_text": "DG3: D ISCOVER PATTERNS AND A NOMALIES The domain experts we spoke to make use of existing commercial and open source tools. These tools support basic exploration and comparison of Functional Safety projects but may fall short when the scale and complexity of data increases. Thus, SafetyLens should support discovering interesting patterns within and across projects.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "S UMMARY V IEW, shown in Figure 5, provides a summary of the projects selected in the Dashboard View (DG6). There are juxtaposed heatmap visualizations for Node Type, Link Relation, and ASIL respectively positioned next to each other. For each attribute table, projects are along the column axis and the corresponding attribute values are along the row axis. An additional column titled \u201cS\u201d is added to show the number of entities (nodes and links) that are shared among these projects. The cells show the per-project entity counts for the corresponding attribute, colored using a continuous color scale (white-to-gray) to help the user discover patterns within as well as across projects (DG3)",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "heatmap",
        "axial_code": ["Repetition"],
        "componenet_code": ["heatmap"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 115,
    "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles",
    "pub_year": 2021,
    "domain": "Functional Safety",
    "requirement": {
      "requirement_text": "DG3: D ISCOVER PATTERNS AND A NOMALIES The domain experts we spoke to make use of existing commercial and open source tools. These tools support basic exploration and comparison of Functional Safety projects but may fall short when the scale and complexity of data increases. Thus, SafetyLens should support discovering interesting patterns within and across projects.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "VISUALIZATION CANVAS shows a node-link group visualiza- tion. Each node (small circle) is an Element of a functional safety project. These nodes are clustered together based on attributes (e.g., Type). The largest circles represent the group nodes and are labeled with the Type and the number of nodes that are part of it. The boundary marking the extent of the groups (convex hull) is highlighted. The nodes are positioned in each others\u2019 vicinity using an implementation of the circle packing algorithm. The node size and color can be mapped to attributes such as {ASIL, Type, Degree (number of edges to a node)}which in the presence of multiple nodes across multiple projects will create visual clusters leading to discoveries of patterns and anomalies. ",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "scatter",
        "axial_code": ["Repetition"],
        "componenet_code": ["scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 116,
    "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles",
    "pub_year": 2021,
    "domain": "Functional Safety",
    "requirement": {
      "requirement_text": "DG4: FACILITATE T RACEABILITY AND D ECOMPOSITION OF ASIL S An important task for domain experts is to trace the ASIL from one node to another (e.g., {MB \u2192 HzE \u2192 SG \u2192 FSR \u2192 TSR}). Since ASILs determine the extent of safety mechanisms for elements, any discrepancy such as an element assigned an ASIL=A instead of ASIL=D is an important concern. To diagnose the problem, users should be able to decompose the ASIL into its Severity (S), Exposure (E), and Controllability (C). Thus, it is an important design goal for SafetyLens to unify tasks allowing users to ef\ufb01ciently detect, diagnose, and \ufb01x ASIL assignment issues",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "SafetyLens first checks if a path exists between the two nodes and overlays it onto the node-link group visualization. It also returns a linearized node-link diagram showing the entire route from source to destination. Below the node link diagram is a heatmap showing the S-E-C (Severity Exposure-Controllability) break up of the ASILs.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "scatter+table+heatmap",
        "axial_code": ["Stack"],
        "componenet_code": ["heatmap", "table", "scatter"]
      },
      {
        "solution_text": "TRACE TAB allows the user to find and visualize if a path exists between two nodes as well as trace their ASILs (e.g., tracing the ASIL from a System Behavior to a Technical Software Requirement). The user can set a node as Source and another as Destination.",
        "solution_category": "interaction",
        "solution_axial": "Selecting,",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 117,
    "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles",
    "pub_year": 2021,
    "domain": "Functional Safety",
    "requirement": {
      "requirement_text": "DG6: P ROVIDE A S UMMARY OF K EY M ETRICS The user tasks suggested that key metrics should be readily available (e.g., total number of nodes, total number of nodes with ASIL=D, etc).",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "S UMMARY V IEW, shown in Figure 5, provides a summary of the projects selected in the Dashboard View (DG6). There are juxtaposed heatmap visualizations for Node Type, Link Relation, and ASIL respectively positioned next to each other. For each attribute table, projects are along the column axis and the corresponding attribute values are along the row axis. An additional column titled \u201cS\u201d is added to show the number of entities (nodes and links) that are shared among these projects. The cells show the per-project entity counts for the corresponding attribute, colored using a continuous color scale (white-to-gray) to help the user discover patterns within as well as across projects (DG3)",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "heatmap",
        "axial_code": ["Repetition"],
        "componenet_code": ["heatmap"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 119,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "P1: Obtain the spatial overview of the bus network and its performance. The experts requested to see a map-based overview of the bus network similar to other GIS software [20, 53]. Such an overview should help users to rapidly orient themselves in the spatial context and grasp the spatial distribution of routes. The overview should also include the visualization of the network performance to guide users in performing a drill-down analysis on the potentially ineffective routes.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "geometry": 1, "categorical": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "For the network-level analysis, a spatial aggregation view is designed to provide a spatial overview of the entire network and help the users filter routes with spatial constraints",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "map+radar+area",
        "axial_code": ["Nesting"],
        "componenet_code": ["radar", "area", "map"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 120,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "P1: Obtain the spatial overview of the bus network and its performance. The experts requested to see a map-based overview of the bus network similar to other GIS software [20, 53]. Such an overview should help users to rapidly orient themselves in the spatial context and grasp the spatial distribution of routes. The overview should also include the visualization of the network performance to guide users in performing a drill-down analysis on the potentially ineffective routes.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "geometry": 1, "categorical": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "For the route-level analysis, a route ranking view is implemented to depict the performance of the routes, allowing the users to find inefficient routes based on performance criteria.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "table+area",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 121,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "P1: Obtain the spatial overview of the bus network and its performance. The experts requested to see a map-based overview of the bus network similar to other GIS software [20, 53]. Such an overview should help users to rapidly orient themselves in the spatial context and grasp the spatial distribution of routes. The overview should also include the visualization of the network performance to guide users in performing a drill-down analysis on the potentially ineffective routes.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "geometry": 1, "categorical": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "6.1.1 Network-Level Analysis An overview of the bus network is essential in locating the areas where the inef\ufb01cient routes most likely exist. The spatial aggregation view (Fig. 1B), designed for the network-level analysis, comprises three linked layers, namely, the map, route, and aggregation layers, to help users analyze the network on a broad scale. The map layer comprises a base map. The route layer draws all routes on the map in blue with opacity. However, the route layer cannot depict the network topology because of the overlapping routes. Therefore, we designed the aggregation layer to visualize the topology with an aggregation graph. Each node in the aggregation graph corresponds to a group of bus stops aggregated spatially with hierarchical clustering [33] by balancing the number of stops in each group. These groups divide the city into transportation zones. The boundaries of these zones are computed with a Voronoi diagram [23] by unifying the polygons that enclose the bus stops inside the zones. In addition, the numbers of the routes between the zones are encoded with the link widths. A zone glyph (Fig. 1D) is placed at the centroid of each transportation zone to summarize the key statistics of this zone. The radar at the glyph center encodes six averaged criteria, namely, route length (RL), number of stops (NS), passenger volume (PV), average load (AL), route directness (DR), and service cost (SC).",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "map+radar+area",
        "axial_code": ["Nesting"],
        "componenet_code": ["radar", "area", "map"]
      },
      {
        "solution_text": "Users can con\ufb01gure the visibility and order of the dimensions \ufb02exibly in the context menu. Two diverging circular distributions around the glyph visualize the amount of passenger \ufb02ows by the geographical directions in which the passengers in this zone leave (green) or enter (orange). Double clicks magnify the glyphs, allowing users to obtain a clearer view of the radars inside. The design of this glyph is kept simple yet informative, such that the users can naturally obtain and compare the performance of different zones with a number of glyphs.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 122,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "P1: Obtain the spatial overview of the bus network and its performance. The experts requested to see a map-based overview of the bus network similar to other GIS software [20, 53]. Such an overview should help users to rapidly orient themselves in the spatial context and grasp the spatial distribution of routes. The overview should also include the visualization of the network performance to guide users in performing a drill-down analysis on the potentially ineffective routes.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "geometry": 1, "categorical": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "6.1.2 Route-Level Analysis In the route-level analysis, we focus on assisting the users in \ufb01nding inef\ufb01cient routes based on performance criteria in complementary to the spatial information. Inspired by LineUp [27], a table-based ranking visualization is included in the route ranking view to facilitate the multicriteria analysis of the routes.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "table+area",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "area"]
      },
      {
        "solution_text": "As illustrated in Fig. 1F, the columns represent six criteria similar to the zone glyphs, and each row is a route that can be ranked by selecting any column. The criteria can also be grouped and sorted with different weights. In addition, the criterion distributions are shown in the column headers, providing an overview and range \ufb01lters for the routes in the table.",
        "solution_category": "interaction",
        "solution_axial": "Reconfigure",
        "solution_compoent": "",
        "axial_code": ["Reconfigure"],
        "componenet_code": ["reconfigure"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 123,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "P2: Analyze the passenger \ufb02ows of bus routes to \ufb01nd weaknesses. The movement of passengers through the bus network provides key insights for the experts to evaluate the performance of the routes. For example, some parts of a route might be non-functional if few passengers were getting on or off in these parts. Therefore, intuitive visualization of passenger \ufb02ows is highly demanded to facilitate the identi\ufb01cation of de\ufb01cient routes. Moreover, such a visualization should also enable the experts to analyze the transfers among multiple bus routes, which may reveal patterns to improve route design.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "geometry": 1, "categorical": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "For the network-level analysis, a spatial aggregation view is designed to provide a spatial overview of the entire network and help the users filter routes with spatial constraints.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "map+radar+area",
        "axial_code": ["Nesting"],
        "componenet_code": ["radar", "area", "map"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 124,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "P2: Analyze the passenger \ufb02ows of bus routes to \ufb01nd weaknesses. The movement of passengers through the bus network provides key insights for the experts to evaluate the performance of the routes. For example, some parts of a route might be non-functional if few passengers were getting on or off in these parts. Therefore, intuitive visualization of passenger \ufb02ows is highly demanded to facilitate the identi\ufb01cation of de\ufb01cient routes. Moreover, such a visualization should also enable the experts to analyze the transfers among multiple bus routes, which may reveal patterns to improve route design.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "geometry": 1, "categorical": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "For the stop-level analysis, a route matrix view is proposed to visualize the passenger flows and transfers among the stops in a selected route with matrices, establishing fine-grained inspection of route performance.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "matrix+bar",
        "axial_code": ["Coordinate"],
        "componenet_code": ["bar", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 125,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "P2: Analyze the passenger \ufb02ows of bus routes to \ufb01nd weaknesses. The movement of passengers through the bus network provides key insights for the experts to evaluate the performance of the routes. For example, some parts of a route might be non-functional if few passengers were getting on or off in these parts. Therefore, intuitive visualization of passenger \ufb02ows is highly demanded to facilitate the identi\ufb01cation of de\ufb01cient routes. Moreover, such a visualization should also enable the experts to analyze the transfers among multiple bus routes, which may reveal patterns to improve route design.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "geometry": 1, "categorical": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "6.1.1 Network-Level Analysis An overview of the bus network is essential in locating the areas where the inef\ufb01cient routes most likely exist. The spatial aggregation view (Fig. 1B), designed for the network-level analysis, comprises three linked layers, namely, the map, route, and aggregation layers, to help users analyze the network on a broad scale. The map layer comprises a base map. The route layer draws all routes on the map in blue with opacity. However, the route layer cannot depict the network topology because of the overlapping routes. Therefore, we designed the aggregation layer to visualize the topology with an aggregation graph. Each node in the aggregation graph corresponds to a group of bus stops aggregated spatially with hierarchical clustering [33] by balancing the number of stops in each group. These groups divide the city into transportation zones. The boundaries of these zones are computed with a Voronoi diagram [23] by unifying the polygons that enclose the bus stops inside the zones. In addition, the numbers of the routes between the zones are encoded with the link widths. A zone glyph (Fig. 1D) is placed at the centroid of each transportation zone to summarize the key statistics of this zone. The radar at the glyph center encodes six averaged criteria, namely, route length (RL), number of stops (NS), passenger volume (PV), average load (AL), route directness (DR), and service cost (SC).",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "map+radar+area",
        "axial_code": ["Nesting"],
        "componenet_code": ["radar", "area", "map"]
      },
      {
        "solution_text": "Users can con\ufb01gure the visibility and order of the dimensions \ufb02exibly in the context menu. Two diverging circular distributions around the glyph visualize the amount of passenger \ufb02ows by the geographical directions in which the passengers in this zone leave (green) or enter (orange). Double clicks magnify the glyphs, allowing users to obtain a clearer view of the radars inside. The design of this glyph is kept simple yet informative, such that the users can naturally obtain and compare the performance of different zones with a number of glyphs.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 126,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "P2: Analyze the passenger \ufb02ows of bus routes to \ufb01nd weaknesses. The movement of passengers through the bus network provides key insights for the experts to evaluate the performance of the routes. For example, some parts of a route might be non-functional if few passengers were getting on or off in these parts. Therefore, intuitive visualization of passenger \ufb02ows is highly demanded to facilitate the identi\ufb01cation of de\ufb01cient routes. Moreover, such a visualization should also enable the experts to analyze the transfers among multiple bus routes, which may reveal patterns to improve route design.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "tables": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "6.1.3 Stop-Level Analysis The stop-level analysis enables the users to explore and evaluate the passenger \ufb02ows and transfers among the stops in a selected route. A \ufb02ow matrix (Fig. 4A) is designed to visualize the passenger \ufb02ows and transfers. The columns and rows of the matrix correspond to the bus stops, and the color intensity of each cell in the matrix encodes the number of passengers traveling between the stops. The color intensity is computed by normalizing the number of passengers into [0, 1] against a global passenger \ufb02ow threshold, which can be changed by users. The horizon charts (Fig. 4B) visualize the number of passengers, aggregated by time, checking in or out at the corresponding stops. The visibility of the horizon charts can be toggled in the context menu to simplify the view. Three-band horizon charts are chosen over bar charts because the estimation accuracy of horizon charts is better than that of bar charts when chart height is limited [30]. In addition, the bars (Fig. 4C) encoding the number of passengers per stop are positioned to the bottom and right of the matrix. In case of long bus routes, users can right click on the matrix to select stops they wish to keep in the view. Massive transfers may indicate that the routes are not well planned and discourage the use of bus transportation [75]. To visualize transfer information, we encode the number of passengers transferred to or from other routes with the opacity of the circles (Fig. 4D) next to the station names. The numbers on the circles indicate how many routes passengers have transferred to or from. The minimum opacity of the circles has been tuned to maintain the visibility of the numbers inside them. Clicking on a circle expands a list of the associated routes (Fig. 4E), each preceded by a small pie chart indicating the percentage of passenger \ufb02ows transferred to or from the route. Selecting a route in the list reveals another \ufb02ow matrix visualizing this route, which is aligned and linked to the original matrix (Fig. 4F). Inspired by MatrixWave [79], we rotate the matrices by 45 \u00b0 clockwise to accommodate them linearly for enhanced scalability. An overview of the matrices (Fig. 4G) is provided at the bottom-left of the view, where each matrix is represented with a square. The currently focused matrix is outlined in the overview, and the numbers of transferred routes are enclosed in the dashed squares.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "matrix+bar",
        "axial_code": ["Coordinate"],
        "componenet_code": ["bar", "matrix"]
      },
      {
        "solution_text": "6.1.3 Stop-Level Analysis The stop-level analysis enables the users to explore and evaluate the passenger \ufb02ows and transfers among the stops in a selected route. A \ufb02ow matrix (Fig. 4A) is designed to visualize the passenger \ufb02ows and transfers. The columns and rows of the matrix correspond to the bus stops, and the color intensity of each cell in the matrix encodes the number of passengers traveling between the stops. The color intensity is computed by normalizing the number of passengers into [0, 1] against a global passenger \ufb02ow threshold, which can be changed by users. The horizon charts (Fig. 4B) visualize the number of passengers, aggregated by time, checking in or out at the corresponding stops. The visibility of the horizon charts can be toggled in the context menu to simplify the view. Three-band horizon charts are chosen over bar charts because the estimation accuracy of horizon charts is better than that of bar charts when chart height is limited [30]. In addition, the bars (Fig. 4C) encoding the number of passengers per stop are positioned to the bottom and right of the matrix. In case of long bus routes, users can right click on the matrix to select stops they wish to keep in the view. Massive transfers may indicate that the routes are not well planned and discourage the use of bus transportation [75]. To visualize transfer information, we encode the number of passengers transferred to or from other routes with the opacity of the circles (Fig. 4D) next to the station names. The numbers on the circles indicate how many routes passengers have transferred to or from. The minimum opacity of the circles has been tuned to maintain the visibility of the numbers inside them. Clicking on a circle expands a list of the associated routes (Fig. 4E), each preceded by a small pie chart indicating the percentage of passenger \ufb02ows transferred to or from the route. Selecting a route in the list reveals another \ufb02ow matrix visualizing this route, which is aligned and linked to the original matrix (Fig. 4F). Inspired by MatrixWave [79], we rotate the matrices by 45 \u00b0 clockwise to accommodate them linearly for enhanced scalability. An overview of the matrices (Fig. 4G) is provided at the bottom-left of the view, where each matrix is represented with a square. The currently focused matrix is outlined in the overview, and the numbers of transferred routes are enclosed in the dashed squares.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 127,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "M1: Generate a set of alternative routes based on the constraints. The experts prefer to interact with the model and generate the Paretooptimal routes as the potential replacements of the selected ineffective route. To minimize the disruption caused by route changes, the experts may preserve several primary stops from the original route. Moreover, certain constraints, such as the construction cost and maximum route length, may be imposed on route generation. An easy-to-use interface is required for translating these constraints into the complex parameters required by the model.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "tables": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "This subsection summarizes the core idea of Weng et al.\u2019s method [63] that searches Pareto-optimal transit routes based on the Monte-Carlo search tree. The Monte-Carlo search tree [14] is studied to search the best next move in a game. Starting from a given game state, the search repeats four stages, namely, selection, expansion, simulation, and backpropagation. First, the most promising state is selected. Then, a new state is created based on the estimated best next move of the selected state. Next, the game result is obtained via simulation. Finally, the estimated value of the states in the tree is updated accordingly.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "The model results will be streamed and visualized in real-time to help the users determine the quality of the generated routes.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "map+area",
        "axial_code": ["Stack"],
        "componenet_code": ["area", "map"]
      },
      {
        "solution_text": "After a deficient route has been determined with the exploration inter- face, the users can obtain replacement routes from the manipulation interface. This interface allows the users to control the model by speci- fying model parameters, criterion filters, and anchored stops.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 128,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "M2: Inspect the quality of the generated routes in real time. Considering that the model is progressive and does not stop automatically, the experts need to know when the results are suf\ufb01ciently good to stop the route generation process. Hence, tailored visualizations are required to depict the current status of the generation process in real-time and provide the early quality preview of the alternative routes as the process continues. Moreover, such visualizations should allow some undesired routes to be removed from the search space to interactively guide and accelerate the search process.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "tables": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The model results will be streamed and visualized in real-time to help the users determine the quality of the generated routes.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "map+area",
        "axial_code": ["Stack"],
        "componenet_code": ["area", "map"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 129,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "L1: Compare the generated routes based on topologies. To help the experts identify the most promising ones from the generated routes, the proposed system must reveal the topological similarities and differences among these routes. The experts may want to know: What stops do these two routes share? Which pair of consecutive stops is the most frequently selected? How much does a route deviate from another one by taking a detour? Integrating topological information can help experts estimate the performance of these routes and eliminate the undesired ones that share similar characteristics",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "geometry": 1, "categorical": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "To aid the users in such progressive decision-making processes, the routes and their topologies are depicted with conflict markers on the map.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "matrix+bar",
        "axial_code": ["Coordinate"],
        "componenet_code": ["bar", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 130,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "L2: Compare the generated routes based on multiple criteria. The most promising alternative route should also be determined in terms of the performance criteria. However, experts may treat each criterion differently under different circumstances. For example, the distances between the stops in a suburban bus route will be considerably longer than those between the stops in a city bus route. To facilitate a judicious decision-making process, the system should enable the experts to inspect the criteria of the generated routes and identify the most optimal one ef\ufb01ciently with tailored ranking models.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "geometry": 1, "categorical": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "The criteria of the available choices are visualized in the ranking view to facilitate the analysis of route performance",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "table+area",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 131,
    "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
    "pub_year": 2021,
    "domain": "cheminformatics",
    "requirement": {
      "requirement_text": "R1: Overview and detailed analysis of a molecular ensemble in the low-dimensional space. For large datasets, scatter plots, which are commonly used to represent the DR output, may suffer from occlusion problems for large datasets. Therefore, the tool should provide visual support for the analysis of data on different levels of abstraction, from the overall distribution of the compounds within the 2D space to the detailed view of individual compounds for a selected region of interest.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1
      }
    },
    "solution": [
      {
        "solution_text": "User can get an overview of the distribution of compounds in a selected DR projection using a given molecular representation. ",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "The core of ChemVA consists of 2D plots, which give the user an overview of the distribution of compounds in a selected DR projection using a given molecular representation. This overview is supported by the Hexagonal view, a well-adopted and commonly used approach to visualize the outcome of DR techniques [57]. The Hexagonal view aims to overcome the overplotting problem, in which the projected data items overlap and cause visual clutter , thus limiting the interpretability, especially for datasets evincing high similarity between data items. The Hexagonal view of ChemVA seeks to solve this problem by aggregating individual data items into individual hexagons. The user can interactively select a subset of hexagons of interest and explore the distribution of individual data items within the Detail view. The combination of the Hexagonal view and the Detail view aims to ful\ufb01ll requirement R1. Finally, since ChemVA is tailored to support the visual comparison of different projections, it also offers the Difference view. This view was speci\ufb01cally designed to address that task, which is stated in requirements R2 and R3.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "hexagonarea+scatter",
        "axial_code": ["Repetition"],
        "componenet_code": ["hexagonarea", "scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 132,
    "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
    "pub_year": 2021,
    "domain": "cheminformatics",
    "requirement": {
      "requirement_text": "R2: Visual inspection of multiple projections. A set of compounds can be expressed by different vector-based molecular representations, each yielding a different DR projection. The tool should enable the user to intuitively combine information encoded in the individual projections to allow studying at once the similarity between compounds based on different molecular representations. More speci\ufb01cally, this includes exploring similarities and differences between chemical compounds, expressed by the different projections. Additionally, the visual representations and interactions should help the domain experts evaluate the suitability of the selected DR model.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1
      }
    },
    "solution": [
      {
        "solution_text": "User can get an overview of the distribution of compounds in a selected DR projection using a given molecular representation. ",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "In order to support the task of comparing the outputs of different 2D projections, as stated in our requirement R2, we propose a novel view called Difference view, that combines and contrasts two selected 2D Hexagonal views, A and B. Initially it displays a hexagonal layout similar to that presented in the Hexagonal view, where the opacity of each hexagon encodes the computed correlation score of the trustworthiness of the projections A and B under study (requirement R3)",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "hexagonarea",
        "axial_code": ["Coordinate"],
        "componenet_code": ["hexagonarea"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 133,
    "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
    "pub_year": 2021,
    "domain": "cheminformatics",
    "requirement": {
      "requirement_text": "R3: Evaluation of the trustworthiness of projections. Users need proper visual support for assessing the trustworthiness of a lowdimensional projection based on the distortion with regard to pairwise distances between compounds in the original space. When such trustworthiness can be compared on different DR projections, users can focus the exploration on a subset of molecular representations.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1
      }
    },
    "solution": [
      {
        "solution_text": "User can get an overview of the distribution of compounds in a selected DR projection using a given molecular representation. ",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "The color of a hexagon encodes the prevailing trend among its compounds for a selected feature, which is by default their bioactivity but can be switched to other molecular properties, including the trustworthiness of the projection.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "hexagonarea",
        "axial_code": ["Repetition"],
        "componenet_code": ["hexagonarea"]
      },
      {
        "solution_text": "The color of a hexagon encodes the prevailing trend among its compounds for a selected feature, which is by default their bioactivity but can be switched to other molecular properties, including the trustworthiness of the projection.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 134,
    "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
    "pub_year": 2021,
    "domain": "cheminformatics",
    "requirement": {
      "requirement_text": "R3: Evaluation of the trustworthiness of projections. Users need proper visual support for assessing the trustworthiness of a lowdimensional projection based on the distortion with regard to pairwise distances between compounds in the original space. When such trustworthiness can be compared on different DR projections, users can focus the exploration on a subset of molecular representations.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1
      }
    },
    "solution": [
      {
        "solution_text": "In addition, all properties and features described in Section 3 can be color-encoded on points representing each compound. These features are selected from a drop-down menu, and their color encodings are chosen according to their type, i.e., quantitative, such as molecular weight, or categorical, such as bioactivity towards a target protein. Another quantitative property that can be used for color encoding corresponds to the correlation scores, which encode the trustworthiness of the DR projection of the compound (requirement R3). These scores were computed using Pearson and Kendall correlation, whose calculation is explained in the Supplementary Material.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Upon selecting a subset of compounds in the Hexagonal view, the user can explore the selected data in the Detail view, depicted in Fig_x0002_ure 3. In this view, the compounds are represented using a standard scatter plot, enhanced by a subtle overlay of the same hexagonal grid as in the Hexagonal view, which helps users keep the correspondence between the zoom level in these views. ",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "hexagonarea+scatter",
        "axial_code": ["Nesting"],
        "componenet_code": ["hexagonarea", "scatter"]
      },
      {
        "solution_text": "The Detail view displays only the selected hexagons zoomed in after the selection. To further enhance the link between the Hexagonal and Detail views, the corresponding hexagons are highlighted when the user hovers over them in any of these views. In order to perform a selection of individual compounds in this view, a lasso-shaped selector is supported. The selected compounds are then displayed in the 3D view and also highlighted in the Table view (Section 5.1.2).",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 135,
    "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
    "pub_year": 2021,
    "domain": "cheminformatics",
    "requirement": {
      "requirement_text": "R3: Evaluation of the trustworthiness of projections. Users need proper visual support for assessing the trustworthiness of a lowdimensional projection based on the distortion with regard to pairwise distances between compounds in the original space. When such trustworthiness can be compared on different DR projections, users can focus the exploration on a subset of molecular representations.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1
      }
    },
    "solution": [
      {
        "solution_text": "User can get an overview of the distribution of compounds in a selected DR projection using a given molecular representation. ",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "In order to support the task of comparing the outputs of different 2D projections, as stated in our requirement R2, we propose a novel view called Difference view, that combines and contrasts two selected 2D Hexagonal views, A and B. Initially it displays a hexagonal layout similar to that presented in the Hexagonal view, where the opacity of each hexagon encodes the computed correlation score of the trustworthiness of the projections A and B under study (requirement R3)",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "hexagonarea",
        "axial_code": ["Coordinate"],
        "componenet_code": ["hexagonarea"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 136,
    "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
    "pub_year": 2021,
    "domain": "cheminformatics",
    "requirement": {
      "requirement_text": "R4: Comparison of compounds according to features related to drug-likeness. Chemical compounds have many features and descriptors related to drug-likeness that can complement other molecular representations in the task of virtual screening. Therefore, it is desirable to provide users with an option to visualize these additional features along with the projections.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1
      }
    },
    "solution": [
      {
        "solution_text": "Besides from the vector-based molecular representations used in the DR projections and displayed in our 2D plot views, there are several other molecular features related to drug-likeness that should be taken into consideration when analyzing the compounds, as stated in requirement R4. ChemVA enables the users to explore such features, listed in Section 3.2, by means of a Table view which offers advanced interaction options. We adopted a well-established tool, published by Gratzl et al. [13], and its extension [11]. As these tools perfectly \ufb01t to our needs, we incorporate them to ChemVA. Further details about the broad range of interaction possibilities can be found in the original papers. In addition to the list of compounds, the Table view provides the users with graphical elements in the form of juxtaposed bar charts and box plots in the right side panel. By default, the compounds in the table are logically grouped by their membership to hexagons in the Hexagonal view. These groups can be either expanded or compressed. When compressed, the table displays the box plots of the distribution of the values for each feature in the hexagon.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "bar+box",
        "axial_code": ["Stack"],
        "componenet_code": ["box", "bar"]
      },
      {
        "solution_text": "As shown in Figure 5. This view is interactively linked with the other visual components of ChemVA. When performing a selection in the 2D plot views, the corresponding compounds are automatically highlighted in the Table view. Conversely, when the user selects compounds in the Table view, the corresponding compounds are highlighted in the Detail view, displayed in the 3D view, and the corresponding hexagons are highlighted in the Hexagonal view",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 137,
    "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
    "pub_year": 2021,
    "domain": "cheminformatics",
    "requirement": {
      "requirement_text": "R5: Comprehensible viewing of 3D structural similarity. The tool should support the inspection of individual compounds in terms of their 3D geometry, as well as the visual comparison of common 3D substructures in a selected set of compounds. For multiple compounds, such a view should convey the information about similarities and differences in their 3D structure.",
      "requirement_code": {
        "describe_observation_item": 1,
        "compare_entities": 1
      }
    },
    "data": {
      "data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1
      }
    },
    "solution": [
      {
        "solution_text": "Compound similarity can be better perceived when the compounds are structurally aligned in the view. To serve this purpose, we use a structural alignment functionality provided by the OpenBabel tool [43]. Once molecules are aligned, the user should be able to easily identify their common parts, i.e., the subsets of atoms and bonds that are present in most of the selected compounds.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "3Dstructure",
        "axial_code": ["Co-axis"],
        "componenet_code": ["3Dstructure"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 138,
    "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
    "pub_year": 2021,
    "domain": "cheminformatics",
    "requirement": {
      "requirement_text": "R6: Possibility to add new compounds and comparison with the existing data. The tool should support the process of exploration of different features and bioactivity for newly added compounds. The new compound should be projected using the DR model and integrated into the remaining views, so that the user can compare its features to those of the compounds in the existing dataset.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1
      }
    },
    "solution": [
      {
        "solution_text": "ChemVA provides an option to add new compounds to the dataset being studied in order to explore their features and compare them with those of other compounds. By means of this functionality, the expert can assess the potential of this newly added compound prior to extensive wet lab testing.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 155,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.1 Match source code with binary cod",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "The source code view displays a single source code file. By default, it displays the one with the most data, but the file can be changed in the interface.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "text"]
      },
      {
        "solution_text": "The source code view displays a single source code file. By default, it displays the one with the most data, but the file can be changed in the interface.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 156,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.1 Match source code with binary cod",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Multiple lines can be selected and will be highlighted across other views, supporting the task of matching the source code and disassembly.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 157,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.1 Match source code with binary cod",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "The disassembly represents the ground truth of the compiled program. One strategy commonly employed by users was to use linked navigation to get close to an area of interest not otherwise selectable with information from our automated analysis and then search by scrolling from there, so we include it in its entirety.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "text"]
      },
      {
        "solution_text": "The disassembly represents the ground truth of the compiled program. One strategy commonly employed by users was to use linked navigation to get close to an area of interest not otherwise selectable with information from our automated analysis and then search by scrolling from there, so we include it in its entirety.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 158,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.1 Match source code with binary cod",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Basic blocks (nodes) in the CFG can be selected individually or by brush and will update all views. The CFG view also supports panning and zooming.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 159,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.2 Identify/Relate structures with code",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding",
        "solution_compoent": "",
        "axial_code": ["Excluding"],
        "componenet_code": ["excluding"]
      },
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "text"]
      },
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 160,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.2 Identify/Relate structures with code",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "The CFG view shows a subgraph of the full binary CFG, based on the current selection. Prior work on CFGs by the visualization experts led to this project. However, early meetings indicated matching of source and disassembly was the main workflow. Thus, our initial prototypes did not include a CFG. In subsequent meetings, we observed our domain experts had difficulty understanding structures such as loops with only matching or nesting. We thus chose to provide such context with a CFG view.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "network+contour+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["network", "contour", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 161,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.2 Identify/Relate structures with code",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      },
      {
        "solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "text",
        "axial_code": ["Repetition"],
        "componenet_code": ["text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 162,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.2 Identify/Relate structures with code",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Identifying or navigating to a particular loop is a common operation, so we chose to directly support it by creating a loop-centric view.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "tree+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["text", "tree"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 163,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.2 Identify/Relate structures with code",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Consistent with the function inlining view, selections in other views will filter this one, providing loop context to those other views.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 164,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.2 Identify/Relate structures with code",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "The call graph view shows a subgraph of the full call graph, with all functions reported by our analysis regardless of whether they were inlined. This view provides a way to relate selected disassembly to the functions and call stack.  Inlined calls are shown with a dashed red line to help identify them.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "network+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["network", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 165,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.3 Annotate relations",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding",
        "solution_compoent": "",
        "axial_code": ["Excluding"],
        "componenet_code": ["excluding"]
      },
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "text"]
      },
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 166,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.3 Annotate relations",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Annotating the disassembly with source code variable names is a common task. While our automated analysis provides a best-effort annotation, it is incomplete. We allow the user to manually add annotations with this view. The view further summarizes all active renamings.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 167,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.4 Trace variable",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding",
        "solution_compoent": "",
        "axial_code": ["Excluding"],
        "componenet_code": ["excluding"]
      },
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "text"]
      },
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 168,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.4 Trace variable",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "The call graph view shows a subgraph of the full call graph, with all functions reported by our analysis regardless of whether they were inlined. This view provides a way to relate selected disassembly to the functions and call stack.  Inlined calls are shown with a dashed red line to help identify them.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "network+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["network", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 169,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T2.1 Find areas of interest",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Another change from CFGExplorer is filtering the graph to a k-hop region of interest around selected basic blocks. Our data creates CFGs that are too large for Sugiyama-style layouts. To support the winnowing of data to find areas of interest, k is configurable via the interface, with a default of k=3 determined through our users\u2019 experience.",
        "solution_category": "interaction",
        "solution_axial": "Filtering,Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration", "Filtering"],
        "componenet_code": ["participation/collaboration", "filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 170,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T2.1 Find areas of interest",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Function inlining is one of the most common optimizations performed by compiler and is of great interest to our collaborators. Thus, we design a separate panel for inlining information to help identify and navigate to them.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "tree+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["text", "tree"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 171,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T2.1 Find areas of interest",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Identifying or navigating to a particular loop is a common operation, so we chose to directly support it by creating a loop-centric view.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "tree+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["text", "tree"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 172,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T2.1 Find areas of interest",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Consistent with the function inlining view, selections in other views will filter this one, providing loop context to those other views.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 173,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T2.2 Identify optimizations",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      },
      {
        "solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "text",
        "axial_code": ["Repetition"],
        "componenet_code": ["text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 174,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T2.2 Identify optimizations",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Function inlining is one of the most common optimizations performed by compiler and is of great interest to our collaborators. Thus, we design a separate panel for inlining information to help identify and navigate to them.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "tree+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["text", "tree"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 175,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T2.2 Identify optimizations",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "The call graph view shows a subgraph of the full call graph, with all functions reported by our analysis regardless of whether they were inlined. This view provides a way to relate selected disassembly to the functions and call stack.  Inlined calls are shown with a dashed red line to help identify them.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "network+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["network", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 176,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T2.3 Assess optimizations",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      },
      {
        "solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "text",
        "axial_code": ["Repetition"],
        "componenet_code": ["text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 178,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R1. Identify relevant dimensions that exhibit high levels of bias.Users should be able to see which dimensions exhibit high lev-els of selection bias and understand which of those high-biasdimensions, and groups of dimensions, are most relevant for theiranalytical question.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "The split icicle plot was developed to visualize shifts in distribution for dimensions in large hierarchies, indicating potential selection bias [6]. Although effective for communicating this information, it exhibits some limitations when used for DR. We therefore developed the icicle table to address these limitations and add functionality useful for DR.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "icicle",
        "axial_code": ["Repetition"],
        "componenet_code": ["icicle"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 179,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R1. Identify relevant dimensions that exhibit high levels of bias.Users should be able to see which dimensions exhibit high lev-els of selection bias and understand which of those high-biasdimensions, and groups of dimensions, are most relevant for theiranalytical question.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "The split icicle plot modifies the strict hierarchical icicle plot layout by splitting certain nodes, enabling more effective sorting of the plot by the maximum distance along each path from a leaf node to the root. Thus areas with large shifts can be sorted toward the top of the plot to help the user prioritize dimensions to investigate for selection bias.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "icicle",
        "axial_code": ["Repetition"],
        "componenet_code": ["icicle"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 180,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R1. Identify relevant dimensions that exhibit high levels of bias.Users should be able to see which dimensions exhibit high lev-els of selection bias and understand which of those high-biasdimensions, and groups of dimensions, are most relevant for theiranalytical question.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "tables": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "Integrating a table with the split icicle plot enables the inclusion of multi-attribute information to help the user select appropriate dimen- sions for reweighting.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "icicle+table",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "icicle"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 181,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R1. Identify relevant dimensions that exhibit high levels of bias.Users should be able to see which dimensions exhibit high lev-els of selection bias and understand which of those high-biasdimensions, and groups of dimensions, are most relevant for theiranalytical question.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "tables": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "Three additional visualizations show the effect of reweighting on per- dimension distances and outcome correlations for the baseline and focus cohorts and enable the selection of reweighting dimensions: a scatter plot, contour plot, and vector plot. Each view shows correlation with outcome along the x-axis and focus-to-baseline distance along the y-axis. The user can switch views on demand, with linked selection between the three views and the icicle table.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "scatter+contour+vector",
        "axial_code": ["Stack"],
        "componenet_code": ["vector", "scatter", "contour"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 182,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R2. Apply bias correction based on user-selected dimensions.Users should be able to specify one or more dimensions for biascorrection and have the system automatically determine the re-quired sample weighting to perform the correction.",
      "requirement_code": { "evaluate_hypothesis": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "The split icicle plot was developed to visualize shifts in distribution for dimensions in large hierarchies, indicating potential selection bias [6]. Although effective for communicating this information, it exhibits some limitations when used for DR. We therefore developed the icicle table to address these limitations and add functionality useful for DR.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "icicle",
        "axial_code": ["Repetition"],
        "componenet_code": ["icicle"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 183,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R2. Apply bias correction based on user-selected dimensions.Users should be able to specify one or more dimensions for biascorrection and have the system automatically determine the re-quired sample weighting to perform the correction.",
      "requirement_code": { "evaluate_hypothesis": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "tables": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "To further support the DR workflow, we have added a replace reweight mode. When fine-tuning the reweighting configuration, the user may wish to adjust a reweight dimension by moving up in the hierarchy to a more general type or down to a more specific type. To do so, the user can enter replace reweight mode, causing the icicle plot to show only the reweight dimension, its ancestors, and its descendants. All ancestors, the reweight dimension, and two levels of children are marked as salient, providing a detailed view of the local neighborhood around the reweight dimension. The user can then select a dimension to take its place in the list of reweight dimensions.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "icicle+table",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "icicle"]
      },
      {
        "solution_text": "To further support the DR workflow, we have added a replace reweight mode. When fine-tuning the reweighting configuration, the user may wish to adjust a reweight dimension by moving up in the hierarchy to a more general type or down to a more specific type. To do so, the user can enter replace reweight mode, causing the icicle plot to show only the reweight dimension, its ancestors, and its descendants. All ancestors, the reweight dimension, and two levels of children are marked as salient, providing a detailed view of the local neighborhood around the reweight dimension. The user can then select a dimension to take its place in the list of reweight dimensions.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 184,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R2. Apply bias correction based on user-selected dimensions.Users should be able to specify one or more dimensions for biascorrection and have the system automatically determine the re-quired sample weighting to perform the correction.",
      "requirement_code": { "evaluate_hypothesis": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "tables": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "The balance panel includes visualizations and controls for the DR process. A reweight list shows all dimensions selected for reweighting. The user can remove dimensions from the list, initiate the reweighting process, and use a slider to control the amount of reweighting. A detailed view of the per-cohort subgroups used for reweighting is shown in the reweight set visualization.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 185,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R3. Understand the effect of bias correction. This includes two keyaspects. First (R3.1), as correcting for a small set of speci\ufb01eddimensions will affect a larger set of dimensions, the user shouldbe made aware of how widespread the effects of reweighting are,where bias was reduced, and where and how much bias remains.Second (R3.2), users must understand the effect of bias correctionon the visualizations driving their primary analysis.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "tables": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "An improved aggregate distance measure is included to better sum- marize the effect of reweighting upon a cohort (R3.1). In a typical cohort exhibiting selection bias, the vast majority of dimensions un- dergo small to moderate shifts in distribution, whereas a smaller number of dimensions that are highly correlated with the dimensions used for selection will undergo larger shifts. It is typically these high-bias di- mensions that the user wishes to correct.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "As the event sequence analysis capabilities of Cadence are used to filter existing cohorts to create new cohorts, representations of each cohort and their provenance are shown in the cohort provenance tree (Figure 1-a). Each cohort is represented by a glyph encoding cohort size and aggregate distance across all data dimensions, along with icons indicating the baseline and focus cohorts (Figure 3-a and b).",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "tree+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "tree"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 186,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R3. Understand the effect of bias correction. This includes two keyaspects. First (R3.1), as correcting for a small set of speci\ufb01eddimensions will affect a larger set of dimensions, the user shouldbe made aware of how widespread the effects of reweighting are,where bias was reduced, and where and how much bias remains.Second (R3.2), users must understand the effect of bias correctionon the visualizations driving their primary analysis.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "The split icicle plot was developed to visualize shifts in distribution for dimensions in large hierarchies, indicating potential selection bias [6]. Although effective for communicating this information, it exhibits some limitations when used for DR. We therefore developed the icicle table to address these limitations and add functionality useful for DR.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "icicle",
        "axial_code": ["Repetition"],
        "componenet_code": ["icicle"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 187,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R3. Understand the effect of bias correction. This includes two keyaspects. First (R3.1), as correcting for a small set of speci\ufb01eddimensions will affect a larger set of dimensions, the user shouldbe made aware of how widespread the effects of reweighting are,where bias was reduced, and where and how much bias remains.Second (R3.2), users must understand the effect of bias correctionon the visualizations driving their primary analysis.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "tables": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "Three additional visualizations show the effect of reweighting on per- dimension distances and outcome correlations for the baseline and focus cohorts and enable the selection of reweighting dimensions: a scatter plot, contour plot, and vector plot. Each view shows correlation with outcome along the x-axis and focus-to-baseline distance along the y-axis. The user can switch views on demand, with linked selection between the three views and the icicle table.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "scatter+contour+vector",
        "axial_code": ["Stack"],
        "componenet_code": ["vector", "scatter", "contour"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 188,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R3. Understand the effect of bias correction. This includes two keyaspects. First (R3.1), as correcting for a small set of speci\ufb01eddimensions will affect a larger set of dimensions, the user shouldbe made aware of how widespread the effects of reweighting are,where bias was reduced, and where and how much bias remains.Second (R3.2), users must understand the effect of bias correctionon the visualizations driving their primary analysis.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "tables": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "Data type-dependent visualizations supporting R3 show the distribution of any dimension selected via the icicle table or distance vs. correlation plots, enabling the user to view information such as which cohort has a higher percentage of heart disease or what the gender breakdown is for each cohort. Figure shows the distributions of nicotine dependence in the baseline (top) and focus (bottom) using the designs from Figure 3-d. In this example, the weighted focus cohort\u2019s distribution has shifted to match that of the baseline cohort. This design is also incorporated into a histogram visualization for numeric dimensions (e.g., age) and a dumbbell plot for categorical dimensions (e.g., gender or race).",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "distancevs.correlationplots",
        "axial_code": ["Repetition"],
        "componenet_code": ["correlationplots", "distancevs"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 189,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R4. Prevent over\ufb01tting for poorly represented subgroups. Insome cases the weighted samples used for bias correction canexcessively amplify poorly sampled subgroups, similar to theproblem of model over\ufb01tting. Users must be able to understandwhen a proposed bias correction poses a risk of over\ufb01tting dueto limitations in the underlying data, and be able to revise thereweighting con\ufb01guration appropriately.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "tables": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "The danger score alerts the user to potential problems with the current reweighting configuration. A score is computed for each cohort to identify those with poorly sampled subgroups. An indicator is shown next to any cohort with a score approaching a system-defined threshold, indicating that adjustments to the current reweighting configuration may be warranted.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 190,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R4. Prevent over\ufb01tting for poorly represented subgroups. Insome cases the weighted samples used for bias correction canexcessively amplify poorly sampled subgroups, similar to theproblem of model over\ufb01tting. Users must be able to understandwhen a proposed bias correction poses a risk of over\ufb01tting dueto limitations in the underlying data, and be able to revise thereweighting con\ufb01guration appropriately.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "tables": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "To further support the DR workflow, we have added a replace reweight mode. When fine-tuning the reweighting configuration, the user may wish to adjust a reweight dimension by moving up in the hierarchy to a more general type or down to a more specific type. To do so, the user can enter replace reweight mode, causing the icicle plot to show only the reweight dimension, its ancestors, and its descendants. All ancestors, the reweight dimension, and two levels of children are marked as salient, providing a detailed view of the local neighborhood around the reweight dimension. The user can then select a dimension to take its place in the list of reweight dimensions.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "icicle+table",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "icicle"]
      },
      {
        "solution_text": "To further support the DR workflow, we have added a replace reweight mode. When fine-tuning the reweighting configuration, the user may wish to adjust a reweight dimension by moving up in the hierarchy to a more general type or down to a more specific type. To do so, the user can enter replace reweight mode, causing the icicle plot to show only the reweight dimension, its ancestors, and its descendants. All ancestors, the reweight dimension, and two levels of children are marked as salient, providing a detailed view of the local neighborhood around the reweight dimension. The user can then select a dimension to take its place in the list of reweight dimensions.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 191,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R4. Prevent over\ufb01tting for poorly represented subgroups. Insome cases the weighted samples used for bias correction canexcessively amplify poorly sampled subgroups, similar to theproblem of model over\ufb01tting. Users must be able to understandwhen a proposed bias correction poses a risk of over\ufb01tting dueto limitations in the underlying data, and be able to revise thereweighting con\ufb01guration appropriately.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "tables": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "The balance panel includes visualizations and controls for the DR process. A reweight list shows all dimensions selected for reweighting. The user can remove dimensions from the list, initiate the reweighting process, and use a slider to control the amount of reweighting. A detailed view of the per-cohort subgroups used for reweighting is shown in the reweight set visualization.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 192,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R4. Prevent over\ufb01tting for poorly represented subgroups. Insome cases the weighted samples used for bias correction canexcessively amplify poorly sampled subgroups, similar to theproblem of model over\ufb01tting. Users must be able to understandwhen a proposed bias correction poses a risk of over\ufb01tting dueto limitations in the underlying data, and be able to revise thereweighting con\ufb01guration appropriately.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "tables": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "The reweight set visualization is based on Upset [26]. It shows how the reweight dimensions combine to form subgroups of each cohort to be reweighted (Section 6), and enables identification and cor- rection of potential reweighting issues that may result in unreasonably high weights for small subgroups of individuals.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "matrix+bar",
        "axial_code": ["Stack"],
        "componenet_code": ["bar", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 193,
    "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection",
    "pub_year": 2021,
    "domain": "autonomous driving",
    "requirement": {
      "requirement_text": "RD1: Human-friendly data representation and summarization. This issue arises from the nature of high dimensionality and sheer volume of images. We need a representation to capture the intrinsic attributes of images in a lower dimension space and then summarize them in a human-friendly way [27,28].",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "In the second module, two core learning components, disentangled representation learning and semantic adversarial learning, are introduced to augment our analysis and meet the requirements of RD1 and RD2. Disentangled representation learning extracts intrinsic and interpretable attributes of traffic lights, such as colors, brightness, and background (Fig. 4b1 ). This component first provides a human-friendly data presentation, and also offers a data space where an adversarial generation can efficiently search. Semantic adversarial learning learns prediction behaviors of a detector, and generates meaningful adversarial examples (Fig. 4b2 ) on top of disentangled representation learning. After this, both acquired data (Fig. 4b3 ) and unseen data (Fig. 4b4 ) are passed to the detector to obtain detection results.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "Disentangledrepresentationandsemanticadversariallearning.",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 194,
    "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection",
    "pub_year": 2021,
    "domain": "autonomous driving",
    "requirement": {
      "requirement_text": "RD1: Human-friendly data representation and summarization. This issue arises from the nature of high dimensionality and sheer volume of images. We need a representation to capture the intrinsic attributes of images in a lower dimension space and then summarize them in a human-friendly way [27,28].",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "Disentangled Representation Learning (DRL) is introduced to extract semantic latent representation of traffic lights (e.g. colors, background, rotation, etc.), shown in Fig.5-a and generate more data with controllable semantics for data augmentation. The semantic latent representation also serves as a cornerstone for both human-friendly data summarization (RD1) and semantic adversarial learning (RD2).",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "Semanticlatentrepresentationextractionfortrafficlights.",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 195,
    "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection",
    "pub_year": 2021,
    "domain": "autonomous driving",
    "requirement": {
      "requirement_text": "RD2: Efficient generation of unseen test cases. We seek for a method generating edge cases to probe model robustness. These test cases should be different from the imperceivable noises that learned from traditional adversarial approaches, and have semantic meanings to guide human to improve the robustness [10,23].",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "In the second module, two core learning components, disentangled representation learning and semantic adversarial learning, are introduced to augment our analysis and meet the requirements of RD1 and RD2. Disentangled representation learning extracts intrinsic and interpretable attributes of traffic lights, such as colors, brightness, and background (Fig. 4b1 ). This component first provides a human-friendly data presentation, and also offers a data space where an adversarial generation can efficiently search. Semantic adversarial learning learns prediction behaviors of a detector, and generates meaningful adversarial examples (Fig. 4b2 ) on top of disentangled representation learning. After this, both acquired data (Fig. 4b3 ) and unseen data (Fig. 4b4 ) are passed to the detector to obtain detection results.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "Disentangledrepresentationandsemanticadversariallearning.",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 196,
    "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection",
    "pub_year": 2021,
    "domain": "autonomous driving",
    "requirement": {
      "requirement_text": "RD2: Efficient generation of unseen test cases. We seek for a method generating edge cases to probe model robustness. These test cases should be different from the imperceivable noises that learned from traditional adversarial approaches, and have semantic meanings to guide human to improve the robustness [10,23].",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "Disentangled Representation Learning (DRL) is introduced to extract semantic latent representation of traffic lights (e.g. colors, background, rotation, etc.), shown in Fig.5-a and generate more data with controllable semantics for data augmentation. The semantic latent representation also serves as a cornerstone for both human-friendly data summarization (RD1) and semantic adversarial learning (RD2).",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "Semanticlatentrepresentationextractionfortrafficlights.",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 197,
    "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection",
    "pub_year": 2021,
    "domain": "autonomous driving",
    "requirement": {
      "requirement_text": "RP1: A contextualized understanding for model performance. In_x0002_stead of using an aggregated metric to evaluate models [18], we would like to put a single score into the contexts of various sizes, IoU thresholds and confident score ranges.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "In the second module, two core learning components, disentangled representation learning and semantic adversarial learning, are introduced to augment our analysis and meet the requirements of RD1 and RD2. Disentangled representation learning extracts intrinsic and interpretable attributes of traffic lights, such as colors, brightness, and background (Fig. 4b1 ). This component first provides a human-friendly data presentation, and also offers a data space where an adversarial generation can efficiently search. Semantic adversarial learning learns prediction behaviors of a detector, and generates meaningful adversarial examples (Fig. 4b2 ) on top of disentangled representation learning. After this, both acquired data (Fig. 4b3 ) and unseen data (Fig. 4b4 ) are passed to the detector to obtain detection results.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "Disentangledrepresentationandsemanticadversariallearning.",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "The metric charts are coordinated with each other to filter data and support multi-faceted performance analysis for accuracy and robustness in other views. This is designed to support contextualized understanding of performance.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "bar"]
      },
      {
        "solution_text": "The metric charts are coordinated with each other to filter data and support multi-faceted performance analysis for accuracy and robustness in other views. This is designed to support contextualized understanding of performance.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 198,
    "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection",
    "pub_year": 2021,
    "domain": "autonomous driving",
    "requirement": {
      "requirement_text": "RP2: Performance interpretation for both accuracy and robustness. . It is challenging to understand how the accuracy and robustness of detectors are impacted by different semantic characteristics of data [11,28,39], including colors of traffic lights (red, green, yellow, off), illumination settings (sun glare or darkness in the tree), distances (large or small size object in the scene), or confusing background (similar but irrelevant objects).",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "In the second module, two core learning components, disentangled representation learning and semantic adversarial learning, are introduced to augment our analysis and meet the requirements of RD1 and RD2. Disentangled representation learning extracts intrinsic and interpretable attributes of traffic lights, such as colors, brightness, and background (Fig. 4b1 ). This component first provides a human-friendly data presentation, and also offers a data space where an adversarial generation can efficiently search. Semantic adversarial learning learns prediction behaviors of a detector, and generates meaningful adversarial examples (Fig. 4b2 ) on top of disentangled representation learning. After this, both acquired data (Fig. 4b3 ) and unseen data (Fig. 4b4 ) are passed to the detector to obtain detection results.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "Disentangledrepresentationandsemanticadversariallearning.",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "We design a view of performance landscape, TileScape, to summarize visual characteristics and corresponding performance over tens of thou- sands of objects. In TileScape, each cell (tile) is an aggregated bin of many detec- tions which are similar in the semantic latent space learned from DRL. Each data-point is a detection, located in the latent space by its latent vector encoded from a detected image patch. Then, the data-points are aggregated into bins, called as tiles, according to a view range and bin size. Also, one object is selected as the representative one for each tile. Here, we use the object with median score from a bin. Additionally, in the background of TileScape, a contour density map shows the data distribution in the current space.",
        "solution_category": "visualization",
        "solution_axial": "Visualization",
        "solution_compoent": "Coordinate",
        "axial_code": ["Visualization"],
        "componenet_code": ["Coordinate"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 199,
    "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection",
    "pub_year": 2021,
    "domain": "autonomous driving",
    "requirement": {
      "requirement_text": "RP3: Injecting human intelligence for performance improvement with minimal human interaction. The ultimate goal of model evaluation and interpretation is to improve its performance with human knowledge in the loop [12]. Meanwhile, as we are working with domain experts, we found they have limited bandwidth to conduct in-depth exploration, but focus on key insights with few interactions [8]. This calls for maximazing insight generation and injection with minimal interaction.",
      "requirement_code": { "knowledge_injection": 1 }
    },
    "data": {
      "data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "In the second module, two core learning components, disentangled representation learning and semantic adversarial learning, are introduced to augment our analysis and meet the requirements of RD1 and RD2. Disentangled representation learning extracts intrinsic and interpretable attributes of traffic lights, such as colors, brightness, and background (Fig. 4b1 ). This component first provides a human-friendly data presentation, and also offers a data space where an adversarial generation can efficiently search. Semantic adversarial learning learns prediction behaviors of a detector, and generates meaningful adversarial examples (Fig. 4b2 ) on top of disentangled representation learning. After this, both acquired data (Fig. 4b3 ) and unseen data (Fig. 4b4 ) are passed to the detector to obtain detection results.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "Disentangledrepresentationandsemanticadversariallearning.",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "Semantic interpretation by coordinating with TileScape. Two mechanisms are introduced to help users understand dimension seman- tics and also interpret their impact over performance.",
        "solution_category": "visualization",
        "solution_axial": "Annotation",
        "solution_compoent": "matrix+image",
        "axial_code": ["Annotation"],
        "componenet_code": ["image", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 200,
    "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection",
    "pub_year": 2021,
    "domain": "autonomous driving",
    "requirement": {
      "requirement_text": "RP3: Injecting human intelligence for performance improvement with minimal human interaction. The ultimate goal of model evaluation and interpretation is to improve its performance with human knowledge in the loop [12]. Meanwhile, as we are working with domain experts, we found they have limited bandwidth to conduct in-depth exploration, but focus on key insights with few interactions [8]. This calls for maximazing insight generation and injection with minimal interaction.",
      "requirement_code": { "knowledge_injection": 1 }
    },
    "data": {
      "data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "Finally, with minimal human interaction from visual interface, actionable insights are derived to generate more data that attempt to \u201clift distribution up\u201d via data augmentation (Fig. 4d ). This also enables us inject human intelligence to improve model accuracy and robustness, aiming at the requirement RP3.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      },
      {
        "solution_text": "Finally, with minimal human interaction from visual interface, actionable insights are derived to generate more data that attempt to \u201clift distribution up\u201d via data augmentation (Fig. 4d ). This also enables us inject human intelligence to improve model accuracy and robustness, aiming at the requirement RP3.",
        "solution_category": "data_manipulation",
        "solution_axial": "UserInput",
        "solution_compoent": "Generateactionableinsights,augmentdatadistribution.",
        "axial_code": ["UserInput"],
        "componenet_code": ["user_input"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 201,
    "paper_title": "Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images",
    "pub_year": 2021,
    "domain": "x-ray image classi\ufb01cation",
    "requirement": {
      "requirement_text": "T1. Analysis in Model Spaces: Investigate scienti\ufb01c images withinthe spaces of ACT, PRD, and FEA, for users to understand how theimages are modeled by the ResNet in the feature space and then classi-\ufb01ed by fully connected layers in the prediction space, with respect tothe real labels. Users can study ResNet model performance by com-paring the distributions of images after feature extraction (FEA), afterclassi\ufb01cation (PRD), and with actual labels (ACT). This study needs tobe performed in an exploratory process. Therefore, it is important tovisualize the images in the three spaces at the same time",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We utilize an open x-ray scattering dataset [36], and an updated ResNet model [33] that was designed for multiple attributes classification of the dataset. About 1,000 x-ray scattering images were employed in our visualization system. They include different types of images including semiconductors, nano-particles, polymer, lithographic gratings, and so on. The attributes in these images are either labeled by domain experts or synthetically generated by a simulation algorithm [33]. Each image thus has an actual attribute vector (ACT vector) consisting of 17 Boolean (0 or 1) values to show if the image has a number of the 17 attributes.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "categorical": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "The goal is to allow users to interactively select, compare, and study images of interest. Therefore, the 2048-dimensional FEA space and 17-dimensional ACT and PRD spaces are projected to the embedded 2D spaces to fulfill the goal. In our system, we have included two commonly used dimension reduction (DR) algorithms, t-SNE and PCA, for deep learning visualization. Other DR methods may further be added.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "Coordinated Visualization in Embedded Spaces: Images are visualized in the 2D canvases of ACT, PRD and FEA spaces, respectively. The goal is to allow users to easily observe many x-ray images and their relationships in these spaces simultaneously.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "scatter",
        "axial_code": ["Repetition"],
        "componenet_code": ["scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 202,
    "paper_title": "Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images",
    "pub_year": 2021,
    "domain": "x-ray image classi\ufb01cation",
    "requirement": {
      "requirement_text": "T2. Analysis with Group Behaviors: Select and examine speci\ufb01cgroups of images in the ACT, PRD, and FEA spaces, in order to \ufb01ndimportant clusters and outliers with respect to the learning model.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "We utilize an open x-ray scattering dataset [36], and an updated ResNet model [33] that was designed for multiple attributes classification of the dataset. About 1,000 x-ray scattering images were employed in our visualization system. They include different types of images including semiconductors, nano-particles, polymer, lithographic gratings, and so on. The attributes in these images are either labeled by domain experts or synthetically generated by a simulation algorithm [33]. Each image thus has an actual attribute vector (ACT vector) consisting of 17 Boolean (0 or 1) values to show if the image has a number of the 17 attributes.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "categorical": 1
      }
    },
    "solution": [
      {
        "solution_text": "Image Group Selection and Visualization: Within the embedded spaces, users are enabled to flexibly select images into groups at each embedded space by lasso tools. Then the selected images in each group are visually highlighted in other spaces. This function is very important for users to freely explore images of interest and conduct comparative analysis among the three spaces. The grouped images are also visualized by a statistic view of image metrics and an image gallery view. They can be further clustered for drill-down study and comparison.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "radialbarchart",
        "axial_code": ["Repetition"],
        "componenet_code": ["radialbarchart"]
      },
      {
        "solution_text": "Image Group Selection and Visualization: Within the embedded spaces, users are enabled to flexibly select images into groups at each embedded space by lasso tools. Then the selected images in each group are visually highlighted in other spaces. This function is very important for users to freely explore images of interest and conduct comparative analysis among the three spaces. The grouped images are also visualized by a statistic view of image metrics and an image gallery view. They can be further clustered for drill-down study and comparison.",
        "solution_category": "interaction",
        "solution_axial": "Selecting,Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration", "Selecting"],
        "componenet_code": ["participation/collaboration", "selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 203,
    "paper_title": "Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images",
    "pub_year": 2021,
    "domain": "x-ray image classi\ufb01cation",
    "requirement": {
      "requirement_text": "T3. Analysis with Image Attributes: Identify important image in-stances with the performance metrics of individual attributes and co-existent attributes to perform the \ufb01rst two tasks.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "We utilize an open x-ray scattering dataset [36], and an updated ResNet model [33] that was designed for multiple attributes classification of the dataset. About 1,000 x-ray scattering images were employed in our visualization system. They include different types of images including semiconductors, nano-particles, polymer, lithographic gratings, and so on. The attributes in these images are either labeled by domain experts or synthetically generated by a simulation algorithm [33]. Each image thus has an actual attribute vector (ACT vector) consisting of 17 Boolean (0 or 1) values to show if the image has a number of the 17 attributes.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "categorical": 1
      }
    },
    "solution": [
      {
        "solution_text": "Attribute Co-existence Visualization: The model perfor- mance with the relations of co-existing attributes is visualized in an interactive view. Users can then define image groups based on the visual cues of model performance.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table",
        "axial_code": ["Repetition"],
        "componenet_code": ["table"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 204,
    "paper_title": "Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images",
    "pub_year": 2021,
    "domain": "x-ray image classi\ufb01cation",
    "requirement": {
      "requirement_text": "T4. Analysis with Comparisons: Compare individual images andimage clusters for the model prediction performance.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "We utilize an open x-ray scattering dataset [36], and an updated ResNet model [33] that was designed for multiple attributes classification of the dataset. About 1,000 x-ray scattering images were employed in our visualization system. They include different types of images including semiconductors, nano-particles, polymer, lithographic gratings, and so on. The attributes in these images are either labeled by domain experts or synthetically generated by a simulation algorithm [33]. Each image thus has an actual attribute vector (ACT vector) consisting of 17 Boolean (0 or 1) values to show if the image has a number of the 17 attributes.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "categorical": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "Group Comparative View and Image Comparison: The selected groups can be easily investigated and compared with group panels for detailed views. Through interactive selection over all the above visualizations, users can also open multiple images to compare their details of raw data and model predictions.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "image+matrix",
        "axial_code": ["Repetition"],
        "componenet_code": ["image", "matrix"]
      },
      {
        "solution_text": "Group Comparative View and Image Comparison: The selected groups can be easily investigated and compared with group panels for detailed views. Through interactive selection over all the above visualizations, users can also open multiple images to compare their details of raw data and model predictions.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 221,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T1 Detect Outbreak. Is there an outbreak? When the number ofinfected patients rises above a normal level within a certain period,i.e., the endemic level, an outbreak occurs. Depending on thepathogen, this endemic level can be two or more patients. Aspatients may not be tested or screened generally, an outbreak isdetermined by manual inspection.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Epidemic Curve View shows the number of infected persons per day in order to support Task 1\u2013 outbreak detection. The infection control expert can select the total number of infections or only new ones (i.e., without copystrains). To see how it relates to the endemic level, a moving average of the user-selected time period is shown. This view can show data for the hospital or specific wards. It supports longer time periods via focus-and-context methods inspired by [77].",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      },
      {
        "solution_text": "Epidemic Curve View shows the number of infected persons per day in order to support Task 1\u2013 outbreak detection. The infection control expert can select the total number of infections or only new ones (i.e., without copystrains). To see how it relates to the endemic level, a moving average of the user-selected time period is shown. This view can show data for the hospital or specific wards. It supports longer time periods via focus-and-context methods inspired by [77].",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 222,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.1 Determine transmission contacts. Did contacts occur be-tween patients that could have led to pathogen transmis-sion? If yes, where, when and with whom?",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": { "tables": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Patient lines pass close to each other for every potential contact.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "area+line",
        "axial_code": ["Coordinate"],
        "componenet_code": ["line", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 223,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.1 Determine transmission contacts. Did contacts occur be-tween patients that could have led to pathogen transmis-sion? If yes, where, when and with whom?",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The circle color encodes the type of relevant contact. We show all possible transmission events \u2013 i.e., several flashback lines.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "area+line",
        "axial_code": ["Coordinate"],
        "componenet_code": ["line", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 224,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.1 Determine transmission contacts. Did contacts occur be-tween patients that could have led to pathogen transmis-sion? If yes, where, when and with whom?",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "We propose a modified storyline [46] layout algorithm to support our tasks. While typical approaches to storyline drawing optimize the number of edge crossings and minimize bends, we have the constraint of patient locations, including ward (Task 3), and patient contacts (Task 2). Fast layout is required for interactive exploration, as sets of patients in the view can change when filters are applied, and data sets are loaded. Thus, we prioritize runtime over crossing optimization and minimizing bends (see Sect. 8). We build upon existing layouts and combine them and adapt them for our purposes.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "area+line",
        "axial_code": ["Coordinate"],
        "componenet_code": ["line", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 225,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.1 Determine transmission contacts. Did contacts occur be-tween patients that could have led to pathogen transmis-sion? If yes, where, when and with whom?",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The Transmission Pathway View shows patient infection status and contacts over time and across locations (Fig. 1 (3)). Outbreak duration, potential transmission contacts between patients, and patient locations are visible (Task 2\u20134). Each line represents a patient and the x-axis encodes time (Task 4). Each patient line starts with the earliest recorded admission to hospital and ends with the last recorded stay. Temporary home stays are also shown. This helps in detecting hospital-associated transmissions during previous stays (Task 3.1). Patient lines pass close to each other for every potential contact (Task 2.1). The y-axis encodes patient location. Fixed vertical positions for individual wards (as in Baling et al. [8]) is not scalable due to the larger number of wards and patients (hundreds), but our layout still aims at preserving the vertical position of wards [4]. Line color conveys infection status, and background color is used to encode location information.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "area+line",
        "axial_code": ["Coordinate"],
        "componenet_code": ["line", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 226,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.1 Determine transmission contacts. Did contacts occur be-tween patients that could have led to pathogen transmis-sion? If yes, where, when and with whom?",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Finalization Lines are ordered from top to bottom according to the order patients entered the ward (see Fig. 4e). Even though this may cause more edge crossings, this order helps support Task 2: tracing transmissions.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 227,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.1 Determine transmission contacts. Did contacts occur be-tween patients that could have led to pathogen transmis-sion? If yes, where, when and with whom?",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Infection status is encoded using color (see Fig. 5 left) in order to help in tracing transmissions (Task 2). Differentiating between \u2018unknown\u2019 and \u2018unknown-will be infected\u2019 helps infection control experts track patients over long time periods, reducing the requirement to pan and zoom. Details on microbiological data are shown on demand through a tooltip. The contact location (Task 3), specifically the ward, is shown on demand by a colored background hull.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "area+line",
        "axial_code": ["Coordinate"],
        "componenet_code": ["line", "area"]
      },
      {
        "solution_text": "Details on microbiological data are shown on demand through a tooltip.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 228,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.1 Determine transmission contacts. Did contacts occur be-tween patients that could have led to pathogen transmis-sion? If yes, where, when and with whom?",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Using a process inspired by [31,68], patient lines are drawn smoothly. As short periods of contact can lead to transmission, wiggles in the line indicate new contacts. We emphasize contacts between patients on the same ward by minimizing the line width for vertical lines.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 229,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.1 Determine transmission contacts. Did contacts occur be-tween patients that could have led to pathogen transmis-sion? If yes, where, when and with whom?",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Our interactive approach supports 1) backward tracing \u2013 finding transmission events and patients in the past that could have infected a selected patient. The interaction enables the search for an index patient, i.e., patient zero (Task 2).",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 230,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.2 Determine index patient (patient zero). Who is the source ofthe outbreak? Identifying the transmission pathway shouldlead back to the index patient, i.e., the patient zero.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "We propose a modified storyline [46] layout algorithm to support our tasks. While typical approaches to storyline drawing optimize the number of edge crossings and minimize bends, we have the constraint of patient locations, including ward (Task 3), and patient contacts (Task 2). Fast layout is required for interactive exploration, as sets of patients in the view can change when filters are applied, and data sets are loaded. Thus, we prioritize runtime over crossing optimization and minimizing bends (see Sect. 8). We build upon existing layouts and combine them and adapt them for our purposes.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "area+line",
        "axial_code": ["Coordinate"],
        "componenet_code": ["line", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 231,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.2 Determine index patient (patient zero). Who is the source ofthe outbreak? Identifying the transmission pathway shouldlead back to the index patient, i.e., the patient zero.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The Transmission Pathway View shows patient infection status and contacts over time and across locations (Fig. 1 (3)). Outbreak duration, potential transmission contacts between patients, and patient locations are visible (Task 2\u20134). Each line represents a patient and the x-axis encodes time (Task 4). Each patient line starts with the earliest recorded admission to hospital and ends with the last recorded stay. Temporary home stays are also shown. This helps in detecting hospital-associated transmissions during previous stays (Task 3.1). Patient lines pass close to each other for every potential contact (Task 2.1). The y-axis encodes patient location. Fixed vertical positions for individual wards (as in Baling et al. [8]) is not scalable due to the larger number of wards and patients (hundreds), but our layout still aims at preserving the vertical position of wards [4]. Line color conveys infection status, and background color is used to encode location information.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "area+line",
        "axial_code": ["Coordinate"],
        "componenet_code": ["line", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 232,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.2 Determine index patient (patient zero). Who is the source ofthe outbreak? Identifying the transmission pathway shouldlead back to the index patient, i.e., the patient zero.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Finalization Lines are ordered from top to bottom according to the order patients entered the ward (see Fig. 4e). Even though this may cause more edge crossings, this order helps support Task 2: tracing transmissions.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 233,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.2 Determine index patient (patient zero). Who is the source ofthe outbreak? Identifying the transmission pathway shouldlead back to the index patient, i.e., the patient zero.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Infection status is encoded using color (see Fig. 5 left) in order to help in tracing transmissions (Task 2). Differentiating between \u2018unknown\u2019 and \u2018unknown-will be infected\u2019 helps infection control experts track patients over long time periods, reducing the requirement to pan and zoom. Details on microbiological data are shown on demand through a tooltip. The contact location (Task 3), specifically the ward, is shown on demand by a colored background hull.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "area+line",
        "axial_code": ["Coordinate"],
        "componenet_code": ["line", "area"]
      },
      {
        "solution_text": "Details on microbiological data are shown on demand through a tooltip.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 234,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.2 Determine index patient (patient zero). Who is the source ofthe outbreak? Identifying the transmission pathway shouldlead back to the index patient, i.e., the patient zero.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Using a process inspired by [31,68], patient lines are drawn smoothly. As short periods of contact can lead to transmission, wiggles in the line indicate new contacts. We emphasize contacts between patients on the same ward by minimizing the line width for vertical lines.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 235,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.2 Determine index patient (patient zero). Who is the source ofthe outbreak? Identifying the transmission pathway shouldlead back to the index patient, i.e., the patient zero.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Our interactive approach supports 1) backward tracing \u2013 finding transmission events and patients in the past that could have infected a selected patient. The interaction enables the search for an index patient, i.e., patient zero (Task 2).",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 236,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.3 Distinguish between a single or multiple outbreaks. Is the observed outbreak a single outbreak or multiple, simulta_x0002_neous outbreaks of similar pathogens? Depending on the answer, there could be one or more index patients.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "We propose a modified storyline [46] layout algorithm to support our tasks. While typical approaches to storyline drawing optimize the number of edge crossings and minimize bends, we have the constraint of patient locations, including ward (Task 3), and patient contacts (Task 2). Fast layout is required for interactive exploration, as sets of patients in the view can change when filters are applied, and data sets are loaded. Thus, we prioritize runtime over crossing optimization and minimizing bends (see Sect. 8). We build upon existing layouts and combine them and adapt them for our purposes.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "area+line",
        "axial_code": ["Coordinate"],
        "componenet_code": ["line", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 237,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.3 Distinguish between a single or multiple outbreaks. Is the observed outbreak a single outbreak or multiple, simulta_x0002_neous outbreaks of similar pathogens? Depending on the answer, there could be one or more index patients.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The Transmission Pathway View shows patient infection status and contacts over time and across locations (Fig. 1 (3)). Outbreak duration, potential transmission contacts between patients, and patient locations are visible (Task 2\u20134). Each line represents a patient and the x-axis encodes time (Task 4). Each patient line starts with the earliest recorded admission to hospital and ends with the last recorded stay. Temporary home stays are also shown. This helps in detecting hospital-associated transmissions during previous stays (Task 3.1). Patient lines pass close to each other for every potential contact (Task 2.1). The y-axis encodes patient location. Fixed vertical positions for individual wards (as in Baling et al. [8]) is not scalable due to the larger number of wards and patients (hundreds), but our layout still aims at preserving the vertical position of wards [4]. Line color conveys infection status, and background color is used to encode location information.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "area+line",
        "axial_code": ["Coordinate"],
        "componenet_code": ["line", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 238,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.3 Distinguish between a single or multiple outbreaks. Is the observed outbreak a single outbreak or multiple, simulta_x0002_neous outbreaks of similar pathogens? Depending on the answer, there could be one or more index patients.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Finalization Lines are ordered from top to bottom according to the order patients entered the ward (see Fig. 4e). Even though this may cause more edge crossings, this order helps support Task 2: tracing transmissions.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 239,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.3 Distinguish between a single or multiple outbreaks. Is the observed outbreak a single outbreak or multiple, simulta_x0002_neous outbreaks of similar pathogens? Depending on the answer, there could be one or more index patients.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Infection status is encoded using color (see Fig. 5 left) in order to help in tracing transmissions (Task 2). Differentiating between \u2018unknown\u2019 and \u2018unknown-will be infected\u2019 helps infection control experts track patients over long time periods, reducing the requirement to pan and zoom. Details on microbiological data are shown on demand through a tooltip. The contact location (Task 3), specifically the ward, is shown on demand by a colored background hull.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "area+line",
        "axial_code": ["Coordinate"],
        "componenet_code": ["line", "area"]
      },
      {
        "solution_text": "Details on microbiological data are shown on demand through a tooltip.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 240,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.3 Distinguish between a single or multiple outbreaks. Is the observed outbreak a single outbreak or multiple, simulta_x0002_neous outbreaks of similar pathogens? Depending on the answer, there could be one or more index patients.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Using a process inspired by [31,68], patient lines are drawn smoothly. As short periods of contact can lead to transmission, wiggles in the line indicate new contacts. We emphasize contacts between patients on the same ward by minimizing the line width for vertical lines.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 241,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T2.3 Distinguish between a single or multiple outbreaks. Is the observed outbreak a single outbreak or multiple, simulta_x0002_neous outbreaks of similar pathogens? Depending on the answer, there could be one or more index patients.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Our interactive approach supports 1) backward tracing \u2013 finding transmission events and patients in the past that could have infected a selected patient. The interaction enables the search for an index patient, i.e., patient zero (Task 2).",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 242,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T3.1 Determine if the outbreak is hospital-associated.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": { "tables": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Each patient line starts with the earliest recorded admission to hospital and ends with the last recorded stay. Temporary home stays are also shown. This helps in detecting hospital-associated transmissions during previous stays.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 243,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T3.1 Determine if the outbreak is hospital-associated.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": { "tables": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "When optimizing the layout with the force-directed algorithm, we use a constraint-based approach [19] to enforce these locations and use additional forces to encourage the desired properties of the layout. The first constraint preserves the temporal order of movements along the x-axis. The second constraint restricts movement outside y-axis areas for the three location types.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 244,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T3.1 Determine if the outbreak is hospital-associated.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "We propose a modified storyline [46] layout algorithm to support our tasks. While typical approaches to storyline drawing optimize the number of edge crossings and minimize bends, we have the constraint of patient locations, including ward (Task 3), and patient contacts (Task 2). Fast layout is required for interactive exploration, as sets of patients in the view can change when filters are applied, and data sets are loaded. Thus, we prioritize runtime over crossing optimization and minimizing bends (see Sect. 8). We build upon existing layouts and combine them and adapt them for our purposes.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "area+line",
        "axial_code": ["Coordinate"],
        "componenet_code": ["line", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 245,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T3.1 Determine if the outbreak is hospital-associated.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Infection status is encoded using color (see Fig. 5 left) in order to help in tracing transmissions (Task 2). Differentiating between \u2018unknown\u2019 and \u2018unknown-will be infected\u2019 helps infection control experts track patients over long time periods, reducing the requirement to pan and zoom. Details on microbiological data are shown on demand through a tooltip. The contact location (Task 3), specifically the ward, is shown on demand by a colored background hull.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "area+line",
        "axial_code": ["Coordinate"],
        "componenet_code": ["line", "area"]
      },
      {
        "solution_text": "Details on microbiological data are shown on demand through a tooltip.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 246,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T3.1 Determine if the outbreak is hospital-associated.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "We assume the starting time moment of the contact interval (ts,te) where tt = ts as the transmission event time, Lt is transmission location (Task 3) and Pt is transmission contact. Note, P can have several potential transmis_x0002_sion events \u2013 different persons and locations and different times. This analysis is repeated, especially for contacts with unknown infection status, as they could be potentially infected if they had contact to an infected patient before. These critical contacts are computed using a constrained path search in the DAG used for the layout. There is currently no bound on how far back or forward in time the transmission events are searched, but user-specified bounds could be implemented, depending on the specifics of the investigated pathogen.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "Useconstrainedpathsearchtofindcriticalcontacts.",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 247,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T3.2 Locate ward(s) with pathogen transmissions.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": { "geometry": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Additional forces are used to maximize the stability of the y position of a patient during a stay in a ward to help represent contact location.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 248,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T3.2 Locate ward(s) with pathogen transmissions.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "We propose a modified storyline [46] layout algorithm to support our tasks. While typical approaches to storyline drawing optimize the number of edge crossings and minimize bends, we have the constraint of patient locations, including ward (Task 3), and patient contacts (Task 2). Fast layout is required for interactive exploration, as sets of patients in the view can change when filters are applied, and data sets are loaded. Thus, we prioritize runtime over crossing optimization and minimizing bends (see Sect. 8). We build upon existing layouts and combine them and adapt them for our purposes.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "area+line",
        "axial_code": ["Coordinate"],
        "componenet_code": ["line", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 249,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T3.2 Locate ward(s) with pathogen transmissions.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Infection status is encoded using color (see Fig. 5 left) in order to help in tracing transmissions (Task 2). Differentiating between \u2018unknown\u2019 and \u2018unknown-will be infected\u2019 helps infection control experts track patients over long time periods, reducing the requirement to pan and zoom. Details on microbiological data are shown on demand through a tooltip. The contact location (Task 3), specifically the ward, is shown on demand by a colored background hull.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "area+line",
        "axial_code": ["Coordinate"],
        "componenet_code": ["line", "area"]
      },
      {
        "solution_text": "Details on microbiological data are shown on demand through a tooltip.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 250,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T3.2 Locate ward(s) with pathogen transmissions.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "We assume the starting time moment of the contact interval (ts,te) where tt = ts as the transmission event time, Lt is transmission location (Task 3) and Pt is transmission contact. Note, P can have several potential transmis_x0002_sion events \u2013 different persons and locations and different times. This analysis is repeated, especially for contacts with unknown infection status, as they could be potentially infected if they had contact to an infected patient before. These critical contacts are computed using a constrained path search in the DAG used for the layout. There is currently no bound on how far back or forward in time the transmission events are searched, but user-specified bounds could be implemented, depending on the specifics of the investigated pathogen.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "Useconstrainedpathsearchtofindcriticalcontacts.",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 251,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T4 Quantify outbreak duration.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": { "tables": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Each line represents a patient and the x-axis encodes time (Task 4). Each patient line starts with the earliest recorded admission to hospital and ends with the last recorded stay. Temporary home stays are also shown.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 252,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T4 Quantify outbreak duration.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": { "tables": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Patients that were in the ward for longer periods of time are more likely to be involved in transmission events. Finally, individual nodes are placed at the precise time of their events along the x-axis, which is important for determining outbreak duration and time of possible pathogen transmissions.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 253,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T4 Quantify outbreak duration.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "We show all possible transmission events \u2013 i.e., several flashback lines. Connections indicate the length of time the potential infection was not detected by the screening or testing.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "area+line",
        "axial_code": ["Coordinate"],
        "componenet_code": ["line", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 254,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T5 Identify potentially infected patients.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": { "geometry": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Contact Network View shows the contacts of selected patients for determining putative infected patients.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "network",
        "axial_code": ["Co-axis"],
        "componenet_code": ["network"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 255,
    "paper_title": "In Search of Patient Zero: Visual Analytics of Pathogen Transmission Pathways in Hospitals",
    "pub_year": 2021,
    "domain": "infection control",
    "requirement": {
      "requirement_text": "T5 Identify potentially infected patients.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The visual analytics system was applied to real data from a large German hospital by an infection control expert. The anonymized data includes hospital location data of \u223c180,000 patients and \u223c900,000 microbiological data over four years",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Our interactive approach supports forward tracing \u2013 finding patients that could be infected by a selected infected patient at a later point in time, i.e., putative infected patients (Task 5).",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 256,
    "paper_title": "A Visual Analytics Framework for Contrastive Network Analysis",
    "pub_year": 2020,
    "domain": "Network analysis",
    "requirement": {
      "requirement_text": "DC1: Support the discovery of whether a target network isunique compared to a background network, and which part of thenetwork relates to the uniqueness. The uniqueness of a target datasetrelative to the base is embedded in the contrastive representation YTgenerated by CL-based representation learning methods, includingcNRL. Many previous works attempted to display this data to revealthe uniqueness [2, 3, 27]. However, because YT only contains theinformation of the target network G T , reviewing only YT is notsufficient to understand how well the CL method finds uniqueness.Also, it is difficult to identify which data points (i.e., network nodesin our case) highly relate to the found uniqueness. The visual an-alytics framework should support discovering the uniqueness andthe associated nodes by presenting the information in both the targetand background networks",
      "requirement_code": { "compare_entities": 1, "explain_differences": 1 }
    },
    "data": {
      "data_text": "Real_x0002_world networks; We generated random networks (Random 1, 2) with Gilbert\u2019s random graph [10] and scale-free networks (Price 1, 2) with the Price\u2019s preferential attachment mod_x0002_els [73], as well as used several public datasets.",
      "data_code": { "network_and_trees": 1 }
    },
    "solution": [
      {
        "solution_text": "NRL with DeepGL. Using DeepGL [77] as NRL, i-cNRL gener_x0002_ates feature matrices XT and XB with interpretable network features. The features consist of the base feature x and relational function f. CL with cPCA. From the target and background feature matrices XT and XB, cPCA [2] produces contrastive principal components (cPCs), which are analogous to principal components (PCs) in ordi_x0002_nary PCA [54]. cPCs are low-dimensional representative directions in which XT has high variance but XB has low variance. That is, YT , an embedding of XT with cPCs, depicts unique characteristics (with the consideration of variance) of a target network GT relative to a background network GB.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Modeling"],
        "componenet_code": ["dimensionality_reduction", "modeling"]
      },
      {
        "solution_text": "With the results generated by i-cNRL, the first step of our analysis workflow, ContraNA\u2019s contrastive representation view visualizes the results to reveal whether or not there is uniqueness in the target network compared to the background network, serving as the following step.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "network",
        "axial_code": ["Repetition"],
        "componenet_code": ["network"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 257,
    "paper_title": "A Visual Analytics Framework for Contrastive Network Analysis",
    "pub_year": 2020,
    "domain": "Network analysis",
    "requirement": {
      "requirement_text": "DC2: Enhance the interpretability of the features learned byNRL and the cPCs generated by CL. Investigating the relationshipsamong the network features, cPCs, and the representation YT is im-portant to interpret the uniqueness of G T . While i-cNRL is designedto provide interpretable network features and cPCs, understandingthem from i-cNRL\u2019s direct outputs is not straightforward. For exam-ple, DeepGL could generate a sophisticated relational function suchas (\u03a6+sum \u25e6 \u03a6max \u25e6 \u03a6\u2212mean)(x). Moreover, examining cPC loadingsfor each feature would be time-consuming when DeepGL producesmany network features. The framework should provide visualiza-tions to facilitate easy understanding of the above information.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "Real_x0002_world networks; We generated random networks (Random 1, 2) with Gilbert\u2019s random graph [10] and scale-free networks (Price 1, 2) with the Price\u2019s preferential attachment mod_x0002_els [73], as well as used several public datasets.",
      "data_code": { "network_and_trees": 1 }
    },
    "solution": [
      {
        "solution_text": "NRL with DeepGL. Using DeepGL [77] as NRL, i-cNRL gener_x0002_ates feature matrices XT and XB with interpretable network features. The features consist of the base feature x and relational function f. CL with cPCA. From the target and background feature matrices XT and XB, cPCA [2] produces contrastive principal components (cPCs), which are analogous to principal components (PCs) in ordi_x0002_nary PCA [54]. cPCs are low-dimensional representative directions in which XT has high variance but XB has low variance. That is, YT , an embedding of XT with cPCs, depicts unique characteristics (with the consideration of variance) of a target network GT relative to a background network GB.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Modeling"],
        "componenet_code": ["dimensionality_reduction", "modeling"]
      },
      {
        "solution_text": "With the above observation from the contrastive representation view, we move on to interpret the network features and cPCs with the feature contribution view.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "table+matrix",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 258,
    "paper_title": "A Visual Analytics Framework for Contrastive Network Analysis",
    "pub_year": 2020,
    "domain": "Network analysis",
    "requirement": {
      "requirement_text": "DC3: Offer intuitiveness in understanding a target network\u2019suniqueness by relating it to common network visualizations. Thecontrastive representation YT generated could contain complicatedpatterns that are dif\ufb01cult to understand. Thus, it is not intuitiveenough to just view these patterns directly based on the i-cNRLresults in the embedding space. To help analyze such patterns, theframework should provide links between the results of i-cNRL andcommonly used visualizations for network analysis, such as laid-outnetworks and probability distributions of network centralities.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "Real_x0002_world networks; We generated random networks (Random 1, 2) with Gilbert\u2019s random graph [10] and scale-free networks (Price 1, 2) with the Price\u2019s preferential attachment mod_x0002_els [73], as well as used several public datasets.",
      "data_code": { "network_and_trees": 1 }
    },
    "solution": [
      {
        "solution_text": "NRL with DeepGL. Using DeepGL [77] as NRL, i-cNRL gener_x0002_ates feature matrices XT and XB with interpretable network features. The features consist of the base feature x and relational function f. CL with cPCA. From the target and background feature matrices XT and XB, cPCA [2] produces contrastive principal components (cPCs), which are analogous to principal components (PCs) in ordi_x0002_nary PCA [54]. cPCs are low-dimensional representative directions in which XT has high variance but XB has low variance. That is, YT , an embedding of XT with cPCs, depicts unique characteristics (with the consideration of variance) of a target network GT relative to a background network GB.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Modeling"],
        "componenet_code": ["dimensionality_reduction", "modeling"]
      },
      {
        "solution_text": "With above results, we further analyze the uniqueness by relating F8 to common network visualizations. ContraNA pro- vides two perspectives for network analysis (DC3-Intuitiveness): probability distributions and laid-out networks [10]. Probability distributions are often used to compare the distributions of target and background networks\u2019 centralities (e.g., whether the degree distribu- tion follows the power law [10]), and laid-out networks are helpful for viewing the topological differences (e.g., whether multiple com- munities exist).",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "line+network",
        "axial_code": ["Stack"],
        "componenet_code": ["line", "network"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 259,
    "paper_title": "A Visual Analytics Framework for Contrastive Network Analysis",
    "pub_year": 2020,
    "domain": "Network analysis",
    "requirement": {
      "requirement_text": "DC4: Provide the \ufb02exibility to interactively adjust the i-cNRLparameters to generate results based on the analysts\u2019 interest. Theresults of i-cNRL heavily depend on the parameters used for eachembedding step. For example, changing a value of the contrastparameter \u03b1 might reveal different unique characteristics in G T . Foranalysts with advanced knowledge on NRL and CL, the frameworkshould provide abilities for interactively tuning the i-cNRL resultsbased on their needs.",
      "requirement_code": { "parameter_setting": 1 }
    },
    "data": {
      "data_text": "Real_x0002_world networks; We generated random networks (Random 1, 2) with Gilbert\u2019s random graph [10] and scale-free networks (Price 1, 2) with the Price\u2019s preferential attachment mod_x0002_els [73], as well as used several public datasets.",
      "data_code": { "network_and_trees": 1 }
    },
    "solution": [
      {
        "solution_text": "NRL with DeepGL. Using DeepGL [77] as NRL, i-cNRL gener_x0002_ates feature matrices XT and XB with interpretable network features. The features consist of the base feature x and relational function f. CL with cPCA. From the target and background feature matrices XT and XB, cPCA [2] produces contrastive principal components (cPCs), which are analogous to principal components (PCs) in ordi_x0002_nary PCA [54]. cPCs are low-dimensional representative directions in which XT has high variance but XB has low variance. That is, YT , an embedding of XT with cPCs, depicts unique characteristics (with the consideration of variance) of a target network GT relative to a background network GB.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Modeling"],
        "componenet_code": ["dimensionality_reduction", "modeling"]
      },
      {
        "solution_text": "With the results generated by i-cNRL, the first step of our analysis workflow, ContraNA\u2019s contrastive representation view visualizes the results to reveal whether or not there is uniqueness in the target network compared to the background network, serving as the following step.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "network",
        "axial_code": ["Repetition"],
        "componenet_code": ["network"]
      },
      {
        "solution_text": "The cPCA used in i-cNRL automatically selects the contrastive pa- rameter \u03b1and computes cPCs to generate the optimized contrastive representations, i.e., maximizing the variation in XT while simul- taneously minimizing the variation in XB [36]. However, the analyst may want to loosen or strengthen the reduction of the variation of XB in order to elucidate the found patterns or discover different patterns. Also, the resultant cPCs might not apt to interpret visually found patterns. To handle such cases, ContraNA supports interactive adjustments of \u03b1and cPCs (DC4-Flexibility).",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 260,
    "paper_title": "ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data",
    "pub_year": 2020,
    "domain": "Time-series analysis",
    "requirement": {
      "requirement_text": "DR1: Provide an overview of concept drift occurrences overtime. The drift occurrences over time can help analysts identifythe interesting time segment. When the analysts\u2019 preferences areunknown, interactive and hierarchical exploration of occurrencesalong time intervals is needed [34, 46].",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We employ the air pollutant data [55] from four nationally-controlled air-quality monitoring sites in Beijing, which was collected every hour from March 1st, 2013 to February 28th, 2017 (34,536 data records per site). 22 meteorology-related dimen_x0002_sions are applied to predict if the air quality index (AQI) is higher than 100 (i.e., worse than mild pollution) after 24 hours. The employed dataset contains player records from three servers (647,800 player records from Server17, 702,125 player records from Server164, and 585,048 player records from Server230) of a MMORPG from August 16th, 2013 to January 19th, 2014. Three servers were started at different timestamps: Server17, Sever164, and Server230, which are in order of time, that is, players on different servers register for the game at different time periods. For each player, 21 attributes, like equipment (i.e., the combat effectiveness score of the player\u2019s equipment), practice (i.e., the level of practice, improved by learning and improving skills and finishing tasks), are recorded every day. The consumption records for the upcoming week of players form a group of time-series.",
      "data_code": {
        "categorical": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The timeline navigator view presents the en- tire timeline and the indices of concept drifts from multiple data sources. Each row corresponds to a data source. ConceptExplorer assigns a unique color to each data source. Due to the limited horizontal space, the distribution of concept drifts may be dense. ConceptExplorer employs a \u201c\u00d7\u201d to mark a concept drift, which can highlight the specific moment by its intersection. Time segments, in which the drift level exceeds a certain value (initialized as the warning level, namely, 2) are highlighted by \u201c\u2212\u201d. These marks indicate various patterns along the timeline, like dense occurrences, outliers, inconsistency with other data sources, periodicity.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 261,
    "paper_title": "ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data",
    "pub_year": 2020,
    "domain": "Time-series analysis",
    "requirement": {
      "requirement_text": "DR2: Integrate features of concept drifts from the predictionmodels. Concept drifts hinder existing prediction models (the modeltrained from historical data) from accurate predictions in new envi-ronments. The accuracy fluctuation of the prediction model indicatesthe occurrence of concept drifts [7, 30, 42, 52, 54]. In addition, pa-rameters of prediction models can reflect the relationship betweendifferent inputs and the label, that is, the model\u2019s understanding ofthe concept [9]",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "We employ the air pollutant data [55] from four nationally-controlled air-quality monitoring sites in Beijing, which was collected every hour from March 1st, 2013 to February 28th, 2017 (34,536 data records per site). 22 meteorology-related dimen_x0002_sions are applied to predict if the air quality index (AQI) is higher than 100 (i.e., worse than mild pollution) after 24 hours. The employed dataset contains player records from three servers (647,800 player records from Server17, 702,125 player records from Server164, and 585,048 player records from Server230) of a MMORPG from August 16th, 2013 to January 19th, 2014. Three servers were started at different timestamps: Server17, Sever164, and Server230, which are in order of time, that is, players on different servers register for the game at different time periods. For each player, 21 attributes, like equipment (i.e., the combat effectiveness score of the player\u2019s equipment), practice (i.e., the level of practice, improved by learning and improving skills and finishing tasks), are recorded every day. The consumption records for the upcoming week of players form a group of time-series.",
      "data_code": {
        "categorical": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "5.2 The Prediction Model ViewThe prediction model view (Figure 1(c)) supports DR2.5.2.1 The Accuracy Fluctuation ChartThe line charts on the left (Figure 1(c)) show the accuracy fluctuationof the prediction models trained by the data from each data source.The occurrences of concept drifts are labeled by \u201c\u00d7\u201d, which isthe same as that in the timeline navigator view. In addition, themoments with warnings are encoded by hollow dots. To explainconcept drift detection, the accuracy fluctuation chart visualizes themagnitude of the accuracy drop of the time segments whose driftlevels are above the warning level (Figure 4(a)). Different datasources may issue drift warnings at similar time segments. To avoidmisunderstandings caused by overlaps, shifted stripes are employedto highlight warning time segments (Figure 4(b)). It can be seenthat even when the warning segments of different data sources arestaggered, the start and end moments of different time segmentscan be clearly distinguished. Vertical stripes are used because theycan emphasize the height, that is, the magnitude of accuracy drops.The results from the consistency judgment model are also shownin the accuracy fluctuation chart. If a concept drift is detectedduring a time segment that is not included by the result from theconsistency judgment model, we emphasize them by a triangle markto distinguish from circles representing others (Figure 4(c))",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "5.2 The Prediction Model ViewThe prediction model view (Figure 1(c)) supports DR2.5.2.1 The Accuracy Fluctuation ChartThe line charts on the left (Figure 1(c)) show the accuracy fluctuationof the prediction models trained by the data from each data source.The occurrences of concept drifts are labeled by \u201c\u00d7\u201d, which isthe same as that in the timeline navigator view. In addition, themoments with warnings are encoded by hollow dots. To explainconcept drift detection, the accuracy fluctuation chart visualizes themagnitude of the accuracy drop of the time segments whose driftlevels are above the warning level (Figure 4(a)). Different datasources may issue drift warnings at similar time segments. To avoidmisunderstandings caused by overlaps, shifted stripes are employedto highlight warning time segments (Figure 4(b)). It can be seenthat even when the warning segments of different data sources arestaggered, the start and end moments of different time segmentscan be clearly distinguished. Vertical stripes are used because theycan emphasize the height, that is, the magnitude of accuracy drops.The results from the consistency judgment model are also shownin the accuracy fluctuation chart. If a concept drift is detectedduring a time segment that is not included by the result from theconsistency judgment model, we emphasize them by a triangle markto distinguish from circles representing others (Figure 4(c))",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 262,
    "paper_title": "ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data",
    "pub_year": 2020,
    "domain": "Time-series analysis",
    "requirement": {
      "requirement_text": "DR2: Integrate features of concept drifts from the predictionmodels. Concept drifts hinder existing prediction models (the modeltrained from historical data) from accurate predictions in new envi-ronments. The accuracy fluctuation of the prediction model indicatesthe occurrence of concept drifts [7, 30, 42, 52, 54]. In addition, pa-rameters of prediction models can reflect the relationship betweendifferent inputs and the label, that is, the model\u2019s understanding ofthe concept [9]",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "We employ the air pollutant data [55] from four nationally-controlled air-quality monitoring sites in Beijing, which was collected every hour from March 1st, 2013 to February 28th, 2017 (34,536 data records per site). 22 meteorology-related dimen_x0002_sions are applied to predict if the air quality index (AQI) is higher than 100 (i.e., worse than mild pollution) after 24 hours. The employed dataset contains player records from three servers (647,800 player records from Server17, 702,125 player records from Server164, and 585,048 player records from Server230) of a MMORPG from August 16th, 2013 to January 19th, 2014. Three servers were started at different timestamps: Server17, Sever164, and Server230, which are in order of time, that is, players on different servers register for the game at different time periods. For each player, 21 attributes, like equipment (i.e., the combat effectiveness score of the player\u2019s equipment), practice (i.e., the level of practice, improved by learning and improving skills and finishing tasks), are recorded every day. The consumption records for the upcoming week of players form a group of time-series.",
      "data_code": {
        "categorical": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "5.2.2 The Projected Parameter ViewThe model parameters updated after each batch during the entiretraining process are projected into a two-dimensional plane usingprincipal components analysis (PCA) based on singular value decom-position (SVD) [47]. The points projected by parameters of the samedata source are connected in order to form a curve. The distancebetween each pair of projected points illustrates the similarity ofcorresponding model parameters, namely, the concept similarity de-scribed by prediction models. Evolution patterns, like intersectionsand bundles [5, 19] can be identi\ufb01ed from the formed curves [10].The convergence and dispersal of curve segments representing differ-ent data sources indicate the agreement and disagreement of relatedmodels on the understanding of the concept. The curve segmentscorresponding to time segments selected in the timeline navigatorview is colored by transparency, from which analysts can learn aboutthe temporal order. The view is automatically zoomed in or out so asto \ufb01t the curves of the entire training process or the selected time seg-ment within the window, as shown in Figure 5. With the backgroundof the entire trajectories (Figure 5(a)), analysts can better measurerelative distances. After zooming in, it can be seen (Figure 5(b))that curves are not overlapped but with similar directions, that is,data sources have similar drifts. Analysts can drag the handle onthe time axis of the accuracy \ufb02uctuation chart to move the circles,which highlight the projected parameters corresponding to the samemoment.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "5.2.2 The Projected Parameter ViewThe model parameters updated after each batch during the entiretraining process are projected into a two-dimensional plane usingprincipal components analysis (PCA) based on singular value decom-position (SVD) [47]. The points projected by parameters of the samedata source are connected in order to form a curve. The distancebetween each pair of projected points illustrates the similarity ofcorresponding model parameters, namely, the concept similarity de-scribed by prediction models. Evolution patterns, like intersectionsand bundles [5, 19] can be identi\ufb01ed from the formed curves [10].The convergence and dispersal of curve segments representing differ-ent data sources indicate the agreement and disagreement of relatedmodels on the understanding of the concept. The curve segmentscorresponding to time segments selected in the timeline navigatorview is colored by transparency, from which analysts can learn aboutthe temporal order. The view is automatically zoomed in or out so asto \ufb01t the curves of the entire training process or the selected time seg-ment within the window, as shown in Figure 5. With the backgroundof the entire trajectories (Figure 5(a)), analysts can better measurerelative distances. After zooming in, it can be seen (Figure 5(b))that curves are not overlapped but with similar directions, that is,data sources have similar drifts. Analysts can drag the handle onthe time axis of the accuracy \ufb02uctuation chart to move the circles,which highlight the projected parameters corresponding to the samemoment.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 263,
    "paper_title": "ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data",
    "pub_year": 2020,
    "domain": "Time-series analysis",
    "requirement": {
      "requirement_text": "DR3: Identify the context of concepts and allow adjustments.Analysts need to know the context of the analyzed data records.Considering that analysts may miss details or may disagree withthe navigation, interactive adjustments for recommended results areneeded [23, 27]",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "We employ the air pollutant data [55] from four nationally-controlled air-quality monitoring sites in Beijing, which was collected every hour from March 1st, 2013 to February 28th, 2017 (34,536 data records per site). 22 meteorology-related dimen_x0002_sions are applied to predict if the air quality index (AQI) is higher than 100 (i.e., worse than mild pollution) after 24 hours. The employed dataset contains player records from three servers (647,800 player records from Server17, 702,125 player records from Server164, and 585,048 player records from Server230) of a MMORPG from August 16th, 2013 to January 19th, 2014. Three servers were started at different timestamps: Server17, Sever164, and Server230, which are in order of time, that is, players on different servers register for the game at different time periods. For each player, 21 attributes, like equipment (i.e., the combat effectiveness score of the player\u2019s equipment), practice (i.e., the level of practice, improved by learning and improving skills and finishing tasks), are recorded every day. The consumption records for the upcoming week of players form a group of time-series.",
      "data_code": {
        "categorical": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The concept-time view displays the time segments in different data sources that are integrated for concept analysis. To display the sources of the applied data records, as mentioned in requirement, each data source is listed in a row to distinguish different data sources. Then, their data records are divided individually to introduce a specific time. Analysts need to make the trade-off between the number of data records and the clarity of concepts for appropriate adjustments. To facilitate decision-making, the drift level and the size of each batch are encoded with the color and height of the bar, respectively. The batches that compose the data records to be analyzed are highlighted. The number of these batches and the total number of the related data records are counted.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 264,
    "paper_title": "ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data",
    "pub_year": 2020,
    "domain": "Time-series analysis",
    "requirement": {
      "requirement_text": "DR4: Study the relationship between attributes and labels.While concepts have not an explicit de\ufb01nition, it is essential to pro-vide a visual explanation. The relationship between labels andattributes are considered to be an important description of con-cepts [14, 18]",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "We employ the air pollutant data [55] from four nationally-controlled air-quality monitoring sites in Beijing, which was collected every hour from March 1st, 2013 to February 28th, 2017 (34,536 data records per site). 22 meteorology-related dimen_x0002_sions are applied to predict if the air quality index (AQI) is higher than 100 (i.e., worse than mild pollution) after 24 hours. The employed dataset contains player records from three servers (647,800 player records from Server17, 702,125 player records from Server164, and 585,048 player records from Server230) of a MMORPG from August 16th, 2013 to January 19th, 2014. Three servers were started at different timestamps: Server17, Sever164, and Server230, which are in order of time, that is, players on different servers register for the game at different time periods. For each player, 21 attributes, like equipment (i.e., the combat effectiveness score of the player\u2019s equipment), practice (i.e., the level of practice, improved by learning and improving skills and finishing tasks), are recorded every day. The consumption records for the upcoming week of players form a group of time-series.",
      "data_code": {
        "categorical": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The data source isconsidered as an attribute to label the context of the data records.For other attributes, the correlation for each batch of data recordsis quantified by the cosine similarity. The attributes are sorted bythe average correlation of the selected batches.",
        "solution_category": "data_manipulation",
        "solution_axial": "SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["SimilarityCalculation"],
        "componenet_code": ["similarity_calculation"]
      },
      {
        "solution_text": "A correlation matrix (Figure 1(e)) is employed to support DR4because of its representation ability [44, 48]. The data source isconsidered as an attribute to label the context of the data records.For other attributes, the correlation for each batch of data recordsis quantified by the cosine similarity. The attributes are sorted bythe average correlation of the selected batches. ConceptExplorer draws correlation matrices for the data source and analyst-specifiednumber of the attributes with the highest correlations subject to theconcept.For each cell, the horizontal and vertical axes of each matrix aredefined by two attributes. A square in non-diagonal cells representsa set of data records whose two attributes fall into the value rangeswhich are encoded with the position of the square. The differencesbetween the number of records with positive labels and those withnegative labels are counted for each square. The ratio of the dif-ference of two counts over their sum (i.e., #Positives\u2212#Negatives#Positives+#Negatives ) isencoded in color (ranging from red to blue). When the label dis-tribution in the dataset is nonuniform, analysts can reset the colormapping and encode the percentage difference in all data recordsin white. In some specific contexts, certain cells may be empty.To distinguish squares without a record, strokes are added in thesquares with more than one record. Darker strokes imply that therecord number of the square is larger than 5% of the amount ofchosen data records. The matrix view exhibits a symmetrical layout.Taking advantage of this feature, the current correlation pattern canbe compared with the other one. Each cell on the diagonal presentsa pair of histograms (i.e., a grounded histogram for lower-left cornerand an inverted histogram for upper-right corner).",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "matrix+bar",
        "axial_code": ["Nesting"],
        "componenet_code": ["bar", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 265,
    "paper_title": "ConceptExplorer: Visual Analysis of Concept Drifts in Multi-source Time-series Data",
    "pub_year": 2020,
    "domain": "Time-series analysis",
    "requirement": {
      "requirement_text": "DR5: Compare concepts in different contexts. Comparing dif-ferent concepts facilitates the understanding of the evolving conceptsand their contexts, e.g., the trends and outliers. There is a need tocompare a newly identi\ufb01ed concept with previously studied onesand record the identi\ufb01ed concepts [49]. Comparing concepts relatedto a concept drift also favors the understanding of the drift",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "We employ the air pollutant data [55] from four nationally-controlled air-quality monitoring sites in Beijing, which was collected every hour from March 1st, 2013 to February 28th, 2017 (34,536 data records per site). 22 meteorology-related dimen_x0002_sions are applied to predict if the air quality index (AQI) is higher than 100 (i.e., worse than mild pollution) after 24 hours. The employed dataset contains player records from three servers (647,800 player records from Server17, 702,125 player records from Server164, and 585,048 player records from Server230) of a MMORPG from August 16th, 2013 to January 19th, 2014. Three servers were started at different timestamps: Server17, Sever164, and Server230, which are in order of time, that is, players on different servers register for the game at different time periods. For each player, 21 attributes, like equipment (i.e., the combat effectiveness score of the player\u2019s equipment), practice (i.e., the level of practice, improved by learning and improving skills and finishing tasks), are recorded every day. The consumption records for the upcoming week of players form a group of time-series.",
      "data_code": {
        "categorical": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Analysts can identify and record significant concepts that may be involved in subsequent analysis. The identified concepts can be compared with other concepts.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "matrix",
        "axial_code": ["Repetition"],
        "componenet_code": ["matrix"]
      },
      {
        "solution_text": "Analysts can identify and record significant concepts that may be involved in subsequent analysis. The identified concepts can be compared with other concepts.",
        "solution_category": "interaction",
        "solution_axial": "Selecting,Connect/Relate",
        "solution_compoent": "",
        "axial_code": ["Connect/Relate", "Selecting"],
        "componenet_code": ["connect/relate", "selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 266,
    "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports",
    "pub_year": 2020,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T1 Adjust the weights of different attributes. Domain expertsusually focus on only one attribute or a weighted combinationof multiple attributes for analysis. For example, a domainexpert may only want to know about the position of a player,or may be more interested in knowing the relative positionbetween the player and the ball. Analysts should be allowedto adjust the weight of different attributes to focus on theattributes of interest",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "MDLalgorithmformultivariatepatterns.Weight,merge,lengthcontrol.ImprovedalgorithmMinDL.",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 267,
    "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports",
    "pub_year": 2020,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T1 Adjust the weights of different attributes. Domain expertsusually focus on only one attribute or a weighted combinationof multiple attributes for analysis. For example, a domainexpert may only want to know about the position of a player,or may be more interested in knowing the relative positionbetween the player and the ball. Analysts should be allowedto adjust the weight of different attributes to focus on theattributes of interest",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "First, the weight of each attribute. Analysts can input the weights of different attributesto adjust their focus of analysis (T1). Our measure considers eachattribute with weight to enable the multivariate pattern mining. Theweight of the i \u2212 th attribute a i is denoted as w i.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      },
      {
        "solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "MDLalgorithmformultivariatepatterns.Weight,merge,lengthcontrol.ImprovedalgorithmMinDL.",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 268,
    "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports",
    "pub_year": 2020,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T1 Adjust the weights of different attributes. Domain expertsusually focus on only one attribute or a weighted combinationof multiple attributes for analysis. For example, a domainexpert may only want to know about the position of a player,or may be more interested in knowing the relative positionbetween the player and the ball. Analysts should be allowedto adjust the weight of different attributes to focus on theattributes of interest",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "MDLalgorithmformultivariatepatterns.Weight,merge,lengthcontrol.ImprovedalgorithmMinDL.",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "We design Attribute Editor to adjust the weights of different attributes and group similar values. We use a list to show all attributes because the list is clear and concise for domain experts. For each attribute, we display the three-level data structure of \u201cattribute \u2192 groups of similar values \u2192 values\u201d. At the top, the name of the attribute is shown on the left. The checkbox before the name controls whether the attribute is used in pattern mining and visualized in the system. On the right, a slider is used to control the weight of the attribute. To recommend a suitable weight as default, we calculate the diversity of the attribute based on information entropy [36]. As the information entropy of an attribute increases, its amount of information also increases and thus should be assigned a higher weight. The diversity is encoded with the vertical line on the slider. Under the top line, all the groups are shown and can be renamed.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+bara",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "bara"]
      },
      {
        "solution_text": "We design Attribute Editor to adjust the weights of different attributes and group similar values. We use a list to show all attributes because the list is clear and concise for domain experts. For each attribute, we display the three-level data structure of \u201cattribute \u2192 groups of similar values \u2192 values\u201d. At the top, the name of the attribute is shown on the left. The checkbox before the name controls whether the attribute is used in pattern mining and visualized in the system. On the right, a slider is used to control the weight of the attribute. To recommend a suitable weight as default, we calculate the diversity of the attribute based on information entropy [36]. As the information entropy of an attribute increases, its amount of information also increases and thus should be assigned a higher weight. The diversity is encoded with the vertical line on the slider. Under the top line, all the groups are shown and can be renamed.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 269,
    "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports",
    "pub_year": 2020,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T2 Display multiple attributes simultaneously. Domain expertsdesire to obtain insights comprehensively by seeing multi-ple attributes of an event simultaneously. Studies on visualanalytics of sports data prove that an intuitive glyph is pre-ferred [13, 30, 43, 49] because it can use visual metaphors thatcan correspond to the real scene to show multiple attributessimultaneously. Moreover, the system should provide designtemplates of glyphs suitable for many sports",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "MDLalgorithmformultivariatepatterns.Weight,merge,lengthcontrol.ImprovedalgorithmMinDL.",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "We use glyphs to show multiple attributes of an event simultaneously because of their effectiveness [30] and the common usage for encoding multivariate sports data [13, 43, 49]. Our glyph designs use six encoding methods, namely, color, matrix, bar, shape, donut, and circular bar, to cover all the attributes in our datasets, following the design principles proposed by Chung et al.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 270,
    "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports",
    "pub_year": 2020,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T3 Merge the patterns. Domain experts prefer to discern differ-ent playing styles when analyzing a player. A playing stylecan be depicted by multiple similar patterns. For example, thesuccessive use of technique Drive and that of technique Volleyare similar, both of which reflect the offensive playing styles intennis. Merging similar patterns based on domain knowledgehelps domain experts evaluate a player\u2019s playing styles quickly.Therefore, the system should support the domain experts tomerge similar patterns based on their knowledge",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "MDLalgorithmformultivariatepatterns.Weight,merge,lengthcontrol.ImprovedalgorithmMinDL.",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 271,
    "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports",
    "pub_year": 2020,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T3 Merge the patterns. Domain experts prefer to discern differ-ent playing styles when analyzing a player. A playing stylecan be depicted by multiple similar patterns. For example, thesuccessive use of technique Drive and that of technique Volleyare similar, both of which reflect the offensive playing styles intennis. Merging similar patterns based on domain knowledgehelps domain experts evaluate a player\u2019s playing styles quickly.Therefore, the system should support the domain experts tomerge similar patterns based on their knowledge",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Second, the groups of similar values. Experts can group similarvalues to merge similar patterns (T3). In our algorithm, within acertain attribute ai, each value is mapped to one and only one group,where one group is a set of similar values. More speci\ufb01cally, eachgroup Gcategorical in a categorical attribute is a set of discrete valuesthat can be considered similar (e.g., the Offensive group includesall techniques to attack). The numerical attributes are quantized todiscrete ranges, such as Gnumerical = {v | min \u2264 v < max}, wheremin and max are the boundary value of the range.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Groupsimilarvaluestomergesimilarpatterns.",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 272,
    "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports",
    "pub_year": 2020,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T3 Merge the patterns. Domain experts prefer to discern differ-ent playing styles when analyzing a player. A playing stylecan be depicted by multiple similar patterns. For example, thesuccessive use of technique Drive and that of technique Volleyare similar, both of which reflect the offensive playing styles intennis. Merging similar patterns based on domain knowledgehelps domain experts evaluate a player\u2019s playing styles quickly.Therefore, the system should support the domain experts tomerge similar patterns based on their knowledge",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "MDLalgorithmformultivariatepatterns.Weight,merge,lengthcontrol.ImprovedalgorithmMinDL.",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "We design Attribute Editor to adjust the weights of different attributes and group similar values. We use a list to show all attributes because the list is clear and concise for domain experts. For each attribute, we display the three-level data structure of \u201cattribute \u2192 groups of similar values \u2192 values\u201d. At the top, the name of the attribute is shown on the left. The checkbox before the name controls whether the attribute is used in pattern mining and visualized in the system. On the right, a slider is used to control the weight of the attribute. To recommend a suitable weight as default, we calculate the diversity of the attribute based on information entropy [36]. As the information entropy of an attribute increases, its amount of information also increases and thus should be assigned a higher weight. The diversity is encoded with the vertical line on the slider. Under the top line, all the groups are shown and can be renamed.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+bara",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "bara"]
      },
      {
        "solution_text": "We design Attribute Editor to adjust the weights of different attributes and group similar values. We use a list to show all attributes because the list is clear and concise for domain experts. For each attribute, we display the three-level data structure of \u201cattribute \u2192 groups of similar values \u2192 values\u201d. At the top, the name of the attribute is shown on the left. The checkbox before the name controls whether the attribute is used in pattern mining and visualized in the system. On the right, a slider is used to control the weight of the attribute. To recommend a suitable weight as default, we calculate the diversity of the attribute based on information entropy [36]. As the information entropy of an attribute increases, its amount of information also increases and thus should be assigned a higher weight. The diversity is encoded with the vertical line on the slider. Under the top line, all the groups are shown and can be renamed.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 273,
    "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports",
    "pub_year": 2020,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T3 Merge the patterns. Domain experts prefer to discern differ-ent playing styles when analyzing a player. A playing stylecan be depicted by multiple similar patterns. For example, thesuccessive use of technique Drive and that of technique Volleyare similar, both of which reflect the offensive playing styles intennis. Merging similar patterns based on domain knowledgehelps domain experts evaluate a player\u2019s playing styles quickly.Therefore, the system should support the domain experts tomerge similar patterns based on their knowledge",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "MDLalgorithmformultivariatepatterns.Weight,merge,lengthcontrol.ImprovedalgorithmMinDL.",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Users can expand the bottom to view detailed information of values in an attribute and group similar values. The distribution of each attribute is visualized as either a bar chart or an area, depending on whether it is a numerical or categorical attribute.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "bar+area",
        "axial_code": ["Stack"],
        "componenet_code": ["area", "bar"]
      },
      {
        "solution_text": "Users can expand the bottom to view detailed information of values in an attribute and group similar values. The distribution of each attribute is visualized as either a bar chart or an area, depending on whether it is a numerical or categorical attribute.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore,Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration", "OverviewandExplore"],
        "componenet_code": [
          "participation/collaboration",
          "overview_and_explore"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 274,
    "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports",
    "pub_year": 2020,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T4 Provide a comparison with multiple levels of detail.Domain experts need to quickly compare a player with othersto \ufb01nd his/her individual tactical patterns. They also need tocompare the performance of a player in different situations toobtain insights about his/her playing styles comprehensively.Thus, the proposed system should allow users to divide thedataset into two subsets with different conditions, such aswho serves and whether it is a crucial time. Then, interactivevisualization is used to visually and interactively comparethe two subsets and \ufb01nd the differences. However, comparingmultivariate event sequences can lead to visual clutter becauseof the massive information. Therefore, the proposed systemshould support a comparison with multiple levels of detail",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "MDLalgorithmformultivariatepatterns.Weight,merge,lengthcontrol.ImprovedalgorithmMinDL.",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "We design Pattern Comparator to help experts compare patterns one-to-one, where juxtaposition is a commonly used method [21,34]. In our system, each subset occupies one column (Fig. 1(C9)), and each pattern occupies one row (Fig. 1(C4)), where patterns are arranged in descending order according to their frequency in the subset. For each pattern, a series of glyphs represent the events in the pattern and are placed from left to right in the order in which events occur. The color of the glyph encodes the player who hit the ball, where the color of the subset represents the server of the subset, and the dark color represents the opponents. Bar charts in the middle support comparison on frequency (Fig. 1(C7)). The height of a bar encodes the frequency of the corresponding pattern. The scale can be adjusted by the controller (Fig. 1(C3)) above the bar charts, where the two numbers on the side show the total frequency of the corresponding subsets. The luminance of the bar encodes the winning rate naturally because the winning rate is a numerical value ranging from 0 to 1 [37]. When users hover on a bar, the frequency and the winning rate are shown in a tooltip. To compare the frequency of a specific pattern (especially when its glyphs in two subsets are not at the same row, e.g., C4 and C6 in Fig. 1) and further compare the distribution of pattern frequency, we design a line chart with two lines (Fig. 1(C5)). The solid line with the color of the subset connects the tops of all the bars on the hovered side. The dashed line with the color of the other subset shows the frequency of each pattern in the other subset. To avoid visual clutter, users need to hover on the area of bar charts to view the line chart.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph+bar+line",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph", "line", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 275,
    "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports",
    "pub_year": 2020,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T4 Provide a comparison with multiple levels of detail.Domain experts need to quickly compare a player with othersto \ufb01nd his/her individual tactical patterns. They also need tocompare the performance of a player in different situations toobtain insights about his/her playing styles comprehensively.Thus, the proposed system should allow users to divide thedataset into two subsets with different conditions, such aswho serves and whether it is a crucial time. Then, interactivevisualization is used to visually and interactively comparethe two subsets and \ufb01nd the differences. However, comparingmultivariate event sequences can lead to visual clutter becauseof the massive information. Therefore, the proposed systemshould support a comparison with multiple levels of detail",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "This section introduces our MDL-based algorithm for multivariatesequential pattern mining, which has two main technical contribu-tions. First, the task of mining patterns from multivariate eventsequences is one of the most critical tasks in racquet sports dataanalysis. Our algorithm supports such a task effectively by solvingthree domain-speci\ufb01c issues, namely, 1) adjusting the weights ofdifferent attributes (T1), 2) merging similar patterns (T3), and 3)controlling the length/authenticity/continuity of the extracted pat-terns to meet domain\u2019s needs. To the best of our knowledge, ouralgorithm is novel and no other existing algorithms can solve all thethree issues. Second, our algorithm represents a new general way forpattern mining of multivariate event sequences. It can be generalizedto many applications for analysis and exploration of multivariateevent sequences, rather than limited to racquet sports data analysis.We improved an algorithm called MinDL [12], which employsthe MDL principle to extract patterns in event sequences. MinDLuses a two-part representation to describe each sequence, namely,the extracted pattern and the corrections needed to reconstruct theoriginal sequence from the pattern (e.g., insertion and deletion ofevents). MinDL initially regards each sequence as a pattern withoutany corrections and continuously merges two patterns to obtain anew pattern with additional corrections. MinDL makes a trade-offbetween the description cost of the two parts to obtain appropriatepatterns. However, MinDL cannot be directly applied to racquetsports data analysis as it cannot meet the above three issues. Thus,we introduce a new measure (Algorithm 1) to calculate the descrip-tion cost, such that the domain requirements can be satis\ufb01ed; Projection technology can effi_x0002_ciently abstract data.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "AlgorithmicCalculation"],
        "componenet_code": [
          "dimensionality_reduction",
          "algorithmic_calculation"
        ]
      },
      {
        "solution_text": "Scatterplot help users overview the patterns of two subsets and quickly focus on interesting ones. Projection technology can effi_x0002_ciently abstract data. Among the commonly used dimensionality reduction algorithm for feature projection, such as PCA, LDA, t_x0002_SNE, etc., we choose the metric MDS algorithm to project patterns for two considerations [3]. 1) Compared to PCA, LDA, and non_x0002_metric MDS, metric MDS can use the distance matrix to project multidimensional data instead of the Euclidean distance. We can hardly define the Euclidean distance between two patterns because patterns have different lengths, and some categorical attributes can hardly define Euclidean distance between values. By contrast, we can use the editing cost between each pair of two patterns to con_x0002_struct the distance matrix. 2) Although t-SNE can also accept a distance matrix, metric MDS is much efficient because t-SNE re_x0002_quires multiple iterations, but metric MDS does not. We project patterns into a plane coordinate system, where each point represents a pattern, and the distance between them shows the similarity between the patterns. For each point, a pie chart encodes statistical data of the pattern, which helps users quickly compare the performance of a pattern in two subsets. The size of the pie encodes the frequency that a pattern appears in two subsets. The entire pie is divided into two sections, each of which represents a subset with the corresponding color. The radian of a section encodes the frequency of the pattern in the corresponding subset. The length of the arc out_x0002_side the pie chart with color encodes the winning percentage of the pattern in the subset, where a semicircle represents all the winnings.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "scatter+pie",
        "axial_code": ["Nesting"],
        "componenet_code": ["pie", "scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 276,
    "paper_title": "Visual Analytics of Multivariate Event Sequence Data in Racquet Sports",
    "pub_year": 2020,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T5 Show detailed sequences within a pattern. Domain expertsmay examine the raw data for detailed analysis and veri\ufb01cation.Thus, a detailed view should be provided to show the raw data",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Our tennis/badminton datasets are provided by the two teams of domain experts who work for the national teams. The data of each match is manually recorded by undergraduates in sports science. As far as we know, the data quality is high, and we have not encountered any data quality/uncertainty issue. The two original datasets are in different formats. Thus, we designed a unified data format to normal_x0002_ize the two datasets. In the unified data format, each match includes a set of sequences. Each sequence S represents a rally and comprises an ordered list of events S = (e1, e2, e3,...,en) and information about who serves, who wins, whether it is in a tiebreaker, and so on. Each event represents a hit and is described by multiple attributes, denoted as ei = {a1 = vi 1,a2 = vi 2,...,an = vi n}.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "All the sequences with a certain pattern are shown in the Instance View for detailed information, when either the point or the row of glyphs is clicked. In the view, each row displays one sequence. For each row, the rectangle on the left encodes the outcome of the rally, where a solid one represents that the server wins, and a hollow one represents that the opponent wins. A circle in the row represents an event in the sequence. We use a solid circle to encode the event in the pattern and a hollow circle for the event not in the pattern. For a solid circle, the number on it represents the index of the event in the pattern.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      },
      {
        "solution_text": "All the sequences with a certain pattern are shown in the Instance View for detailed information, when either the point or the row of glyphs is clicked. In the view, each row displays one sequence. For each row, the rectangle on the left encodes the outcome of the rally, where a solid one represents that the server wins, and a hollow one represents that the opponent wins. A circle in the row represents an event in the sequence. We use a solid circle to encode the event in the pattern and a hollow circle for the event not in the pattern. For a solid circle, the number on it represents the index of the event in the pattern.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+glyph",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "glyph"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 278,
    "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data",
    "pub_year": 2022,
    "domain": "career mobility",
    "requirement": {
      "requirement_text": "T1 What are the general characteristics of career mobility?Experts require a quick overview of the data to iden-tify the regions of interest for further analysis. Peoplewith vertical movements (i.e., who changed job lev-els, such as promotion and demotion) or attainedhigh job levels are more likely to attract the experts\u2019attention.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.",
      "data_code": { "tables": 1, "temporal": 1, "sequential": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "The Parameter View is a filtering dashboard for users to choose a population of interest. From top to bottom, users can select a particular career length, time period, and different official backgrounds.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 279,
    "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data",
    "pub_year": 2022,
    "domain": "career mobility",
    "requirement": {
      "requirement_text": "T1 What are the general characteristics of career mobility?Experts require a quick overview of the data to iden-tify the regions of interest for further analysis. Peoplewith vertical movements (i.e., who changed job lev-els, such as promotion and demotion) or attainedhigh job levels are more likely to attract the experts\u2019attention.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.",
      "data_code": { "tables": 1, "temporal": 1, "sequential": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. ",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "In the Distribution View, three bar charts show the ver- tical movement statistics of the chosen population. They include the population distribution of origin and destination job levels, and job level distance of the whole career. Users can quickly locate the data of interest for further analysis.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      },
      {
        "solution_text": "In the Distribution View, three bar charts show the ver- tical movement statistics of the chosen population. They include the population distribution of origin and destination job levels, and job level distance of the whole career. Users can quickly locate the data of interest for further analysis.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 280,
    "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data",
    "pub_year": 2022,
    "domain": "career mobility",
    "requirement": {
      "requirement_text": "T1 What are the general characteristics of career mobility?Experts require a quick overview of the data to iden-tify the regions of interest for further analysis. Peoplewith vertical movements (i.e., who changed job lev-els, such as promotion and demotion) or attainedhigh job levels are more likely to attract the experts\u2019attention.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.",
      "data_code": { "tables": 1, "temporal": 1, "sequential": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. ",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "The Mobility Rate Timeline shows the vertical mobility rate changing over time, giving an over- view of the data filtered in the Distribution View.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 281,
    "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data",
    "pub_year": 2022,
    "domain": "career mobility",
    "requirement": {
      "requirement_text": "T2 What special features do the groups with vertical move-ments have at different time periods?Further investiga-tion is required to explain the mobility patterns indifferent historical periods. Experts want to \ufb01nd outif there are common features among the promotion/demotion groups.",
      "requirement_code": {
        "discover_observation": 1,
        "evaluate_hypothesis": 1
      }
    },
    "data": {
      "data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.",
      "data_code": { "tables": 1, "temporal": 1, "sequential": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. ",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "The Mobility Rate Timeline shows the vertical mobility rate changing over time, giving an over- view of the data filtered in the Distribution View.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      },
      {
        "solution_text": "The Mobility Rate Timeline shows the vertical mobility rate changing over time, giving an over- view of the data filtered in the Distribution View. Hover- ing on a bar unit, a glyph is shown to compare statistics of different groups (i.e., promotion, demotion, and steady groups), thereby revealing the salient features influ- encing vertical mobility at different periods.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      },
      {
        "solution_text": "The Mobility Rate Timeline shows the vertical mobility rate changing over time, giving an over- view of the data filtered in the Distribution View. Hover- ing on a bar unit, a glyph is shown to compare statistics of different groups (i.e., promotion, demotion, and steady groups), thereby revealing the salient features influ- encing vertical mobility at different periods.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "glyph",
        "axial_code": ["Co-axis"],
        "componenet_code": ["glyph"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 282,
    "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data",
    "pub_year": 2022,
    "domain": "career mobility",
    "requirement": {
      "requirement_text": "T3 What are the characteristics of different social groups?Identifying latent groups based on similarity in pat-terns of vertical movements is essential to \ufb01ndinghidden rules in career mobility. The system alsoneeds to provide statistics for groups to help \ufb01ndindividual similarities.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. Besides latent group identification, we also distilled three social relations and computed several mobility indices. Social Relationships. We have extracted three types of social relations based on experts\u2019 suggestions: colleagues (working in the same department at certain periods), towns_x0002_men (from the same Birthplace), and classmates who passed the exam in the same year. For colleagues, we store the over_x0002_lapped periods and the departments they served together. Career Mobility Indices. There are three indices reflecting career mobility at the individual and group levels [49].",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "Description. The Group View (Fig. 2D) lists latent groups(T3) detected by the algorithm in Section 4 given the offi-cials chosen from the Distribution View. Each group is repre-sented by an intuitive node-link sequence showing thegroup sequential pattern. Numbers in rectangles form intogroup patterns, and two circles at the beginning and endshow the actual minimum and maximum job levels withinthe group. Groups are sorted by the numbers of officials indescending order. Experts require a statistical summary ofeach group to check whether officials have commonalities,so we design a folded information card (Fig. 2D1). We com-pute the average job-level distance and give four horizontalproportional bars to summarize the demographic profile(i.e., Ethnicity, Birthplace, Family Background, and ExamDegree).",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph", "bar"]
      },
      {
        "solution_text": "Experts can quickly identify whether a value domi-nates an attribute, thus finding commonalities. They canalso hover on each horizontal bar to check detailed statisticsof this attribute.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 283,
    "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data",
    "pub_year": 2022,
    "domain": "career mobility",
    "requirement": {
      "requirement_text": "T4 What is the mobility pattern for each group?After identi-fying groups, experts want to check the mobility pat-terns at the job-level and department levels within agroup to compare career path similarities (e.g., thepromotion speed and department transfer routines).",
      "requirement_code": {
        "discover_observation": 1,
        "evaluate_hypothesis": 1
      }
    },
    "data": {
      "data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. Besides latent group identification, we also distilled three social relations and computed several mobility indices. Social Relationships. We have extracted three types of social relations based on experts\u2019 suggestions: colleagues (working in the same department at certain periods), towns_x0002_men (from the same Birthplace), and classmates who passed the exam in the same year. For colleagues, we store the over_x0002_lapped periods and the departments they served together. Career Mobility Indices. There are three indices reflecting career mobility at the individual and group levels [49].",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "The Population Flow displays a detailed career mobility pattern in two modes (i.e., job-level and depart- ment) with two types of time (i.e., absolute and relative), which can be switched by two buttons. We provide a novel flow design adopting a multi-scale approach to form into the superimposition of three layers, namely, overall mobil- ity flow, group subflow, and individual career thread. The groups of interest or indi- vidual social relationships are embedded into the over- all population. This allows for the study of the mobility of a specific population within its broader historical context.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "sankey",
        "axial_code": ["Repetition"],
        "componenet_code": ["sankey"]
      },
      {
        "solution_text": "The Population Flow displays a detailed career mobility pattern in two modes (i.e., job-level and depart- ment) with two types of time (i.e., absolute and relative), which can be switched by two buttons. We provide a novel flow design adopting a multi-scale approach to form into the superimposition of three layers, namely, overall mobil- ity flow, group subflow, and individual career thread. The groups of interest or indi- vidual social relationships are embedded into the over- all population. This allows for the study of the mobility of a specific population within its broader historical context.",
        "solution_category": "interaction",
        "solution_axial": "Reconfigure",
        "solution_compoent": "",
        "axial_code": ["Reconfigure"],
        "componenet_code": ["reconfigure"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 284,
    "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data",
    "pub_year": 2022,
    "domain": "career mobility",
    "requirement": {
      "requirement_text": "T4 What is the mobility pattern for each group?After identi-fying groups, experts want to check the mobility pat-terns at the job-level and department levels within agroup to compare career path similarities (e.g., thepromotion speed and department transfer routines).",
      "requirement_code": {
        "discover_observation": 1,
        "evaluate_hypothesis": 1
      }
    },
    "data": {
      "data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. Besides latent group identification, we also distilled three social relations and computed several mobility indices. Social Relationships. We have extracted three types of social relations based on experts\u2019 suggestions: colleagues (working in the same department at certain periods), towns_x0002_men (from the same Birthplace), and classmates who passed the exam in the same year. For colleagues, we store the over_x0002_lapped periods and the departments they served together. Career Mobility Indices. There are three indices reflecting career mobility at the individual and group levels [49].",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "The latent groups and individuals\u2019 social relationships can be highlighted in the Population Flow (Fig. 3C) for a detailed investigation (T4, T6). Two modes are supported to portray the career mobility of the selected individuals. The first mode (Fig. 3C2) aggregates the career paths within a group into a subflow, where each flow seg- ment is embedded into the corresponding overall flow seg- ment. It is intuitive and scalable, allowing for inspecting group mobility and proportions relative to the whole population.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "sankey",
        "axial_code": ["Repetition"],
        "componenet_code": ["sankey"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 285,
    "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data",
    "pub_year": 2022,
    "domain": "career mobility",
    "requirement": {
      "requirement_text": "T5 What are the mobility characteristics for different individ-uals? Besides macro-level analysis, experts also wishto examine speci\ufb01c individuals at a micro-level.Those more in\ufb02uential with long careers and highpositions are interesting targets.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.",
      "data_code": { "tables": 1, "temporal": 1, "sequential": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. ",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "The Person View (Fig. 2E) lists a visual summary of poten-tial in\ufb02uential of\ufb01cials (T5). Since domain experts wish to\ufb01lter in\ufb02uential of\ufb01cials \ufb02exibly using career length and\ufb01nal job level, we thus provide these two constraints. Wealso provide two methods to sort of\ufb01cials (i.e., career start-ing years or career lengths). Each horizontal bar shows anindividual\u2019s job level information. The total length encodesthe career length. It comprises multiple small bars withlength encoding corresponding working periods and colorencoding different job levels. According to our experts, wedivide job levels into four levels represented by four colorcategories: red for high-level jobs (from level 3 to 1), brownfor middle-level (from level 6 to 4), blue for low-level (fromlevel 9 to 7), and grey for the lowest level (level 10). Insideeach level, we use saturation to distinguish job levels. Thedarker the color, the higher the job level. The color schemefor job levels is consistent in the whole system. Users canhave a quick overview of these individuals and \ufb01nd thosewith long careers and great promotion distance",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      },
      {
        "solution_text": "Since domain experts wish to\ufb01lter in\ufb02uential of\ufb01cials \ufb02exibly using career length and\ufb01nal job level, we thus provide these two constraints. Wealso provide two methods to sort of\ufb01cials (i.e., career start-ing years or career lengths). Each horizontal bar shows anindividual\u2019s job level information.",
        "solution_category": "interaction",
        "solution_axial": "Filtering,Reconfigure",
        "solution_compoent": "",
        "axial_code": ["Reconfigure", "Filtering"],
        "componenet_code": ["reconfigure", "filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 286,
    "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data",
    "pub_year": 2022,
    "domain": "career mobility",
    "requirement": {
      "requirement_text": "T6 How do the mobility patterns of each individual andhis social relationships change over time? To betterunderstand differences in opportunities, expertswish to inspect individuals\u2019 different social rela-tionships and how their mobility evolves andinteracts with them.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. Besides latent group identification, we also distilled three social relations and computed several mobility indices. Social Relationships. We have extracted three types of social relations based on experts\u2019 suggestions: colleagues (working in the same department at certain periods), towns_x0002_men (from the same Birthplace), and classmates who passed the exam in the same year. For colleagues, we store the over_x0002_lapped periods and the departments they served together. Career Mobility Indices. There are three indices reflecting career mobility at the individual and group levels [49].",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "The Population Flow displays a detailed career mobility pattern in two modes (i.e., job-level and depart- ment) with two types of time (i.e., absolute and relative), which can be switched by two buttons. We provide a novel flow design adopting a multi-scale approach to form into the superimposition of three layers, namely, overall mobil- ity flow, group subflow, and individual career thread. The groups of interest or indi- vidual social relationships are embedded into the over- all population. This allows for the study of the mobility of a specific population within its broader historical context.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "sankey",
        "axial_code": ["Repetition"],
        "componenet_code": ["sankey"]
      },
      {
        "solution_text": "The Population Flow displays a detailed career mobility pattern in two modes (i.e., job-level and depart- ment) with two types of time (i.e., absolute and relative), which can be switched by two buttons. We provide a novel flow design adopting a multi-scale approach to form into the superimposition of three layers, namely, overall mobil- ity flow, group subflow, and individual career thread. The groups of interest or indi- vidual social relationships are embedded into the over- all population. This allows for the study of the mobility of a specific population within its broader historical context.",
        "solution_category": "interaction",
        "solution_axial": "Reconfigure",
        "solution_compoent": "",
        "axial_code": ["Reconfigure"],
        "componenet_code": ["reconfigure"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 287,
    "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data",
    "pub_year": 2022,
    "domain": "career mobility",
    "requirement": {
      "requirement_text": "T6 How do the mobility patterns of each individual andhis social relationships change over time? To betterunderstand differences in opportunities, expertswish to inspect individuals\u2019 different social rela-tionships and how their mobility evolves andinteracts with them.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "After choosing the population of interest, we apply latent group detection to classify officials and distill career pat_x0002_terns. Here we focus on the vertical career mobility analysis. We define a latent social group as a cluster of individuals with similar vertical (i.e., job level) movement sequences. As discussed in Section 2.1, latent group identification methods in social science are mostly ad-hoc and insufficient for our experts to understand promotion and demotion rules. Besides latent group identification, we also distilled three social relations and computed several mobility indices. Social Relationships. We have extracted three types of social relations based on experts\u2019 suggestions: colleagues (working in the same department at certain periods), towns_x0002_men (from the same Birthplace), and classmates who passed the exam in the same year. For colleagues, we store the over_x0002_lapped periods and the departments they served together. Career Mobility Indices. There are three indices reflecting career mobility at the individual and group levels [49].",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "The latent groups and individuals\u2019 social relationships can be highlighted in the Population Flow (Fig. 3C) for a detailed investigation (T4, T6). Two modes are supported to portray the career mobility of the selected individuals. The first mode (Fig. 3C2) aggregates the career paths within a group into a subflow, where each flow seg- ment is embedded into the corresponding overall flow seg- ment. It is intuitive and scalable, allowing for inspecting group mobility and proportions relative to the whole population.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "sankey",
        "axial_code": ["Repetition"],
        "componenet_code": ["sankey"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 288,
    "paper_title": "Interactive Visual Exploration of Longitudinal Historical Career Mobility Data",
    "pub_year": 2022,
    "domain": "career mobility",
    "requirement": {
      "requirement_text": "T6 How do the mobility patterns of each individual andhis social relationships change over time? To betterunderstand differences in opportunities, expertswish to inspect individuals\u2019 different social rela-tionships and how their mobility evolves andinteracts with them.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "One such newly-constructed quantitative historical dataset is the CGED-Q (China Government Employee Database_x0002_Qing) [3], [4]. It records the career trajectories of over 340,000 government officials in the bureaucracy of Qing China from 1760 to 1912.",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Besides latent group identification, we also distilled three social relations and computed several mobility indices. Social Relationships. We have extracted three types of social relations based on experts\u2019 suggestions: colleagues (working in the same department at certain periods), towns_x0002_men (from the same Birthplace), and classmates who passed the exam in the same year. For colleagues, we store the over_x0002_lapped periods and the departments they served together. Career Mobility Indices. There are three indices reflecting career mobility at the individual and group levels [49].",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "To show the social relationships of a selected individual (KP ), users can select specific social relationships in a drop-down menu. The career threads that have social connections with KP will be highlighted over the overall mobility flow. Each thread consists of two line types, namely, solid lines indicating the time period this official worked with KP , and the dashed line meaning they did not work in the same department. Different department glyphs in two colors (with the same encodings in department flow) are overlaid onto the KP \u2019s career thread. The characters of the glyphs are abbreviations of different departments.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "sankey+line",
        "axial_code": ["Nesting"],
        "componenet_code": ["line", "sankey"]
      },
      {
        "solution_text": "To show the social relationships of a selected individual (KP ), users can select specific social relationships in a drop-down menu. The career threads that have social connections with KP will be highlighted over the overall mobility flow.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 289,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R1] Graph query design and matching: This requirementaims to provide the user with a tool to interactively design/construct a query. This query will serve as a starting pointto find all the occurrences on the graph. In the previousexample, the user can easily depict the authors and the con-ferences linking these authors with various venue types asjournals or conferences (e.g. TVCG, ICDM, and KDD).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Fig. 2 shows the query view interface. This view allows theuser to de\ufb01ne the query structure (nodes, edges node/edge-attributes) via interactive functionalities [R1, R3]. This inter-face contains a drawing canvas, two built-in toolbars and astatus bar. The drawing canvas (Fig. 2a) is the area wherethe query is constructed and where interactions happen.The design toolbar (Fig. 2b) provides different actions tobuild the query. From top to bottom, these actions are: adda node of a speci\ufb01c layer, add an edge of a speci\ufb01c layer,delete a node/edge, launch the edge suggestion mechanismand arrange the layout of the query by applying a forcedirected algorithm. The standard toolbar (Fig. 2c) includesfrequently used features such as load, create, and save aquery structure. It also contains the Search button that runsthe graph engine. Finally, the status bar (Fig. 2d) providestextual information about the current state of the view.Actions such as specifying a node type or an attributevalue, specifying an edge type, starting the query matchingprocess and launching the edge suggestion mechanismrequire speci\ufb01c interactions described below",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "Network+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "Network"]
      },
      {
        "solution_text": "The design toolbar (Fig. 2b) provides different actions tobuild the query. From top to bottom, these actions are: adda node of a speci\ufb01c layer, add an edge of a speci\ufb01c layer,delete a node/edge, launch the edge suggestion mechanismand arrange the layout of the query by applying a forcedirected algorithm. The standard toolbar (Fig. 2c) includesfrequently used features such as load, create, and save aquery structure. It also contains the Search button that runsthe graph engine. Finally, the status bar (Fig. 2d) providestextual information about the current state of the view.Actions such as specifying a node type or an attributevalue, specifying an edge type, starting the query matchingprocess and launching the edge suggestion mechanismrequire speci\ufb01c interactions described below",
        "solution_category": "interaction",
        "solution_axial": "Extractionoffeatures",
        "solution_compoent": "",
        "axial_code": ["Extractionoffeatures"],
        "componenet_code": ["extraction_of_features"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 290,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R1] Graph query design and matching: This requirementaims to provide the user with a tool to interactively design/construct a query. This query will serve as a starting pointto find all the occurrences on the graph. In the previousexample, the user can easily depict the authors and the con-ferences linking these authors with various venue types asjournals or conferences (e.g. TVCG, ICDM, and KDD).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "The right part of the figure shows the query after adding the above-mentioned suggestions. To accept a suggested edge, the user clicks on the internal (dashed line) or external edge (pie slice). A new internal edge is displayed as a continuous line KDD and SDM links). A new external edge is displayed as a link to a node toward its slice. The query suggestion mechanism provides visual elements to guide the user in the incremental con- struction of the query. Thus, each time the query is extended, the user can retrieve embeddings and further refine his/her searching process. The query suggestion mechanism is activated from the design toolbar.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "Network+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "Network"]
      },
      {
        "solution_text": "The right part of the figure shows the query after adding the above-mentioned suggestions. To accept a suggested edge, the user clicks on the internal (dashed line) or external edge (pie slice). A new internal edge is displayed as a continuous line KDD and SDM links). A new external edge is displayed as a link to a node toward its slice. The query suggestion mechanism provides visual elements to guide the user in the incremental con- struction of the query. Thus, each time the query is extended, the user can retrieve embeddings and further refine his/her searching process. The query suggestion mechanism is activated from the design toolbar.",
        "solution_category": "interaction",
        "solution_axial": "Extractionoffeatures",
        "solution_compoent": "",
        "axial_code": ["Extractionoffeatures"],
        "componenet_code": ["extraction_of_features"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 291,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R2] Navigation and exploration of the results: This require-ment aims to provide visualizations and interactions tofacilitate the exploration of the results, or the embeddings5at different granularity levels.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to improve the SRT [R5], VERTIGo tightlyinteracts with the SuMGra system to enable the user to start,pause, and resume the query engine with the possibility tonavigate/explore partially collected embeddings [R2] (thisfunctionality is described in Section 5)",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "Network+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "Network"]
      },
      {
        "solution_text": "In order to improve the SRT [R5], VERTIGo tightlyinteracts with the SuMGra system to enable the user to start,pause, and resume the query engine with the possibility tonavigate/explore partially collected embeddings [R2] (thisfunctionality is described in Section 5)",
        "solution_category": "interaction",
        "solution_axial": "Extractionoffeatures",
        "solution_compoent": "",
        "axial_code": ["Extractionoffeatures"],
        "componenet_code": ["extraction_of_features"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 292,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R2] Navigation and exploration of the results: This require-ment aims to provide visualizations and interactions tofacilitate the exploration of the results, or the embeddings5at different granularity levels.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "We adopt a node-link diagram with a force directed lay- out [33] to produce a suitable visualization of the graph. Working with large graphs entails some challenges: aes- thetic criteria, scalability and reasonable interaction time. To deal with these issues, we leverage a multilevel tech- nique named Multipole Multilevel Method (FM3) [34]. FM3 is a well-known approach in the graph drawing field that allows scaling up to large graphs with a good trade off between time and visualization performances. An  example of a graph layout obtained by VERTIGo is pre- sented in Fig. 4a. The input graph is a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types. We note that such a visualization has the advantage to stand out groups of nodes that clearly form communities. In this particular example, we can easily highlight five major groups (clus- ters) of nodes. Edges of the graph layout are not depicted to avoid cluttering issues.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 293,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R2] Navigation and exploration of the results: This require-ment aims to provide visualizations and interactions tofacilitate the exploration of the results, or the embeddings5at different granularity levels.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to display a large number of embeddings, Pientaet al. [11] reduced them to points. This kind of abstractionavoids a cluttered visualization; however, the spatial distri-bution of its elements is not depicted. With the aim to displayin detail the topology of the embeddings (i. e., nodes andedges) [R2], we designed the Embeddings view (Fig. 5). Inthis view, we show in detail the structure of the embeddings(Fig. 5b) containing a given set of nodes (Fig. 5a) (interactionsto select these nodes are described in Section 5).The goal of the Embeddings view is to allow the user tosample the embeddings for any combination of nodesinvolved in the embeddings. For example, considering theembeddings of the query shown in Fig. 2, the Fig. 5b showsthe list of embeddings where Shixia Liu and Yangqiu Songare present. This behaviour enhances the exploration task[R2] since it not only shows the embeddings, but also a sub-set of them.In order to convey the spatial distribution of the embed-dings, we use the Minimum Bounding Rectangle (MBR)value that bounds the nodes of an embedding in the Graphview. This value transmits the extent of the nodes within anembedding. For instance, a low MBR value conveys thecloseness of its nodes, while a high MBR value means thatone or more nodes are far apart from the others, possibly indifferent clusters. The Embeddings view allows users tosort results in an ascending/descending order based ontheir MBR values [R2]. This metric is also used to \ufb01lterresults using a histogram (Fig. 1e) that groups MBR valuesinto n ranges (interactions are described in Section 5.2).With the aim to depict the topological structure of theembeddings, we must consider that some embeddings couldinvolve the same set of nodes, with a different edge topology.Based on this feature, we aggregate embeddings with thesame set of nodes into a single fusion embedding. Fig. 6 showsan example where the fusion embedding (c) is the result ofthe fusion of the embeddings (a) and (b). Thus, this fusionmaintains the topology of nodes, but the topology of edgesevolves according to the links in their aggregated embed-dings. The number of aggregations is represented by thethickness of the edges in Fig. 6c.Each row in the list of embeddings (Fig. 5b) is composedby a fusion embedding and its associated metadata (i. e., theMBR value and the number of aggregated embeddings).This metadata can be particularly useful to explore embed-dings according to their spatial/topological features.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "Network",
        "axial_code": ["Repetition"],
        "componenet_code": ["Network"]
      },
      {
        "solution_text": "For instance, in a co-authorship network, given a set of authors(Fig. 5a), if the user is interested in knowing other authorswho often collaborate with them, he/she can order up thelist of embeddings (Fig. 5b) by clicking on the last column ofthe view, and recover the embedding(s) with the highestnumber of aggregations (Fig. 5c).",
        "solution_category": "interaction",
        "solution_axial": "Reconfigure",
        "solution_compoent": "",
        "axial_code": ["Reconfigure"],
        "componenet_code": ["reconfigure"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 294,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R2] Navigation and exploration of the results: This require-ment aims to provide visualizations and interactions tofacilitate the exploration of the results, or the embeddings5at different granularity levels.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Considering the overview, we employ a heatmap [35] thatdynamically illustrates the portion of the graph in whichembeddings are located. Fig. 4a shows an example of a heat-map of embeddings. The density distribution is representedusing the viridis colour gradient [36] which offers a highrange of perceived values while avoiding red-green colorblindness. Liu and Heer [37] demonstrate that this gradientperforms well to encode quantitative data in terms of timeand error through an experimentation involving 9 competi-tors. The viridis scheme varies in a sequential multi-hue pal-ette from blue (low-dense areas) to yellow (high-denseareas). In the example depicted in Fig. 4b, we can highlightthat the Graph view shows different communities since thelayout algorithm highlights different groups of nodes. Here,the embeddings are not equally distributed among thesegroups. As an interesting point, we can note a high density(high concentration of embeddings depicted in yellow) inthe center of the image while a lower concentration ofembeddings appears in the upper-right corner. Our heatmapcan be employed to visually understand if a query is specificto a certain portion of the graph or not, and thus provides theuser an entry point for the exploration task [R2].We would like to note that VERTIGo allows the user topause and resume the underlying query process (Fig. 7b)This can be particularly useful if the number of embeddingsgrows quickly [R5]. In that scenario, the process can bepaused at any moment, the embeddings are retrieved con-sidering the current state of the query engine and the corre-sponding heatmap (Fig. 7e) is produced or updated.Finally, the user can resume the query process from wherehe/she had stopped.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "scatter+heatmap",
        "axial_code": ["Coordinate"],
        "componenet_code": ["heatmap", "scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 295,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R2] Navigation and exploration of the results: This require-ment aims to provide visualizations and interactions tofacilitate the exploration of the results, or the embeddings5at different granularity levels.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "The abstraction supplied by the heatmap makes it dif\ufb01cultto know the exact number/distribution of embeddingsfound in a region. To deal with this issue, VERTIGo allowsthe users to change the levels of details zooming on someparticular area [R2] (Fig. 7f).",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      },
      {
        "solution_text": "In this case, the nodes of theembeddings are shown in a contrasted color (see the bluenodes in Fig. 8). We use the area of the node to depict thenumber of embeddings it belongs to: bigger nodes corre-spond to nodes occurring in more embeddings. In order toenhance the effectiveness of the exploration task [R2], weshow the labels of embedding nodes. Labeling the nodes isnot a trivial task, since showing labels for all nodes maycause cluttering issues. To overcome this problem, we showoverlapping-free labels based on node weight (i. e., thenumber of times it appears in an embedding), by using agreedy algorithm. It \ufb01rst orders the set of nodes by weight.Next, for each node, it calculates the bounding rectangle ofthe label, and it checks if there is no overlap with the previ-ous labeled nodes. If there is no overlap, the label is shown,otherwise it is not. The complexity of this algorithm isO\u00f0n2\u00de, where n is the set of embedding nodes. The resultdepends on the zoom level. Thus, when the user zooms in/out, the algorithm computes again the displayed labels.Fig. 8 shows an example. Notice that some node labels arebigger (e.g., N. Ramakrishnan) than their neighbors.The labeling algorithm clearly displays the most weigh-ted node labels. However, depending on the zoom level,some nodes with low weight may be underneath others. Toovercome this problem, we implement a text \ufb01eld searchfacility to retrieve nodes by their name. Fig. 8a shows thisoption. Once the node name is selected, the focus of theGraph view is automatically redirected to the node locationand an animation shows its exact location.From the detailed level, the user can select a set ofembedding nodes for further analysis by clicking on them.Their border color becomes red and they are automaticallyadded to the Embeddings view (Fig. 7g). The user can thusvisualize the list of the associated embeddings (Fig. 7h). Inthe following section, we describe the exploration of embed-dings using both: the Embeddings and the Graph views.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "In this case, the nodes of theembeddings are shown in a contrasted color (see the bluenodes in Fig. 8). We use the area of the node to depict thenumber of embeddings it belongs to: bigger nodes corre-spond to nodes occurring in more embeddings. In order toenhance the effectiveness of the exploration task [R2], weshow the labels of embedding nodes. Labeling the nodes isnot a trivial task, since showing labels for all nodes maycause cluttering issues. To overcome this problem, we showoverlapping-free labels based on node weight (i. e., thenumber of times it appears in an embedding), by using agreedy algorithm. It \ufb01rst orders the set of nodes by weight.Next, for each node, it calculates the bounding rectangle ofthe label, and it checks if there is no overlap with the previ-ous labeled nodes. If there is no overlap, the label is shown,otherwise it is not. The complexity of this algorithm isO\u00f0n2\u00de, where n is the set of embedding nodes. The resultdepends on the zoom level. Thus, when the user zooms in/out, the algorithm computes again the displayed labels.Fig. 8 shows an example. Notice that some node labels arebigger (e.g., N. Ramakrishnan) than their neighbors.The labeling algorithm clearly displays the most weigh-ted node labels. However, depending on the zoom level,some nodes with low weight may be underneath others. Toovercome this problem, we implement a text \ufb01eld searchfacility to retrieve nodes by their name. Fig. 8a shows thisoption. Once the node name is selected, the focus of theGraph view is automatically redirected to the node locationand an animation shows its exact location.From the detailed level, the user can select a set ofembedding nodes for further analysis by clicking on them.Their border color becomes red and they are automaticallyadded to the Embeddings view (Fig. 7g). The user can thusvisualize the list of the associated embeddings (Fig. 7h). Inthe following section, we describe the exploration of embed-dings using both: the Embeddings and the Graph views.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "scatter+network",
        "axial_code": ["Nesting"],
        "componenet_code": ["network", "scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 296,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R2] Navigation and exploration of the results: This require-ment aims to provide visualizations and interactions tofacilitate the exploration of the results, or the embeddings5at different granularity levels.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Besides the Kelp-based representation, VERTIGo pro-vides a histogram to explore embedding results based ontheir MBR value [R2]. MBR values are grouped into nranges de\ufb01ned by the user, where 1 _x0014_ n _x0014_ 10. Fig. 1e dis-plays a histogram grouped into \ufb01ve ranges. In this example,we can highlight that there is a concentration of embeddingswith a low MBR value, which re\ufb02ects their proximity in thegraph (maybe in the same cluster). On the other hand, thelast two ranges show a low concentration of embeddingswith a high MBR value (nodes of these embeddings mightbe in different clusters). This histogram allows the user to\ufb01lter embeddings by clicking over the desired bar. Thank tothis functionality, users can focus on a speci\ufb01c group or dis-cover anomalous embeddings (please refer to the exampleon Section 6.1 to see this functionality).",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Besides the Kelp-based representation, VERTIGo pro-vides a histogram to explore embedding results based ontheir MBR value [R2]. MBR values are grouped into nranges de\ufb01ned by the user, where 1 _x0014_ n _x0014_ 10. Fig. 1e dis-plays a histogram grouped into \ufb01ve ranges. In this example,we can highlight that there is a concentration of embeddingswith a low MBR value, which re\ufb02ects their proximity in thegraph (maybe in the same cluster). On the other hand, thelast two ranges show a low concentration of embeddingswith a high MBR value (nodes of these embeddings mightbe in different clusters). This histogram allows the user to\ufb01lter embeddings by clicking over the desired bar. Thank tothis functionality, users can focus on a speci\ufb01c group or dis-cover anomalous embeddings (please refer to the exampleon Section 6.1 to see this functionality).",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 297,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R3] Handling multilayer graphs: This requirement aims toallow the user to model and depict a traditional graph or amultilayer graph, i. e., a graph where each layer containsedges of a certain type. Referring to the previous example, alayer can be a specific venue in the co-authorship network.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Fig. 2 shows the query view interface. This view allows theuser to de\ufb01ne the query structure (nodes, edges node/edge-attributes) via interactive functionalities [R1, R3]. This inter-face contains a drawing canvas, two built-in toolbars and astatus bar. The drawing canvas (Fig. 2a) is the area wherethe query is constructed and where interactions happen.The design toolbar (Fig. 2b) provides different actions tobuild the query. From top to bottom, these actions are: adda node of a speci\ufb01c layer, add an edge of a speci\ufb01c layer,delete a node/edge, launch the edge suggestion mechanismand arrange the layout of the query by applying a forcedirected algorithm. The standard toolbar (Fig. 2c) includesfrequently used features such as load, create, and save aquery structure. It also contains the Search button that runsthe graph engine. Finally, the status bar (Fig. 2d) providestextual information about the current state of the view.Actions such as specifying a node type or an attributevalue, specifying an edge type, starting the query matchingprocess and launching the edge suggestion mechanismrequire speci\ufb01c interactions described below",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "Network+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "Network"]
      },
      {
        "solution_text": "The design toolbar (Fig. 2b) provides different actions tobuild the query. From top to bottom, these actions are: adda node of a speci\ufb01c layer, add an edge of a speci\ufb01c layer,delete a node/edge, launch the edge suggestion mechanismand arrange the layout of the query by applying a forcedirected algorithm. The standard toolbar (Fig. 2c) includesfrequently used features such as load, create, and save aquery structure. It also contains the Search button that runsthe graph engine. Finally, the status bar (Fig. 2d) providestextual information about the current state of the view.Actions such as specifying a node type or an attributevalue, specifying an edge type, starting the query matchingprocess and launching the edge suggestion mechanismrequire speci\ufb01c interactions described below",
        "solution_category": "interaction",
        "solution_axial": "Extractionoffeatures",
        "solution_compoent": "",
        "axial_code": ["Extractionoffeatures"],
        "componenet_code": ["extraction_of_features"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 298,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R3] Handling multilayer graphs: This requirement aims toallow the user to model and depict a traditional graph or amultilayer graph, i. e., a graph where each layer containsedges of a certain type. Referring to the previous example, alayer can be a specific venue in the co-authorship network.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "VERTIGo allows the users to specify the types of nodes inthe query, which fit the multilayer aspect of the inputgraph [R3]. In this case, the list of matched embeddingsonly contains the nodes holding the selected types, and theSRT of the query process decreases [R5]. Nodes of a giventype are represented by the same icon showing the typesemantic (for instance, a person in Fig. 2e). A specific attribute value can also be added to a node of thequery in order to decrease the number of embeddings and theSRT of the query process [R5]. The query view enables this action by right-clicking on the desired node and selecting avalue from the displayed pop-up list. For instance, in Fig. 2e,one can select an author name in a co-authorship network.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 299,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R3] Handling multilayer graphs: This requirement aims toallow the user to model and depict a traditional graph or amultilayer graph, i. e., a graph where each layer containsedges of a certain type. Referring to the previous example, alayer can be a specific venue in the co-authorship network.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "VERTIGo allows the users to query traditional graphs ormultilayer graphs [R3]. We adopt a well-known way tovisualize the edge types (or layers) by color coding. When apair of nodes is connected by several edges, some scalabilitychallenges arise i) how to organize these links and ii) how todeal with many edge types. To overcome these problems,edges between a pair of nodes are arranged in a parallelway. The relative distance between parallel edges remainsthe same even when the number of links increases, whichallows the users to clearly see all the links, as shown inFig. 2. The upper limit for the number of edges is 10 becauseof the known problem of distinguishing small regions ofcolors [30]. Edges are labeled to enhance the type recogni-tion. In order to add an edge to a query, the user must \ufb01rstchoose an edge type from the drop-down edges button onthe design toolbar (Fig. 2b). Then, using the mouse pointer,the user draws the edge from one node to another. Finally,the edge is automatically arranged. When more than twoedges connect a pair of nodes, the Boolean logical operatorAND is used to perform a logical conjunction between them.Fig. 2 shows the query example described in Section 3,where an author published a TVCG paper with one authorand he/she also published an ICDM and a KDD paper withanother author. More precisely, we explicitly state that theauthor corresponding to the central node has publishedboth an ICDM and a KDD article with the same co-author,and a TVCG paper with another co-author. We can observethat the multilayer graph structure allows us to easily spec-ify multiple constraints between the same pair of nodes",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "Network+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "Network"]
      },
      {
        "solution_text": "VERTIGo allows the users to query traditional graphs ormultilayer graphs [R3]. We adopt a well-known way tovisualize the edge types (or layers) by color coding. When apair of nodes is connected by several edges, some scalabilitychallenges arise i) how to organize these links and ii) how todeal with many edge types. To overcome these problems,edges between a pair of nodes are arranged in a parallelway. The relative distance between parallel edges remainsthe same even when the number of links increases, whichallows the users to clearly see all the links, as shown inFig. 2. The upper limit for the number of edges is 10 becauseof the known problem of distinguishing small regions ofcolors [30]. Edges are labeled to enhance the type recogni-tion. In order to add an edge to a query, the user must \ufb01rstchoose an edge type from the drop-down edges button onthe design toolbar (Fig. 2b). Then, using the mouse pointer,the user draws the edge from one node to another. Finally,the edge is automatically arranged. When more than twoedges connect a pair of nodes, the Boolean logical operatorAND is used to perform a logical conjunction between them.Fig. 2 shows the query example described in Section 3,where an author published a TVCG paper with one authorand he/she also published an ICDM and a KDD paper withanother author. More precisely, we explicitly state that theauthor corresponding to the central node has publishedboth an ICDM and a KDD article with the same co-author,and a TVCG paper with another co-author. We can observethat the multilayer graph structure allows us to easily spec-ify multiple constraints between the same pair of nodes",
        "solution_category": "interaction",
        "solution_axial": "Extractionoffeatures",
        "solution_compoent": "",
        "axial_code": ["Extractionoffeatures"],
        "componenet_code": ["extraction_of_features"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 300,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R3] Handling multilayer graphs: This requirement aims toallow the user to model and depict a traditional graph or amultilayer graph, i. e., a graph where each layer containsedges of a certain type. Referring to the previous example, alayer can be a specific venue in the co-authorship network.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Since a multilayer graph involves different nodes and edgetypes, standard query engines such as TurboISO [22] orVF3 [26] are not adapted for such graphs. To deal with mul-tilayer graphs [R3], we integrate into VERTIGo a recent mul-tilayer graph query engine named SuMGra [31]. It exploitsspecialized indexing techniques to speed the backtrackingalgorithm that is commonly employed to deal with the sub-graph isomorphism problem [32]. The Search button in thestandard toolbar (Fig. 2c) starts the query process.",
        "solution_category": "interaction",
        "solution_axial": "Extractionoffeatures",
        "solution_compoent": "",
        "axial_code": ["Extractionoffeatures"],
        "componenet_code": ["extraction_of_features"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 301,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R4] Query suggestion: This requirement aims to suggestan extension of the initial query to the user. Mentioning theprevious example, it could propose another conferencevenue based on the graph structure that links two authorsin order to re\ufb01ne the results.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "The right part of the figure shows the query after adding the above-mentioned suggestions. To accept a suggested edge, the user clicks on the internal (dashed line) or external edge (pie slice). A new internal edge is displayed as a continuous line KDD and SDM links). A new external edge is displayed as a link to a node toward its slice. The query suggestion mechanism provides visual elements to guide the user in the incremental con- struction of the query. Thus, each time the query is extended, the user can retrieve embeddings and further refine his/her searching process. The query suggestion mechanism is activated from the design toolbar.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "Network+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "Network"]
      },
      {
        "solution_text": "The right part of the figure shows the query after adding the above-mentioned suggestions. To accept a suggested edge, the user clicks on the internal (dashed line) or external edge (pie slice). A new internal edge is displayed as a continuous line KDD and SDM links). A new external edge is displayed as a link to a node toward its slice. The query suggestion mechanism provides visual elements to guide the user in the incremental con- struction of the query. Thus, each time the query is extended, the user can retrieve embeddings and further refine his/her searching process. The query suggestion mechanism is activated from the design toolbar.",
        "solution_category": "interaction",
        "solution_axial": "Extractionoffeatures",
        "solution_compoent": "",
        "axial_code": ["Extractionoffeatures"],
        "componenet_code": ["extraction_of_features"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 302,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R4] Query suggestion: This requirement aims to suggestan extension of the initial query to the user. Mentioning theprevious example, it could propose another conferencevenue based on the graph structure that links two authorsin order to re\ufb01ne the results.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Based on the initial query structure, VERTIGo automaticallysuggests k edges to the user\u2019s query intention [R4]. To obtaincandidate edges, VIIQ [14] uses a correlation of a subset ofedges of a user log. This approach only ranks a subset ofedges which leads to suggest only a part of the query struc-ture. VERTIGo overcomes this drawback by ranking all theadjacent edges for every node of the embeddings and thensuggests the top-k frequent type of edges found. The graphstructure is used to obtain the adjacent edges for eachembedding node. This mechanism suggests interestingextensions of the query since it leverages informationobtained by the underlying graph",
        "solution_category": "data_manipulation",
        "solution_axial": "SimilarityCalculation",
        "solution_compoent": "Leveragesgraphstructuretorankalladjacentedgesforeachnodeofembeddings.",
        "axial_code": ["SimilarityCalculation"],
        "componenet_code": ["similarity_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 303,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R5] Scalability: This requirement aims to handle queryingon large graphs and recover the embeddings in a reasonableSystem Response Time (SRT). SRT is de\ufb01ned as the timetaken by the matching engine to evaluate the entire query.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "VERTIGo allows the users to specify the types of nodes inthe query, which fit the multilayer aspect of the inputgraph [R3]. In this case, the list of matched embeddingsonly contains the nodes holding the selected types, and theSRT of the query process decreases [R5]. Nodes of a giventype are represented by the same icon showing the typesemantic (for instance, a person in Fig. 2e). A specific attribute value can also be added to a node of thequery in order to decrease the number of embeddings and theSRT of the query process [R5]. The query view enables this action by right-clicking on the desired node and selecting avalue from the displayed pop-up list. For instance, in Fig. 2e,one can select an author name in a co-authorship network.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 304,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R5] Scalability: This requirement aims to handle queryingon large graphs and recover the embeddings in a reasonableSystem Response Time (SRT). SRT is de\ufb01ned as the timetaken by the matching engine to evaluate the entire query.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "A common paradigm in query process is based on a Quer-y!Result approach, i. e., after a query construction, send therequest to the query engine and then visualize the retrievedresults [12]. However, as interactions with the query engineare not available in this paradigm, the process can be affectedby huge SRT when the number of results grows exponen-tially. In order to improve the SRT [R5], VERTIGo tightlyinteracts with the SuMGra system to enable the user to start,pause, and resume the query engine with the possibility tonavigate/explore partially collected embeddings [R2] (thisfunctionality is described in Section 5).",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "Network+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "Network"]
      },
      {
        "solution_text": "A common paradigm in query process is based on a Quer-y!Result approach, i. e., after a query construction, send therequest to the query engine and then visualize the retrievedresults [12]. However, as interactions with the query engineare not available in this paradigm, the process can be affectedby huge SRT when the number of results grows exponen-tially. In order to improve the SRT [R5], VERTIGo tightlyinteracts with the SuMGra system to enable the user to start,pause, and resume the query engine with the possibility tonavigate/explore partially collected embeddings [R2] (thisfunctionality is described in Section 5).",
        "solution_category": "interaction",
        "solution_axial": "Extractionoffeatures",
        "solution_compoent": "",
        "axial_code": ["Extractionoffeatures"],
        "componenet_code": ["extraction_of_features"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 305,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R5] Scalability: This requirement aims to handle queryingon large graphs and recover the embeddings in a reasonableSystem Response Time (SRT). SRT is de\ufb01ned as the timetaken by the matching engine to evaluate the entire query.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "We adopt a node-link diagram with a force directed lay- out [33] to produce a suitable visualization of the graph. Working with large graphs entails some challenges: aes- thetic criteria, scalability and reasonable interaction time. To deal with these issues, we leverage a multilevel tech- nique named Multipole Multilevel Method (FM3) [34]. FM3 is a well-known approach in the graph drawing field that allows scaling up to large graphs with a good trade off between time and visualization performances. An  example of a graph layout obtained by VERTIGo is pre- sented in Fig. 4a. The input graph is a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types. We note that such a visualization has the advantage to stand out groups of nodes that clearly form communities. In this particular example, we can easily highlight five major groups (clus- ters) of nodes. Edges of the graph layout are not depicted to avoid cluttering issues.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 306,
    "paper_title": "VERTIGo: A Visual Platform for Querying and Exploring Large Multilayer Networks",
    "pub_year": 2022,
    "domain": "Network",
    "requirement": {
      "requirement_text": "[R5] Scalability: This requirement aims to handle queryingon large graphs and recover the embeddings in a reasonableSystem Response Time (SRT). SRT is de\ufb01ned as the timetaken by the matching engine to evaluate the entire query.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "a biological dataset [3] modeled as a multilayer graph. It is composed of 38,936 nodes, 310,664 edges and 7 edge types; a real offshore dataset revealed by the International Consortium of Investigative Journalists. This dataset exposes the relationship between entities (persons, companies, clients, and addresses) in tax heavens around the world through four investigations called the Panama Papers, the Offshore Leaks, the Bahamas Leaks, and the Paradise Papers.",
      "data_code": { "network_and_trees": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "We would like to note that VERTIGo allows the user to pause and resume the underlying query process. This can be particularly useful if the number of embeddings grows quickly. In that scenario, the process can be paused at any moment, the embeddings are retrieved con- sidering the current state of the query engine and the corre- sponding  is produced or updated. Finally, the user can resume the query process from where he/she had stopped.",
        "solution_category": "interaction",
        "solution_axial": "History",
        "solution_compoent": "",
        "axial_code": ["History"],
        "componenet_code": ["history"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 307,
    "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data",
    "pub_year": 2022,
    "domain": "Spatial cascade",
    "requirement": {
      "requirement_text": "R1 Provide location overviews. First, the experts need toidentify and select interesting locations like Lianget al. did in their study [28]. They pay attention to thespatial context and temporal distribution of thelocations\u2019 infection events. The system should allowaccess to the spatiotemporal overview for eachlocation.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Maps are widely used in urban analysis scenar- ios [59]. We use a level-of-detail mechanism for the map. Each location is represented by a circle, and the size and opac- ity of the circles encode the number of events. Zooming in on the map reveals the spatiotemporal overviews for the locations. Inspired by the widely used radial layout for temporal data [32], [46], we encode the tem- poral distribution of events with a radial heatmap around the circle for each location. The opacity of each sector encodes the number of events aggregated from the corresponding time- span. Different aggregation strategies can be applied. For example, for traffic congestion events, each sector represents an hour of every day, resulting in 24 sectors; and for air pollu- tion events, 12 sectors correspond to 12 months of a year. The map also depicts the cascading patterns inferred by the model as spatial networks. Each location is assigned with a unique color that is consistent throughout the system.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "map+donut",
        "axial_code": ["Coordinate"],
        "componenet_code": ["donut", "map"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 308,
    "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data",
    "pub_year": 2022,
    "domain": "Spatial cascade",
    "requirement": {
      "requirement_text": "R2 Recommend potential locations. The experts can haveno specific prior knowledge for a target area to ini-tialize the analysis. The system should recommendseveral potential locations, where strong infectiondependencies and valuable patterns may berevealed. In particular, the experts believe that thetemporal co-occurrences of infection events canenhance this recommendation",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The projection (A1) reveals interesting location sub- sets when users have no exact prior knowledge. If 1) a pair of locations are close to each other and 2) the infection events in these two locations frequently co-occur, then these two locations are highly likely infected simulta- neously. The first condition can be easily checked on the map. We design the projection based on a two-level distance measurement to capture the event co-occurrences between two locations, such as, m and n.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "The projection (A1) reveals interesting location sub- sets when users have no exact prior knowledge. If 1) a pair of locations are close to each other and 2) the infection events in these two locations frequently co-occur, then these two locations are highly likely infected simulta- neously. The first condition can be easily checked on the map. We design the projection based on a two-level distance measurement to capture the event co-occurrences between two locations, such as, m and n.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 309,
    "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data",
    "pub_year": 2022,
    "domain": "Spatial cascade",
    "requirement": {
      "requirement_text": "R3 Summarize various cascades. A cascading networkcomprises massive cascades of infection events. Eachcascade involves multiple location in\ufb02uences due tothe same contagion. Summarizing these cascadesprior to the in\ufb02uence analysis is strongly required tolearn how contagions propagate; for example, whichlocations are all infected by contagions frequently",
      "requirement_code": { "identify_main_cause_aggregate": 1 }
    },
    "data": {
      "data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "This section presents the mining framework for inferring cascading patterns in general spatiotemporal domains.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Modeling"],
        "componenet_code": ["algorithmic_calculation", "modeling"]
      },
      {
        "solution_text": "The influence view adopts a table-based layout to organize and summarize cascades in an occlusion-free manner, as illustrated in figure.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "matrix",
        "axial_code": ["Repetition"],
        "componenet_code": ["matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 310,
    "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data",
    "pub_year": 2022,
    "domain": "Spatial cascade",
    "requirement": {
      "requirement_text": "R4 Display the uncertainties of in\ufb02uences. An infectionevent has many possible upstream infection events.Although the algorithm has estimated the mostlikely upstream, the system should reveal other pos-sibilities to understand the reliability and uncer-tainty of the inference result, such as how does thepossibility of the inferred upstream differ from that ofothers?",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "This section presents the mining framework for inferring cascading patterns in general spatiotemporal domains.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Modeling"],
        "componenet_code": ["algorithmic_calculation", "modeling"]
      },
      {
        "solution_text": "Uncertainty Visualization. A cascading network is summa-rized from long-term observations. Thus, for a speci\ufb01c infec-tion event ev, other possibilities ems (m 6 \u00bc u) with a largerlikelihood f\u00f0evje m\u00de than the inferred f\u00f0evje u\u00de may exist.Uncertainty visualization should re\ufb02ect how much theinferred upstream eu exceeds ems.According to the experts\u2019 suggestions, those ems with alarger likelihood and longer duration are denoted morelikely causes. For the in\ufb02uences comprised by the same cell,we derive an uncertainty metric by dividing the number ofthe more likely causes by the number of in\ufb02uences. Themetric is then encoded with the cell luminance. For exam-ple, most cells in Fig. 4 B5, except those at the bottom, arebright, indicating that most of the in\ufb02uences are certain.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "matrix",
        "axial_code": ["Repetition"],
        "componenet_code": ["matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 311,
    "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data",
    "pub_year": 2022,
    "domain": "Spatial cascade",
    "requirement": {
      "requirement_text": "R4 Display the uncertainties of in\ufb02uences. An infectionevent has many possible upstream infection events.Although the algorithm has estimated the mostlikely upstream, the system should reveal other pos-sibilities to understand the reliability and uncer-tainty of the inference result, such as how does thepossibility of the inferred upstream differ from that ofothers?",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "This section presents the mining framework for inferring cascading patterns in general spatiotemporal domains.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Modeling"],
        "componenet_code": ["algorithmic_calculation", "modeling"]
      },
      {
        "solution_text": "We design the uncertainty and influence visu- alizations in the influence view to reveal location influences from the two aspects. Users must know uncertainties before the influence analysis. Thus, the uncertainty vis- ualizations are displayed by default. ",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph+matrix",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 312,
    "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data",
    "pub_year": 2022,
    "domain": "Spatial cascade",
    "requirement": {
      "requirement_text": "R5 Characterize in\ufb02uencing processes. The experts canunderstand how a downstream event depends on itsupstream and what happened between them byexamining location in\ufb02uences. The following ques-tions may be asked: Does the downstream get easilyinfected when its upstream is infected? Are the infectionsituations similar?Therefore, the system should visu-ally depict in\ufb02uencing processes.",
      "requirement_code": { "evaluate_hypothesis": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "However, the visually intermittent appearance preventsusers from analyzing these in\ufb02uences, for example, grasp-ing the distribution of start time differences. Improving thereadability is also an order optimization problem like thatin the in\ufb02uence view. We address this issue based on theHamiltonian path problem again. Here, the weights areeuclidean distances between in\ufb02uence vectors (e.g., hts\u00f0eu\u00de ts\u00f0e v\u00de; te\u00f0eu\u00de  ts\u00f0e v\u00de; te\u00f0ev\u00de  ts\u00f0ev\u00dei in Fig. 6 B1). Such vec-tors properly portray the appearance of the in\ufb02uence glyphin the horizontal direction. We obtain a clear appearance inthe end (Fig. 6 D)",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "In\ufb02uence Visualization. We visualize every single in\ufb02uencewith a timeline-based in\ufb02uence glyph (Fig. 6 B), where the twoinfection events are aligned along the timeline. We placethese two events on the same horizontal position and makethe overlapping part bold for highlight, thereby improvingthe scalability and readability. Colors are assigned accordingto the corresponding locations, and the overlapping partmixes both. Subsequently, we visualize multiple in\ufb02uences bystacking the multiple glyphs and aligning them according tothe downstream events\u2019 start times (Fig. 6 C).",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 313,
    "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data",
    "pub_year": 2022,
    "domain": "Spatial cascade",
    "requirement": {
      "requirement_text": "R5 Characterize in\ufb02uencing processes. The experts canunderstand how a downstream event depends on itsupstream and what happened between them byexamining location in\ufb02uences. The following ques-tions may be asked: Does the downstream get easilyinfected when its upstream is infected? Are the infectionsituations similar?Therefore, the system should visu-ally depict in\ufb02uencing processes.",
      "requirement_code": { "evaluate_hypothesis": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "This section presents the mining framework for inferring cascading patterns in general spatiotemporal domains.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Modeling"],
        "componenet_code": ["algorithmic_calculation", "modeling"]
      },
      {
        "solution_text": "We design the uncertainty and influence visu- alizations in the influence view to reveal location influences from the two aspects. Users must know uncertainties before the influence analysis. Thus, the uncertainty vis- ualizations are displayed by default. ",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph+matrix",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 314,
    "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data",
    "pub_year": 2022,
    "domain": "Spatial cascade",
    "requirement": {
      "requirement_text": "R6 Investigate the temporal characteristics of cascades. Cas-cades can occur many times and lead to differentdegrees of infections. The experts aim to obtain thetemporal distribution of cascades (When do the cas-cades usually happen? Winter? Rush hour?) and the cas-cading effects on the involved locations (Does thecascades last long?). Thus, temporal visualizations forcascades should be implemented in the system",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "This section presents the mining framework for inferring cascading patterns in general spatiotemporal domains.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Modeling"],
        "componenet_code": ["algorithmic_calculation", "modeling"]
      },
      {
        "solution_text": "We visualize many cascades along a long timeline and expose their temporal characteristics to users. As illustrated in C3, the temporal chart folds the timeline from top to bottom and left to right. Each cascade is represented as a bar. The positions of its upper and lower borders encode the start and end times of the cascade, respectively. The bars\u2019 occurrences are aggregated in both horizontal and vertical directions as the heatmaps above and to the right of the chart, respectively.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "matrix+bar",
        "axial_code": ["Nesting"],
        "componenet_code": ["bar", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 315,
    "paper_title": "Visual Cascade Analytics of Large-Scale Spatiotemporal Data",
    "pub_year": 2022,
    "domain": "Spatial cascade",
    "requirement": {
      "requirement_text": "R7 Unfold individual cascades. The experts tend to accessdetailed infections and obtain convincing results byunfolding the cascades. They must understand howthe contagion spreads over the locations involved in a cascade to recover the reasons behind urban deterio-ration. Questions may be asked, for instance, when didA get infected? Were B and C infected after A got infected?How long was the delay? How severe was the cascade?",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "Taxi trajectories from local authorities recorded the trajectories of 8,816 taxis between March and April 2016. Low-level roads, such as village roads, were fil_x0002_tered and resulted in 944 major road segments. We com_x0002_puted the average speed of the taxis passing through each road for every 10 minutes as the travel speed, which consti_x0002_tutes a set of spatial time series. A road segment is consid_x0002_ered congested if the travel speed did not exceed 20 km/h (see Section 4.1). Finally, 944 time series data of congestion events were extracted and fed into VisCas; An air pollution event is detected if the PM2:5 concentration is larger than 75 mgm_x0004_3 (Sec_x0002_tion 4.1). Our dataset comprises the hourly readings of the PM2:5 concentration from 482 major air monitoring stations in China during 2018. 482 infection event series of air pollu_x0002_tion were extracted and then fed into VisCas.",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "This section presents the mining framework for inferring cascading patterns in general spatiotemporal domains.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Modeling"],
        "componenet_code": ["algorithmic_calculation", "modeling"]
      },
      {
        "solution_text": "Each card presents an individual cascade in detail and mainly constitutes multiple timelines. Each one indicates how a specific contagion infects an involved location and is colored according to the location. The card header displays the start and end times of the cas- cade. The cascading card design is borrowed from the experts\u2019 hand-draft of cascades.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 317,
    "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches",
    "pub_year": 2021,
    "domain": "sport",
    "requirement": {
      "requirement_text": "G1: Steerable projection of all tactics. An overview is re-quired to facilitate the identi\ufb01cation of key tactics (R1). Gen-erally, projection methods are used to provide an overviewof massive high-dimension data. Moreover, to ef\ufb01cientlyand comprehensively identify the key tactics, projections ofall tactics from diverse perspectives are required. Therefore,the projection should be steerable for users to generate anon-demand layout.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "The summary view contains a steerable projection plot and a control panel. In the projection plot, each tactic is embedded and projected as a point. For each point, we use color hue to encode different players since color hue is the most effective visual channel for categorical attributes besides spatial position [56]. The area of a point encodes the frequency, and the opacity encodes the scoring rate of a tactic.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "scatter+glyph",
        "axial_code": ["Stack"],
        "componenet_code": ["glyph", "scatter"]
      },
      {
        "solution_text": "In the control panel, analysts can switch the projection layout by selecting the attributes they want to focus on figure. In addition, analysts can filter the tactics in the plot according to the hit player, the frequency, and the scoring rate.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 318,
    "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches",
    "pub_year": 2021,
    "domain": "sport",
    "requirement": {
      "requirement_text": "G2: Effective glyphs of tactics. An effective glyph formultivariate tactic attributes is necessary for illustrationand investigation of tactics (R2, R3). Experts are oftenoverwhelmed when investigating the multi-dimensional at-tributes of multiple tactics (Fig. 2). A glyph can help solvethis issue [38], [39], [40]. However, the glyph in previouswork [1] cannot be scaled to more attributes and are notef\ufb01cient enough when applied to analysis of massive tacticdata. Therefore, a more scalable and effective glyph is re-quired to facilitate an ef\ufb01cient and comprehensive analysis.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "We placed the shared attribute values onthe plot as hints (Fig. 3(B2)). The widget below the plot dis-plays the details of a particular tactic with glyphs (G2) (to beintroduced in the detail view). Analysts can hover over thepoints to examine this information (Fig. 3(B3)). In the controlpanel, analysts can switch the projection layout by selectingthe attributes they want to focus on (Fig. 3(C1))",
        "solution_category": "visualization",
        "solution_axial": "Annotation",
        "solution_compoent": "scatter+text",
        "axial_code": ["Annotation"],
        "componenet_code": ["scatter", "text"]
      },
      {
        "solution_text": "Analysts can hover over thepoints to examine this information (Fig. 3(B3)). In the controlpanel, analysts can switch the projection layout by selectingthe attributes they want to focus on (Fig. 3(C1))",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore,Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering", "OverviewandExplore"],
        "componenet_code": ["filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 319,
    "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches",
    "pub_year": 2021,
    "domain": "sport",
    "requirement": {
      "requirement_text": "G2: Effective glyphs of tactics. An effective glyph formultivariate tactic attributes is necessary for illustrationand investigation of tactics (R2, R3). Experts are oftenoverwhelmed when investigating the multi-dimensional at-tributes of multiple tactics (Fig. 2). A glyph can help solvethis issue [38], [39], [40]. However, the glyph in previouswork [1] cannot be scaled to more attributes and are notef\ufb01cient enough when applied to analysis of massive tacticdata. Therefore, a more scalable and effective glyph is re-quired to facilitate an ef\ufb01cient and comprehensive analysis.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "We used a novel glyph to encode a stroke and used three glyphs to represent a tactic. Analysts can control the visibility of the attributes on the glyph. The glyph is designed to be circular to cater to the metaphor of the ball.",
        "solution_category": "visualization",
        "solution_axial": "Basic",
        "solution_compoent": "glyph",
        "axial_code": ["Basic"],
        "componenet_code": ["glyph"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 320,
    "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches",
    "pub_year": 2021,
    "domain": "sport",
    "requirement": {
      "requirement_text": "G3: Comparative analysis of different tactics. Expertscan discover the differences of performances between tacticsby comparing their effects in the matches (e.g., temporalchange of scoring rates and using rates in the match)(R2). Moreover, through comparisons of particular tactic at-tributes, they can determine the reasons for the performanceof speci\ufb01c tactics (R3) through comparisons. They need toknow how a tactic differs from others in terms of technicaland contextual attributes. These differences can be used toidentify the key attributes that affect tactic performance.",
      "requirement_code": { "compare_entities": 1, "explain_differences": 1 }
    },
    "data": {
      "data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "The tactic list illustrates the most details of the selected tactics with glyphs in structure-driven placement for comparative and correlation analyses.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 321,
    "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches",
    "pub_year": 2021,
    "domain": "sport",
    "requirement": {
      "requirement_text": "G3: Comparative analysis of different tactics. Expertscan discover the differences of performances between tacticsby comparing their effects in the matches (e.g., temporalchange of scoring rates and using rates in the match)(R2). Moreover, through comparisons of particular tactic at-tributes, they can determine the reasons for the performanceof speci\ufb01c tactics (R3) through comparisons. They need toknow how a tactic differs from others in terms of technicaland contextual attributes. These differences can be used toidentify the key attributes that affect tactic performance.",
      "requirement_code": { "compare_entities": 1, "explain_differences": 1 }
    },
    "data": {
      "data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "We also provided the previous andsubsequent strokes of each tactic (Fig. 3(E1)) for correlationanalysis (G3) of the reciprocal relation between tactics ofthe two players",
        "solution_category": "visualization",
        "solution_axial": "Basic",
        "solution_compoent": "glyph",
        "axial_code": ["Basic"],
        "componenet_code": ["glyph"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 322,
    "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches",
    "pub_year": 2021,
    "domain": "sport",
    "requirement": {
      "requirement_text": "G3: Comparative analysis of different tactics. Expertscan discover the differences of performances between tacticsby comparing their effects in the matches (e.g., temporalchange of scoring rates and using rates in the match)(R2). Moreover, through comparisons of particular tactic at-tributes, they can determine the reasons for the performanceof speci\ufb01c tactics (R3) through comparisons. They need toknow how a tactic differs from others in terms of technicaland contextual attributes. These differences can be used toidentify the key attributes that affect tactic performance.",
      "requirement_code": { "compare_entities": 1, "explain_differences": 1 }
    },
    "data": {
      "data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "We provided context information to facilitate comparative and correlation analyses. The point beside each tactic encodes the frequency and scoring rate by its area and transparency, respectively. Additionally, we displayed the distribution of the frequency and scoring rate of a tactic in different conditions defined by the domain experts. The frequency and scoring rate were encoded using the height and luminance of the bar, respectively.",
        "solution_category": "visualization",
        "solution_axial": "Annotation",
        "solution_compoent": "bubble+bar",
        "axial_code": ["Annotation"],
        "componenet_code": ["bubble", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 323,
    "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches",
    "pub_year": 2021,
    "domain": "sport",
    "requirement": {
      "requirement_text": "G4: Correlation analysis of adjacent tactics. Correlationanalysis is crucial for performance characterizing and rea-soning (R2, R3). A tactic is highly related to the previousand subsequent tactics since it shares two strokes with eachof them (Fig. 2). According to the experts, the subsequentand current tactics can help assess the performance of thecurrent tactic, and the previous and current tactics can helpinterpret the performance of the current tactic (R3)",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "The tactic list illustrates the most details of the selected tactics with glyphs in structure-driven placement for comparative and correlation analyses.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 324,
    "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches",
    "pub_year": 2021,
    "domain": "sport",
    "requirement": {
      "requirement_text": "G4: Correlation analysis of adjacent tactics. Correlationanalysis is crucial for performance characterizing and rea-soning (R2, R3). A tactic is highly related to the previousand subsequent tactics since it shares two strokes with eachof them (Fig. 2). According to the experts, the subsequentand current tactics can help assess the performance of thecurrent tactic, and the previous and current tactics can helpinterpret the performance of the current tactic (R3)",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "We provided context information to facilitate comparative and correlation analyses. The point beside each tactic encodes the frequency and scoring rate by its area and transparency, respectively. Additionally, we displayed the distribution of the frequency and scoring rate of a tactic in different conditions defined by the domain experts. The frequency and scoring rate were encoded using the height and luminance of the bar, respectively.",
        "solution_category": "visualization",
        "solution_axial": "Annotation",
        "solution_compoent": "bubble+bar",
        "axial_code": ["Annotation"],
        "componenet_code": ["bubble", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 325,
    "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches",
    "pub_year": 2021,
    "domain": "sport",
    "requirement": {
      "requirement_text": "G5: Videos sessions of each tactic. Organizing the rawvideos based on the tactic data can help experts ef\ufb01cientlyexamine the video contents they need. Since the analysisobject is tactic, videos clipped based on tactics can helpquickly validate the analysis results of particular tactics (R4)",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "We provided context information to facilitate comparative and correlation analyses. The point beside each tactic encodes the frequency and scoring rate by its area and transparency, respectively. Additionally, we displayed the distribution of the frequency and scoring rate of a tactic in different conditions defined by the domain experts. The frequency and scoring rate were encoded using the height and luminance of the bar, respectively.",
        "solution_category": "visualization",
        "solution_axial": "Annotation",
        "solution_compoent": "bubble+bar",
        "axial_code": ["Annotation"],
        "componenet_code": ["bubble", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 326,
    "paper_title": "Tac-Miner: Visual Tactic Mining for Multiple Table Tennis Matches",
    "pub_year": 2021,
    "domain": "sport",
    "requirement": {
      "requirement_text": "G5: Videos sessions of each tactic. Organizing the rawvideos based on the tactic data can help experts ef\ufb01cientlyexamine the video contents they need. Since the analysisobject is tactic, videos clipped based on tactics can helpquickly validate the analysis results of particular tactics (R4)",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "Fine-grained table tennis data are manually collected from videos by the domain experts. A match is recorded by hundreds of strokes, and each stroke is depicted by approxi_x0002_mately 20 attributes. Experts divide these attributes into two categories (i.e., technical ones and contextual ones (Fig. 1)). Technical attributes describe the technical characteristics of a stroke. Contextual attributes record the match contexts of a stroke. They are vital indicators for assessing the effect of a tactic. According to the definition of \u201ctactic\u201d, we only considered the rallies containing more than two strokes.",
      "data_code": {
        "categorical": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The display view provides a replay of each tactic for validation and further investigation. Analysts can click on the button near the context information to unfold the display view, which includes the video clips and animation of a particular tactic. The display view mainly includes a rally list, a display table, and a video widget.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      },
      {
        "solution_text": "The display view provides a replay of each tactic for validation and further investigation. Analysts can click on the button near the context information to unfold the display view, which includes the video clips and animation of a particular tactic. The display view mainly includes a rally list, a display table, and a video widget.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "video+animation",
        "axial_code": ["Stack"],
        "componenet_code": ["animation", "video"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 329,
    "paper_title": "HisVA: A Visual Analytics System for Studying History",
    "pub_year": 2022,
    "domain": "education",
    "requirement": {
      "requirement_text": "Presents a reduced but effective exploration space.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "textual": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48].",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 330,
    "paper_title": "HisVA: A Visual Analytics System for Studying History",
    "pub_year": 2022,
    "domain": "education",
    "requirement": {
      "requirement_text": "Presents a reduced but effective exploration space.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "textual": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48]. Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,Excluding",
        "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.",
        "axial_code": ["Excluding", "Modeling"],
        "componenet_code": ["excluding", "modeling"]
      },
      {
        "solution_text": "The event view presents multiple event charts and a summary chart. We place twenty event charts that are con- structed based on computed topics and time information. The charts are initially ordered as produced by the modeling algorithm, but users can scroll the view, interac- tively adjust the chart order, and hide and show any event chart they choose. Each chart shows 10 topic keywords, which are ordered by their contribution to each topic.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "area+bubble",
        "axial_code": ["Repetition"],
        "componenet_code": ["bubble", "area"]
      },
      {
        "solution_text": "The charts are initially ordered as produced by the modeling algorithm, but users can scroll the view, interac- tively adjust the chart order, and hide and show any event chart they choose.",
        "solution_category": "interaction",
        "solution_axial": "Reconfigure,Filtering",
        "solution_compoent": "",
        "axial_code": ["Reconfigure", "Filtering"],
        "componenet_code": ["reconfigure", "filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 331,
    "paper_title": "HisVA: A Visual Analytics System for Studying History",
    "pub_year": 2022,
    "domain": "education",
    "requirement": {
      "requirement_text": "Presents a reduced but effective exploration space.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "textual": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48]. Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,Excluding",
        "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.",
        "axial_code": ["Excluding", "Modeling"],
        "componenet_code": ["excluding", "modeling"]
      },
      {
        "solution_text": "Summary Chart. The summary chart (a line chart at the bottom of the event view, A2) shows the aggregated number of the articles by time to let users see the number of events across time and important events in specific time ranges. For example, the summary chart shows two peaks from 1910 to 1920 and from 1940 to 1945. The peaks mean that, given the document corpus, some events in the two time ranges have the greatest number of articles related to World War I and World War II. In addition, the summary chart provides two vertical gray bars at each side which users use for filtering time ranges. If the bars are at each end, the entire data set is used for computing the number of events by time.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "bar+area",
        "axial_code": ["Stack"],
        "componenet_code": ["area", "bar"]
      },
      {
        "solution_text": "In addition, the summary chart provides two vertical gray bars at each side which users use for filtering time ranges. If the bars are at each end, the entire data set is used for computing the number of events by time.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 332,
    "paper_title": "HisVA: A Visual Analytics System for Studying History",
    "pub_year": 2022,
    "domain": "education",
    "requirement": {
      "requirement_text": "Presents a reduced but effective exploration space.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "textual": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48]. Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,Excluding",
        "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.",
        "axial_code": ["Excluding", "Modeling"],
        "componenet_code": ["excluding", "modeling"]
      },
      {
        "solution_text": "Displaying Important Articles. The events with higher importance weights (i.e., topic contribution or page-rank scores) than the threshold value are presented as a pink dot at the top of the event chart. When a series of adja- cent dots occlude each other, they are aggregated into a wide dot (i.e., ellipse). For example, the third event chart in figure shows that the topic of the chart is \u201cItalian,\u201d and there are seven important events and six wide dots which contain more than two important events.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "area+bubble",
        "axial_code": ["Repetition"],
        "componenet_code": ["bubble", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 333,
    "paper_title": "HisVA: A Visual Analytics System for Studying History",
    "pub_year": 2022,
    "domain": "education",
    "requirement": {
      "requirement_text": "Presents a reduced but effective exploration space.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.",
      "data_code": {
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "sequential": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48]. Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,Excluding",
        "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.",
        "axial_code": ["Excluding", "Modeling"],
        "componenet_code": ["excluding", "modeling"]
      },
      {
        "solution_text": "For efficient exploration, the list view allows users to sort the events by date, important weight, and topic. Events with a importance weight higher than the threshold value are highlighted in gray (e.g., \u201cSecond Balkan War\u201d in figure).",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "image+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["image", "text"]
      },
      {
        "solution_text": "For efficient exploration, the list view allows users to sort the events by date, important weight, and topic. Events with a importance weight higher than the threshold value are highlighted in gray (e.g., \u201cSecond Balkan War\u201d in figure).",
        "solution_category": "interaction",
        "solution_axial": "Reconfigure,Filtering",
        "solution_compoent": "",
        "axial_code": ["Reconfigure", "Filtering"],
        "componenet_code": ["reconfigure", "filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 334,
    "paper_title": "HisVA: A Visual Analytics System for Studying History",
    "pub_year": 2022,
    "domain": "education",
    "requirement": {
      "requirement_text": "Supplies spatial and temporal contexts of historical events.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "textual": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding",
        "solution_compoent": "Dateandlocationentityextraction.Selectsmostfrequentlyshowninformationasrepresentativetemporalandspatialdata.",
        "axial_code": ["Excluding"],
        "componenet_code": ["excluding"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 335,
    "paper_title": "HisVA: A Visual Analytics System for Studying History",
    "pub_year": 2022,
    "domain": "education",
    "requirement": {
      "requirement_text": "Supplies spatial and temporal contexts of historical events.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "textual": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48]. Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,Excluding",
        "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.",
        "axial_code": ["Excluding", "Modeling"],
        "componenet_code": ["excluding", "modeling"]
      },
      {
        "solution_text": "The event view presents multiple event charts and a summary chart. We place twenty event charts that are con- structed based on computed topics and time information. The charts are initially ordered as produced by the modeling algorithm, but users can scroll the view, interac- tively adjust the chart order, and hide and show any event chart they choose. Each chart shows 10 topic keywords, which are ordered by their contribution to each topic.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "area+bubble",
        "axial_code": ["Repetition"],
        "componenet_code": ["bubble", "area"]
      },
      {
        "solution_text": "The charts are initially ordered as produced by the modeling algorithm, but users can scroll the view, interac- tively adjust the chart order, and hide and show any event chart they choose.",
        "solution_category": "interaction",
        "solution_axial": "Reconfigure,Filtering",
        "solution_compoent": "",
        "axial_code": ["Reconfigure", "Filtering"],
        "componenet_code": ["reconfigure", "filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 336,
    "paper_title": "HisVA: A Visual Analytics System for Studying History",
    "pub_year": 2022,
    "domain": "education",
    "requirement": {
      "requirement_text": "Supplies spatial and temporal contexts of historical events.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "textual": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48]. Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,Excluding",
        "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.",
        "axial_code": ["Excluding", "Modeling"],
        "componenet_code": ["excluding", "modeling"]
      },
      {
        "solution_text": "Summary Chart. The summary chart (a line chart at the bottom of the event view, A2) shows the aggregated number of the articles by time to let users see the number of events across time and important events in specific time ranges. For example, the summary chart shows two peaks from 1910 to 1920 and from 1940 to 1945. The peaks mean that, given the document corpus, some events in the two time ranges have the greatest number of articles related to World War I and World War II. In addition, the summary chart provides two vertical gray bars at each side which users use for filtering time ranges. If the bars are at each end, the entire data set is used for computing the number of events by time.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "bar+area",
        "axial_code": ["Stack"],
        "componenet_code": ["area", "bar"]
      },
      {
        "solution_text": "In addition, the summary chart provides two vertical gray bars at each side which users use for filtering time ranges. If the bars are at each end, the entire data set is used for computing the number of events by time.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 337,
    "paper_title": "HisVA: A Visual Analytics System for Studying History",
    "pub_year": 2022,
    "domain": "education",
    "requirement": {
      "requirement_text": "Assists in the acquisition of information for investi- gation of historical events.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "textual": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "We used two methods for recommending importantarticles to help users begin their explorations (R3). The first is topic contribution, which is computed by a topic model-ing algorithm and indicates how much an article contributesto forming each topic. As a second method, we use thearticles\u2019 centrality (i.e., page-rank) in a network, computedfrom the Wikipedia clickstream. The page-rank value of anarticle indicates its popularity, because page-rank valuesare proportional to the article\u2019s number of views. HisVAprovides a toggle button for the selection between contribu-tion-based (TOPIC_REC) and popularity-based recommen-dations, as shown at the top of Fig. 2. In addition to thetoggle, there is the slider bar, which can be used to adjustthreshold values. If an article\u2019s contribution or popularityvalue is greater than the threshold value, it is recommendedas an important article.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,AlgorithmicCalculation",
        "solution_compoent": "Recommendationbytopiccontributionandcentrality.",
        "axial_code": ["AlgorithmicCalculation", "Modeling"],
        "componenet_code": ["algorithmic_calculation", "modeling"]
      },
      {
        "solution_text": "We used two methods for recommending importantarticles to help users begin their explorations (R3). The first is topic contribution, which is computed by a topic model-ing algorithm and indicates how much an article contributesto forming each topic. As a second method, we use thearticles\u2019 centrality (i.e., page-rank) in a network, computedfrom the Wikipedia clickstream. The page-rank value of anarticle indicates its popularity, because page-rank valuesare proportional to the article\u2019s number of views. HisVAprovides a toggle button for the selection between contribu-tion-based (TOPIC_REC) and popularity-based recommen-dations, as shown at the top of Fig. 2. In addition to thetoggle, there is the slider bar, which can be used to adjustthreshold values. If an article\u2019s contribution or popularityvalue is greater than the threshold value, it is recommendedas an important article.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 338,
    "paper_title": "HisVA: A Visual Analytics System for Studying History",
    "pub_year": 2022,
    "domain": "education",
    "requirement": {
      "requirement_text": "Assists in the acquisition of information for investi- gation of historical events.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.",
      "data_code": {
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "sequential": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48]. Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,Excluding",
        "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.",
        "axial_code": ["Excluding", "Modeling"],
        "componenet_code": ["excluding", "modeling"]
      },
      {
        "solution_text": "To help users efficiently investigate historical events with detailed information, HisVA provides a resource view comprising four sub-views\u2014list, result, article, and annotation views. The list view showcases historical events, allowing users to sequentially access a series of events, along with meta information, such as a thumbnail, event date, topic number associated with the article, topic weight, and page- rank value. ",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "image+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["image", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 339,
    "paper_title": "HisVA: A Visual Analytics System for Studying History",
    "pub_year": 2022,
    "domain": "education",
    "requirement": {
      "requirement_text": "Promotes linking diverse events and finding rela- tionships among them.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.",
      "data_code": {
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "sequential": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "To provide an overview of the historical article collection(R1), we used the topics produced by topic modeling algo-rithms. To find an appropriate model, we tested topicmodeling methods with various topic counts (e.g., 10\u201350topics), including LSI (Latent Semantic Indexing) [44], HDP(Hierarchical Dirichlet Process) [45], LDA (Latent DirichletAllocation) [46], and LDA Mallet [47]. After reviewing themodeling results, we decided to use LDA Mallet with 20topics, due to its highest coherence score (0.47), which meas-ures the similarity of the words in each topic [48]. We usedthe topic modeling algorithms implemented in Gensim [49]and the coherence model to measure the coherence scorebased on normalized point-wise mutual information(NPMI) and the cosine similarity (called coherence CV) [48]. Next, we extracted date and location entities from eacharticle and performed pre-processing to provide users withthe spatial and temporal contexts of an event (R2). For theextraction, we used the 7-class model Stanford NamedEntity Recognizer (SNER) [41], trained on the MessageUnderstanding Conference (MUC) 6 and 7 [42] trainingdata sets. We then counted the number of dates and loca-tions associated with each event and set those with the mostfrequently shown information as the representative tempo-ral and spatial information of each event. For example,\u201cGermany\u201d and \u201cMarch 1945\u201d are mentioned 98 and 6times, respectively, in the article \u201cWorld War II,\u201d so weused them to represent the location and date of \u201cWorld WarII.\u201d We extracted geocoordinates (i.e., latitude and longi-tude) of the locations using Geopy [43]",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,Excluding",
        "solution_compoent": "OverviewofhistoricalarticlecollectionbyutilizestopicmodelingalgorithmsLDAMallet.",
        "axial_code": ["Excluding", "Modeling"],
        "componenet_code": ["excluding", "modeling"]
      },
      {
        "solution_text": "The result view lists articles in response to the user\u2019s search in the search bar. We present articles with the linear structure for easy content navigation in studying [60]. When users click on an event, a new window pops up and presents the clicked event with other events that are most relevant to the clicked event.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "image+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["image", "text"]
      },
      {
        "solution_text": "The result view lists articles in response to the user\u2019s search in the search bar. We present articles with the linear structure for easy content navigation in studying [60]. When users click on an event, a new window pops up and presents the clicked event with other events that are most relevant to the clicked event.",
        "solution_category": "interaction",
        "solution_axial": "Extractionoffeatures",
        "solution_compoent": "",
        "axial_code": ["Extractionoffeatures"],
        "componenet_code": ["extraction_of_features"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 340,
    "paper_title": "HisVA: A Visual Analytics System for Studying History",
    "pub_year": 2022,
    "domain": "education",
    "requirement": {
      "requirement_text": "Promotes linking diverse events and finding rela- tionships among them.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "We initially collected 5,467 Wikipedia articles, of which 3,019 marked as \u201cevents\u201d were used.",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "textual": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "To facilitate the task of linking events and finding rela- tionships among them, HisVA utilizes CMVs [11] with quick and easy interactions. For example, when a time range is set in the summary chart, the  historical events in the user-specified time range are pre- sented in all map, article, and event views. When the event\u2019s importance or frequency weight is changed due to user interactions with the filters, the visu- alization and articles in each view are also updated. In the same context, when users select regions by drawing a box or clicking countries in the map view, visualiza- tions and articles in other views are updated accord- ingly. When users select an event from a cluster marker, the article view is auto-scrolled to present the part of the article associated with the selected event. When users hover the mouse over an article in the list view, the cen- ter of the map view is moved to the location associated with the selected article and the cluster marker associ- ated with the selected article changes to the spiral repre- sentation with the clicked article highlighted in red.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 341,
    "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison",
    "pub_year": 2022,
    "domain": "anomaly detection",
    "requirement": {
      "requirement_text": "R1. Provide Inspection on the Possible Anomalies From Unla-beled Datasets. Real-world event sequence datasets often con-tain a large number of unlabeled sequences. Users oftenneed to narrow the inspection scope to a smaller group ofsequences that require attention. For example, the medicalexperts commented that they typically filter out problematicpatients based on specific criteria before drilling intodetailed clinical events",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.",
      "data_code": { "tables": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2); After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z; To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal; To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,SimilarityCalculation,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "Modeling",
          "SimilarityCalculation"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "modeling",
          "similarity_calculation"
        ]
      },
      {
        "solution_text": "The analysis starts from the anomaly overview, which provides an MDS projection of the latent vectors for all anomalous sequences in the dataset and allows users to select an anoma- lous sequence for subsequent analysis.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "contour+scatter",
        "axial_code": ["Coordinate"],
        "componenet_code": ["scatter", "contour"]
      },
      {
        "solution_text": "The analysis starts from the anomaly overview, which provides an MDS projection of the latent vectors for all anomalous sequences in the dataset and allows users to select an anoma- lous sequence for subsequent analysis.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 342,
    "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison",
    "pub_year": 2022,
    "domain": "anomaly detection",
    "requirement": {
      "requirement_text": "R1. Provide Inspection on the Possible Anomalies From Unla-beled Datasets. Real-world event sequence datasets often con-tain a large number of unlabeled sequences. Users oftenneed to narrow the inspection scope to a smaller group ofsequences that require attention. For example, the medicalexperts commented that they typically filter out problematicpatients based on specific criteria before drilling intodetailed clinical events",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.",
      "data_code": { "tables": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "Latentvectorsforanomaloussequencesdetection.",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 343,
    "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison",
    "pub_year": 2022,
    "domain": "anomaly detection",
    "requirement": {
      "requirement_text": "R1. Provide Inspection on the Possible Anomalies From Unla-beled Datasets. Real-world event sequence datasets often con-tain a large number of unlabeled sequences. Users oftenneed to narrow the inspection scope to a smaller group ofsequences that require attention. For example, the medicalexperts commented that they typically filter out problematicpatients based on specific criteria before drilling intodetailed clinical events",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.",
      "data_code": { "tables": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2)",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "Sequence-to-SequenceVAEforinterpretableanomalydetection.",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 344,
    "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison",
    "pub_year": 2022,
    "domain": "anomaly detection",
    "requirement": {
      "requirement_text": "R2. Facilitate Interpretation by Identifying Anomalous EventsWithin Abnormal Sequences. The interpretability of the anom-alous sequences highly relies on the analysis of low-levelevents. For example, the clinical path of a patient may bedetected as an anomaly due to a misused medicine, andsoftware may be considered suspicious due to abnormal \ufb01leexecutions. However, real-world event sequences can belong in length and heterogeneous in types, which makes itchallenging to identify anomalous events",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.",
      "data_code": { "tables": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2); After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z; To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal; To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,SimilarityCalculation,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "Modeling",
          "SimilarityCalculation"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "modeling",
          "similarity_calculation"
        ]
      },
      {
        "solution_text": "The main panel supports visual interpretation of the selected anomalous sequence via sequence comparison. Specifically, the main view is vertically divided into three major parts, including an anomalous sequence view showing the progression of the selected anomalous sequence, with the type of abnormality being marked out on anoma- lous events.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "bar+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 345,
    "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison",
    "pub_year": 2022,
    "domain": "anomaly detection",
    "requirement": {
      "requirement_text": "R2. Facilitate Interpretation by Identifying Anomalous EventsWithin Abnormal Sequences. The interpretability of the anom-alous sequences highly relies on the analysis of low-levelevents. For example, the clinical path of a patient may bedetected as an anomaly due to a misused medicine, andsoftware may be considered suspicious due to abnormal \ufb01leexecutions. However, real-world event sequences can belong in length and heterogeneous in types, which makes itchallenging to identify anomalous events",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.",
      "data_code": { "tables": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2)",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "Sequence-to-SequenceVAEforinterpretableanomalydetection.",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 346,
    "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison",
    "pub_year": 2022,
    "domain": "anomaly detection",
    "requirement": {
      "requirement_text": "R2. Facilitate Interpretation by Identifying Anomalous EventsWithin Abnormal Sequences. The interpretability of the anom-alous sequences highly relies on the analysis of low-levelevents. For example, the clinical path of a patient may bedetected as an anomaly due to a misused medicine, andsoftware may be considered suspicious due to abnormal \ufb01leexecutions. However, real-world event sequences can belong in length and heterogeneous in types, which makes itchallenging to identify anomalous events",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.",
      "data_code": { "tables": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "Interpretingsequenceanomalieswithreconstructionprobabilities.",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 347,
    "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison",
    "pub_year": 2022,
    "domain": "anomaly detection",
    "requirement": {
      "requirement_text": "R3. Support Anomaly Analysis in the Context of EntireSequence Progressions. Instead of focusing on the anomalousevents, analyzing anomalies within the context of entiresequence progressions can help illustrate the cause and consequences of the anomalous events. Speci\ufb01cally, themedical experts stated that early prevention could beachieved if we know what led to the occurrence of ananomaly.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.",
      "data_code": { "tables": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2); After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z; To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal; To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,SimilarityCalculation,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "Modeling",
          "SimilarityCalculation"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "modeling",
          "similarity_calculation"
        ]
      },
      {
        "solution_text": "The main panel supports visual interpretation of the selected anomalous sequence via sequence comparison. Specifically, the main view is vertically divided into three major parts, including an anomalous sequence view showing the progression of the selected anomalous sequence, with the type of abnormality being marked out on anoma- lous events.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "bar+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 348,
    "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison",
    "pub_year": 2022,
    "domain": "anomaly detection",
    "requirement": {
      "requirement_text": "R3. Support Anomaly Analysis in the Context of EntireSequence Progressions. Instead of focusing on the anomalousevents, analyzing anomalies within the context of entiresequence progressions can help illustrate the cause and consequences of the anomalous events. Speci\ufb01cally, themedical experts stated that early prevention could beachieved if we know what led to the occurrence of ananomaly.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.",
      "data_code": { "tables": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2); After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z; To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal; To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,SimilarityCalculation,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "Modeling",
          "SimilarityCalculation"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "modeling",
          "similarity_calculation"
        ]
      },
      {
        "solution_text": "The selected anomalous sequence is organized intosequence segments aligning with the time slots (Fig. 4 (2)).Events in adjacent time slots are separated by a label of thesequence ID. In addition, events in each time slot are verti-cally aligned with events in the reconstruction view to helpidentify the event labels.We emphasize the event anomalies in the anomaloussequence with a set of glyphs (Figs. 4a, 4b, and 4c) repre-senting the three anomaly types. The glyphs are designedbased on the metaphor of editing symbols, intending to con-vey insights on operations that are required to transform ananomalous sequence into normal. Speci\ufb01cally, we representevent redundancy with a delete symbol (Fig. 4a), event miss-ing with an insert symbol (Fig. 4b), and temporal anomalywith a move arrow (Fig. 4c) pointing from the observed timeslot to the expected time slot. Event missing and the end-point of the temporal anomaly is encoded using a whiterectangular node with a dashed border, indicating that anevent is expected to occur but is not present.Additionally, we retrieve the subgroup of normalsequences for each type of event anomaly that \u201csupport\u201dthe corresponding event to be abnormal as the comparisongroup. For example, in Fig. 3, the comparison group for theevent redundancy in the \ufb01rst slot (\u201c#0\u201d) shall be the top twoclusters in the normal sequence view below where the event isalso not presented. We summarize the abnormality ofevents analyzed by matching the anomalous sequence to allselected normal sequences and integrating all anomalytypes identi\ufb01ed for each event. By default, we select thedominant type of anomaly that has the largest comparisongroup for display. However, users can change their focusby selecting a different subgroup of normal sequences dur-ing analysis for comparison. We calculate a support rate asthe proportion of the selected normal sequences in the com-parison group, and the anomaly score as the average match-ing cost for each event anomaly to help users justify thelevel and the con\ufb01dence of abnormality. The anomaly scoreis displayed with the height of a red peak above, and thesupport rate is encoded with the height of a blue peak underthe anomalous event",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "bar+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "bar"]
      },
      {
        "solution_text": "However, users can change their focusby selecting a different subgroup of normal sequences dur-ing analysis for comparison. ",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 349,
    "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison",
    "pub_year": 2022,
    "domain": "anomaly detection",
    "requirement": {
      "requirement_text": "R4. Allow Case-Based Reasoning to Gain User Trust and HelpExplore Higher-Level Anomalous Patterns. The lack of explain-ability in deep learning models inhibits user trust in theanalysis result. Recent studies tackled this issue throughcase-based reasoning [47], which generates explanationsbased on similar cases in the dataset. Medical expertsexpressed similar interests in following treatment plansunder normal and abnormal circumstances to understandhow the anomalous patient deviates from typical cases.Moreover, comparing normal and abnormal sequences canreveal higher-level anomalous patterns (e.g., anomaloussub-sequences or event ordering) beyond low-level ones.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.",
      "data_code": { "tables": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2); After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z; To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal; To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,SimilarityCalculation,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "Modeling",
          "SimilarityCalculation"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "modeling",
          "similarity_calculation"
        ]
      },
      {
        "solution_text": "The similarity view (Fig. 3 (2)) displays the distribution of all normalsequences and their similarities to the selected anomaloussequence, which is derived from the distance of correspond-ing latent vectors. In particular, we measure the sequencesimilarity in two ways: their cost for matching events in thenormal sequences to the selected anomalous sequence (notedas matching cost) and the distance between the latent vectorsof normal sequences and the anomalous sequence (noted assequence distance). Sequences with small matching costs gen-erally have more similar events, while sequences with smalldistances to the anomalous sequence usually contain keyprogression patterns, such as an indicator of a particulartype of disease. From this view, users can switch betweendifferent similarity measurements to inspect different distri-butions and select a group of normal sequences to comparewith the anomalous sequence in the main panel (R4).",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      },
      {
        "solution_text": "From this view, users can switch betweendifferent similarity measurements to inspect different distri-butions and select a group of normal sequences to comparewith the anomalous sequence in the main panel (R4).",
        "solution_category": "interaction",
        "solution_axial": "Filtering,Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting", "Filtering"],
        "componenet_code": ["selecting", "filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 350,
    "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison",
    "pub_year": 2022,
    "domain": "anomaly detection",
    "requirement": {
      "requirement_text": "R4. Allow Case-Based Reasoning to Gain User Trust and HelpExplore Higher-Level Anomalous Patterns. The lack of explain-ability in deep learning models inhibits user trust in theanalysis result. Recent studies tackled this issue throughcase-based reasoning [47], which generates explanationsbased on similar cases in the dataset. Medical expertsexpressed similar interests in following treatment plansunder normal and abnormal circumstances to understandhow the anomalous patient deviates from typical cases.Moreover, comparing normal and abnormal sequences canreveal higher-level anomalous patterns (e.g., anomaloussub-sequences or event ordering) beyond low-level ones.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.",
      "data_code": { "tables": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal.",
        "solution_category": "data_manipulation",
        "solution_axial": "SimilarityCalculation",
        "solution_compoent": "Comparinganomaloussequenceswithnormalonesusingasequencecomparisonmethod.",
        "axial_code": ["SimilarityCalculation"],
        "componenet_code": ["similarity_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 351,
    "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison",
    "pub_year": 2022,
    "domain": "anomaly detection",
    "requirement": {
      "requirement_text": "R4. Allow Case-Based Reasoning to Gain User Trust and HelpExplore Higher-Level Anomalous Patterns. The lack of explain-ability in deep learning models inhibits user trust in theanalysis result. Recent studies tackled this issue throughcase-based reasoning [47], which generates explanationsbased on similar cases in the dataset. Medical expertsexpressed similar interests in following treatment plansunder normal and abnormal circumstances to understandhow the anomalous patient deviates from typical cases.Moreover, comparing normal and abnormal sequences canreveal higher-level anomalous patterns (e.g., anomaloussub-sequences or event ordering) beyond low-level ones.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.",
      "data_code": { "tables": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2); After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z; To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal; To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,SimilarityCalculation,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "Modeling",
          "SimilarityCalculation"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "modeling",
          "similarity_calculation"
        ]
      },
      {
        "solution_text": "The main panel supports visual interpretation of the selected anomalous sequence via sequence comparison. Specifically, the main view is vertically divided into three major parts, including a normal sequence view summarizing the progression of similar normal sequences that are selected by the user from the similarity view.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "network+bar",
        "axial_code": ["Nesting"],
        "componenet_code": ["network", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 352,
    "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison",
    "pub_year": 2022,
    "domain": "anomaly detection",
    "requirement": {
      "requirement_text": "R4. Allow Case-Based Reasoning to Gain User Trust and HelpExplore Higher-Level Anomalous Patterns. The lack of explain-ability in deep learning models inhibits user trust in theanalysis result. Recent studies tackled this issue throughcase-based reasoning [47], which generates explanationsbased on similar cases in the dataset. Medical expertsexpressed similar interests in following treatment plansunder normal and abnormal circumstances to understandhow the anomalous patient deviates from typical cases.Moreover, comparing normal and abnormal sequences canreveal higher-level anomalous patterns (e.g., anomaloussub-sequences or event ordering) beyond low-level ones.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.",
      "data_code": { "tables": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2); After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z; To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal; To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,SimilarityCalculation,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "Modeling",
          "SimilarityCalculation"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "modeling",
          "similarity_calculation"
        ]
      },
      {
        "solution_text": "The selected normal sequences are displayed in the normalsequence view (Fig. 3 (5)). To allow comparative analysis atdifferent granularity of sequence aggregation (R5), the nor-mal sequence view is designed to support two visualizationmodes: the sequence mode (Fig. 3 (5B)) for preserving theindividual details of the normal sequences and the clustermode (Fig. 3 (5A)) for enabling inspecting the progressionpaths of a larger number of normal sequences at a time.Users can switch between two visualization mode using thebutton at the top of the main panel (Fig. 3c).Sequence Mode. The sequence mode of the normal sequenceview displays the sequences of normal records individually,aiming to support sequence-to-sequence comparison andef\ufb01cient access to the raw data of normal sequences. Asshown in Fig. 3 (5B), the normal sequences are displayed ina scrollable list. The encoding schema of each individualsequence is kept consistent with the anomalous sequencefor easy comparison. Users can select any individualsequence to compare the anomalous sequence with, and theevent anomalies marked in the anomalous sequence view shallbe updated accordingly. The sequences are ranked from topto bottom with their gradually increasing matching cost orsequence distances to the anomalous sequence in the latentspace, depending on the metric utilized in the selection ofnormal sequences in the similarity view (Fig. 3a). Analystscan focus their comparison to the \ufb01rst few sequences toinvestigate the minimum effort of turning the anomaloussequence into normal, or sequences with similar progres-sion context in order to avoid introducing noisy event com-parison results.Cluster Mode. The cluster mode of the normal sequenceview summarizes the progression of normal sequences intoa \ufb02ow-based visualization by clustering sequence segmentsin each time slot. In particular, the sequence segments ineach slot are clustered using Mean Shift Clustering [55]based on the multi-hot vectors introduced in Section 4. Notethat we include the segments in the anomalous sequence togenerate the clusters, and take the anomalous sequenceaway from the corresponding clusters when displayingonly the normal sequences.The visualization is designed to support comparing theanomalous sequence with subgroups of normal sequencesthat have particular progression patterns. Each cluster isrepresented with a rectangular node (Fig. 4d), consistingof a left-side label displaying the number of sequencesclustered in each node, and the main content depicting theevent occurrence of the clustered sequence segments. Notethat the text in the left-side label is rotated to distinguishfrom the sequence id annotated in the presentation of indi-vidual sequences. The occurrence of each event is encodedwith a vertical bar (Fig. 4e), horizontally aligned with thesame type of event in the reconstruction view and the anoma-lous sequence view. The height of each bar is proportional tothe number of sequences in each cluster having the corre-sponding event occurrence. The color encoding is consis-tent with other views showing the occurrence probability.We set the vertical position of cluster nodes in each timeslot using a layout algorithm introduced in [56] to illus-trate the similarity between clusters and minimize linkcrossing intuitively. Cluster nodes with similar eventoccurrences are grouped together, illustrating a higher-level structure of sequence progression. Cluster nodes inadjacent time slots are connected with light grey links(Fig. 4f) to uncover the transition patterns of sequencesbetween clusters.The cluster nodes can be expanded to show speci\ufb01csequence segments within and contracted back to a summa-rized cluster node in response to a double click (as shown inFig. 3h). This design aims to help users decide the granular-ity of analysis with more \ufb02exibility. The system also sup-ports clustering analysis with the selected anomaloussequence incorporated to help analysts overview how theprogression of anomalous record deviates from the normalgroup. By switching on the overlay button (Fig. 3d), theanomalous sequence segments are added back to the clus-ters. The cluster nodes and transition links that the anoma-lous sequence progress through are highlighted in red (asshown in Fig. 6 (1)) so that users can quickly identify prob-lematic time slots in which the anomalous sequence fall intothe clusters with a small population",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "network+bar",
        "axial_code": ["Nesting"],
        "componenet_code": ["network", "bar"]
      },
      {
        "solution_text": "This design aims to help users decide the granular-ity of analysis with more \ufb02exibility. The system also sup-ports clustering analysis with the selected anomaloussequence incorporated to help analysts overview how theprogression of anomalous record deviates from the normalgroup. By switching on the overlay button (Fig. 3d), theanomalous sequence segments are added back to the clus-ters. The cluster nodes and transition links that the anoma-lous sequence progress through are highlighted in red (asshown in Fig. 6 (1)) so that users can quickly identify prob-lematic time slots in which the anomalous sequence fall intothe clusters with a small population",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 353,
    "paper_title": "Interpretable Anomaly Detection in Event Sequences via Sequence Matching and Visual Comparison",
    "pub_year": 2022,
    "domain": "anomaly detection",
    "requirement": {
      "requirement_text": "R5. Provide Multi-Level Aggregation and Comparison toExplore the Full Hierarchy and Interpret Various Sequence Anal-ysis Results. Applying different levels of aggregation for agroup of sequences can result in distinct interpretations ofthe result. For example, the anomalous events detected bycomparing an anomalous sequence with an individual nor-mal sequence may be different from the result when com-paring with a subgroup.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "MIMIC [64], a publicly accessible critical care database with de-identified electronic health records for 46,520 patients with 12,487 event types; The dataset contains ten types of mile_x0002_stone events of 40 university professors, such as receiving degrees, publishing papers, and changing academic posi_x0002_tions.",
      "data_code": { "tables": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "To address these challenges, we adapted a Sequence-to-Sequence VAE to detect anomalies in event sequences inter-pretably. In particular, we leverage the merits of deep neu-ral networks in learning complex sequential patterns toaddress the \ufb01rst challenge and the probabilistic foundationof VAE in capturing data variability to solve the second.Finally, we employ the reconstruction probabilities output from the VAE to facilitate the interpretation of the anoma-lous sequences. As shown in Fig. 2, the algorithm consistsof four major steps. In the \ufb01rst step, we train a VAE-basedmodel to extract low-dimensional feature representations(i.e., the latent vector zz) to characterize the progression ofeach input sequence. The second step employs the latentvectors to measure the outlierness for each sequence basedon their Local Outlier Factor (LOF), which is then used toidentify anomalous sequences (R1). The latent vectors arefed to the decoder of the VAE model for sequence recon-struction in the third step, which recovers the expectedprobabilities for each event in each time slot of the inputsequence. In the \ufb01nal step, the anomalous sequence ismatched with normal sequence utilizing a matching metricbased on the event probabilities derived from sequencereconstruction so as to detect event anomalies (R2); After training the model, we employ the latent vector zz ofeach input sequence to detect anomalous sequences in thedataset (R1). Although prior VAE-based anomaly detectionmethods typically use reconstruction probabilities as thedetection metric [27], [28], they mostly focus on detectinganomalous data points in time-series. As stated in Sec-tion 4.2, the reconstruction loss is trained to maximize thelikelihood of individual events, which may fail to representthe overall sequence progression. In contrast, the latent vec-tors are trained to learn a feature for each event sequence inthe context of the entire sequence dataset, and can be bettersuited for identifying anomalous sequences from thesequence dataset. Therefore, we employ the Local OutlierFactor (LOF) [51] to evaluate the outlierness of eachsequence in the latent space using the latent vector z; To support detecting anomalous events in a more inter-pretable manner, we further incorporate the progression ofnormal sequences (R4) into our analytical context. Speci\ufb01-cally, we compare each sequence with a group of normalsequences that are close to the anomalous sequence in thelatent space to investigate their differences. Wongsuphasa-wat et al. [54] introduced a sequence comparison methodthat aligns two sequences by events and quanti\ufb01es eventdifferences with a Match & Mismatch Measure. However,this measurement treats all types of mismatched eventsequally, which may not accord with the real-world situa-tion, because not all events that occur in normal sequencesshould appear in the anomalous sequence. For example, inthe medical scenario, patients in the normal group may bediagnosed with a certain complication that does not appearin the anomalous sequence. The complication, however, isnot likely to occur under the progression context (i.e., previ-ous lab test events or treatments) of the anomaloussequence, and should not be considered abnormal; To facilitate the interpretation of sequence anomalies, we further identify anomalous events that contribute tosequence abnormality (R2). As mentioned earlier, the recon-struction probabilities are restored from the latent vector zzthat is sampled from the the latent space where the majorityof the sequences are normal, and the training objectiveensures that the reconstruction probabilities are also similarto the original input sequence. Therefore, the reconstructionprobabilities of the anomalous sequences can be used toinfer an expected occurrence likelihood of events in typicalcases, where xx0i represents the expected occurrence likeli-hood of all events in the ith time slot. Intuitively, we canconsider events that violate their expected occurrence prob-abilities as abnormal. For example, if an event has a highlikelihood of occurrence but is not presented in the anoma-lous sequence, it is likely to be an event missing anomaly.On the contrary, if an event has a low likelihood of occur-rence but appears in the anomalous sequence, it is likely tobe an event redundancy anomaly. However, the reconstruc-tion probabilities are not always reliable as the valueshighly rely on the precedent occurrence of events. For exam-ple, the reconstruction probabilities of all events in the timesteps at the beginning of the sequence are generally verylow, due to the lack of progression context from precedentevents. In addition, the model provides little explanation onhow the probabilities are estimated, and users may find itdifficult to apply the probabilities for determining theboundary of the anomaly.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,SimilarityCalculation,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "Modeling",
          "SimilarityCalculation"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "modeling",
          "similarity_calculation"
        ]
      },
      {
        "solution_text": "The selected normal sequences are displayed in the normalsequence view (Fig. 3 (5)). To allow comparative analysis atdifferent granularity of sequence aggregation (R5), the nor-mal sequence view is designed to support two visualizationmodes: the sequence mode (Fig. 3 (5B)) for preserving theindividual details of the normal sequences and the clustermode (Fig. 3 (5A)) for enabling inspecting the progressionpaths of a larger number of normal sequences at a time.Users can switch between two visualization mode using thebutton at the top of the main panel (Fig. 3c).Sequence Mode. The sequence mode of the normal sequenceview displays the sequences of normal records individually,aiming to support sequence-to-sequence comparison andef\ufb01cient access to the raw data of normal sequences. Asshown in Fig. 3 (5B), the normal sequences are displayed ina scrollable list. The encoding schema of each individualsequence is kept consistent with the anomalous sequencefor easy comparison. Users can select any individualsequence to compare the anomalous sequence with, and theevent anomalies marked in the anomalous sequence view shallbe updated accordingly. The sequences are ranked from topto bottom with their gradually increasing matching cost orsequence distances to the anomalous sequence in the latentspace, depending on the metric utilized in the selection ofnormal sequences in the similarity view (Fig. 3a). Analystscan focus their comparison to the \ufb01rst few sequences toinvestigate the minimum effort of turning the anomaloussequence into normal, or sequences with similar progres-sion context in order to avoid introducing noisy event com-parison results.Cluster Mode. The cluster mode of the normal sequenceview summarizes the progression of normal sequences intoa \ufb02ow-based visualization by clustering sequence segmentsin each time slot. In particular, the sequence segments ineach slot are clustered using Mean Shift Clustering [55]based on the multi-hot vectors introduced in Section 4. Notethat we include the segments in the anomalous sequence togenerate the clusters, and take the anomalous sequenceaway from the corresponding clusters when displayingonly the normal sequences.The visualization is designed to support comparing theanomalous sequence with subgroups of normal sequencesthat have particular progression patterns. Each cluster isrepresented with a rectangular node (Fig. 4d), consistingof a left-side label displaying the number of sequencesclustered in each node, and the main content depicting theevent occurrence of the clustered sequence segments. Notethat the text in the left-side label is rotated to distinguishfrom the sequence id annotated in the presentation of indi-vidual sequences. The occurrence of each event is encodedwith a vertical bar (Fig. 4e), horizontally aligned with thesame type of event in the reconstruction view and the anoma-lous sequence view. The height of each bar is proportional tothe number of sequences in each cluster having the corre-sponding event occurrence. The color encoding is consis-tent with other views showing the occurrence probability.We set the vertical position of cluster nodes in each timeslot using a layout algorithm introduced in [56] to illus-trate the similarity between clusters and minimize linkcrossing intuitively. Cluster nodes with similar eventoccurrences are grouped together, illustrating a higher-level structure of sequence progression. Cluster nodes inadjacent time slots are connected with light grey links(Fig. 4f) to uncover the transition patterns of sequencesbetween clusters.The cluster nodes can be expanded to show speci\ufb01csequence segments within and contracted back to a summa-rized cluster node in response to a double click (as shown inFig. 3h). This design aims to help users decide the granular-ity of analysis with more \ufb02exibility. The system also sup-ports clustering analysis with the selected anomaloussequence incorporated to help analysts overview how theprogression of anomalous record deviates from the normalgroup. By switching on the overlay button (Fig. 3d), theanomalous sequence segments are added back to the clus-ters. The cluster nodes and transition links that the anoma-lous sequence progress through are highlighted in red (asshown in Fig. 6 (1)) so that users can quickly identify prob-lematic time slots in which the anomalous sequence fall intothe clusters with a small population",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "network+bar",
        "axial_code": ["Nesting"],
        "componenet_code": ["network", "bar"]
      },
      {
        "solution_text": "This design aims to help users decide the granular-ity of analysis with more \ufb02exibility. The system also sup-ports clustering analysis with the selected anomaloussequence incorporated to help analysts overview how theprogression of anomalous record deviates from the normalgroup. By switching on the overlay button (Fig. 3d), theanomalous sequence segments are added back to the clus-ters. The cluster nodes and transition links that the anoma-lous sequence progress through are highlighted in red (asshown in Fig. 6 (1)) so that users can quickly identify prob-lematic time slots in which the anomalous sequence fall intothe clusters with a small population",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 354,
    "paper_title": "DeHumor: Visual Analytics for Decomposing Humor",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "R1: Analyze Text and Audio Simultaneously to Reveal TheirCorrelations. Our experts confirmed that both speech contentand vocal delivery are considered necessary for a humorouseffect. It is difficult to capture both of them by watching thevideos. Therefore, at each level, the system needs to presenttextual and audio features concurrently to help users reasonabout the effective use of words and voice.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We process the collected data such that (1) the text script and audio are aligned to support multimodal analysis, and (2) the full speech data is segmented into humor snippets to support context and sentence-level analy_x0002_sis. To achieve the alignment, we first detect each word\u2019s starting time and ending time in the transcript using P2FA [45]. Thereafter, we align the audio and text modality together at the word level. As for humor snippet segmenta_x0002_tion, we regard a sentence immediately before a laughter marker as a punchline (i.e., the most important sentence that triggers the audience response). We treat all of the sentences between two punchlines as the candidate context paragraph for the second punchline. The intuition is that all the infor_x0002_mation that occurs after a punchline are potentially useful for building up the next punchline. More concrete context recognition (shown in Section 5.1.3) should come from these candidate sentences. As a result, we split the transcripts at laughter markers. Each resulting humor snippet contains exactly one punchline (i.e., the last sentence of the segment) and its contexts (all the preceding sentences). The audio is clipped correspondingly through the starting and ending times of the sentences. Eventually, we organize the raw speech data into aligned audio and transcripts per snippet, per sentence, and per word; We compute and encode three types of semantic features at the sentence level: incongruity, sentiment, and phonetics. For each feature, a meaningful threshold is used to iden- tify important words or phrases in the sentence, which are annotated with intuitive glyphs. These thresholds can be interactively adjusted by users according to the feature distribution in figure.",
        "solution_category": "data_manipulation",
        "solution_axial": "Explainability,Excluding,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["Explainability", "Excluding", "AlgorithmicCalculation"],
        "componenet_code": [
          "explainability",
          "excluding",
          "algorithmic_calculation"
        ]
      },
      {
        "solution_text": "Besides sentence- and context-level, we design an aug- mented time matrix that provides an overview of distribution of humor occurrences and the related features of speech content and vocal delivery at the speech level.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "matrix+bar",
        "axial_code": ["Stack"],
        "componenet_code": ["bar", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 355,
    "paper_title": "DeHumor: Visual Analytics for Decomposing Humor",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "R2: Visualize a Speech Level Overview That Shows Vocal andVerbal Styles of Humor, as well as Their Distribution. At thespeech level, the system should summarize the timing ofhumor-related properties\u2014the humor is injected how fre-quently, under what condition (Or, what topic \ufb02ow), to which part of the speech, and with what verbal and vocalstyles. The visual summary of these properties serves asguidance and should help users \ufb01nd speci\ufb01c humor snip-pets within a speech. For example, a communication coachmight prioritize the very \ufb01rst humorous punchline (when),to show students how to provide an impressive opening(objective) by making small talks or sharing personal lives(e.g., \u201cMy brother\u2019s in prison.\u201d). Besides, as suggested by theexperts, the visual summary of the humor distributionshould be integrated with temporal information, along withthe topic \ufb02ow and verbal feature statistics.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We process the collected data such that (1) the text script and audio are aligned to support multimodal analysis, and (2) the full speech data is segmented into humor snippets to support context and sentence-level analy_x0002_sis. To achieve the alignment, we first detect each word\u2019s starting time and ending time in the transcript using P2FA [45]. Thereafter, we align the audio and text modality together at the word level. As for humor snippet segmenta_x0002_tion, we regard a sentence immediately before a laughter marker as a punchline (i.e., the most important sentence that triggers the audience response). We treat all of the sentences between two punchlines as the candidate context paragraph for the second punchline. The intuition is that all the infor_x0002_mation that occurs after a punchline are potentially useful for building up the next punchline. More concrete context recognition (shown in Section 5.1.3) should come from these candidate sentences. As a result, we split the transcripts at laughter markers. Each resulting humor snippet contains exactly one punchline (i.e., the last sentence of the segment) and its contexts (all the preceding sentences). The audio is clipped correspondingly through the starting and ending times of the sentences. Eventually, we organize the raw speech data into aligned audio and transcripts per snippet, per sentence, and per word; We compute and encode three types of semantic features at the sentence level: incongruity, sentiment, and phonetics. For each feature, a meaningful threshold is used to iden- tify important words or phrases in the sentence, which are annotated with intuitive glyphs. These thresholds can be interactively adjusted by users according to the feature distribution in figure.",
        "solution_category": "data_manipulation",
        "solution_axial": "Explainability,Excluding,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["Explainability", "Excluding", "AlgorithmicCalculation"],
        "componenet_code": [
          "explainability",
          "excluding",
          "algorithmic_calculation"
        ]
      },
      {
        "solution_text": "Visualizing Contextual Repetitions: We design a context linking graph to display the extracted inter-sentence repetition occurrences. As shown in figure, the graph follows a three- stage design, such that it gradually reveals the concrete con- text information to the user and traverse from the context- level to the sentence-level.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "text",
        "axial_code": ["Repetition"],
        "componenet_code": ["text"]
      },
      {
        "solution_text": "Visualizing Contextual Repetitions: We design a context linking graph to display the extracted inter-sentence repetition occurrences. As shown in figure, the graph follows a three- stage design, such that it gradually reveals the concrete con- text information to the user and traverse from the context- level to the sentence-level.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 356,
    "paper_title": "DeHumor: Visual Analytics for Decomposing Humor",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "R2: Visualize a Speech Level Overview That Shows Vocal andVerbal Styles of Humor, as well as Their Distribution. At thespeech level, the system should summarize the timing ofhumor-related properties\u2014the humor is injected how fre-quently, under what condition (Or, what topic \ufb02ow), to which part of the speech, and with what verbal and vocalstyles. The visual summary of these properties serves asguidance and should help users \ufb01nd speci\ufb01c humor snip-pets within a speech. For example, a communication coachmight prioritize the very \ufb01rst humorous punchline (when),to show students how to provide an impressive opening(objective) by making small talks or sharing personal lives(e.g., \u201cMy brother\u2019s in prison.\u201d). Besides, as suggested by theexperts, the visual summary of the humor distributionshould be integrated with temporal information, along withthe topic \ufb02ow and verbal feature statistics.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We process the collected data such that (1) the text script and audio are aligned to support multimodal analysis, and (2) the full speech data is segmented into humor snippets to support context and sentence-level analy_x0002_sis. To achieve the alignment, we first detect each word\u2019s starting time and ending time in the transcript using P2FA [45]. Thereafter, we align the audio and text modality together at the word level. As for humor snippet segmenta_x0002_tion, we regard a sentence immediately before a laughter marker as a punchline (i.e., the most important sentence that triggers the audience response). We treat all of the sentences between two punchlines as the candidate context paragraph for the second punchline. The intuition is that all the infor_x0002_mation that occurs after a punchline are potentially useful for building up the next punchline. More concrete context recognition (shown in Section 5.1.3) should come from these candidate sentences. As a result, we split the transcripts at laughter markers. Each resulting humor snippet contains exactly one punchline (i.e., the last sentence of the segment) and its contexts (all the preceding sentences). The audio is clipped correspondingly through the starting and ending times of the sentences. Eventually, we organize the raw speech data into aligned audio and transcripts per snippet, per sentence, and per word; We compute and encode three types of semantic features at the sentence level: incongruity, sentiment, and phonetics. For each feature, a meaningful threshold is used to iden- tify important words or phrases in the sentence, which are annotated with intuitive glyphs. These thresholds can be interactively adjusted by users according to the feature distribution in figure.",
        "solution_category": "data_manipulation",
        "solution_axial": "Explainability,Excluding,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["Explainability", "Excluding", "AlgorithmicCalculation"],
        "componenet_code": [
          "explainability",
          "excluding",
          "algorithmic_calculation"
        ]
      },
      {
        "solution_text": "Besides sentence- and context-level, we design an aug- mented time matrix that provides an overview of distribution of humor occurrences and the related features of speech content and vocal delivery at the speech level.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "matrix+bar",
        "axial_code": ["Stack"],
        "componenet_code": ["bar", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 357,
    "paper_title": "DeHumor: Visual Analytics for Decomposing Humor",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "R3: Provide a Context-Level Overview That Shows Build-UpElements of Humor, as well as Their Relationships. Once zoomedin to a speci\ufb01c snippet, context level exploration is necessaryfor evaluating how a humorous story is written (e.g., how thekey concepts in the punchline are \ufb01rst introduced and howthey connect the pieces of humor stories), as well as a sum-mary of delivery skills that are frequently used to help conveythe story. Both researchers and communication coachesviewed the contextual analysis of humor build-ups to be themost demanding. Therefore, our system should primarilysupport users at this level.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We process the collected data such that (1) the text script and audio are aligned to support multimodal analysis, and (2) the full speech data is segmented into humor snippets to support context and sentence-level analy_x0002_sis. To achieve the alignment, we first detect each word\u2019s starting time and ending time in the transcript using P2FA [45]. Thereafter, we align the audio and text modality together at the word level. As for humor snippet segmenta_x0002_tion, we regard a sentence immediately before a laughter marker as a punchline (i.e., the most important sentence that triggers the audience response). We treat all of the sentences between two punchlines as the candidate context paragraph for the second punchline. The intuition is that all the infor_x0002_mation that occurs after a punchline are potentially useful for building up the next punchline. More concrete context recognition (shown in Section 5.1.3) should come from these candidate sentences. As a result, we split the transcripts at laughter markers. Each resulting humor snippet contains exactly one punchline (i.e., the last sentence of the segment) and its contexts (all the preceding sentences). The audio is clipped correspondingly through the starting and ending times of the sentences. Eventually, we organize the raw speech data into aligned audio and transcripts per snippet, per sentence, and per word; We compute and encode three types of semantic features at the sentence level: incongruity, sentiment, and phonetics. For each feature, a meaningful threshold is used to iden- tify important words or phrases in the sentence, which are annotated with intuitive glyphs. These thresholds can be interactively adjusted by users according to the feature distribution in figure.",
        "solution_category": "data_manipulation",
        "solution_axial": "Explainability,Excluding,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["Explainability", "Excluding", "AlgorithmicCalculation"],
        "componenet_code": [
          "explainability",
          "excluding",
          "algorithmic_calculation"
        ]
      },
      {
        "solution_text": "Visualizing Contextual Repetitions: We design a context linking graph to display the extracted inter-sentence repetition occurrences. As shown in figure, the graph follows a three- stage design, such that it gradually reveals the concrete con- text information to the user and traverse from the context- level to the sentence-level.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "text",
        "axial_code": ["Repetition"],
        "componenet_code": ["text"]
      },
      {
        "solution_text": "Visualizing Contextual Repetitions: We design a context linking graph to display the extracted inter-sentence repetition occurrences. As shown in figure, the graph follows a three- stage design, such that it gradually reveals the concrete con- text information to the user and traverse from the context- level to the sentence-level.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 358,
    "paper_title": "DeHumor: Visual Analytics for Decomposing Humor",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "R4: Highlight the Pairing of Individual Content Words andHumor-Related Verbal Delivery Units. We need to expose theco-occurrence between textual and audio features withineach individual sentence, so to demonstrate the humor strate-gies with relevant concrete examples (e.g., words and utter-ance). Within a snippet, the punchline is the most importantsentence since it immediately triggers laughter.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We process the collected data such that (1) the text script and audio are aligned to support multimodal analysis, and (2) the full speech data is segmented into humor snippets to support context and sentence-level analy_x0002_sis. To achieve the alignment, we first detect each word\u2019s starting time and ending time in the transcript using P2FA [45]. Thereafter, we align the audio and text modality together at the word level. As for humor snippet segmenta_x0002_tion, we regard a sentence immediately before a laughter marker as a punchline (i.e., the most important sentence that triggers the audience response). We treat all of the sentences between two punchlines as the candidate context paragraph for the second punchline. The intuition is that all the infor_x0002_mation that occurs after a punchline are potentially useful for building up the next punchline. More concrete context recognition (shown in Section 5.1.3) should come from these candidate sentences. As a result, we split the transcripts at laughter markers. Each resulting humor snippet contains exactly one punchline (i.e., the last sentence of the segment) and its contexts (all the preceding sentences). The audio is clipped correspondingly through the starting and ending times of the sentences. Eventually, we organize the raw speech data into aligned audio and transcripts per snippet, per sentence, and per word; We compute and encode three types of semantic features at the sentence level: incongruity, sentiment, and phonetics. For each feature, a meaningful threshold is used to iden- tify important words or phrases in the sentence, which are annotated with intuitive glyphs. These thresholds can be interactively adjusted by users according to the feature distribution in figure.",
        "solution_category": "data_manipulation",
        "solution_axial": "Explainability,Excluding,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["Explainability", "Excluding", "AlgorithmicCalculation"],
        "componenet_code": [
          "explainability",
          "excluding",
          "algorithmic_calculation"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 359,
    "paper_title": "DeHumor: Visual Analytics for Decomposing Humor",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "R4: Highlight the Pairing of Individual Content Words andHumor-Related Verbal Delivery Units. We need to expose theco-occurrence between textual and audio features withineach individual sentence, so to demonstrate the humor strate-gies with relevant concrete examples (e.g., words and utter-ance). Within a snippet, the punchline is the most importantsentence since it immediately triggers laughter.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We process the collected data such that (1) the text script and audio are aligned to support multimodal analysis, and (2) the full speech data is segmented into humor snippets to support context and sentence-level analy_x0002_sis. To achieve the alignment, we first detect each word\u2019s starting time and ending time in the transcript using P2FA [45]. Thereafter, we align the audio and text modality together at the word level. As for humor snippet segmenta_x0002_tion, we regard a sentence immediately before a laughter marker as a punchline (i.e., the most important sentence that triggers the audience response). We treat all of the sentences between two punchlines as the candidate context paragraph for the second punchline. The intuition is that all the infor_x0002_mation that occurs after a punchline are potentially useful for building up the next punchline. More concrete context recognition (shown in Section 5.1.3) should come from these candidate sentences. As a result, we split the transcripts at laughter markers. Each resulting humor snippet contains exactly one punchline (i.e., the last sentence of the segment) and its contexts (all the preceding sentences). The audio is clipped correspondingly through the starting and ending times of the sentences. Eventually, we organize the raw speech data into aligned audio and transcripts per snippet, per sentence, and per word; We compute and encode three types of semantic features at the sentence level: incongruity, sentiment, and phonetics. For each feature, a meaningful threshold is used to iden- tify important words or phrases in the sentence, which are annotated with intuitive glyphs. These thresholds can be interactively adjusted by users according to the feature distribution in figure.",
        "solution_category": "data_manipulation",
        "solution_axial": "Explainability,Excluding,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["Explainability", "Excluding", "AlgorithmicCalculation"],
        "componenet_code": [
          "explainability",
          "excluding",
          "algorithmic_calculation"
        ]
      },
      {
        "solution_text": "The graph first provides a context distribution summary, which shows how the sentences are connected to each other through repeated concepts. A rounded gray rectangle repre- sents a sentence, and its horizontal length encodes the sen- tence length. We highlight the most important punchline with a denser gray color. We connect rectangles with arc links if their corresponding sentences share repetitive concepts, and add thin lines below the rectangles to denote the presence of these concept. The combination of the links and the lines help highlight different structures of build-up for humor.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph+arc+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["arc", "glyph", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 360,
    "paper_title": "DeHumor: Visual Analytics for Decomposing Humor",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "R5: Support Intuitive Interactions for Helping Users TraverseAmong Different Levels, and Reveal the Different Level of Details.For example, our communication coaches suggested thatthe original audio and scripts of the speech should also beincluded in the system, in addition to a visual summary ofhumor. The system should allow users to rapidly locate thesegments of interest in the speech and playback the corre-sponding audio clips",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Details-on-Demand Through Clicking. Once a user clicks a speech of interest in the control panel, the humor exploration will be updated accordingly. When the user clicks on a key-word or in the augmented time matrix, the corresponding humor snippet will be scrolled to the top in the content exploration, and the corresponding sentence in the context linking graph and the transcript will be highlighted.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 361,
    "paper_title": "DeHumor: Visual Analytics for Decomposing Humor",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "R5: Support Intuitive Interactions for Helping Users TraverseAmong Different Levels, and Reveal the Different Level of Details.For example, our communication coaches suggested thatthe original audio and scripts of the speech should also beincluded in the system, in addition to a visual summary ofhumor. The system should allow users to rapidly locate thesegments of interest in the speech and playback the corre-sponding audio clips",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.",
      "data_code": { "sequential": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Linked Scrolling. When users scroll in the content explora- tion, the time range of the visible humor snippets will be highlighted in the augmented time matrix.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 362,
    "paper_title": "DeHumor: Visual Analytics for Decomposing Humor",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "R5: Support Intuitive Interactions for Helping Users TraverseAmong Different Levels, and Reveal the Different Level of Details.For example, our communication coaches suggested thatthe original audio and scripts of the speech should also beincluded in the system, in addition to a visual summary ofhumor. The system should allow users to rapidly locate thesegments of interest in the speech and playback the corre-sponding audio clips",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Given a humorous speech, we collect four kinds of data from it: (1) We collect the meta-information (e.g., title, speakers, and categories) for indexing and query_x0002_ing a specific speech, so to enhance the usability of DeHu_x0002_mor; (2) We label humor occurrence within a speech based on the audience behavior markers (i.e., [LAUGHTER]) that are annotated in the transcripts; Previous studies have veri_x0002_fied that laughter can reliably indicate humor [49], [55], [56], [57]; (3) We use the transcripts for content analysis, and (4) the audio sequences for verbal delivery analysis. For demonstration purpose, we collect two speech data_x0002_sets from TED Talks and Comedy Central Stand-up.",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Active Query Through Searching, Sorting, and Filtering. Users  can search a speech or sort speeches according to a specific cri- terion in the control panel. Also, they can apply filters in the humor focus to find different styles of punchlines. Then, the corresponding humor snippets will be highlighted in the humor exploration view.",
        "solution_category": "interaction",
        "solution_axial": "Reconfigure",
        "solution_compoent": "",
        "axial_code": ["Reconfigure"],
        "componenet_code": ["reconfigure"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 363,
    "paper_title": "MVNet: Multi-Variate Multi-View Brain Network Comparison Over Uncertain Data",
    "pub_year": 2022,
    "domain": "brain",
    "requirement": {
      "requirement_text": "R1: Eliminate Uncertainty in Brain Connectivity Data. Theneurology doctor in our study described the comparison ofstructural brain connectivity as an infrequent scenario intheir clinical practice. The key reason is attributed to theuncertainty of the brain connectivity data and the follow-upbrain network construction process. The neuroscientist andcomputer scientist echoed the same uncertainty issue. Theydemand analyzing valid brain network data that faithfullyrepresents brain connectivity.",
      "requirement_code": { "data_filtering": 1 }
    },
    "data": {
      "data_text": "We developed MV2Net system mainly using the data set from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) \u2013 a public Consortium on collecting, validating, and utilizing AD data [61]. The ADNI data set contains struc_x0002_tural MRIs of 202 subjects recruited and scanned at 16 dif_x0002_ferent sites across North America [1]. On each subject, we computed a structural brain network by the segmentation method distributed with the FreeSurfer tool [62]. FreeSurfer applied the Desikan-Killiany parcellation template [63], which defines 70 ROIs (nodes) on the brain cortex of each subject. The default edge weight between two ROIs is set to the number of fibers on the white matter pathway between them, estimated from the DTI data of each subject.",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "geometry": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "On the connectivity features extracted in Section 4.1, it is found that the feature distributions largely follow normal distribution after removing zero values. Therefore, we apply the parameter-free Grubbs\u2019s test as the anomaly detection algorithm to extract zero-quality outlier feature values and compute the QoC measure for the other feature values. The Grubbs\u2019s test adapted to our quality computa_x0002_tion usage is given below. More details of the QoC measure and its computation algorithm can be found in Appendix C, available in the online supplemental material   To address the visualization problem in displaying discrim_x0002_inative connectivity features (P2 and P3), we apply both uni_x0002_variate and multivariate feature selection algorithms in MV2Net to identify connectivity features that are signifi_x0002_cantly different between the brain networks of contrasting subject groups. The univariate algorithm introduces a statis_x0002_tical hypothesis test on each connectivity feature of the net_x0002_work. Under the normality assumption verified in Section 4.2, the Student\u2019s t-test is legitimately used, which computes a p-value on each feature. When the p-value is no greater than a significance level (0.05 by default), the mean feature values of two contrasting groups on the corresponding feature are said to be significantly different. These selected significant features will be visualized in MV2Net for further analysis. Note that when the feature variance of contrasting groups is unequal, the Welch\u2019s t-test can be applied instead of the standard t-test. Users are allowed to choose between the two tests for every comparison. According to clinical researches on white matter disor_x0002_der, there are two major types of brain connectivity destruc_x0002_tion: one on a certain scope of white matter that mostly affects a single brain network connection; the other on a range of cerebral cortex spanning multiple ROIs (mainly grey matter) that could affect all brain fibers connecting to these ROIs. In the latter case, the connectivity features asso_x0002_ciated with affected ROIs will be correlated in the diseased subjects, e.g., with decreased connectivity strength simulta_x0002_neously. Yet, each individual connectivity feature may not be significantly different between the comparing subject group. The univariate feature selection algorithm will not solve the problem in this case because it analyzes each fea_x0002_ture independently. Lowering the p-value threshold also does not work as this will produce too many discriminative features without detecting the feature correlation. We con_x0002_sider the multivariate feature selection method called group lasso (GL), which is a kind of supervised machine learning algorithm. The algorithm aims to leverage the supervision to guide the selection process for a subset of discriminative features, instead of extracting a single feature in the univari_x0002_ate analysis. The method takes both connectivity features and the brain network structure as input. A set of structur_x0002_ally correlated connectivity features (e.g., on the subgraph of brain network) are then detected. When applied collec_x0002_tively, these features can precisely predict the subject label of a brain network, i.e., patient or control. The prediction accuracy is displayed in the interface to demonstrate the performance of the model. To evaluate the significance of each of these multivariate features, we apply univariate sta_x0002_tistical tests on each feature and display the derived p-val_x0002_ues in the interface for the visual analysis of multivariate bio-markers.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling", "AlgorithmicCalculation"],
        "componenet_code": ["modeling", "algorithmic_calculation"]
      },
      {
        "solution_text": "Visual Quality Analysis on Connectivity Features of the Brain Network. To meet requirement using visual analytics techniques, the MV2Net system is designed to conduct built-in quality analysis on brain network connectivity data. A carefully engineered anomaly detection method is applied to compute the pre-defined feature quality mea- sure. Null connections and possibly mismeasured connec- tivity features are assigned zero or low feature quality. On visualization, an interactive data wrangling mechanism is designed and implemented to allow users to eliminate the data uncertainty before the network comparison analysis.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "matrix",
        "axial_code": ["Repetition"],
        "componenet_code": ["matrix"]
      },
      {
        "solution_text": "Visual Quality Analysis on Connectivity Features of the Brain Network. To meet requirement using visual analytics techniques, the MV2Net system is designed to conduct built-in quality analysis on brain network connectivity data. A carefully engineered anomaly detection method is applied to compute the pre-defined feature quality mea- sure. Null connections and possibly mismeasured connec- tivity features are assigned zero or low feature quality. On visualization, an interactive data wrangling mechanism is designed and implemented to allow users to eliminate the data uncertainty before the network comparison analysis.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 364,
    "paper_title": "MVNet: Multi-Variate Multi-View Brain Network Comparison Over Uncertain Data",
    "pub_year": 2022,
    "domain": "brain",
    "requirement": {
      "requirement_text": "R2: Analyze Brain Connectivity From Multiple Perspectives.All experts agree that the reconstructed fiber tracts repre-sent full details of brain connectivity, much better than thebrain network with a single edge weight of connectivitystrength. For example, in a typical pathological disorder ofbrain tumors, the tumor in the early stage can cause geomet-ric changes to the brain fiber, but without reducing the fiberconnectivity strength. The analysis of brain connectivityshould be conducted from multiple perspectives, on diffu-sion features at the voxel level and on geometric featuresrepresenting the physical shape of fiber tracts.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "We developed MV2Net system mainly using the data set from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) \u2013 a public Consortium on collecting, validating, and utilizing AD data [61]. The ADNI data set contains struc_x0002_tural MRIs of 202 subjects recruited and scanned at 16 dif_x0002_ferent sites across North America [1]. On each subject, we computed a structural brain network by the segmentation method distributed with the FreeSurfer tool [62]. FreeSurfer applied the Desikan-Killiany parcellation template [63], which defines 70 ROIs (nodes) on the brain cortex of each subject. The default edge weight between two ROIs is set to the number of fibers on the white matter pathway between them, estimated from the DTI data of each subject.",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "geometry": 1,
        "media": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "On the connectivity features extracted in Section 4.1, it is found that the feature distributions largely follow normal distribution after removing zero values. Therefore, we apply the parameter-free Grubbs\u2019s test as the anomaly detection algorithm to extract zero-quality outlier feature values and compute the QoC measure for the other feature values. The Grubbs\u2019s test adapted to our quality computa_x0002_tion usage is given below. More details of the QoC measure and its computation algorithm can be found in Appendix C, available in the online supplemental material   To address the visualization problem in displaying discrim_x0002_inative connectivity features (P2 and P3), we apply both uni_x0002_variate and multivariate feature selection algorithms in MV2Net to identify connectivity features that are signifi_x0002_cantly different between the brain networks of contrasting subject groups. The univariate algorithm introduces a statis_x0002_tical hypothesis test on each connectivity feature of the net_x0002_work. Under the normality assumption verified in Section 4.2, the Student\u2019s t-test is legitimately used, which computes a p-value on each feature. When the p-value is no greater than a significance level (0.05 by default), the mean feature values of two contrasting groups on the corresponding feature are said to be significantly different. These selected significant features will be visualized in MV2Net for further analysis. Note that when the feature variance of contrasting groups is unequal, the Welch\u2019s t-test can be applied instead of the standard t-test. Users are allowed to choose between the two tests for every comparison. According to clinical researches on white matter disor_x0002_der, there are two major types of brain connectivity destruc_x0002_tion: one on a certain scope of white matter that mostly affects a single brain network connection; the other on a range of cerebral cortex spanning multiple ROIs (mainly grey matter) that could affect all brain fibers connecting to these ROIs. In the latter case, the connectivity features asso_x0002_ciated with affected ROIs will be correlated in the diseased subjects, e.g., with decreased connectivity strength simulta_x0002_neously. Yet, each individual connectivity feature may not be significantly different between the comparing subject group. The univariate feature selection algorithm will not solve the problem in this case because it analyzes each fea_x0002_ture independently. Lowering the p-value threshold also does not work as this will produce too many discriminative features without detecting the feature correlation. We con_x0002_sider the multivariate feature selection method called group lasso (GL), which is a kind of supervised machine learning algorithm. The algorithm aims to leverage the supervision to guide the selection process for a subset of discriminative features, instead of extracting a single feature in the univari_x0002_ate analysis. The method takes both connectivity features and the brain network structure as input. A set of structur_x0002_ally correlated connectivity features (e.g., on the subgraph of brain network) are then detected. When applied collec_x0002_tively, these features can precisely predict the subject label of a brain network, i.e., patient or control. The prediction accuracy is displayed in the interface to demonstrate the performance of the model. To evaluate the significance of each of these multivariate features, we apply univariate sta_x0002_tistical tests on each feature and display the derived p-val_x0002_ues in the interface for the visual analysis of multivariate bio-markers.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling", "AlgorithmicCalculation"],
        "componenet_code": ["modeling", "algorithmic_calculation"]
      },
      {
        "solution_text": "Visualization of Discriminative Connectivity Features Between Two Groups of Brain Networks (Univariate Bio- Markers). To achieve requirement on individual connectivity features, the system integrates univariate statistical tests (e.g., the Student\u2019s t-test) with interactive brain network visualization. Multiple choices of statistical test algorithms and visual comparison modes are supported in the system. Users can directly manipulate the algorithm parameters and visually analyze the outcome. A visual interface for subject group selection is also introduced to specify the comparison setting.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph+network",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph", "network"]
      },
      {
        "solution_text": "Users can directly manipulate the algorithm parameters and visually analyze the outcome. ",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 365,
    "paper_title": "MVNet: Multi-Variate Multi-View Brain Network Comparison Over Uncertain Data",
    "pub_year": 2022,
    "domain": "brain",
    "requirement": {
      "requirement_text": "R2: Analyze Brain Connectivity From Multiple Perspectives.All experts agree that the reconstructed fiber tracts repre-sent full details of brain connectivity, much better than thebrain network with a single edge weight of connectivitystrength. For example, in a typical pathological disorder ofbrain tumors, the tumor in the early stage can cause geomet-ric changes to the brain fiber, but without reducing the fiberconnectivity strength. The analysis of brain connectivityshould be conducted from multiple perspectives, on diffu-sion features at the voxel level and on geometric featuresrepresenting the physical shape of fiber tracts.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "We developed MV2Net system mainly using the data set from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) \u2013 a public Consortium on collecting, validating, and utilizing AD data [61]. The ADNI data set contains struc_x0002_tural MRIs of 202 subjects recruited and scanned at 16 dif_x0002_ferent sites across North America [1]. On each subject, we computed a structural brain network by the segmentation method distributed with the FreeSurfer tool [62]. FreeSurfer applied the Desikan-Killiany parcellation template [63], which defines 70 ROIs (nodes) on the brain cortex of each subject. The default edge weight between two ROIs is set to the number of fibers on the white matter pathway between them, estimated from the DTI data of each subject.",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "geometry": 1,
        "media": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "On the connectivity features extracted in Section 4.1, it is found that the feature distributions largely follow normal distribution after removing zero values. Therefore, we apply the parameter-free Grubbs\u2019s test as the anomaly detection algorithm to extract zero-quality outlier feature values and compute the QoC measure for the other feature values. The Grubbs\u2019s test adapted to our quality computa_x0002_tion usage is given below. More details of the QoC measure and its computation algorithm can be found in Appendix C, available in the online supplemental material   To address the visualization problem in displaying discrim_x0002_inative connectivity features (P2 and P3), we apply both uni_x0002_variate and multivariate feature selection algorithms in MV2Net to identify connectivity features that are signifi_x0002_cantly different between the brain networks of contrasting subject groups. The univariate algorithm introduces a statis_x0002_tical hypothesis test on each connectivity feature of the net_x0002_work. Under the normality assumption verified in Section 4.2, the Student\u2019s t-test is legitimately used, which computes a p-value on each feature. When the p-value is no greater than a significance level (0.05 by default), the mean feature values of two contrasting groups on the corresponding feature are said to be significantly different. These selected significant features will be visualized in MV2Net for further analysis. Note that when the feature variance of contrasting groups is unequal, the Welch\u2019s t-test can be applied instead of the standard t-test. Users are allowed to choose between the two tests for every comparison. According to clinical researches on white matter disor_x0002_der, there are two major types of brain connectivity destruc_x0002_tion: one on a certain scope of white matter that mostly affects a single brain network connection; the other on a range of cerebral cortex spanning multiple ROIs (mainly grey matter) that could affect all brain fibers connecting to these ROIs. In the latter case, the connectivity features asso_x0002_ciated with affected ROIs will be correlated in the diseased subjects, e.g., with decreased connectivity strength simulta_x0002_neously. Yet, each individual connectivity feature may not be significantly different between the comparing subject group. The univariate feature selection algorithm will not solve the problem in this case because it analyzes each fea_x0002_ture independently. Lowering the p-value threshold also does not work as this will produce too many discriminative features without detecting the feature correlation. We con_x0002_sider the multivariate feature selection method called group lasso (GL), which is a kind of supervised machine learning algorithm. The algorithm aims to leverage the supervision to guide the selection process for a subset of discriminative features, instead of extracting a single feature in the univari_x0002_ate analysis. The method takes both connectivity features and the brain network structure as input. A set of structur_x0002_ally correlated connectivity features (e.g., on the subgraph of brain network) are then detected. When applied collec_x0002_tively, these features can precisely predict the subject label of a brain network, i.e., patient or control. The prediction accuracy is displayed in the interface to demonstrate the performance of the model. To evaluate the significance of each of these multivariate features, we apply univariate sta_x0002_tistical tests on each feature and display the derived p-val_x0002_ues in the interface for the visual analysis of multivariate bio-markers.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling", "AlgorithmicCalculation"],
        "componenet_code": ["modeling", "algorithmic_calculation"]
      },
      {
        "solution_text": "Visualization of Correlated Discriminative Connectivity Features Between Two Groups of Brain Networks (Multivari- ate Bio-Markers). The system also satisfies requirement on correlated connectivity features, e.g., multiple discrimina- tive features on the same brain connection, or the same fea- ture on a subgraph of the brain network. Multivariate feature selection algorithms are applied to extract discrimi- native subgraph features. A high-order com- posite visualization design is introduced to analyze correlated connectivity features on the same brain connection.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "glyph+network",
        "axial_code": ["Coordinate"],
        "componenet_code": ["glyph", "network"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 366,
    "paper_title": "MVNet: Multi-Variate Multi-View Brain Network Comparison Over Uncertain Data",
    "pub_year": 2022,
    "domain": "brain",
    "requirement": {
      "requirement_text": "R3: Detect Brain Connectivity Bio-Markers by StatisticalTests. The clinical practice to detect connectivity bio-markers for disease depends on manually spotting the dif-ference with the naked eye, which is subjective and error-prone. Statistical tests and machine learning algorithms candetect individual connectivity features and a group of corre-lated features that are statistically signi\ufb01cantly differentbetween the comparing subject groups. Domain usersdemand directly looking at the analysis result instead ofanalyzing the raw data themselves.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "We developed MV2Net system mainly using the data set from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) \u2013 a public Consortium on collecting, validating, and utilizing AD data [61]. The ADNI data set contains struc_x0002_tural MRIs of 202 subjects recruited and scanned at 16 dif_x0002_ferent sites across North America [1]. On each subject, we computed a structural brain network by the segmentation method distributed with the FreeSurfer tool [62]. FreeSurfer applied the Desikan-Killiany parcellation template [63], which defines 70 ROIs (nodes) on the brain cortex of each subject. The default edge weight between two ROIs is set to the number of fibers on the white matter pathway between them, estimated from the DTI data of each subject.",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "geometry": 1,
        "media": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "On the connectivity features extracted in Section 4.1, it is found that the feature distributions largely follow normal distribution after removing zero values. Therefore, we apply the parameter-free Grubbs\u2019s test as the anomaly detection algorithm to extract zero-quality outlier feature values and compute the QoC measure for the other feature values. The Grubbs\u2019s test adapted to our quality computa_x0002_tion usage is given below. More details of the QoC measure and its computation algorithm can be found in Appendix C, available in the online supplemental material   To address the visualization problem in displaying discrim_x0002_inative connectivity features (P2 and P3), we apply both uni_x0002_variate and multivariate feature selection algorithms in MV2Net to identify connectivity features that are signifi_x0002_cantly different between the brain networks of contrasting subject groups. The univariate algorithm introduces a statis_x0002_tical hypothesis test on each connectivity feature of the net_x0002_work. Under the normality assumption verified in Section 4.2, the Student\u2019s t-test is legitimately used, which computes a p-value on each feature. When the p-value is no greater than a significance level (0.05 by default), the mean feature values of two contrasting groups on the corresponding feature are said to be significantly different. These selected significant features will be visualized in MV2Net for further analysis. Note that when the feature variance of contrasting groups is unequal, the Welch\u2019s t-test can be applied instead of the standard t-test. Users are allowed to choose between the two tests for every comparison. According to clinical researches on white matter disor_x0002_der, there are two major types of brain connectivity destruc_x0002_tion: one on a certain scope of white matter that mostly affects a single brain network connection; the other on a range of cerebral cortex spanning multiple ROIs (mainly grey matter) that could affect all brain fibers connecting to these ROIs. In the latter case, the connectivity features asso_x0002_ciated with affected ROIs will be correlated in the diseased subjects, e.g., with decreased connectivity strength simulta_x0002_neously. Yet, each individual connectivity feature may not be significantly different between the comparing subject group. The univariate feature selection algorithm will not solve the problem in this case because it analyzes each fea_x0002_ture independently. Lowering the p-value threshold also does not work as this will produce too many discriminative features without detecting the feature correlation. We con_x0002_sider the multivariate feature selection method called group lasso (GL), which is a kind of supervised machine learning algorithm. The algorithm aims to leverage the supervision to guide the selection process for a subset of discriminative features, instead of extracting a single feature in the univari_x0002_ate analysis. The method takes both connectivity features and the brain network structure as input. A set of structur_x0002_ally correlated connectivity features (e.g., on the subgraph of brain network) are then detected. When applied collec_x0002_tively, these features can precisely predict the subject label of a brain network, i.e., patient or control. The prediction accuracy is displayed in the interface to demonstrate the performance of the model. To evaluate the significance of each of these multivariate features, we apply univariate sta_x0002_tistical tests on each feature and display the derived p-val_x0002_ues in the interface for the visual analysis of multivariate bio-markers.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling", "AlgorithmicCalculation"],
        "componenet_code": ["modeling", "algorithmic_calculation"]
      },
      {
        "solution_text": "Visualization of Discriminative Connectivity Features Between Two Groups of Brain Networks (Univariate Bio- Markers). To achieve requirement on individual connectivity features, the system integrates univariate statistical tests (e.g., the Student\u2019s t-test) with interactive brain network visualization. Multiple choices of statistical test algorithms and visual comparison modes are supported in the system. Users can directly manipulate the algorithm parameters and visually analyze the outcome. A visual interface for subject group selection is also introduced to specify the comparison setting.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph+network",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph", "network"]
      },
      {
        "solution_text": "Users can directly manipulate the algorithm parameters and visually analyze the outcome. ",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 367,
    "paper_title": "MVNet: Multi-Variate Multi-View Brain Network Comparison Over Uncertain Data",
    "pub_year": 2022,
    "domain": "brain",
    "requirement": {
      "requirement_text": "R3: Detect Brain Connectivity Bio-Markers by StatisticalTests. The clinical practice to detect connectivity bio-markers for disease depends on manually spotting the dif-ference with the naked eye, which is subjective and error-prone. Statistical tests and machine learning algorithms candetect individual connectivity features and a group of corre-lated features that are statistically signi\ufb01cantly differentbetween the comparing subject groups. Domain usersdemand directly looking at the analysis result instead ofanalyzing the raw data themselves.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "We developed MV2Net system mainly using the data set from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) \u2013 a public Consortium on collecting, validating, and utilizing AD data [61]. The ADNI data set contains struc_x0002_tural MRIs of 202 subjects recruited and scanned at 16 dif_x0002_ferent sites across North America [1]. On each subject, we computed a structural brain network by the segmentation method distributed with the FreeSurfer tool [62]. FreeSurfer applied the Desikan-Killiany parcellation template [63], which defines 70 ROIs (nodes) on the brain cortex of each subject. The default edge weight between two ROIs is set to the number of fibers on the white matter pathway between them, estimated from the DTI data of each subject.",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "geometry": 1,
        "media": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "On the connectivity features extracted in Section 4.1, it is found that the feature distributions largely follow normal distribution after removing zero values. Therefore, we apply the parameter-free Grubbs\u2019s test as the anomaly detection algorithm to extract zero-quality outlier feature values and compute the QoC measure for the other feature values. The Grubbs\u2019s test adapted to our quality computa_x0002_tion usage is given below. More details of the QoC measure and its computation algorithm can be found in Appendix C, available in the online supplemental material   To address the visualization problem in displaying discrim_x0002_inative connectivity features (P2 and P3), we apply both uni_x0002_variate and multivariate feature selection algorithms in MV2Net to identify connectivity features that are signifi_x0002_cantly different between the brain networks of contrasting subject groups. The univariate algorithm introduces a statis_x0002_tical hypothesis test on each connectivity feature of the net_x0002_work. Under the normality assumption verified in Section 4.2, the Student\u2019s t-test is legitimately used, which computes a p-value on each feature. When the p-value is no greater than a significance level (0.05 by default), the mean feature values of two contrasting groups on the corresponding feature are said to be significantly different. These selected significant features will be visualized in MV2Net for further analysis. Note that when the feature variance of contrasting groups is unequal, the Welch\u2019s t-test can be applied instead of the standard t-test. Users are allowed to choose between the two tests for every comparison. According to clinical researches on white matter disor_x0002_der, there are two major types of brain connectivity destruc_x0002_tion: one on a certain scope of white matter that mostly affects a single brain network connection; the other on a range of cerebral cortex spanning multiple ROIs (mainly grey matter) that could affect all brain fibers connecting to these ROIs. In the latter case, the connectivity features asso_x0002_ciated with affected ROIs will be correlated in the diseased subjects, e.g., with decreased connectivity strength simulta_x0002_neously. Yet, each individual connectivity feature may not be significantly different between the comparing subject group. The univariate feature selection algorithm will not solve the problem in this case because it analyzes each fea_x0002_ture independently. Lowering the p-value threshold also does not work as this will produce too many discriminative features without detecting the feature correlation. We con_x0002_sider the multivariate feature selection method called group lasso (GL), which is a kind of supervised machine learning algorithm. The algorithm aims to leverage the supervision to guide the selection process for a subset of discriminative features, instead of extracting a single feature in the univari_x0002_ate analysis. The method takes both connectivity features and the brain network structure as input. A set of structur_x0002_ally correlated connectivity features (e.g., on the subgraph of brain network) are then detected. When applied collec_x0002_tively, these features can precisely predict the subject label of a brain network, i.e., patient or control. The prediction accuracy is displayed in the interface to demonstrate the performance of the model. To evaluate the significance of each of these multivariate features, we apply univariate sta_x0002_tistical tests on each feature and display the derived p-val_x0002_ues in the interface for the visual analysis of multivariate bio-markers.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling", "AlgorithmicCalculation"],
        "componenet_code": ["modeling", "algorithmic_calculation"]
      },
      {
        "solution_text": "Visualization of Correlated Discriminative Connectivity Features Between Two Groups of Brain Networks (Multivari- ate Bio-Markers). The system also satisfies requirement on correlated connectivity features, e.g., multiple discrimina- tive features on the same brain connection, or the same fea- ture on a subgraph of the brain network. Multivariate feature selection algorithms are applied to extract discrimi- native subgraph features. A high-order com- posite visualization design is introduced to analyze correlated connectivity features on the same brain connection.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "glyph+network",
        "axial_code": ["Coordinate"],
        "componenet_code": ["glyph", "network"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 368,
    "paper_title": "MVNet: Multi-Variate Multi-View Brain Network Comparison Over Uncertain Data",
    "pub_year": 2022,
    "domain": "brain",
    "requirement": {
      "requirement_text": "R4: Iteratively Compare Multiple Subject Groups and Synthe-size Results From Multiple Comparisons. To identify uniqueconnectivity bio-markers associated with a particular dis-ease type, neurology doctors often not only compare thegroup of patients having this disease type with the healthycontrols, but also compare the patient group having theother type of disease with controls, and compare betweenthe patients with different disease types. The patient groupswith changing severity of disease are also compared withthe healthy controls to understand the disease progression.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "We developed MV2Net system mainly using the data set from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) \u2013 a public Consortium on collecting, validating, and utilizing AD data [61]. The ADNI data set contains struc_x0002_tural MRIs of 202 subjects recruited and scanned at 16 dif_x0002_ferent sites across North America [1]. On each subject, we computed a structural brain network by the segmentation method distributed with the FreeSurfer tool [62]. FreeSurfer applied the Desikan-Killiany parcellation template [63], which defines 70 ROIs (nodes) on the brain cortex of each subject. The default edge weight between two ROIs is set to the number of fibers on the white matter pathway between them, estimated from the DTI data of each subject.",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "geometry": 1,
        "media": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "Visual Analysis of Multiple Group-Based Brain Net- work Comparisons. To meet requirement, MV2Net visualization is designed with multiple comparison views. Both correlation and progressive patterns in these compari- sons can be visually detected. Composite visualizations are also supported to integrate the multiple comparisons into the same view for analysis. To allow users to examine bio- markers on low-level connectivity, a 3D comparison view is provided to drill down to detailed geometric/diffusion fea- tures on the fiber tract level.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "3Dstructure",
        "axial_code": ["Repetition"],
        "componenet_code": ["3Dstructure"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 369,
    "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization",
    "pub_year": 2022,
    "domain": "crime patterns ",
    "requirement": {
      "requirement_text": "T1. Analyze Street-Level Crime Hotspots. De\ufb01ne and depictcrime hotspots on a street level of detail, aiming to identifylocations with similar crime patterns",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling", "AlgorithmicCalculation"],
        "componenet_code": ["modeling", "algorithmic_calculation"]
      },
      {
        "solution_text": "Illustrated in Fig. 7b, the hotspot scatter view shows the Prob-ability_x0001_Intensity scatter plot. To identify anchor points as hot-spots, one can use the linear discriminant filter functiondescribed in Section 5 by tuning the parameters a and pct (per-centage of anchor points to be considered hotspots), although an interactive brushing mechanism is also available.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      },
      {
        "solution_text": "Section 5 by tuning the parameters a and pct (per-centage of anchor points to be considered hotspots), although an interactive brushing mechanism is also available.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 370,
    "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization",
    "pub_year": 2022,
    "domain": "crime patterns ",
    "requirement": {
      "requirement_text": "T2. Probability _x0001_ Intensity Crime Hotspots. Enable a mecha-nism capable of identifying hotspots based not only on theabsolute number of crimes but also on the probability oftheir occurrence.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling", "AlgorithmicCalculation"],
        "componenet_code": ["modeling", "algorithmic_calculation"]
      },
      {
        "solution_text": "This view, depicted in Fig. 7a, enables the visualization ofanchor points\u2019 geographical location. Each node\u2019s colordepends on the group it belongs to, which is computed asdescribed in Section 6. The location view is particularly use-ful to overview the spatial distribution of hotspots and theirsimilarity. Besides, it is possible to show additional informa-tion in the background coloring the census units accordingto a given property, for example, socioeconomic and social vulnerability index (this resource will be exploited in thecase studies). Location view provides 2D and 3D visualiza-tion of the hotspots, being possible to change visualizationproperties such as size and opacity of the hotspots. Coloropacity and elevation, the latter only available in the 3Dview, can be set to correspond to the intensity or probabilityof the hotspots.Anchor Point Selection. By a simple clicking in an anchorpoint, Physical View shows the Google Street View photos inthe surroundings of the selected hotspots",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "map+scatter",
        "axial_code": ["Coordinate"],
        "componenet_code": ["scatter", "map"]
      },
      {
        "solution_text": "Location view provides 2D and 3D visualiza-tion of the hotspots, being possible to change visualizationproperties such as size and opacity of the hotspots. Coloropacity and elevation, the latter only available in the 3Dview, can be set to correspond to the intensity or probabilityof the hotspots.Anchor Point Selection. By a simple clicking in an anchorpoint, Physical View shows the Google Street View photos inthe surroundings of the selected hotspots",
        "solution_category": "interaction",
        "solution_axial": "Encode,OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore", "Encode"],
        "componenet_code": ["overview_and_explore", "encode"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 371,
    "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization",
    "pub_year": 2022,
    "domain": "crime patterns ",
    "requirement": {
      "requirement_text": "T3. Crime Patterns Analysis. Enable the analysis of crimepatterns in a particular hotspot or a group of hotspots",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling", "AlgorithmicCalculation"],
        "componenet_code": ["modeling", "algorithmic_calculation"]
      },
      {
        "solution_text": "This view uses Google Street View to extract and organizephotos of the surroundings of selected hotspots over time(see Fig. 7f). Each photo is a collage of many photosextracted during spatial padding. This padding is accom-plished for each time slice. Physical View helps domainexperts to understand the relationship between crime pat-terns and the urban infrastructure over time.",
        "solution_category": "visualization",
        "solution_axial": "Annotation",
        "solution_compoent": "area+image",
        "axial_code": ["Annotation"],
        "componenet_code": ["image", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 372,
    "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization",
    "pub_year": 2022,
    "domain": "crime patterns ",
    "requirement": {
      "requirement_text": "T4. Analyze the Surroundings of a Hotspot. Scrutinize urbancharacteristics around a hotspot.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling", "AlgorithmicCalculation"],
        "componenet_code": ["modeling", "algorithmic_calculation"]
      },
      {
        "solution_text": "Illustrated in Fig. 7b, the hotspot scatter view shows the Prob-ability_x0001_Intensity scatter plot. To identify anchor points as hot-spots, one can use the linear discriminant filter functiondescribed in Section 5 by tuning the parameters a and pct (per-centage of anchor points to be considered hotspots), although an interactive brushing mechanism is also available.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      },
      {
        "solution_text": "Section 5 by tuning the parameters a and pct (per-centage of anchor points to be considered hotspots), although an interactive brushing mechanism is also available.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      },
      {
        "solution_text": "Once hotspots have been selected with Hotspot Scatter View,they are grouped according to similarity, and the multidi-mensional projection is performed to reveal their distribu-tion in the feature space.",
        "solution_category": "data_manipulation",
        "solution_axial": "Filtering,Clustering&Grouping,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": [
          "DimensionalityReduction",
          "Filtering",
          "Clustering&Grouping"
        ],
        "componenet_code": [
          "dimensionality_reduction",
          "filtering",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "Once hotspots have been selected with Hotspot Scatter View,they are grouped according to similarity, and the multidi-mensional projection is performed to reveal their distribu-tion in the feature space. As shown in Fig. 7c, hotspots arecolored according to their groups in the projection. More-over, the legend on the right encodes the label and the num-ber of elements in each group.Group Selection. By clicking in the label of a group, Loca-tion View, Hotspot Scatter View, Between-Group Chart, andWithin-Group Joy Chart are updated to highlight only thedata in the selected group, making it easier for users to focustheir analysis on the selected group of hotspots.Filtering. It is possible to select hotspots using a lasso selec-tion. The \ufb01ltered hotspots are highlighted in the LocationView, Hotspot Scatter View, Between-Group Chart, and Within-Group Joy Chart. Besides, one can analyze individual timeseries of selected hotspots using the Within-Group Joy Chart",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      },
      {
        "solution_text": "Group Selection. By clicking in the label of a group, Loca-tion View, Hotspot Scatter View, Between-Group Chart, andWithin-Group Joy Chart are updated to highlight only thedata in the selected group, making it easier for users to focustheir analysis on the selected group of hotspots.Filtering. It is possible to select hotspots using a lasso selec-tion. The \ufb01ltered hotspots are highlighted in the LocationView, Hotspot Scatter View, Between-Group Chart, and Within-Group Joy Chart. Besides, one can analyze individual timeseries of selected hotspots using the Within-Group Joy Chart",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 373,
    "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization",
    "pub_year": 2022,
    "domain": "crime patterns ",
    "requirement": {
      "requirement_text": "T4. Analyze the Surroundings of a Hotspot. Scrutinize urbancharacteristics around a hotspot.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling", "AlgorithmicCalculation"],
        "componenet_code": ["modeling", "algorithmic_calculation"]
      },
      {
        "solution_text": "To better visualize the intra-cluster patterns, we create avisual representation that summarizes the crime pattern ineach group (see Fig. 7d). This visualization shows the aver-age time series of each group and the standard deviation ofthe group\u2019s time series. The rectangular glyphs, whose sizere\ufb02ects the number of hotspots in the group, are arrangedto keep the most similar groups closer to each other in thelayout, following the proximity relation observed in theCrime Pattern Projection View. The rectangular glyphsarrangement is computed from an optimization proceduresimilar to the method described in [38]. The Between-GroupChart is useful to understand the crime patterns present inthe data.Group Selection. By clicking in a rectangular glyph, Loca-tion View shows additional information encoded on the geo-map, such as the socioeconomic and social vulnerabilityindex.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "area",
        "axial_code": ["Repetition"],
        "componenet_code": ["area"]
      },
      {
        "solution_text": "By clicking in a rectangular glyph, Loca-tion View shows additional information encoded on the geo-map, such as the socioeconomic and social vulnerabilityindex.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 374,
    "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization",
    "pub_year": 2022,
    "domain": "crime patterns ",
    "requirement": {
      "requirement_text": "T4. Analyze the Surroundings of a Hotspot. Scrutinize urbancharacteristics around a hotspot.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling", "AlgorithmicCalculation"],
        "componenet_code": ["modeling", "algorithmic_calculation"]
      },
      {
        "solution_text": "This view relies on \ufb01lled line plots (see Fig. 7e) to present, ineach line of the chart, the crime time series of the hotspotsof a speci\ufb01c group. This visualization aims to provide adetailed visualization of the crime pattern in the hotspotgroup.Time Series Selection. By clicking in a particular timeseries, Physical View shows the photos of their surroundings.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "area",
        "axial_code": ["Repetition"],
        "componenet_code": ["area"]
      },
      {
        "solution_text": "By clicking in a particular timeseries, Physical View shows the photos of their surroundings.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 375,
    "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization",
    "pub_year": 2022,
    "domain": "crime patterns ",
    "requirement": {
      "requirement_text": "T5. Group Similar Hotspots. Group hotspots according tothe similarity of their patterns to visualize the spatial distri-bution of similar hotspots and possible causes for theobserved pattern",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling", "AlgorithmicCalculation"],
        "componenet_code": ["modeling", "algorithmic_calculation"]
      },
      {
        "solution_text": "To better visualize the intra-cluster patterns, we create avisual representation that summarizes the crime pattern ineach group (see Fig. 7d). This visualization shows the aver-age time series of each group and the standard deviation ofthe group\u2019s time series. The rectangular glyphs, whose sizere\ufb02ects the number of hotspots in the group, are arrangedto keep the most similar groups closer to each other in thelayout, following the proximity relation observed in theCrime Pattern Projection View. The rectangular glyphsarrangement is computed from an optimization proceduresimilar to the method described in [38]. The Between-GroupChart is useful to understand the crime patterns present inthe data.Group Selection. By clicking in a rectangular glyph, Loca-tion View shows additional information encoded on the geo-map, such as the socioeconomic and social vulnerabilityindex.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "area",
        "axial_code": ["Repetition"],
        "componenet_code": ["area"]
      },
      {
        "solution_text": "By clicking in a rectangular glyph, Loca-tion View shows additional information encoded on the geo-map, such as the socioeconomic and social vulnerabilityindex.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 376,
    "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization",
    "pub_year": 2022,
    "domain": "crime patterns ",
    "requirement": {
      "requirement_text": "T5. Group Similar Hotspots. Group hotspots according tothe similarity of their patterns to visualize the spatial distri-bution of similar hotspots and possible causes for theobserved pattern",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling", "AlgorithmicCalculation"],
        "componenet_code": ["modeling", "algorithmic_calculation"]
      },
      {
        "solution_text": "This view relies on \ufb01lled line plots (see Fig. 7e) to present, ineach line of the chart, the crime time series of the hotspotsof a speci\ufb01c group. This visualization aims to provide adetailed visualization of the crime pattern in the hotspotgroup.Time Series Selection. By clicking in a particular timeseries, Physical View shows the photos of their surroundings.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "area",
        "axial_code": ["Repetition"],
        "componenet_code": ["area"]
      },
      {
        "solution_text": "By clicking in a particular timeseries, Physical View shows the photos of their surroundings.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 377,
    "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization",
    "pub_year": 2022,
    "domain": "crime patterns ",
    "requirement": {
      "requirement_text": "T6. Compare Hotspots\u2019 Groups. Support the comparison ofhotspot groups according, making it possible to analyze thepatterns of different groups and the dispersion of hotspotswithin a group.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling", "AlgorithmicCalculation"],
        "componenet_code": ["modeling", "algorithmic_calculation"]
      },
      {
        "solution_text": "Illustrated in Fig. 7b, the hotspot scatter view shows the Prob-ability_x0001_Intensity scatter plot. To identify anchor points as hot-spots, one can use the linear discriminant filter functiondescribed in Section 5 by tuning the parameters a and pct (per-centage of anchor points to be considered hotspots), although an interactive brushing mechanism is also available.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      },
      {
        "solution_text": "Section 5 by tuning the parameters a and pct (per-centage of anchor points to be considered hotspots), although an interactive brushing mechanism is also available.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      },
      {
        "solution_text": "Once hotspots have been selected with Hotspot Scatter View,they are grouped according to similarity, and the multidi-mensional projection is performed to reveal their distribu-tion in the feature space.",
        "solution_category": "data_manipulation",
        "solution_axial": "Filtering,Clustering&Grouping,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": [
          "DimensionalityReduction",
          "Filtering",
          "Clustering&Grouping"
        ],
        "componenet_code": [
          "dimensionality_reduction",
          "filtering",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "Once hotspots have been selected with Hotspot Scatter View,they are grouped according to similarity, and the multidi-mensional projection is performed to reveal their distribu-tion in the feature space. As shown in Fig. 7c, hotspots arecolored according to their groups in the projection. More-over, the legend on the right encodes the label and the num-ber of elements in each group.Group Selection. By clicking in the label of a group, Loca-tion View, Hotspot Scatter View, Between-Group Chart, andWithin-Group Joy Chart are updated to highlight only thedata in the selected group, making it easier for users to focustheir analysis on the selected group of hotspots.Filtering. It is possible to select hotspots using a lasso selec-tion. The \ufb01ltered hotspots are highlighted in the LocationView, Hotspot Scatter View, Between-Group Chart, and Within-Group Joy Chart. Besides, one can analyze individual timeseries of selected hotspots using the Within-Group Joy Chart",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      },
      {
        "solution_text": "Group Selection. By clicking in the label of a group, Loca-tion View, Hotspot Scatter View, Between-Group Chart, andWithin-Group Joy Chart are updated to highlight only thedata in the selected group, making it easier for users to focustheir analysis on the selected group of hotspots.Filtering. It is possible to select hotspots using a lasso selec-tion. The \ufb01ltered hotspots are highlighted in the LocationView, Hotspot Scatter View, Between-Group Chart, and Within-Group Joy Chart. Besides, one can analyze individual timeseries of selected hotspots using the Within-Group Joy Chart",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 378,
    "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization",
    "pub_year": 2022,
    "domain": "crime patterns ",
    "requirement": {
      "requirement_text": "T6. Compare Hotspots\u2019 Groups. Support the comparison ofhotspot groups according, making it possible to analyze thepatterns of different groups and the dispersion of hotspotswithin a group.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling", "AlgorithmicCalculation"],
        "componenet_code": ["modeling", "algorithmic_calculation"]
      },
      {
        "solution_text": "To better visualize the intra-cluster patterns, we create avisual representation that summarizes the crime pattern ineach group (see Fig. 7d). This visualization shows the aver-age time series of each group and the standard deviation ofthe group\u2019s time series. The rectangular glyphs, whose sizere\ufb02ects the number of hotspots in the group, are arrangedto keep the most similar groups closer to each other in thelayout, following the proximity relation observed in theCrime Pattern Projection View. The rectangular glyphsarrangement is computed from an optimization proceduresimilar to the method described in [38]. The Between-GroupChart is useful to understand the crime patterns present inthe data.Group Selection. By clicking in a rectangular glyph, Loca-tion View shows additional information encoded on the geo-map, such as the socioeconomic and social vulnerabilityindex.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "area",
        "axial_code": ["Repetition"],
        "componenet_code": ["area"]
      },
      {
        "solution_text": "By clicking in a rectangular glyph, Loca-tion View shows additional information encoded on the geo-map, such as the socioeconomic and social vulnerabilityindex.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 379,
    "paper_title": "CriPAV: Street-Level Crime Patterns Analysis and Visualization",
    "pub_year": 2022,
    "domain": "crime patterns ",
    "requirement": {
      "requirement_text": "T6. Compare Hotspots\u2019 Groups. Support the comparison ofhotspot groups according, making it possible to analyze thepatterns of different groups and the dispersion of hotspotswithin a group.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "Real crime data from Sao Paulo (the city of S~ao Paulo) - Brazil. All case studies use crime records assembled by domain experts and provided by the Police Department of S~ao Paulo (the largest South American city with around 12 million inhabitants). The data set consists of information on about 1,650,000 crime incidents recorded from 2006 to 2017. We have worked with three crime types: passerby, commercial establishment, and vehicle robbery. Each record contains the census unit\u2019s identification ID where the crime took place, the date and time of the event, type, and geocode location (i.e., latitude and longitude) information.",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "As illustrated on the left of Fig. 1, hotspot identification is a primary component of CriPAV. Hotspots are visually defined from a \u2018Probability _x0001_ Intensity\u2019 scatter plot, where each dot corresponds to an anchor point. The intensity axis of each anchor point is the temporally aggregated number of crime events in the anchor point divide by the maximum number of crime events among all anchor points. The com_x0002_putation of how likely crimes are in each anchor point is more intricate; Another essential task that our methodology must accom_x0002_plish is identifying hotspots with similar temporal behavior (see Fig. 1-Finding Similar Hotspots). Finding the tempo_x0002_rally similar hotspot means searching for a similar time series, which is a difficult problem. Methods such as Dis_x0002_crete-Time Wrapping can be used to this end but with the price a high computational cost and instability to noise [36]. Instead, we opt for a deep learning embedding technique we called Hotspot2Vec.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling", "AlgorithmicCalculation"],
        "componenet_code": ["modeling", "algorithmic_calculation"]
      },
      {
        "solution_text": "This view relies on \ufb01lled line plots (see Fig. 7e) to present, ineach line of the chart, the crime time series of the hotspotsof a speci\ufb01c group. This visualization aims to provide adetailed visualization of the crime pattern in the hotspotgroup.Time Series Selection. By clicking in a particular timeseries, Physical View shows the photos of their surroundings.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "area",
        "axial_code": ["Repetition"],
        "componenet_code": ["area"]
      },
      {
        "solution_text": "By clicking in a particular timeseries, Physical View shows the photos of their surroundings.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 381,
    "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",
    "pub_year": 2022,
    "domain": "Fairness in graph mining",
    "requirement": {
      "requirement_text": "Which portion of nodes are the most important.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Our modular design enables analysts to freely integrate any graph-based ranking models for use as the target or base model. For demonstration purposes, we apply PageRank as the base model and AttriRank and a debiased PageRank (InFoRM) as the target models.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "Our framework is designed to enable a flexible definition of ranks and attributes to be considered when diagnosing fairness. Recent research [8, 35, 37] emphasizes that the top-k elements will receive more attention, and ranking bias is typically explored with respect to the top-k ranks. In our proposed framework, a data setting panel is configured to enable the analyst to select the top-k nodes. This is facilitated by the Ranking Score Density Histogram, which shows the ranking score distribution for the target ranking model. The analyst can interactively modify the number of bins by clicking the gear icon, and the histogram supports brush selection to select a specific ranking range. For example, if the analyst cares about potential biases of nodes who have similar ranking scores, then the analyst can brush a particular bin on the histogram and all the nodes within that ranking score range are selected. If the analyst wishes to select a specific ranking position, a slider is configured to enable the analyst to select nodes from rank m to n. In this way, the analysts can explore how attributes are distributed for any specific range of ranks.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      },
      {
        "solution_text": "Our framework is designed to enable a flexible definition of ranks and attributes to be considered when diagnosing fairness. Recent research [8, 35, 37] emphasizes that the top-k elements will receive more attention, and ranking bias is typically explored with respect to the top-k ranks. In our proposed framework, a data setting panel is configured to enable the analyst to select the top-k nodes. This is facilitated by the Ranking Score Density Histogram, which shows the ranking score distribution for the target ranking model. The analyst can interactively modify the number of bins by clicking the gear icon, and the histogram supports brush selection to select a specific ranking range. For example, if the analyst cares about potential biases of nodes who have similar ranking scores, then the analyst can brush a particular bin on the histogram and all the nodes within that ranking score range are selected. If the analyst wishes to select a specific ranking position, a slider is configured to enable the analyst to select nodes from rank m to n. In this way, the analysts can explore how attributes are distributed for any specific range of ranks.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 382,
    "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",
    "pub_year": 2022,
    "domain": "Fairness in graph mining",
    "requirement": {
      "requirement_text": "Which attributes are critically important for fairness.",
      "requirement_code": { "discover_observation": 1, "compare_entities": 1 }
    },
    "data": {
      "data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Attributes View. To support the interactive definition of groups (T1.2),we have designed an attribute setting panel (Figure 1.B) and an attributeview panel (Figure 1.C). The attributes view panel employs a paral-lel set where each selected attribute is visualized with multiple bars.Selected nodes are encoded as curves with different widths. Both theheight of bars and the width of the curves encode the number of nodesmapping to a specific attribute value. Additionally, the distribution ofattributes across the selected nodes is visualized with a histogram (Fig-ure 1.C.1). We use a light grey color to show the attribute distributionfor the entire dataset, and the dark grey color histogram shows the dis-tribution of attributes for the selected nodes. The attribute setting panel(Figure 1.B) enables the flexible selection of one or more attributesby clicking on the multiple selection area (Figure 1.B.1). All corre-sponding views including the attributes view (Figure 1.C), the grouptable view (Figure 1.D), the rank mapping view (Figure 1.E), the groupproportion view (Figure 1.E.3) and the group shift view (Figure 1.E.4)are automatically updated as the selected attributes are changed. Sincegroup fairness is most often based on categorical attribute values, wealso include a customization feature that allows analysts to categorizeattributes that may have continuous values. For example, protectedclasses for age are often grouped into ranges, e.g. under 18, 65+, etc..We also provide another histogram (Figure 1.B.3) to facilitate thecomparison of distribution similarities on selected attributes betweenselected nodes and the entire dataset. The metric for measuring dis-tribution similarities can be customized based on the analysts\u2019 needs.Currently, the framework supports Kullback-Leibler divergence fordemonstration purpose. The height of the bars are mapped to the dif-ferences of the between the distributions of the selected nodes and theentire dataset on a speci\ufb01c attribute.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 383,
    "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",
    "pub_year": 2022,
    "domain": "Fairness in graph mining",
    "requirement": {
      "requirement_text": "Which nodes are advantaged/disadvantaged by the model.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Individual Bias. Individual bias represents how the model guarantees that nodes with similar attributes will receive similar rankings. It is important to understand if individual nodes have been \u201csacrificed\u201d or privileged by the model in order to reduce group bias. To help analysts explore the individual biases among selected nodes, we label the selected nodes as advantaged/disadvantage nodes according to their ranking position changes (increase/decrease) between the base and target model.",
        "solution_category": "data_manipulation",
        "solution_axial": "Explainability",
        "solution_compoent": "AnalyseIndividualBiasbylabelingmodel-drivenrankingshifts.",
        "axial_code": ["Explainability"],
        "componenet_code": ["explainability"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 384,
    "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",
    "pub_year": 2022,
    "domain": "Fairness in graph mining",
    "requirement": {
      "requirement_text": "Which nodes are advantaged/disadvantaged by the model.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Content Bias. In real-world applications, a full ranking of millions of items simply cannot be displayed, and is typically culled to some top-k rank. In this setting, even the nodes who have the same ranking scores can have a large difference in ranking positions, and this problem is referred to as content bias. For example, imagine a list of items where the second through the seventh item have identical ranking scores. The method of display implies inequality in ranking even though ranks two through seven have equal ranking scores. Here, the implicit ordering can lead to significant differences in their exposure rates. To help analysts explore this phenomenon, we group nodes into clusters based on their ranking scores; Group Bias. Many fairness metrics have been proposed for measuring group fairness [10, 14]. These methods attempt to measure the degree of discrimination or bias [27]. However, there is no single term that universally represents bias. We denote group bias as the bias that reflects the ability of the model to achieve statistical parity between groups, where a group is defined with respect to the analysts\u2019 selected sensitive attributes. The goal of the framework is to enable analysts to audit whether the ranking results of a model exhibit direct or indirect preferences towards one or more groups, resulting in lower ranking scores for the disadvantaged groups. Compared with the content bias, where disadvantages can be due to display constraints, group bias can be mitigated algorithmically. To observe the impact on groups\u2019 ranking between the base and the target model, we formalize the ranking changes for each group by computing the average ranking position change; Individual Bias. Individual bias represents how the model guarantees that nodes with similar attributes will receive similar rankings. It is important to understand if individual nodes have been \u201csacrificed\u201d or privileged by the model in order to reduce group bias. To help analysts explore the individual biases among selected nodes, we label the selected nodes as advantaged/disadvantage nodes according to their ranking position changes (increase/decrease) between the base and target model.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,Clustering,AlgorithmicCalculation,Explainability",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "Excluding",
          "Clustering",
          "Explainability"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "excluding",
          "clustering_and_grouping",
          "explainability"
        ]
      },
      {
        "solution_text": "From top to bottom, the nodes are ranked from m to n, and in a cluster, the nodes are mapped from left to right according to their rank (high to low). Each cluster from the base model is connected to a corresponding cluster from the target model by a grey line when they share the same node(s), which illustrates how the ranking of this node changes between models.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "matrix+link",
        "axial_code": ["Nesting"],
        "componenet_code": ["link", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 385,
    "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",
    "pub_year": 2022,
    "domain": "Fairness in graph mining",
    "requirement": {
      "requirement_text": "Which nodes are advantaged/disadvantaged by the model.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Content Bias. In real-world applications, a full ranking of millions of items simply cannot be displayed, and is typically culled to some top-k rank. In this setting, even the nodes who have the same ranking scores can have a large difference in ranking positions, and this problem is referred to as content bias. For example, imagine a list of items where the second through the seventh item have identical ranking scores. The method of display implies inequality in ranking even though ranks two through seven have equal ranking scores. Here, the implicit ordering can lead to significant differences in their exposure rates. To help analysts explore this phenomenon, we group nodes into clusters based on their ranking scores; Group Bias. Many fairness metrics have been proposed for measuring group fairness [10, 14]. These methods attempt to measure the degree of discrimination or bias [27]. However, there is no single term that universally represents bias. We denote group bias as the bias that reflects the ability of the model to achieve statistical parity between groups, where a group is defined with respect to the analysts\u2019 selected sensitive attributes. The goal of the framework is to enable analysts to audit whether the ranking results of a model exhibit direct or indirect preferences towards one or more groups, resulting in lower ranking scores for the disadvantaged groups. Compared with the content bias, where disadvantages can be due to display constraints, group bias can be mitigated algorithmically. To observe the impact on groups\u2019 ranking between the base and the target model, we formalize the ranking changes for each group by computing the average ranking position change; Individual Bias. Individual bias represents how the model guarantees that nodes with similar attributes will receive similar rankings. It is important to understand if individual nodes have been \u201csacrificed\u201d or privileged by the model in order to reduce group bias. To help analysts explore the individual biases among selected nodes, we label the selected nodes as advantaged/disadvantage nodes according to their ranking position changes (increase/decrease) between the base and target model.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,Clustering,AlgorithmicCalculation,Explainability",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "Excluding",
          "Clustering",
          "Explainability"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "excluding",
          "clustering_and_grouping",
          "explainability"
        ]
      },
      {
        "solution_text": "The group shift view is designed to inspect both group bias and individual bias. For group bias, the bar chart on the left shows the average ranking change of each group. The color of the bar encodes the identity of the group. The bar chart on the right shows the distribution of group members in the base model and target model, and the analyst can diagnose group shifts in selected nodes to understand the corresponding fairness trade-offs between models.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 386,
    "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",
    "pub_year": 2022,
    "domain": "Fairness in graph mining",
    "requirement": {
      "requirement_text": "Which groups are advantaged/disadvantaged by the model.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Group Bias. Many fairness metrics have been proposed for measuring group fairness [10, 14]. These methods attempt to measure the degree of discrimination or bias [27]. However, there is no single term that universally represents bias. We denote group bias as the bias that reflects the ability of the model to achieve statistical parity between groups, where a group is defined with respect to the analysts\u2019 selected sensitive attributes. The goal of the framework is to enable analysts to audit whether the ranking results of a model exhibit direct or indirect preferences towards one or more groups, resulting in lower ranking scores for the disadvantaged groups. Compared with the content bias, where disadvantages can be due to display constraints, group bias can be mitigated algorithmically. To observe the impact on groups\u2019 ranking between the base and the target model, we formalize the ranking changes for each group by computing the average ranking position change.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "GroupBiasreduction.Computingtheaveragerankingpositionchange.",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 387,
    "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",
    "pub_year": 2022,
    "domain": "Fairness in graph mining",
    "requirement": {
      "requirement_text": "Which groups are advantaged/disadvantaged by the model.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Content Bias. In real-world applications, a full ranking of millions of items simply cannot be displayed, and is typically culled to some top-k rank. In this setting, even the nodes who have the same ranking scores can have a large difference in ranking positions, and this problem is referred to as content bias. For example, imagine a list of items where the second through the seventh item have identical ranking scores. The method of display implies inequality in ranking even though ranks two through seven have equal ranking scores. Here, the implicit ordering can lead to significant differences in their exposure rates. To help analysts explore this phenomenon, we group nodes into clusters based on their ranking scores; Group Bias. Many fairness metrics have been proposed for measuring group fairness [10, 14]. These methods attempt to measure the degree of discrimination or bias [27]. However, there is no single term that universally represents bias. We denote group bias as the bias that reflects the ability of the model to achieve statistical parity between groups, where a group is defined with respect to the analysts\u2019 selected sensitive attributes. The goal of the framework is to enable analysts to audit whether the ranking results of a model exhibit direct or indirect preferences towards one or more groups, resulting in lower ranking scores for the disadvantaged groups. Compared with the content bias, where disadvantages can be due to display constraints, group bias can be mitigated algorithmically. To observe the impact on groups\u2019 ranking between the base and the target model, we formalize the ranking changes for each group by computing the average ranking position change; Individual Bias. Individual bias represents how the model guarantees that nodes with similar attributes will receive similar rankings. It is important to understand if individual nodes have been \u201csacrificed\u201d or privileged by the model in order to reduce group bias. To help analysts explore the individual biases among selected nodes, we label the selected nodes as advantaged/disadvantage nodes according to their ranking position changes (increase/decrease) between the base and target model.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,Clustering,AlgorithmicCalculation,Explainability",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "Excluding",
          "Clustering",
          "Explainability"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "excluding",
          "clustering_and_grouping",
          "explainability"
        ]
      },
      {
        "solution_text": "Group Proportion View. The group proportion view is designed to illustrate the target ranking model\u2019s effects on each group\u2019s proportion. The group proportion view consists of two sets of bars and each set shows the composition of selected nodes sorted by both ranking models respectively. To facilitate inspection, we support switching the view mode between the proportion mode and the comparison mode. The proportion mode displays the stacked bars to summarize the overall group distribution of the selected nodes, while the comparison mode supports a direct comparison of group proportions between models. In other words, the comparison mode helps analysts perform pair-wise comparisons of the same group proportions between different models. Analysts can toggle between the proportion and comparison mode by using the switch button on the right side of the title bar of the rank mapping view.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      },
      {
        "solution_text": "Analysts can toggle between the proportion and comparison mode by using the switch button on the right side of the title bar of the rank mapping view.",
        "solution_category": "interaction",
        "solution_axial": "Encode",
        "solution_compoent": "",
        "axial_code": ["Encode"],
        "componenet_code": ["encode"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 388,
    "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",
    "pub_year": 2022,
    "domain": "Fairness in graph mining",
    "requirement": {
      "requirement_text": "Which groups are advantaged/disadvantaged by the model.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Content Bias. In real-world applications, a full ranking of millions of items simply cannot be displayed, and is typically culled to some top-k rank. In this setting, even the nodes who have the same ranking scores can have a large difference in ranking positions, and this problem is referred to as content bias. For example, imagine a list of items where the second through the seventh item have identical ranking scores. The method of display implies inequality in ranking even though ranks two through seven have equal ranking scores. Here, the implicit ordering can lead to significant differences in their exposure rates. To help analysts explore this phenomenon, we group nodes into clusters based on their ranking scores; Group Bias. Many fairness metrics have been proposed for measuring group fairness [10, 14]. These methods attempt to measure the degree of discrimination or bias [27]. However, there is no single term that universally represents bias. We denote group bias as the bias that reflects the ability of the model to achieve statistical parity between groups, where a group is defined with respect to the analysts\u2019 selected sensitive attributes. The goal of the framework is to enable analysts to audit whether the ranking results of a model exhibit direct or indirect preferences towards one or more groups, resulting in lower ranking scores for the disadvantaged groups. Compared with the content bias, where disadvantages can be due to display constraints, group bias can be mitigated algorithmically. To observe the impact on groups\u2019 ranking between the base and the target model, we formalize the ranking changes for each group by computing the average ranking position change; Individual Bias. Individual bias represents how the model guarantees that nodes with similar attributes will receive similar rankings. It is important to understand if individual nodes have been \u201csacrificed\u201d or privileged by the model in order to reduce group bias. To help analysts explore the individual biases among selected nodes, we label the selected nodes as advantaged/disadvantage nodes according to their ranking position changes (increase/decrease) between the base and target model.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,Clustering,AlgorithmicCalculation,Explainability",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "Excluding",
          "Clustering",
          "Explainability"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "excluding",
          "clustering_and_grouping",
          "explainability"
        ]
      },
      {
        "solution_text": "The group shift view is designed to inspect both group bias and individual bias. For group bias, the bar chart on the left shows the average ranking change of each group. The color of the bar encodes the identity of the group. The bar chart on the right shows the distribution of group members in the base model and target model, and the analyst can diagnose group shifts in selected nodes to understand the corresponding fairness trade-offs between models.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 389,
    "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",
    "pub_year": 2022,
    "domain": "Fairness in graph mining",
    "requirement": {
      "requirement_text": "Which nodes have similar relevance (ranking scores).",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Content Bias. In real-world applications, a full ranking of millions of items simply cannot be displayed, and is typically culled to some top-k rank. In this setting, even the nodes who have the same ranking scores can have a large difference in ranking positions, and this problem is referred to as content bias. For example, imagine a list of items where the second through the seventh item have identical ranking scores. The method of display implies inequality in ranking even though ranks two through seven have equal ranking scores. Here, the implicit ordering can lead to significant differences in their exposure rates. To help analysts explore this phenomenon, we group nodes into clusters based on their ranking scores.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,Clustering",
        "solution_compoent": "ExploreContentBias.Clusternodeswithsimilarrankingscores.",
        "axial_code": ["Excluding", "Clustering"],
        "componenet_code": ["excluding", "clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 390,
    "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",
    "pub_year": 2022,
    "domain": "Fairness in graph mining",
    "requirement": {
      "requirement_text": "Which nodes have similar relevance (ranking scores).",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Content Bias. In real-world applications, a full ranking of millions of items simply cannot be displayed, and is typically culled to some top-k rank. In this setting, even the nodes who have the same ranking scores can have a large difference in ranking positions, and this problem is referred to as content bias. For example, imagine a list of items where the second through the seventh item have identical ranking scores. The method of display implies inequality in ranking even though ranks two through seven have equal ranking scores. Here, the implicit ordering can lead to significant differences in their exposure rates. To help analysts explore this phenomenon, we group nodes into clusters based on their ranking scores; Group Bias. Many fairness metrics have been proposed for measuring group fairness [10, 14]. These methods attempt to measure the degree of discrimination or bias [27]. However, there is no single term that universally represents bias. We denote group bias as the bias that reflects the ability of the model to achieve statistical parity between groups, where a group is defined with respect to the analysts\u2019 selected sensitive attributes. The goal of the framework is to enable analysts to audit whether the ranking results of a model exhibit direct or indirect preferences towards one or more groups, resulting in lower ranking scores for the disadvantaged groups. Compared with the content bias, where disadvantages can be due to display constraints, group bias can be mitigated algorithmically. To observe the impact on groups\u2019 ranking between the base and the target model, we formalize the ranking changes for each group by computing the average ranking position change; Individual Bias. Individual bias represents how the model guarantees that nodes with similar attributes will receive similar rankings. It is important to understand if individual nodes have been \u201csacrificed\u201d or privileged by the model in order to reduce group bias. To help analysts explore the individual biases among selected nodes, we label the selected nodes as advantaged/disadvantage nodes according to their ranking position changes (increase/decrease) between the base and target model.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,Clustering,AlgorithmicCalculation,Explainability",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "Excluding",
          "Clustering",
          "Explainability"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "excluding",
          "clustering_and_grouping",
          "explainability"
        ]
      },
      {
        "solution_text": "Rank Mapping View. The rank mapping view consists of two columns of stacked rectangles, where the left column shows the ranking results of the base model, and the right column shows the ranking results of the target model. For each column, small squares that represent nodes of the analyst-defined groups are organized into large rectangles, where each rectangle represents a cluster that contains nodes with similar ranking scores.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "matrix+link",
        "axial_code": ["Nesting"],
        "componenet_code": ["link", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 391,
    "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",
    "pub_year": 2022,
    "domain": "Fairness in graph mining",
    "requirement": {
      "requirement_text": "What is each node\u2019s position in the ranking result, and how likely is it that content bias has occurred in similar nodes.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Content Bias. In real-world applications, a full ranking of millions of items simply cannot be displayed, and is typically culled to some top-k rank. In this setting, even the nodes who have the same ranking scores can have a large difference in ranking positions, and this problem is referred to as content bias. For example, imagine a list of items where the second through the seventh item have identical ranking scores. The method of display implies inequality in ranking even though ranks two through seven have equal ranking scores. Here, the implicit ordering can lead to significant differences in their exposure rates. To help analysts explore this phenomenon, we group nodes into clusters based on their ranking scores.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,Clustering",
        "solution_compoent": "ExploreContentBias.Clusternodeswithsimilarrankingscores.",
        "axial_code": ["Excluding", "Clustering"],
        "componenet_code": ["excluding", "clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 392,
    "paper_title": "FairRankVis: A Visual Analytics Framework for Exploring Algorithmic Fairness in Graph Mining Models",
    "pub_year": 2022,
    "domain": "Fairness in graph mining",
    "requirement": {
      "requirement_text": "What is each node\u2019s position in the ranking result, and how likely is it that content bias has occurred in similar nodes.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The Facebook social network dataset [22]. The data is subsampled to a subgraph with 734 nodes and 74254 edges. Each node has 24 attributes that describe the demo_x0002_graphics of a user. All identifiable information is anonymized and some attribute values are suppressed; The social net_x0002_work dataset collected from Weibo [1] where each node consists of four social attributes (gender, fans, account level, and location) about the demographic information of a Weibo user. For demonstration purposes, we subsampled the data down to 781 nodes and 2315 edges.",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Content Bias. In real-world applications, a full ranking of millions of items simply cannot be displayed, and is typically culled to some top-k rank. In this setting, even the nodes who have the same ranking scores can have a large difference in ranking positions, and this problem is referred to as content bias. For example, imagine a list of items where the second through the seventh item have identical ranking scores. The method of display implies inequality in ranking even though ranks two through seven have equal ranking scores. Here, the implicit ordering can lead to significant differences in their exposure rates. To help analysts explore this phenomenon, we group nodes into clusters based on their ranking scores; Group Bias. Many fairness metrics have been proposed for measuring group fairness [10, 14]. These methods attempt to measure the degree of discrimination or bias [27]. However, there is no single term that universally represents bias. We denote group bias as the bias that reflects the ability of the model to achieve statistical parity between groups, where a group is defined with respect to the analysts\u2019 selected sensitive attributes. The goal of the framework is to enable analysts to audit whether the ranking results of a model exhibit direct or indirect preferences towards one or more groups, resulting in lower ranking scores for the disadvantaged groups. Compared with the content bias, where disadvantages can be due to display constraints, group bias can be mitigated algorithmically. To observe the impact on groups\u2019 ranking between the base and the target model, we formalize the ranking changes for each group by computing the average ranking position change; Individual Bias. Individual bias represents how the model guarantees that nodes with similar attributes will receive similar rankings. It is important to understand if individual nodes have been \u201csacrificed\u201d or privileged by the model in order to reduce group bias. To help analysts explore the individual biases among selected nodes, we label the selected nodes as advantaged/disadvantage nodes according to their ranking position changes (increase/decrease) between the base and target model.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,Clustering,AlgorithmicCalculation,Explainability",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "Excluding",
          "Clustering",
          "Explainability"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "excluding",
          "clustering_and_grouping",
          "explainability"
        ]
      },
      {
        "solution_text": "Rank Mapping View. The rank mapping view consists of two columns of stacked rectangles, where the left column shows the ranking results of the base model, and the right column shows the ranking results of the target model. For each column, small squares that represent nodes of the analyst-defined groups are organized into large rectangles, where each rectangle represents a cluster that contains nodes with similar ranking scores.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "matrix+link",
        "axial_code": ["Nesting"],
        "componenet_code": ["link", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 393,
    "paper_title": "VisQA: X-raying Vision and Language Reasoning in Transformers",
    "pub_year": 2022,
    "domain": "Visual Question Answering(HCI)",
    "requirement": {
      "requirement_text": "G1 Examine the performances of each instance for a givenmodel. To investigate bias in VQA systems as introduced in sec. 3.3,it is first important to examine the model predictions, along with itsconfidence score with respect to its inputs. In order to be useful, thosepredictions need to be combined with the ground-truth, to estimate ifthe model is wrong, and how frequent is the ground-truth answer andpredictions. Inputs need to be inspected as well as they may conveyambiguities that may be at the beginning of an explanation for a mis-take. Finally, due to a large amount of data available for inspection,experts may prioritize inputs the more likely to be biased, i.e., thosewith infrequent ground-truth answers and frequent predictions.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The GQA [29] standard dataset provides question/image pairs along with their answers, the ground truth of bounding boxes, and semantic descriptions of questions.",
      "data_code": { "media": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Image ranking-by-feature. In order to ease user exploration over the complete dataset, VISQA displays images in the top bar from left to right based on the likelihood of their questions to be answered using statistical biases. To do so, we classify questions using ground truth answers, as proposed in [32]: top 20% of the most frequent answer will be classified as Head, as opposed to Tail which describes questions with the least frequent answers. We attribute a score to each image based on their Head-questions/Tail-questions ratio. The more an image has Tail-questions over Head-questions, the higher its score is. The underlying hypothesis is that frequent answers will be chosen more likely when a model tends to exploit biases (e.g. \u201cYellow bananas\u201d). Also, frequent questions are harder to analyze since if any bias is exploited by the model, it will answer correctly.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Retrieval",
        "solution_compoent": "",
        "axial_code": ["Retrieval", "AlgorithmicCalculation"],
        "componenet_code": ["retrieval", "algorithmic_calculation"]
      },
      {
        "solution_text": "Image ranking-by-feature. In order to ease user exploration over the complete dataset, VISQA displays images in the top bar from left to right based on the likelihood of their questions to be answered using statistical biases. To do so, we classify questions using ground truth answers, as proposed in [32]: top 20% of the most frequent answer will be classified as Head, as opposed to Tail which describes questions with the least frequent answers. We attribute a score to each image based on their Head-questions/Tail-questions ratio. The more an image has Tail-questions over Head-questions, the higher its score is. The underlying hypothesis is that frequent answers will be chosen more likely when a model tends to exploit biases (e.g. \u201cYellow bananas\u201d). Also, frequent questions are harder to analyze since if any bias is exploited by the model, it will answer correctly.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "text+image+matrix+bar",
        "axial_code": ["Stack"],
        "componenet_code": ["matrix", "image", "bar", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 394,
    "paper_title": "VisQA: X-raying Vision and Language Reasoning in Transformers",
    "pub_year": 2022,
    "domain": "Visual Question Answering(HCI)",
    "requirement": {
      "requirement_text": "G2 Browse the attention of the model for an instance. Analyzingthe attention maps of LXMERT models is crucial for understandingwhat factors in\ufb02uenced its decision, and eventually whether or not themodel attends to both language and vision. While visualizing indi-vidually each attention map is feasible, we aim at improving such anexploration, by contextualizing each attention head with their neighbor-hood (i.e., other attention heads directly connected to it), and positionwithin the model. This is relevant as attention heads get closer to theoutput, they both encapsulate previous attention, and may be more in\ufb02u-ential on models\u2019 decisions. In addition, experts may need to prioritizeheads conveying salient attention, thus those heads need to be summa-rized and/or emphasized. Finally, for in-depth analysis, experts need tovisualize the complete attention map and link each of their elementsto human-understandable information, i.e., words of the question andbounding boxes within the input image.",
      "requirement_code": { "discover_observation": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The GQA [29] standard dataset provides question/image pairs along with their answers, the ground truth of bounding boxes, and semantic descriptions of questions.",
      "data_code": { "media": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Instance view. This view, inspired by VL-Transformer representations illustrated in Fig. 2, is the root of any analysis of attention maps using VISQA. It matches the internal data flow through the internal structure of the model, from left to right: the input image/question pair, layers and heads with intra-modality layers first, and finally the answer output distribution (encoded as horizontal bars). A particular design decision was to display all attention maps at once, using a single-colored rectangle encoding the attention intensity as k-number [47] (see next paragraph for details). In the Instance view, attention heads can be selected with a mouse-over interaction.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "matrix",
        "axial_code": ["Repetition"],
        "componenet_code": ["matrix"]
      },
      {
        "solution_text": "In the Instance view, attention heads can be selected with a mouse-over interaction.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 395,
    "paper_title": "VisQA: X-raying Vision and Language Reasoning in Transformers",
    "pub_year": 2022,
    "domain": "Visual Question Answering(HCI)",
    "requirement": {
      "requirement_text": "G3 Link attention to language tasks. Once a relevant attentionhead is observed by an expert, the user should be able to contextualizeit with the rest of the dataset. In particular, experts are interested inevaluating whether or not this head is responsive to certain tasks pro-vided by the semantics of questions (e.g., \ufb01nd a color), or rather if thehead is responsive to certain topics (e.g., clothing). Such informationcan be provided in the VQA training dataset, considered as categories.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The GQA [29] standard dataset provides question/image pairs along with their answers, the ground truth of bounding boxes, and semantic descriptions of questions.",
      "data_code": { "media": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Image ranking-by-feature. In order to ease user exploration over the complete dataset, VISQA displays images in the top bar from left to right based on the likelihood of their questions to be answered using statistical biases. To do so, we classify questions using ground truth answers, as proposed in [32]: top 20% of the most frequent answer will be classified as Head, as opposed to Tail which describes questions with the least frequent answers. We attribute a score to each image based on their Head-questions/Tail-questions ratio. The more an image has Tail-questions over Head-questions, the higher its score is. The underlying hypothesis is that frequent answers will be chosen more likely when a model tends to exploit biases (e.g. \u201cYellow bananas\u201d). Also, frequent questions are harder to analyze since if any bias is exploited by the model, it will answer correctly.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Retrieval",
        "solution_compoent": "",
        "axial_code": ["Retrieval", "AlgorithmicCalculation"],
        "componenet_code": ["retrieval", "algorithmic_calculation"]
      },
      {
        "solution_text": "Head Statistics. are represented using three charts. A vertical area (leftmost chart) represents the distribution of k-numbers of the selected head over the complete validation dataset (around 1500 image/question pairs). The vertical axis encodes the values of k-numbers, while the horizontal axis encodes the density of the corresponding k-number. The current k-number, for the selected head, which corresponds to the image/question pair loaded in VISQA, is represented as a horizontal red bar positioned on the vertical axis. This area-chart provides insights such as the detection of useless heads with constant high k-number which can be reduced to calculation on average overall items instead of selecting specific items. In contrast, heads with constant low k-number can be interpreted as conveying key information. More specialized heads, with bi-modal k-number distributions, can also be observed. Two stacked bar-charts represent the k-numbers of the selected head grouped by question operations.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "area+bar",
        "axial_code": ["Stack"],
        "componenet_code": ["bar", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 396,
    "paper_title": "VisQA: X-raying Vision and Language Reasoning in Transformers",
    "pub_year": 2022,
    "domain": "Visual Question Answering(HCI)",
    "requirement": {
      "requirement_text": "G4 Explore alternative scenarios. Once cues on how the modeluses its attention to output a decision are gathered, the next step, is totest this knowledge by querying the model on altered input or parame-ters. Ultimately the experts desire to answers questions such as: \u201cwouldthe model have a similar attention if the question were on another objectof the image?\u201d, or \u201cis this head or group of heads relevant for the \ufb01naldecision?\u201d. This can be regrouped into two categories \ufb01rst, the possi-bility to ask free-from questions, and second the possibility to modifythe model\u2019s attention. In order to be usable, and due to the numberof queries an expert may need to execute, both those manipulationsrequire to interact with the model in a reasonable amount of time, e.g.,less than a couple of seconds.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The GQA [29] standard dataset provides question/image pairs along with their answers, the ground truth of bounding boxes, and semantic descriptions of questions.",
      "data_code": { "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Free-form questions. By default, V IS QA loads the GQA dataset [29]to provide images and questions. But at any time, users can type andask free-form open-ended questions (G4). Such an interaction allowsinvestigating the model\u2019s bias exploitation. For instance, when askedthe following question from the GQA dataset \u201cIs this a mirror or asofa\u201d, the model correctly outputs \u201cmirror\u201d. However, when asked thefollowing user-inputted question \u201cIs there a mirror in this image?\u201d, themodel fails and outputs \u201cno\u201d. This suggests that the model might haveexploited biases when it answered the \ufb01rst question, which is supportedby the fact that in the GQA dataset, \u201cmirror\u201d is the correct answer tothe question \u201cIs this a mirror or a sofa\u201d in 85% of all cases.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 397,
    "paper_title": "VisQA: X-raying Vision and Language Reasoning in Transformers",
    "pub_year": 2022,
    "domain": "Visual Question Answering(HCI)",
    "requirement": {
      "requirement_text": "G4 Explore alternative scenarios. Once cues on how the modeluses its attention to output a decision are gathered, the next step, is totest this knowledge by querying the model on altered input or parame-ters. Ultimately the experts desire to answers questions such as: \u201cwouldthe model have a similar attention if the question were on another objectof the image?\u201d, or \u201cis this head or group of heads relevant for the \ufb01naldecision?\u201d. This can be regrouped into two categories \ufb01rst, the possi-bility to ask free-from questions, and second the possibility to modifythe model\u2019s attention. In order to be usable, and due to the numberof queries an expert may need to execute, both those manipulationsrequire to interact with the model in a reasonable amount of time, e.g.,less than a couple of seconds.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The GQA [29] standard dataset provides question/image pairs along with their answers, the ground truth of bounding boxes, and semantic descriptions of questions.",
      "data_code": { "media": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Image ranking-by-feature. In order to ease user exploration over the complete dataset, VISQA displays images in the top bar from left to right based on the likelihood of their questions to be answered using statistical biases. To do so, we classify questions using ground truth answers, as proposed in [32]: top 20% of the most frequent answer will be classified as Head, as opposed to Tail which describes questions with the least frequent answers. We attribute a score to each image based on their Head-questions/Tail-questions ratio. The more an image has Tail-questions over Head-questions, the higher its score is. The underlying hypothesis is that frequent answers will be chosen more likely when a model tends to exploit biases (e.g. \u201cYellow bananas\u201d). Also, frequent questions are harder to analyze since if any bias is exploited by the model, it will answer correctly.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Retrieval",
        "solution_compoent": "",
        "axial_code": ["Retrieval", "AlgorithmicCalculation"],
        "componenet_code": ["retrieval", "algorithmic_calculation"]
      },
      {
        "solution_text": "Users can select attention heads by clicking on them in the instance view, or by their k-number category. Such a selection can then be used to prune the corresponding head for the next forward of the model. Pruning here means that the attention head does not perform any focused attention, but uniformly distributes attention over the full set of items (objects or words). Each row of a pruned attention map is thus the equivalent of an average calculation. At any time, users can request a new forward pass of the model by clicking on the top left button \u201cask\u201d.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 398,
    "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce",
    "pub_year": 2022,
    "domain": "e-commerce livestreams",
    "requirement": {
      "requirement_text": "R1 Highlighting high-risk periods in a video. Browsing a videostream without visual hints may be tedious. Thus, the systemshould automatically detect and highlight critical periods thatmoderators should consider in priority order.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The short videos",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The data-processing procedure comprises video-audio decomposition and feature extraction. Our framework first separates a video into a sequence of frames and audio clips and then employs state-of-the-art techniques [30, 48, 52, 59] to extract multimodal features; To moderate the sheer volume of videos, we adopt a binary classifier as the filter, which can discover potentially deviant videos in accordance with extracted visual and audio features. However, video moderation policies differ across platforms and applications. Obtaining sufficient data to train a classifier to classify e-commerce livestreaming videos is difficult. Following the idea of \u201clearning with reviewing,\u201d we first define a linear classifier to initiate the framework and then adopt the reviewed videos as the ground truths to improve the classifier iteratively.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection", "Modeling"],
        "componenet_code": ["feature_selection", "modeling"]
      },
      {
        "solution_text": "The video view consists of a video player and a segmented timeline that displays the risk distribution of different time periods. The video player integrates a set of common video tools (e.g., play/stop and fast-forward) to assist the moderators in flexibly browsing the video content. To highlight high-risk periods in a video, we propose three alternative designs, namely, a segmented timeline with risk indicators, a timeline with in-line flow visualization, and a timeline with triangle glyphs. The design with triangular glyphs merely highlights the high-risk moments but lacks the necessary risk context, which may lead to the skipping of potentially deviant frames. To overcome this limitation, we insert a flow chart depicting the risk distribution of an entire video into the timeline.",
        "solution_category": "visualization",
        "solution_axial": "largepanel",
        "solution_compoent": "video+line+matrix",
        "axial_code": ["largepanel"],
        "componenet_code": ["line", "video", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 399,
    "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce",
    "pub_year": 2022,
    "domain": "e-commerce livestreams",
    "requirement": {
      "requirement_text": "R1 Highlighting high-risk periods in a video. Browsing a videostream without visual hints may be tedious. Thus, the systemshould automatically detect and highlight critical periods thatmoderators should consider in priority order.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The short videos",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The data-processing procedure comprises video-audio decomposition and feature extraction. Our framework first separates a video into a sequence of frames and audio clips and then employs state-of-the-art techniques [30, 48, 52, 59] to extract multimodal features; To moderate the sheer volume of videos, we adopt a binary classifier as the filter, which can discover potentially deviant videos in accordance with extracted visual and audio features. However, video moderation policies differ across platforms and applications. Obtaining sufficient data to train a classifier to classify e-commerce livestreaming videos is difficult. Following the idea of \u201clearning with reviewing,\u201d we first define a linear classifier to initiate the framework and then adopt the reviewed videos as the ground truths to improve the classifier iteratively.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection", "Modeling"],
        "componenet_code": ["feature_selection", "modeling"]
      },
      {
        "solution_text": "Nevertheless, the flow chart is excessively informative to highlight high-risk moments effectively. To balance the two extremes, we adopt a segmented timeline that is divided into different periods, in which the associated risk categories are indicated by color blocks. We encode the risk categories corresponding to the four moderation policies by using a color scheme extracted from ColorBrewer.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "matrix+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["text", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 400,
    "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce",
    "pub_year": 2022,
    "domain": "e-commerce livestreams",
    "requirement": {
      "requirement_text": "R2 Presenting a multilevel overview of frames. Long videos containnumerous frames that can hinder the exploration of video con-tent. Hence, the system should provide a scalable and compactsummarization of frames to support an effective overview.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The short videos",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The data-processing procedure comprises video-audio decomposition and feature extraction. Our framework first separates a video into a sequence of frames and audio clips and then employs state-of-the-art techniques [30, 48, 52, 59] to extract multimodal features; To moderate the sheer volume of videos, we adopt a binary classifier as the filter, which can discover potentially deviant videos in accordance with extracted visual and audio features. However, video moderation policies differ across platforms and applications. Obtaining sufficient data to train a classifier to classify e-commerce livestreaming videos is difficult. Following the idea of \u201clearning with reviewing,\u201d we first define a linear classifier to initiate the framework and then adopt the reviewed videos as the ground truths to improve the classifier iteratively.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection", "Modeling"],
        "componenet_code": ["feature_selection", "modeling"]
      },
      {
        "solution_text": "The frame comprises a multilevel overview of the video frames and a risk-aware circular glyph. To provide a concrete video summarization, we employ a well-established narrative model [40, 44] that characterizes video content at scene, shot, and frame levels. A shot refers to a set of similar consecutive frames, and a scene refers to a list of similar shots.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "image+donut",
        "axial_code": ["Repetition"],
        "componenet_code": ["image", "donut"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 401,
    "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce",
    "pub_year": 2022,
    "domain": "e-commerce livestreams",
    "requirement": {
      "requirement_text": "R3 Displaying a multifaceted overview of audio content. The lin-ear reading habit makes searching audio clips time consuming.Therefore, the system should provide a multifaceted overview toenable the efficient exploration of audio content.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The short videos",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The data-processing procedure comprises video-audio decomposition and feature extraction. Our framework first separates a video into a sequence of frames and audio clips and then employs state-of-the-art techniques [30, 48, 52, 59] to extract multimodal features; To moderate the sheer volume of videos, we adopt a binary classifier as the filter, which can discover potentially deviant videos in accordance with extracted visual and audio features. However, video moderation policies differ across platforms and applications. Obtaining sufficient data to train a classifier to classify e-commerce livestreaming videos is difficult. Following the idea of \u201clearning with reviewing,\u201d we first define a linear classifier to initiate the framework and then adopt the reviewed videos as the ground truths to improve the classifier iteratively.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection", "Modeling"],
        "componenet_code": ["feature_selection", "modeling"]
      },
      {
        "solution_text": "The audio view provides a multifaceted overview of streamers\u2019 speech, which consists of a horizontal histogram and a storyline-based visualization. The horizontal histogram is designed to demonstrate the frequency distribution of the risk words extracted from the audio content, in which each bar refers to a word. The height of a bar represents the count of the word mentioned in the speech, and its color represents the risk category, which is the same as the circular glyph. We rank the risk words in descending order to enable moderators to review the risky content from highest to lowest. However, moderating audio content by only reviewing the risk word summaries is insufficient, because moderators cannot make a reliable decision without the temporal context. Thus, we adopt storyline-based visualization to reveal the temporal concurrence of risk word.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "bar+line+donut+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["text", "line", "donut", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 402,
    "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce",
    "pub_year": 2022,
    "domain": "e-commerce livestreams",
    "requirement": {
      "requirement_text": "R4 Contextualizing risk information with multimodal content. Riskinformation is extracted from video or audio content. The sys-tem should combine multimodal content with associated risks toprovide concrete visual representations.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The short videos",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The data-processing procedure comprises video-audio decomposition and feature extraction. Our framework first separates a video into a sequence of frames and audio clips and then employs state-of-the-art techniques [30, 48, 52, 59] to extract multimodal features; To moderate the sheer volume of videos, we adopt a binary classifier as the filter, which can discover potentially deviant videos in accordance with extracted visual and audio features. However, video moderation policies differ across platforms and applications. Obtaining sufficient data to train a classifier to classify e-commerce livestreaming videos is difficult. Following the idea of \u201clearning with reviewing,\u201d we first define a linear classifier to initiate the framework and then adopt the reviewed videos as the ground truths to improve the classifier iteratively.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection", "Modeling"],
        "componenet_code": ["feature_selection", "modeling"]
      },
      {
        "solution_text": "The frame comprises a multilevel overview of the video frames and a risk-aware circular glyph. To provide a concrete video summarization, we employ a well-established narrative model [40, 44] that characterizes video content at scene, shot, and frame levels. A shot refers to a set of similar consecutive frames, and a scene refers to a list of similar shots.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "image+donut",
        "axial_code": ["Repetition"],
        "componenet_code": ["image", "donut"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 403,
    "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce",
    "pub_year": 2022,
    "domain": "e-commerce livestreams",
    "requirement": {
      "requirement_text": "R4 Contextualizing risk information with multimodal content. Riskinformation is extracted from video or audio content. The sys-tem should combine multimodal content with associated risks toprovide concrete visual representations.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The short videos",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The data-processing procedure comprises video-audio decomposition and feature extraction. Our framework first separates a video into a sequence of frames and audio clips and then employs state-of-the-art techniques [30, 48, 52, 59] to extract multimodal features; To moderate the sheer volume of videos, we adopt a binary classifier as the filter, which can discover potentially deviant videos in accordance with extracted visual and audio features. However, video moderation policies differ across platforms and applications. Obtaining sufficient data to train a classifier to classify e-commerce livestreaming videos is difficult. Following the idea of \u201clearning with reviewing,\u201d we first define a linear classifier to initiate the framework and then adopt the reviewed videos as the ground truths to improve the classifier iteratively.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection", "Modeling"],
        "componenet_code": ["feature_selection", "modeling"]
      },
      {
        "solution_text": "Moreover, we integrate the circular designs [19] with arepresentative frame whose risk score is the highest in the scene (R4).The radial bar chart visualizes risk tags by using bar sectors and the barheight to encode the risk score, which is easy to interpret. The donutchart encodes the risk score by using the sector angle, which is scalableto sparse risk information. The rose diagram encodes the risk scorewith the sector area, which highlights extreme risk scores effectively.Considering that the three designs demonstrate their strengths in differ-ent tasks, we provide a radio button to enable users to select differentdesigns in accordance with their requirements.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "image+donut",
        "axial_code": ["Repetition"],
        "componenet_code": ["image", "donut"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 404,
    "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce",
    "pub_year": 2022,
    "domain": "e-commerce livestreams",
    "requirement": {
      "requirement_text": "R5 Linking associated risk-aware visualizations in coordinated views.A gap exists in the risk information extracted from multimodalcontent. The system should visually connect risk-aware visualiza-tions in different views to facilitate quick labeling.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The short videos",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Locating high-risk periods. Moderators can locate high-risk periods by using the segmented timeline in figure. The system will update the video content during the located period when moderators click on the blocks.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 405,
    "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce",
    "pub_year": 2022,
    "domain": "e-commerce livestreams",
    "requirement": {
      "requirement_text": "R5 Linking associated risk-aware visualizations in coordinated views.A gap exists in the risk information extracted from multimodalcontent. The system should visually connect risk-aware visualiza-tions in different views to facilitate quick labeling.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The short videos",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Linking frames with risk tags. Moderators can link video frames with associated risk tags to review deviant content quickly. In figure, the system will automatically enlarge the associated frames when moderators hover over the sectors of the risk glyph. Moreover, moderators can update video content by clicking on a frame to locate deviant content promptly.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore,Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering", "OverviewandExplore"],
        "componenet_code": ["filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 406,
    "paper_title": "VideoModerator: A Risk-aware Framework for Multimodal Video Moderation in E-Commerce",
    "pub_year": 2022,
    "domain": "e-commerce livestreams",
    "requirement": {
      "requirement_text": "R5 Linking associated risk-aware visualizations in coordinated views.A gap exists in the risk information extracted from multimodalcontent. The system should visually connect risk-aware visualiza-tions in different views to facilitate quick labeling.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The short videos",
      "data_code": { "sequential": 1, "media": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Exploring audio context. Moderators can effectively explore audio context by clicking on a risk word or a histogram bar in figure. The system will enlarge the links to highlight the associated temporal context. Moreover, moderators can click on a word cloud to specify the period they want to review further.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 407,
    "paper_title": "ThreadStates: State-based Visual Analysis of Disease Progression",
    "pub_year": 2022,
    "domain": "Disease Progression",
    "requirement": {
      "requirement_text": "T1. Characterize value distribution over multiple features at eachstate. States indicate patient statuses and are represented by the valuedistributions of multiple timepoint features. For example, cancer pa-tients may exhibit different genomic mutations at the first cancer di-agnosis and a later cancer relapse. Visualizing the timepoint featuresof each state enables users to interpret the identified states, comparedifferent states, and refine states based on their analysis needs. As aresult, users are able to understand disease progression through theevolution of theses states and their timepoint feature values (G1)",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The synthetic COVID-19 dataset has relatively long sequences with dense timepoint features. For these patients, a daily measurement for a set of indicators (such as heart rate, white blood cell count, etc.) had been generated using a model-based approach based on real data to protect patient privacy; The real cytogenetically normal acute myeloid leukemia (CN-AML) dataset has relatively short se_x0002_quences with sparse timepoint features. This dataset provides the somatic genomic alterations of 50 patients at three critical timepoints of CN_x0002_AML progression (i.e., diagnosis, remission, relapse).",
      "data_code": { "tables": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Algorithm Choice. In ThreadStates, we treat state identification as an unsupervised clustering task. Each input item is a high dimensional vector Oi, j = (x1,x2,...,xm). Oi, j indicates the observation of patient i at timepoint j. (x1, x2...,xm) indicate the value of the m types of observations, such as body temperature, antibody level. The task is to find clusters with similar timepoint observations. For state-based visual analysis of disease progression, previous studies have employed Hidden Markov Models (HMM) to identify states and state transitions synchronously [21]. The synchronous identification of states and transi_x0002_tions enables efficient pattern mining but provides little support for user participation and state refinement. In this study, we use a hierarchical agglomerative clustering method [32] to provide users more flexibility in the process of state identification. For missing values, we replace them with non-missing values at the nearest timepoint of the same patient, based on the assumption that clinicians are more likely to skip an observation that has no dramatic change. The clustering results are influenced by the selected timepoint fea_x0002_tures. The selection of timepoint features will influence the state iden_x0002_tification results, therefore affecting the transition summarization and the analysis conclusions. The integration of users\u2019 domain knowledge can help select appropriate features and identify clinically meaningful states. In our earlier work [15], we proposed a Feature Manager based on Lineup [9]. This Feature Manager calculates a set of variability scores to highlight features that are of potential clinical interest.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Glyph Matrix that explains the characteristics of states based on their value distribution on different features. As shown in figure, the Glyph Matrix employs a small multiples design [33]. Each small chart depicts the value distribution of one state (column) on one feature (row). This Glyph Matrix design is similar to the matrix design proposed in DPVis. However, DPVis only provides statistic summaries (e.g., mean, standard deviation) for the feature distribution. The occurrence frequency (i.e., number of records) of each state is not presented to users in DPVis.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "matrix+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 408,
    "paper_title": "ThreadStates: State-based Visual Analysis of Disease Progression",
    "pub_year": 2022,
    "domain": "Disease Progression",
    "requirement": {
      "requirement_text": "T2. Summarize and compare the occurrence of each state. Usersmight be interested in the occurrence patterns of a state, such as whendoes a state normally occur and how frequently does this state occurin the target cohort. Since disease progression is depicted in terms ofstate transitions, summarizing the occurrence patterns of each state canoffer users a more comprehensive understanding of states, thereforefacilitating the interpretation of disease progressions (G1)",
      "requirement_code": {
        "compare_entities": 1,
        "describe_observation_aggregate": 1
      }
    },
    "data": {
      "data_text": "The synthetic COVID-19 dataset has relatively long sequences with dense timepoint features. For these patients, a daily measurement for a set of indicators (such as heart rate, white blood cell count, etc.) had been generated using a model-based approach based on real data to protect patient privacy; The real cytogenetically normal acute myeloid leukemia (CN-AML) dataset has relatively short se_x0002_quences with sparse timepoint features. This dataset provides the somatic genomic alterations of 50 patients at three critical timepoints of CN_x0002_AML progression (i.e., diagnosis, remission, relapse).",
      "data_code": { "tables": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Algorithm Choice. In ThreadStates, we treat state identification as an unsupervised clustering task. Each input item is a high dimensional vector Oi, j = (x1,x2,...,xm). Oi, j indicates the observation of patient i at timepoint j. (x1, x2...,xm) indicate the value of the m types of observations, such as body temperature, antibody level. The task is to find clusters with similar timepoint observations. For state-based visual analysis of disease progression, previous studies have employed Hidden Markov Models (HMM) to identify states and state transitions synchronously [21]. The synchronous identification of states and transi_x0002_tions enables efficient pattern mining but provides little support for user participation and state refinement. In this study, we use a hierarchical agglomerative clustering method [32] to provide users more flexibility in the process of state identification. For missing values, we replace them with non-missing values at the nearest timepoint of the same patient, based on the assumption that clinicians are more likely to skip an observation that has no dramatic change. The clustering results are influenced by the selected timepoint fea_x0002_tures. The selection of timepoint features will influence the state iden_x0002_tification results, therefore affecting the transition summarization and the analysis conclusions. The integration of users\u2019 domain knowledge can help select appropriate features and identify clinically meaningful states. In our earlier work [15], we proposed a Feature Manager based on Lineup [9]. This Feature Manager calculates a set of variability scores to highlight features that are of potential clinical interest.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Glyph Matrix that explains the characteristics of states based on their value distribution on different features. As shown in figure, the Glyph Matrix employs a small multiples design [33]. Each small chart depicts the value distribution of one state (column) on one feature (row). This Glyph Matrix design is similar to the matrix design proposed in DPVis. However, DPVis only provides statistic summaries (e.g., mean, standard deviation) for the feature distribution. The occurrence frequency (i.e., number of records) of each state is not presented to users in DPVis.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "matrix+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 409,
    "paper_title": "ThreadStates: State-based Visual Analysis of Disease Progression",
    "pub_year": 2022,
    "domain": "Disease Progression",
    "requirement": {
      "requirement_text": "T2. Summarize and compare the occurrence of each state. Usersmight be interested in the occurrence patterns of a state, such as whendoes a state normally occur and how frequently does this state occurin the target cohort. Since disease progression is depicted in terms ofstate transitions, summarizing the occurrence patterns of each state canoffer users a more comprehensive understanding of states, thereforefacilitating the interpretation of disease progressions (G1)",
      "requirement_code": {
        "compare_entities": 1,
        "describe_observation_aggregate": 1
      }
    },
    "data": {
      "data_text": "The synthetic COVID-19 dataset has relatively long sequences with dense timepoint features. For these patients, a daily measurement for a set of indicators (such as heart rate, white blood cell count, etc.) had been generated using a model-based approach based on real data to protect patient privacy; The real cytogenetically normal acute myeloid leukemia (CN-AML) dataset has relatively short se_x0002_quences with sparse timepoint features. This dataset provides the somatic genomic alterations of 50 patients at three critical timepoints of CN_x0002_AML progression (i.e., diagnosis, remission, relapse).",
      "data_code": { "tables": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Algorithm Choice. In ThreadStates, we treat state identification as an unsupervised clustering task. Each input item is a high dimensional vector Oi, j = (x1,x2,...,xm). Oi, j indicates the observation of patient i at timepoint j. (x1, x2...,xm) indicate the value of the m types of observations, such as body temperature, antibody level. The task is to find clusters with similar timepoint observations. For state-based visual analysis of disease progression, previous studies have employed Hidden Markov Models (HMM) to identify states and state transitions synchronously [21]. The synchronous identification of states and transi_x0002_tions enables efficient pattern mining but provides little support for user participation and state refinement. In this study, we use a hierarchical agglomerative clustering method [32] to provide users more flexibility in the process of state identification. For missing values, we replace them with non-missing values at the nearest timepoint of the same patient, based on the assumption that clinicians are more likely to skip an observation that has no dramatic change. The clustering results are influenced by the selected timepoint fea_x0002_tures. The selection of timepoint features will influence the state iden_x0002_tification results, therefore affecting the transition summarization and the analysis conclusions. The integration of users\u2019 domain knowledge can help select appropriate features and identify clinically meaningful states. In our earlier work [15], we proposed a Feature Manager based on Lineup [9]. This Feature Manager calculates a set of variability scores to highlight features that are of potential clinical interest.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "ThreadStates also provides areas to present the temporal distribution of each state. As shown in figure, users can easily compare the temporal distributions of states to better understand their role in disease progression.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "area",
        "axial_code": ["Repetition"],
        "componenet_code": ["area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 410,
    "paper_title": "ThreadStates: State-based Visual Analysis of Disease Progression",
    "pub_year": 2022,
    "domain": "Disease Progression",
    "requirement": {
      "requirement_text": "T2. Summarize and compare the occurrence of each state. Usersmight be interested in the occurrence patterns of a state, such as whendoes a state normally occur and how frequently does this state occurin the target cohort. Since disease progression is depicted in terms ofstate transitions, summarizing the occurrence patterns of each state canoffer users a more comprehensive understanding of states, thereforefacilitating the interpretation of disease progressions (G1)",
      "requirement_code": {
        "compare_entities": 1,
        "describe_observation_aggregate": 1
      }
    },
    "data": {
      "data_text": "The synthetic COVID-19 dataset has relatively long sequences with dense timepoint features. For these patients, a daily measurement for a set of indicators (such as heart rate, white blood cell count, etc.) had been generated using a model-based approach based on real data to protect patient privacy; The real cytogenetically normal acute myeloid leukemia (CN-AML) dataset has relatively short se_x0002_quences with sparse timepoint features. This dataset provides the somatic genomic alterations of 50 patients at three critical timepoints of CN_x0002_AML progression (i.e., diagnosis, remission, relapse).",
      "data_code": { "tables": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Algorithm Choice. In ThreadStates, we treat state identification as an unsupervised clustering task. Each input item is a high dimensional vector Oi, j = (x1,x2,...,xm). Oi, j indicates the observation of patient i at timepoint j. (x1, x2...,xm) indicate the value of the m types of observations, such as body temperature, antibody level. The task is to find clusters with similar timepoint observations. For state-based visual analysis of disease progression, previous studies have employed Hidden Markov Models (HMM) to identify states and state transitions synchronously [21]. The synchronous identification of states and transi_x0002_tions enables efficient pattern mining but provides little support for user participation and state refinement. In this study, we use a hierarchical agglomerative clustering method [32] to provide users more flexibility in the process of state identification. For missing values, we replace them with non-missing values at the nearest timepoint of the same patient, based on the assumption that clinicians are more likely to skip an observation that has no dramatic change. The clustering results are influenced by the selected timepoint fea_x0002_tures. The selection of timepoint features will influence the state iden_x0002_tification results, therefore affecting the transition summarization and the analysis conclusions. The integration of users\u2019 domain knowledge can help select appropriate features and identify clinically meaningful states. In our earlier work [15], we proposed a Feature Manager based on Lineup [9]. This Feature Manager calculates a set of variability scores to highlight features that are of potential clinical interest.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "For example, as shown in (1), it is hard to examine and compare state B and state C. The histograms may also lead to the overlooking of certain value distributions (2). Meanwhile, when using histograms and bar charts, it is ineffective to compare the occurrence frequency of states, which is important for the proposed analysis. Users can only estimate the occurrence frequency of each state by summing up the height of all rectangles. To address this issue, we proposed a novel glyph for the Glyph Matrix (c). This glyph reflected a design trade-off between the ease of reading certain important information and the familiarity of the visualization. We further refined the design by removing glyph color to facilitate comparison of glyphs in the same row, i.e., comparing states across one feature.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "matrix+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 411,
    "paper_title": "ThreadStates: State-based Visual Analysis of Disease Progression",
    "pub_year": 2022,
    "domain": "Disease Progression",
    "requirement": {
      "requirement_text": "T3. Search, rank, and filter state transition patterns. Capturingimportant state transition patterns is crucial for the analysis of diseaseprogression. It enables users to comprehend the evolution of diseases(G1) and find distinctive groups of patients with similar interestedtransition patterns (G2). Sequential pattern mining algorithms cangenerate long lists of patterns, many of which are redundant and do notcontribute to the understanding of disease progression. Therefore, thetool should provide flexible support for users to quickly search, rank,and locate interested patterns",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The synthetic COVID-19 dataset has relatively long sequences with dense timepoint features. For these patients, a daily measurement for a set of indicators (such as heart rate, white blood cell count, etc.) had been generated using a model-based approach based on real data to protect patient privacy; The real cytogenetically normal acute myeloid leukemia (CN-AML) dataset has relatively short se_x0002_quences with sparse timepoint features. This dataset provides the somatic genomic alterations of 50 patients at three critical timepoints of CN_x0002_AML progression (i.e., diagnosis, remission, relapse).",
      "data_code": { "tables": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In phase 2, we aim to identify 1) frequent state transition patterns and 2) patient groups with similar transition patterns. Note that the current version of ThreadStates fo_x0002_cuses on the analysis of sequences and does not consider the interval between timepoints. Event features are considered based on their rela_x0002_tive positions to these timepoints rather than their actual timestamps. Algorithm Choice. Previous studies have proposed a wide range of novel algorithms to tackle various challenges in the visual analytics of sequence data. Instead of proposing additional algorithms, we focus on experimenting and selecting a suitable algorithm from existing ones based on the analysis needs in disease progression.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Apart from the Sankey-based visualization, ThreadStates provides two tables to assist the analysis of different patient groups. The first table enables users to search, sort, filter the frequent patterns of each patient group to better understand the transition patterns.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table",
        "axial_code": ["Repetition"],
        "componenet_code": ["table"]
      },
      {
        "solution_text": "The first table enables users to search, sort, filter the frequent patterns of each patient group to better understand the transition patterns.",
        "solution_category": "interaction",
        "solution_axial": "Extractionoffeatures,Reconfigure,Filtering",
        "solution_compoent": "",
        "axial_code": ["Reconfigure", "Extractionoffeatures", "Filtering"],
        "componenet_code": [
          "reconfigure",
          "extraction_of_features",
          "filtering"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 412,
    "paper_title": "ThreadStates: State-based Visual Analysis of Disease Progression",
    "pub_year": 2022,
    "domain": "Disease Progression",
    "requirement": {
      "requirement_text": "T4. Group state transition patterns. Progression patterns of thesame disease vary across patients and across treatments. Discoveringand disentangling patterns from a target cohort allow researchers to ob-tain a comprehensive understanding of the disease. Grouping differenttransition patterns also provides an effective means to group patientsand find patient subgroups with similar transition patterns (G2).",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The synthetic COVID-19 dataset has relatively long sequences with dense timepoint features. For these patients, a daily measurement for a set of indicators (such as heart rate, white blood cell count, etc.) had been generated using a model-based approach based on real data to protect patient privacy; The real cytogenetically normal acute myeloid leukemia (CN-AML) dataset has relatively short se_x0002_quences with sparse timepoint features. This dataset provides the somatic genomic alterations of 50 patients at three critical timepoints of CN_x0002_AML progression (i.e., diagnosis, remission, relapse).",
      "data_code": { "tables": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "N-gram based methods have exhibited great performance in previous event sequence analysis studies [2, 36]. Based on those previous studies, we implemented the algorithm in ThreadStates as below. Each patient Pi can be represented by a set of N-grams, i.e., N consecutive elements in the state sequence (s1,s2,...,sk). We denote the Ngram set of patient Pi as TN(Pi), which can be represented as TN(Pi) = {(sjsj+1...sj+N\u22121)| j \u2208 [1, k +1\u2212N]}, where k is the number of timepoints, N is the number of consecutive elements. The distance between patient i and patient j is calculated as the Jaccard distance between the two N-gram sets: dist(Pi,Pj) = TN(Pi)\u2229TN(Pj)/TN(Pi)\u222aTN(Pj). Patients are then grouped through hierarchical agglomerative clustering [32] based on their pairwise distances. In other words, patients with similar state transition patterns will be grouped together.",
        "solution_category": "data_manipulation",
        "solution_axial": "Wrangling,SimilarityCalculation,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": [
          "Wrangling",
          "Clustering&Grouping",
          "SimilarityCalculation"
        ],
        "componenet_code": [
          "wrangling",
          "clustering_and_grouping",
          "similarity_calculation"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 413,
    "paper_title": "ThreadStates: State-based Visual Analysis of Disease Progression",
    "pub_year": 2022,
    "domain": "Disease Progression",
    "requirement": {
      "requirement_text": "T5. Reveal the association between patient groups and other fea-tures. Usually, users not only want to summarize disease progressionpatterns but are also interested in revealing the association betweenprogression patterns and other features (G3). These associations cangenerate valuable insights for a number of clinical tasks, such as earlydisease prediction and treatment selection. Since we group patientsbased on progression patterns, the association between progressions andother features can be revealed by analyzing the associations betweenpatient groups and these features",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The synthetic COVID-19 dataset has relatively long sequences with dense timepoint features. For these patients, a daily measurement for a set of indicators (such as heart rate, white blood cell count, etc.) had been generated using a model-based approach based on real data to protect patient privacy; The real cytogenetically normal acute myeloid leukemia (CN-AML) dataset has relatively short se_x0002_quences with sparse timepoint features. This dataset provides the somatic genomic alterations of 50 patients at three critical timepoints of CN_x0002_AML progression (i.e., diagnosis, remission, relapse).",
      "data_code": { "tables": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In phase 2, we aim to identify 1) frequent state transition patterns and 2) patient groups with similar transition patterns. Note that the current version of ThreadStates fo_x0002_cuses on the analysis of sequences and does not consider the interval between timepoints. Event features are considered based on their rela_x0002_tive positions to these timepoints rather than their actual timestamps. Algorithm Choice. Previous studies have proposed a wide range of novel algorithms to tackle various challenges in the visual analytics of sequence data. Instead of proposing additional algorithms, we focus on experimenting and selecting a suitable algorithm from existing ones based on the analysis needs in disease progression.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Apart from the Sankey-based visualization, ThreadStates provides two tables to assist the analysis of different patient groups. The second table summarizes the patient-features of each group to reveal the correlation between state transition patterns and patient features. In each table cell, the patient feature visualization is represented using the same glyph as the one used in the Glyph Matrix to reduce the learning burden.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+glyph",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "glyph"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 414,
    "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports",
    "pub_year": 2022,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T1 Identify the tactics used by a player. Given that each player haspersonal tactics, domain experts usually specify a player of inter-est and analyze his/her personal tactics. Moreover, domain ex-perts usually select many similar opponents (e.g., all left-handedopponents) to know the player\u2019s tactics for competing againstsuch opponents comprehensively. Our system should mine andvisualize the tactics of a certain player.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "The Control Bar allows users to select a dataset from one of several racket sports, one player of interest (IP), and multiple opponents (OPs) in order to analyze the tactics of the IP against these OPs. The system filters the rallies and displays the number of filtered rallies on the right. Throughout the user interface, we distinguish the IP from the OPs by hue because it is the most effective way to differentiate among a small number of categories [43]. We chose orange for the IP because it is an energetic and attention-getting color that domain experts preferred. We chose blue to stand for the OPs because it contrasts with orange.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 415,
    "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports",
    "pub_year": 2022,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T1 Identify the tactics used by a player. Given that each player haspersonal tactics, domain experts usually specify a player of inter-est and analyze his/her personal tactics. Moreover, domain ex-perts usually select many similar opponents (e.g., all left-handedopponents) to know the player\u2019s tactics for competing againstsuch opponents comprehensively. Our system should mine andvisualize the tactics of a certain player.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Our algorithm is highly correlated with our visualizations. Considering that there exist two core analysis targets: the tactics and the tactic progression, we implement our algorithm in two steps: identifying the tactics and discovering the tactic progression. Detailed implementa_x0002_tion of these two steps is as follows. In this step, we introduce a multivariate pattern mining algorithm to summarize a small set of frequent tactics from hundreds of original sequences. For example, in Fig. 4, we mine three tactics (t1 to t3) from four sequences (s1 to s4). Inspired by Ditto [2], we propose a generic multivariate pattern mining algorithm based on MDL. The core idea is similar to the Generative Adversarial Network (GAN), where the algorithm consists of two parts: a pattern generator and a pattern discriminator. The generator continuously generates new candidate patterns based on the current pattern set. The discriminator provides a measure based on MDL to judge whether a pattern is beneficial to sum_x0002_marizing the dataset. The algorithm iteratively adds the good patterns to the pattern set and removes the bad ones until the pattern set no longer changes. We adopt this idea due to two reasons: 1) The generator can selectively generate tactics instead of searching the ample space of versatile tactics, reducing the running time. 2) The discriminator can define a measure flexibly based on domain requirements to filter the tactics, which are meaningful to domain analysis. This step is designed for highly abstracting the complex original se_x0002_quences into a clear Sankey diagram that reveals the tactic progression, based on the mined tactics (Fig. 4(A and B \u2192 D)). Given a pattern set P and original sequences S, the algorithm generates a directed acyclic graph G (i.e., the Sankey diagram).",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Modeling"],
        "componenet_code": ["algorithmic_calculation", "modeling"]
      },
      {
        "solution_text": "The Flow View uses a Sankey diagram to show a ever-changing tactic progression. The diagram is in chronological order from left to right. Each node indicates a tactic along with certain contexts (i.e., specific pre-tactics). Multiple nodes may share the same tactic but have different contexts.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "sankey+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "sankey"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 416,
    "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports",
    "pub_year": 2022,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T1 Identify the tactics used by a player. Given that each player haspersonal tactics, domain experts usually specify a player of inter-est and analyze his/her personal tactics. Moreover, domain ex-perts usually select many similar opponents (e.g., all left-handedopponents) to know the player\u2019s tactics for competing againstsuch opponents comprehensively. Our system should mine andvisualize the tactics of a certain player.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "In this step, we introduce a multivariate pattern mining algorithm to summarize a small set of frequent tactics from hundreds of original sequences. For example, in Fig. 4, we mine three tactics (t1 to t3) from four sequences (s1 to s4). Inspired by Ditto [2], we propose a generic multivariate pattern mining algorithm based on MDL. The core idea is similar to the Generative Adversarial Network (GAN), where the algorithm consists of two parts: a pattern generator and a pattern discriminator. The generator continuously generates new candidate patterns based on the current pattern set. The discriminator provides a measure based on MDL to judge whether a pattern is beneficial to sum_x0002_marizing the dataset. The algorithm iteratively adds the good patterns to the pattern set and removes the bad ones until the pattern set no longer changes. We adopt this idea due to two reasons: 1) The generator can selectively generate tactics instead of searching the ample space of versatile tactics, reducing the running time. 2) The discriminator can define a measure flexibly based on domain requirements to filter the tactics, which are meaningful to domain analysis; The Projection View reveals similarities among tactics. Each tactic is represented by a point, and the distance between two points encodes the similarity between the two tactics. For each point, the area encodes the frequency of the tactic. The hue and the saturation encode the winning rate after using the tactic, consistent with the tail of our glyph design in the Flow View.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,SimilarityCalculation,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": [
          "DimensionalityReduction",
          "Modeling",
          "SimilarityCalculation"
        ],
        "componenet_code": [
          "dimensionality_reduction",
          "modeling",
          "similarity_calculation"
        ]
      },
      {
        "solution_text": "The Projection View reveals similarities among tactics. Each tactic is represented by a point, and the distance between two points encodes the similarity between the two tactics. For each point, the area encodes the frequency of the tactic. The hue and the saturation encode the winning rate after using the tactic, consistent with the tail of our glyph design in the Flow View.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 417,
    "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports",
    "pub_year": 2022,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T1 Identify the tactics used by a player. Given that each player haspersonal tactics, domain experts usually specify a player of inter-est and analyze his/her personal tactics. Moreover, domain ex-perts usually select many similar opponents (e.g., all left-handedopponents) to know the player\u2019s tactics for competing againstsuch opponents comprehensively. Our system should mine andvisualize the tactics of a certain player.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "In this step, we introduce a multivariate pattern mining algorithm to summarize a small set of frequent tactics from hundreds of original sequences. For example, in Fig. 4, we mine three tactics (t1 to t3) from four sequences (s1 to s4). Inspired by Ditto [2], we propose a generic multivariate pattern mining algorithm based on MDL. The core idea is similar to the Generative Adversarial Network (GAN), where the algorithm consists of two parts: a pattern generator and a pattern discriminator. The generator continuously generates new candidate patterns based on the current pattern set. The discriminator provides a measure based on MDL to judge whether a pattern is beneficial to sum_x0002_marizing the dataset. The algorithm iteratively adds the good patterns to the pattern set and removes the bad ones until the pattern set no longer changes. We adopt this idea due to two reasons: 1) The generator can selectively generate tactics instead of searching the ample space of versatile tactics, reducing the running time. 2) The discriminator can define a measure flexibly based on domain requirements to filter the tactics, which are meaningful to domain analysis.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "When users click on the glyph or point of a tactic, the Tactic View can visualize the multivariate tactic in tabular form (T1), where each row represents one attribute, and each column represents one event. Each obround indicates the value of an attribute within an event, while the hue of the background encodes who hits the ball. For each value within the tactic, we directly display this value as text because there are so many possibilities that it does not make sense to design different encodings or to expect users to remember them. For values where the player has multiple choices for applying the tactic, we visualize the frequency of each choice through a bar chart, one bar for one choice. The bar chart helps users understand how the tactic is used in detail by including values ignored by the pattern mining algorithm.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "table+text+bar",
        "axial_code": ["Nesting"],
        "componenet_code": ["table", "bar", "text"]
      },
      {
        "solution_text": "When users click on the glyph or point of a tactic, the Tactic View can visualize the multivariate tactic in tabular form (T1), where each row represents one attribute, and each column represents one event. Each obround indicates the value of an attribute within an event, while the hue of the background encodes who hits the ball. For each value within the tactic, we directly display this value as text because there are so many possibilities that it does not make sense to design different encodings or to expect users to remember them. For values where the player has multiple choices for applying the tactic, we visualize the frequency of each choice through a bar chart, one bar for one choice. The bar chart helps users understand how the tactic is used in detail by including values ignored by the pattern mining algorithm.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 418,
    "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports",
    "pub_year": 2022,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T2 Reveal the tactic progression. When finding a tactic of interest,domain experts need to know what tactics it comes from and what tactics it may progress into. By understanding the various changes,experts can know when a tactic is used and the results that thetactic can lead to. Our system should reveal the progressiverelationships among versatile tactics.",
      "requirement_code": {
        "describe_observation_aggregate": 1,
        "identify_main_cause_aggregate": 1
      }
    },
    "data": {
      "data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Our algorithm is highly correlated with our visualizations. Considering that there exist two core analysis targets: the tactics and the tactic progression, we implement our algorithm in two steps: identifying the tactics and discovering the tactic progression. Detailed implementa_x0002_tion of these two steps is as follows. In this step, we introduce a multivariate pattern mining algorithm to summarize a small set of frequent tactics from hundreds of original sequences. For example, in Fig. 4, we mine three tactics (t1 to t3) from four sequences (s1 to s4). Inspired by Ditto [2], we propose a generic multivariate pattern mining algorithm based on MDL. The core idea is similar to the Generative Adversarial Network (GAN), where the algorithm consists of two parts: a pattern generator and a pattern discriminator. The generator continuously generates new candidate patterns based on the current pattern set. The discriminator provides a measure based on MDL to judge whether a pattern is beneficial to sum_x0002_marizing the dataset. The algorithm iteratively adds the good patterns to the pattern set and removes the bad ones until the pattern set no longer changes. We adopt this idea due to two reasons: 1) The generator can selectively generate tactics instead of searching the ample space of versatile tactics, reducing the running time. 2) The discriminator can define a measure flexibly based on domain requirements to filter the tactics, which are meaningful to domain analysis. This step is designed for highly abstracting the complex original se_x0002_quences into a clear Sankey diagram that reveals the tactic progression, based on the mined tactics (Fig. 4(A and B \u2192 D)). Given a pattern set P and original sequences S, the algorithm generates a directed acyclic graph G (i.e., the Sankey diagram).",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Modeling"],
        "componenet_code": ["algorithmic_calculation", "modeling"]
      },
      {
        "solution_text": "The Flow View uses a Sankey diagram to show a ever-changing tactic progression. The diagram is in chronological order from left to right. Each node indicates a tactic along with certain contexts (i.e., specific pre-tactics). Multiple nodes may share the same tactic but have different contexts.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "sankey+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "sankey"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 419,
    "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports",
    "pub_year": 2022,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T3 Point out the tactics worth analyzing across a full progression.A player may adopt versatile tactics to compete against opponents.Instead of analyzing each tactic and exploring what comes beforeand after, domain experts prefer to analyze tactics that can affectthe overall progression signi\ufb01cantly (e.g., a tactic that can directlyscore a point without tactics coming after it). Our system shouldvisualize the tactics with multiple levels of detail, where thevisualizations at the highest level should help users quickly \ufb01ndthe tactics worth analyzing.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "In this step, we introduce a multivariate pattern mining algorithm to summarize a small set of frequent tactics from hundreds of original sequences. For example, in Fig. 4, we mine three tactics (t1 to t3) from four sequences (s1 to s4). Inspired by Ditto [2], we propose a generic multivariate pattern mining algorithm based on MDL. The core idea is similar to the Generative Adversarial Network (GAN), where the algorithm consists of two parts: a pattern generator and a pattern discriminator. The generator continuously generates new candidate patterns based on the current pattern set. The discriminator provides a measure based on MDL to judge whether a pattern is beneficial to sum_x0002_marizing the dataset. The algorithm iteratively adds the good patterns to the pattern set and removes the bad ones until the pattern set no longer changes. We adopt this idea due to two reasons: 1) The generator can selectively generate tactics instead of searching the ample space of versatile tactics, reducing the running time. 2) The discriminator can define a measure flexibly based on domain requirements to filter the tactics, which are meaningful to domain analysis.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "Multiple nodes may share the same tactic but have different contexts. We introduce a glyph-based design to encode the tactic and its context because glyphs can visualize multidimensional data with intuitive visual metaphors [3], preferred by domain experts. Each flow from one node to another indicates that IP first uses a tactic and then changes into another tactic without other tactics in between, where the width encodes the number of sequences.",
        "solution_category": "visualization",
        "solution_axial": "Basic",
        "solution_compoent": "glyph",
        "axial_code": ["Basic"],
        "componenet_code": ["glyph"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 420,
    "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports",
    "pub_year": 2022,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T3 Point out the tactics worth analyzing across a full progression.A player may adopt versatile tactics to compete against opponents.Instead of analyzing each tactic and exploring what comes beforeand after, domain experts prefer to analyze tactics that can affectthe overall progression signi\ufb01cantly (e.g., a tactic that can directlyscore a point without tactics coming after it). Our system shouldvisualize the tactics with multiple levels of detail, where thevisualizations at the highest level should help users quickly \ufb01ndthe tactics worth analyzing.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "In this step, we introduce a multivariate pattern mining algorithm to summarize a small set of frequent tactics from hundreds of original sequences. For example, in Fig. 4, we mine three tactics (t1 to t3) from four sequences (s1 to s4). Inspired by Ditto [2], we propose a generic multivariate pattern mining algorithm based on MDL. The core idea is similar to the Generative Adversarial Network (GAN), where the algorithm consists of two parts: a pattern generator and a pattern discriminator. The generator continuously generates new candidate patterns based on the current pattern set. The discriminator provides a measure based on MDL to judge whether a pattern is beneficial to sum_x0002_marizing the dataset. The algorithm iteratively adds the good patterns to the pattern set and removes the bad ones until the pattern set no longer changes. We adopt this idea due to two reasons: 1) The generator can selectively generate tactics instead of searching the ample space of versatile tactics, reducing the running time. 2) The discriminator can define a measure flexibly based on domain requirements to filter the tactics, which are meaningful to domain analysis; The Projection View reveals similarities among tactics. Each tactic is represented by a point, and the distance between two points encodes the similarity between the two tactics. For each point, the area encodes the frequency of the tactic. The hue and the saturation encode the winning rate after using the tactic, consistent with the tail of our glyph design in the Flow View.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,SimilarityCalculation,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": [
          "DimensionalityReduction",
          "Modeling",
          "SimilarityCalculation"
        ],
        "componenet_code": [
          "dimensionality_reduction",
          "modeling",
          "similarity_calculation"
        ]
      },
      {
        "solution_text": "The Projection View reveals similarities among tactics. Each tactic is represented by a point, and the distance between two points encodes the similarity between the two tactics. For each point, the area encodes the frequency of the tactic. The hue and the saturation encode the winning rate after using the tactic, consistent with the tail of our glyph design in the Flow View.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 421,
    "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports",
    "pub_year": 2022,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T4 Compare two multivariate tactics. Comparison is essential fordata analysis of sequential data [22, 26, 38]. For two tactics,domain experts usually want to know which is better and why it isbetter than the other. Especially for two similar ones, experts needto know which characteristics make them perform differently. Oursystem should support comparison between two tactics.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "In this step, we introduce a multivariate pattern mining algorithm to summarize a small set of frequent tactics from hundreds of original sequences. For example, in Fig. 4, we mine three tactics (t1 to t3) from four sequences (s1 to s4). Inspired by Ditto [2], we propose a generic multivariate pattern mining algorithm based on MDL. The core idea is similar to the Generative Adversarial Network (GAN), where the algorithm consists of two parts: a pattern generator and a pattern discriminator. The generator continuously generates new candidate patterns based on the current pattern set. The discriminator provides a measure based on MDL to judge whether a pattern is beneficial to sum_x0002_marizing the dataset. The algorithm iteratively adds the good patterns to the pattern set and removes the bad ones until the pattern set no longer changes. We adopt this idea due to two reasons: 1) The generator can selectively generate tactics instead of searching the ample space of versatile tactics, reducing the running time. 2) The discriminator can define a measure flexibly based on domain requirements to filter the tactics, which are meaningful to domain analysis.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "When users click on the glyph or point of a tactic, the Tactic View can visualize the multivariate tactic in tabular form (T1), where each row represents one attribute, and each column represents one event. Each obround indicates the value of an attribute within an event, while the hue of the background encodes who hits the ball. For each value within the tactic, we directly display this value as text because there are so many possibilities that it does not make sense to design different encodings or to expect users to remember them. For values where the player has multiple choices for applying the tactic, we visualize the frequency of each choice through a bar chart, one bar for one choice. The bar chart helps users understand how the tactic is used in detail by including values ignored by the pattern mining algorithm.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "table+text+bar",
        "axial_code": ["Nesting"],
        "componenet_code": ["table", "bar", "text"]
      },
      {
        "solution_text": "When users click on the glyph or point of a tactic, the Tactic View can visualize the multivariate tactic in tabular form (T1), where each row represents one attribute, and each column represents one event. Each obround indicates the value of an attribute within an event, while the hue of the background encodes who hits the ball. For each value within the tactic, we directly display this value as text because there are so many possibilities that it does not make sense to design different encodings or to expect users to remember them. For values where the player has multiple choices for applying the tactic, we visualize the frequency of each choice through a bar chart, one bar for one choice. The bar chart helps users understand how the tactic is used in detail by including values ignored by the pattern mining algorithm.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      },
      {
        "solution_text": "When users then right click on another tactic, this view can enable a comparison mode to compare the two tactics. To reduce users\u2019 learning costs, we simply divide each obround into two parts and juxtapose the corresponding values of the two tactics for a one-to-one comparison [23]. To ensure that similar hits are aligned, we also propose a greedy alignment strategy based on the relative indices and which player made each hit.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "text+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 422,
    "paper_title": "TacticFlow: Visual Analytics of Ever-Changing Tactics in Racket Sports",
    "pub_year": 2022,
    "domain": "sports",
    "requirement": {
      "requirement_text": "T5 Display the raw sequences. Experts need to examine a tacticin the context of a rally in order to know what happened in realgames so that they can communicate each rally to the player. Oursystem should display the raw data for data exploration in detail.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We obtained two datasets from domain experts for case studies \u2014 a tennis dataset DT and a badminton dataset DB. Both datasets are based on several matches during high-level events in 2019, all played between top players in the quarterfinals or later. We only considered rallies that contained five or more hits because athletes generally do not change tactics during short rallies. Dataset DT was collected from six matches, where Djokovic served the ball and competed against six other top-10 players. The dataset recorded three attributes for each event, including the hitting position (26 values), the player\u2019s pose (12 values), and the hitting technique (14 values). Dataset DB was collected from five matches, where Momota Kento played against four other top-10 players. The dataset recorded three attributes, including the shuttle\u2019s height (4 values), the hitting position (6 values), and the hitting technique (15 values). All the attributes and values are listed in the Appendix.",
      "data_code": { "tables": 1, "media": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "In this step, we introduce a multivariate pattern mining algorithm to summarize a small set of frequent tactics from hundreds of original sequences. For example, in Fig. 4, we mine three tactics (t1 to t3) from four sequences (s1 to s4). Inspired by Ditto [2], we propose a generic multivariate pattern mining algorithm based on MDL. The core idea is similar to the Generative Adversarial Network (GAN), where the algorithm consists of two parts: a pattern generator and a pattern discriminator. The generator continuously generates new candidate patterns based on the current pattern set. The discriminator provides a measure based on MDL to judge whether a pattern is beneficial to sum_x0002_marizing the dataset. The algorithm iteratively adds the good patterns to the pattern set and removes the bad ones until the pattern set no longer changes. We adopt this idea due to two reasons: 1) The generator can selectively generate tactics instead of searching the ample space of versatile tactics, reducing the running time. 2) The discriminator can define a measure flexibly based on domain requirements to filter the tactics, which are meaningful to domain analysis; The Projection View reveals similarities among tactics. Each tactic is represented by a point, and the distance between two points encodes the similarity between the two tactics. For each point, the area encodes the frequency of the tactic. The hue and the saturation encode the winning rate after using the tactic, consistent with the tail of our glyph design in the Flow View.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,SimilarityCalculation,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": [
          "DimensionalityReduction",
          "Modeling",
          "SimilarityCalculation"
        ],
        "componenet_code": [
          "dimensionality_reduction",
          "modeling",
          "similarity_calculation"
        ]
      },
      {
        "solution_text": "The Rally View lists all the rallies that contain a tactic selected by users, one per row. The view is divided into two parts according to the outcome of each rally so that users can compare rallies that IP won (the upper parts, indicated by a W on the left) and that he/she lost (the lower part, indicated by an L on the left). The top-right corner of each part displays the total number of wins and losses. For each row, circles on the right represent the hits in the rally, where the hue encodes whether IP or OPs hit the ball, and the number is the index of the hit. We highlight hits within the tactic with a solid circle so that users can know where the tactic is applied. When the user clicks on a rally, the exact values are expanded into a table. Users can also hover over a row to find a video button on the right, which triggers a video of the appropriate segment of the rally.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+circle",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "circle"]
      },
      {
        "solution_text": "When the user clicks on a rally, the exact values are expanded into a table. Users can also hover over a row to find a video button on the right, which triggers a video of the appropriate segment of the rally.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 423,
    "paper_title": "VITALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers & Visual Analytics",
    "pub_year": 2022,
    "domain": "academic literature",
    "requirement": {
      "requirement_text": "Serendipity: Enable serendipitous identification of semantically related articles that do not necessarily have shared keywords through visual exploration.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The dataset has 8 attributes (columns) and 59,232 papers (rows). ",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Embed: We next curated a dataframe of Title, Abstract, Au_x0002_thors, Source, Year, and Keywords and created the GloVe [50] and Specter [14] document embeddings. To create the document embed_x0002_dings for GloVe, we used TF-IDF weightings (instead of mean vectors) and SIF weightings that have been shown to remove noise through PCA reduction [3]. We used the public API to create the Specter em_x0002_beddings [14]. With these document embeddings, we used UMAP to construct 2-D document representations used in the Visualization Canvas (see Figure 4).",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "AlgorithmicCalculation"],
        "componenet_code": [
          "dimensionality_reduction",
          "algorithmic_calculation"
        ]
      },
      {
        "solution_text": "Similarity Search View shows options to find papers similar to one or more input papers. VITALITY supports setting the dimensions (2-Dimensional, n-Dimensional), number of similar papers to return, and the word embedding approach (e.g., Specter) to compute similarity.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "text"]
      },
      {
        "solution_text": "Similarity Search View shows options to find papers similar to one or more input papers. VITALITY supports setting the dimensions (2-Dimensional, n-Dimensional), number of similar papers to return, and the word embedding approach (e.g., Specter) to compute similarity.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 424,
    "paper_title": "VITALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers & Visual Analytics",
    "pub_year": 2022,
    "domain": "academic literature",
    "requirement": {
      "requirement_text": "Familiarity: Facilitate a familiar search functionality to what users are currently accustomed to, such as keyword and author search.",
      "requirement_code": { "data_filtering": 1 }
    },
    "data": {
      "data_text": "The dataset has 8 attributes (columns) and 59,232 papers (rows). ",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Embed: We next curated a dataframe of Title, Abstract, Au_x0002_thors, Source, Year, and Keywords and created the GloVe [50] and Specter [14] document embeddings. To create the document embed_x0002_dings for GloVe, we used TF-IDF weightings (instead of mean vectors) and SIF weightings that have been shown to remove noise through PCA reduction [3]. We used the public API to create the Specter em_x0002_beddings [14].",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Paper Collection View shows the entire corpus of papers in an interactive tabular layout. (1) shows an overview (number of visible papers) and UI controls to perform a global search, show hidden columns, add all papers to the input list of papers in the Similarity Search table, and save all papers to the \u201ccart\u201d in the Saved Papers View. (2) shows the attributes along with UI controls to filter (range sliders for Quantitative attributes, multiselect dropdowns for Nominal attributes), hide a column, and define a column on hover. (3) shows an interactive table of all papers with options to see detail, locate in the UMAP, add to the input list of papers for similarity search, and save to the \u201ccart\u201d. Search and filter capabilities are designed to be an intuitive entry-point into the dataset of academic articles.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "text"]
      },
      {
        "solution_text": "Paper Collection View shows the entire corpus of papers in an interactive tabular layout. (1) shows an overview (number of visible papers) and UI controls to perform a global search, show hidden columns, add all papers to the input list of papers in the Similarity Search table, and save all papers to the \u201ccart\u201d in the Saved Papers View. (2) shows the attributes along with UI controls to filter (range sliders for Quantitative attributes, multiselect dropdowns for Nominal attributes), hide a column, and define a column on hover. (3) shows an interactive table of all papers with options to see detail, locate in the UMAP, add to the input list of papers for similarity search, and save to the \u201ccart\u201d. Search and filter capabilities are designed to be an intuitive entry-point into the dataset of academic articles.",
        "solution_category": "interaction",
        "solution_axial": "Selecting,Filtering",
        "solution_compoent": "",
        "axial_code": ["Selecting", "Filtering"],
        "componenet_code": ["selecting", "filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 425,
    "paper_title": "VITALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers & Visual Analytics",
    "pub_year": 2022,
    "domain": "academic literature",
    "requirement": {
      "requirement_text": "Novelty: Afford users to find semantically related articles by searching based on the author\u2019s own ideas in the form of unpublished sentences / abstract.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The dataset has 8 attributes (columns) and 59,232 papers (rows). ",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Embed: We next curated a dataframe of Title, Abstract, Au_x0002_thors, Source, Year, and Keywords and created the GloVe [50] and Specter [14] document embeddings. To create the document embed_x0002_dings for GloVe, we used TF-IDF weightings (instead of mean vectors) and SIF weightings that have been shown to remove noise through PCA reduction [3]. We used the public API to create the Specter em_x0002_beddings [14]. With these document embeddings, we used UMAP to construct 2-D document representations used in the Visualization Canvas (see Figure 4).",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "AlgorithmicCalculation"],
        "componenet_code": [
          "dimensionality_reduction",
          "algorithmic_calculation"
        ]
      },
      {
        "solution_text": "Similarity Search View shows options to find papers similar to one or more input papers. VITALITY supports setting the dimensions (2-Dimensional, n-Dimensional), number of similar papers to return, and the word embedding approach (e.g., Specter) to compute similarity.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "text"]
      },
      {
        "solution_text": "Similarity Search View shows options to find papers similar to one or more input papers. VITALITY supports setting the dimensions (2-Dimensional, n-Dimensional), number of similar papers to return, and the word embedding approach (e.g., Specter) to compute similarity.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 426,
    "paper_title": "VITALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers & Visual Analytics",
    "pub_year": 2022,
    "domain": "academic literature",
    "requirement": {
      "requirement_text": "Overview: Enable users to interact with a visual overview of a group of papers.",
      "requirement_code": { "discover_observation": 1, "interactivity": 1 }
    },
    "data": {
      "data_text": "The dataset has 8 attributes (columns) and 59,232 papers (rows). ",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Embed: We next curated a dataframe of Title, Abstract, Au_x0002_thors, Source, Year, and Keywords and created the GloVe [50] and Specter [14] document embeddings. To create the document embed_x0002_dings for GloVe, we used TF-IDF weightings (instead of mean vectors) and SIF weightings that have been shown to remove noise through PCA reduction [3]. We used the public API to create the Specter em_x0002_beddings [14]. With these document embeddings, we used UMAP to construct 2-D document representations used in the Visualization Canvas (see Figure 4).",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "AlgorithmicCalculation"],
        "componenet_code": [
          "dimensionality_reduction",
          "algorithmic_calculation"
        ]
      },
      {
        "solution_text": "Visualization Canvas shows a 2-D UMAP projection of the embedding space of the entire paper collection: hovering on a point highlights it, shows the corresponding title in a fixed tooltip below, and automatically scrolls the collection (table) to bring the corresponding paper (row) into the viewport; pressing Shift enables lasso-mode to select multiple points using a free-form lasso operation; zooming and panning support helps navigate the UMAP to specific regions; By default, each point in the UMAP is colored based on the state of the corresponding paper (\u201cDefault\u201d): Unfiltered (unfiltered and visible in the main paper collection table; dark-grey), Filtered (filtered out and not visible in the paper collection table; light-grey), Similarity Input (added to the By Papers section in the Similarity Search View; pink), Similarity Output (in the Output Similar table; orange), and Saved (added to the Saved Papers table; red). Other options to color include Source, Year, CitationCounts, and Similarity Score.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      },
      {
        "solution_text": "Visualization Canvas shows a 2-D UMAP projection of the embedding space of the entire paper collection: hovering on a point highlights it, shows the corresponding title in a fixed tooltip below, and automatically scrolls the collection (table) to bring the corresponding paper (row) into the viewport; pressing Shift enables lasso-mode to select multiple points using a free-form lasso operation; zooming and panning support helps navigate the UMAP to specific regions; By default, each point in the UMAP is colored based on the state of the corresponding paper (\u201cDefault\u201d): Unfiltered (unfiltered and visible in the main paper collection table; dark-grey), Filtered (filtered out and not visible in the paper collection table; light-grey), Similarity Input (added to the By Papers section in the Similarity Search View; pink), Similarity Output (in the Output Similar table; orange), and Saved (added to the Saved Papers table; red). Other options to color include Source, Year, CitationCounts, and Similarity Score.",
        "solution_category": "interaction",
        "solution_axial": "Selecting,Filtering",
        "solution_compoent": "",
        "axial_code": ["Selecting", "Filtering"],
        "componenet_code": ["selecting", "filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 427,
    "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers",
    "pub_year": 2022,
    "domain": "academic career",
    "requirement": {
      "requirement_text": "T1 How does a factor influence career success over time? For lon-gitudinal comparison, the expert wishes to know how the impact ofa specific factor on career success develops over time. This couldbe related to the development of specific academic fields.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].",
      "data_code": {
        "tables": 1,
        "textual": 1,
        "ordinal": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).",
        "solution_category": "data_manipulation",
        "solution_axial": "UserInput,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["UserInput", "Clustering&Grouping"],
        "componenet_code": ["user_input", "clustering_and_grouping"]
      },
      {
        "solution_text": "The Horizon Chart Group summarizes the trends of factor impacts and supports the comparison among factors. Description: Each horizon chart represents a factor which is extended from a line chart. The x-axis encodes the time and the y-axis represents the explanatory power of a factor. The line chart is divided into layered bands with uniform ranges. The y value is encoded by a gradient color scheme in blue. The darker the color, the higher the value. Then the bands are shifted to the center and distributed within a fixed height. Two y scales are provided: a unified scale for impact comparison across factors and an independent scale for temporal inspection within a factor. Users can choose a factor for further analysis. Justification: Initially, we used line charts with two modes. However, a multi-line graph that includes all the factors in one coordinate suffered great visual clutters. Small multiples where each line chart represented a factor were also not appropriate, since the height of each line chart was too narrow to show the temporal trends, let alone the comparison among factors. Thus, we chose the horizon chart to show the impact of multiple factors in a compact way.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "area",
        "axial_code": ["Repetition"],
        "componenet_code": ["area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 428,
    "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers",
    "pub_year": 2022,
    "domain": "academic career",
    "requirement": {
      "requirement_text": "T2 How do multiple factors differ in their impacts on career suc-cess? The expert wants to know the effects of different factorsat a speci\ufb01c time as a cross-sectional comparison to determinethe dominant factors in\ufb02uencing career success. Speci\ufb01cally, it isinteresting to compare the impacts of individual and social factors.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].",
      "data_code": {
        "tables": 1,
        "textual": 1,
        "ordinal": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).",
        "solution_category": "data_manipulation",
        "solution_axial": "UserInput,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["UserInput", "Clustering&Grouping"],
        "componenet_code": ["user_input", "clustering_and_grouping"]
      },
      {
        "solution_text": "The Horizon Chart Group summarizes the trends of factor impacts and supports the comparison among factors. Description: Each horizon chart represents a factor which is extended from a line chart. The x-axis encodes the time and the y-axis represents the explanatory power of a factor. The line chart is divided into layered bands with uniform ranges. The y value is encoded by a gradient color scheme in blue. The darker the color, the higher the value. Then the bands are shifted to the center and distributed within a fixed height. Two y scales are provided: a unified scale for impact comparison across factors and an independent scale for temporal inspection within a factor. Users can choose a factor for further analysis. Justification: Initially, we used line charts with two modes. However, a multi-line graph that includes all the factors in one coordinate suffered great visual clutters. Small multiples where each line chart represented a factor were also not appropriate, since the height of each line chart was too narrow to show the temporal trends, let alone the comparison among factors. Thus, we chose the horizon chart to show the impact of multiple factors in a compact way.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "area",
        "axial_code": ["Repetition"],
        "componenet_code": ["area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 429,
    "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers",
    "pub_year": 2022,
    "domain": "academic career",
    "requirement": {
      "requirement_text": "T3 How does a category within a factor change over time to affectcareer success? The impact of a category can change at differentperiods. It re\ufb02ects the change of the roles of this category.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].",
      "data_code": {
        "tables": 1,
        "textual": 1,
        "ordinal": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).",
        "solution_category": "data_manipulation",
        "solution_axial": "UserInput,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["UserInput", "Clustering&Grouping"],
        "componenet_code": ["user_input", "clustering_and_grouping"]
      },
      {
        "solution_text": "The Horizon Chart Group summarizes the trends of factor impacts and supports the comparison among factors. Description: Each horizon chart represents a factor which is extended from a line chart. The x-axis encodes the time and the y-axis represents the explanatory power of a factor. The line chart is divided into layered bands with uniform ranges. The y value is encoded by a gradient color scheme in blue. The darker the color, the higher the value. Then the bands are shifted to the center and distributed within a fixed height. Two y scales are provided: a unified scale for impact comparison across factors and an independent scale for temporal inspection within a factor. Users can choose a factor for further analysis. Justification: Initially, we used line charts with two modes. However, a multi-line graph that includes all the factors in one coordinate suffered great visual clutters. Small multiples where each line chart represented a factor were also not appropriate, since the height of each line chart was too narrow to show the temporal trends, let alone the comparison among factors. Thus, we chose the horizon chart to show the impact of multiple factors in a compact way.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "area",
        "axial_code": ["Repetition"],
        "componenet_code": ["area"]
      },
      {
        "solution_text": "After specifying the factor in the Horizon Chart Group, users can use the Impact Timeline to study the detailed impacts of the factor obtained from the regression model.Users can observe the temporal trends of the impact and compare the effects of different categories within the factor through intuitive interactions.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      },
      {
        "solution_text": "Description: The Impact Timeline consists of two parts: a MatrixLine showing the detailed impact of a time window and a Navigator revealing the impact evolvement over time. Justification: We designed two line charts for two values as the navigator while it was space-wasting. We then assembled them into a dual-y chart with an area encoding the p-value. However, after trying the system, our expert suggested that the ranges of the p-values were more practical to learn the statistical significance than the raw values. Thus, we adopted a stepped line graph to fulfill the requirement.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "pie+bar+matrix+line",
        "axial_code": ["Stack"],
        "componenet_code": ["matrix", "line", "pie", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 430,
    "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers",
    "pub_year": 2022,
    "domain": "academic career",
    "requirement": {
      "requirement_text": "T3 How does a category within a factor change over time to affectcareer success? The impact of a category can change at differentperiods. It re\ufb02ects the change of the roles of this category.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].",
      "data_code": {
        "tables": 1,
        "textual": 1,
        "ordinal": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).",
        "solution_category": "data_manipulation",
        "solution_axial": "UserInput,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["UserInput", "Clustering&Grouping"],
        "componenet_code": ["user_input", "clustering_and_grouping"]
      },
      {
        "solution_text": "Navigator. A Navigator provides a temporal overview of two values(i.e., p-value and coefficient) of a selected category mentioned aboveor the numerical independent variables (i.e., IV10 \u2212 IV12 ) (T3). It isa timeline with a dual y-axis that represents p-value and coefficient,respectively. A stepped line graph in green shows the p-values and theline chart in black reveals the coefficients. The color of the circle on the line chart encodes the positive and negative values with the same colorscheme in the matrix. Users can quickly \ufb01nd the period of interest anddrag to focus the time window in MatrixLine. For the numerical factors,users can directly use Navigator to study the time-varying impacts.",
        "solution_category": "visualization",
        "solution_axial": "largepanel",
        "solution_compoent": "bar+line",
        "axial_code": ["largepanel"],
        "componenet_code": ["line", "bar"]
      },
      {
        "solution_text": "Users can quickly \ufb01nd the period of interest anddrag to focus the time window in MatrixLine. For the numerical factors,users can directly use Navigator to study the time-varying impacts.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 431,
    "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers",
    "pub_year": 2022,
    "domain": "academic career",
    "requirement": {
      "requirement_text": "T4 How do the categories within a factor differ from affecting ca-reer success? For each factor, different categories may have differ-ent sizes of impacts. Figuring out those with high impacts can helpexplain and provide guidelines for academic career development.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].",
      "data_code": {
        "tables": 1,
        "textual": 1,
        "ordinal": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).",
        "solution_category": "data_manipulation",
        "solution_axial": "UserInput,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["UserInput", "Clustering&Grouping"],
        "componenet_code": ["user_input", "clustering_and_grouping"]
      },
      {
        "solution_text": "The Horizon Chart Group summarizes the trends of factor impacts and supports the comparison among factors. Description: Each horizon chart represents a factor which is extended from a line chart. The x-axis encodes the time and the y-axis represents the explanatory power of a factor. The line chart is divided into layered bands with uniform ranges. The y value is encoded by a gradient color scheme in blue. The darker the color, the higher the value. Then the bands are shifted to the center and distributed within a fixed height. Two y scales are provided: a unified scale for impact comparison across factors and an independent scale for temporal inspection within a factor. Users can choose a factor for further analysis. Justification: Initially, we used line charts with two modes. However, a multi-line graph that includes all the factors in one coordinate suffered great visual clutters. Small multiples where each line chart represented a factor were also not appropriate, since the height of each line chart was too narrow to show the temporal trends, let alone the comparison among factors. Thus, we chose the horizon chart to show the impact of multiple factors in a compact way.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "area",
        "axial_code": ["Repetition"],
        "componenet_code": ["area"]
      },
      {
        "solution_text": "After specifying the factor in the Horizon Chart Group, users can use the Impact Timeline to study the detailed impacts of the factor obtained from the regression model.Users can observe the temporal trends of the impact and compare the effects of different categories within the factor through intuitive interactions.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      },
      {
        "solution_text": "Description: The Impact Timeline consists of two parts: a MatrixLine showing the detailed impact of a time window and a Navigator revealing the impact evolvement over time. Justification: We designed two line charts for two values as the navigator while it was space-wasting. We then assembled them into a dual-y chart with an area encoding the p-value. However, after trying the system, our expert suggested that the ranges of the p-values were more practical to learn the statistical significance than the raw values. Thus, we adopted a stepped line graph to fulfill the requirement.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "pie+bar+matrix+line",
        "axial_code": ["Stack"],
        "componenet_code": ["matrix", "line", "pie", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 432,
    "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers",
    "pub_year": 2022,
    "domain": "academic career",
    "requirement": {
      "requirement_text": "T4 How do the categories within a factor differ from affecting ca-reer success? For each factor, different categories may have differ-ent sizes of impacts. Figuring out those with high impacts can helpexplain and provide guidelines for academic career development.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].",
      "data_code": {
        "tables": 1,
        "textual": 1,
        "ordinal": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).",
        "solution_category": "data_manipulation",
        "solution_axial": "UserInput,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["UserInput", "Clustering&Grouping"],
        "componenet_code": ["user_input", "clustering_and_grouping"]
      },
      {
        "solution_text": "MatrixLine. Each matrix shows the MIA model results of a timewindow (T4). The design targets independent variables related to se-quence clustering results (i.e., IV1 \u2212 IV9 in Section 4.3). To show theimpacts of different categories (i.e., clusters) in individual\u2019s (IV1 \u2212 IV3 )and his top collaborator\u2019s factors (IV4 \u2212 IV6 ), our MIA model trans-forms these categories into dummy variables and uses post-hoc analysisto produce pairwise category comparisons. The output is an n \u2217 n ma-trix for coef\ufb01cients and p-values in the regression as mentioned inSection 4.3. Thus, in Fig. 4-A, we divide the matrix into an uppertriangle and a lower triangle to show the pairwise p-values and coef-\ufb01cients, respectively. In the upper triangle, each cell represents thepairwise p-value. The p-value is partitioned to four ranges (i.e., [0,0.001), [0.001, 0.01), [0.01, 0.05), [0.05, 1]) based on the traditionalsocial science approaches [11, 57] and our expert\u2019s suggestions to showthe statistical signi\ufb01cance. We use white for range [0.05, 1] (i.e., notstatistically signi\ufb01cant) and green in different saturations for the otherthree ranges to distinguish the statistical signi\ufb01cance at different levels.The smaller the p-values, the darker the green. In the lower triangle,each cell shows the coef\ufb01cient of pairwise categories. Our expert em-phasized the importance of studying both the absolute values and thepositive and negative of the coef\ufb01cients. Thus, we use blue and red toencode the positive and negative values, respectively. The saturationencodes the absolute values. The larger the absolute value, the darkerthe color. The bar charts and pie charts above the matrix summarizethe population and proportions of individuals in each category (i.e.,sequence cluster) respectively. The dark grey area in the pie chartrepresents the proportion of this cluster.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "pie+bar+matrix",
        "axial_code": ["Stack"],
        "componenet_code": ["matrix", "pie", "bar"]
      },
      {
        "solution_text": "To inspect the time-varying impact of a category, users can \ufb01rstspecify a reference category (Fig. 4-A1) to align all the matrices (Sec-tion 4.3) across time. It will skip matrices without the reference cate-gory. Then they can choose a target category by clicking its pie chart.Two cells showing the p-value and coef\ufb01cient will be highlighted acrosstime. The Navigator will also be updated to summarize the temporaltrends. The target category will be added to the Cluster View for furtheranalysis. Users can customize the number of clusters, the length of thetime window and the window moving step to adjust the MIA model.To show the impact of collaboration strength of each collaborator\u2019scategory of a social factor (i.e., IV7 \u2212 IV9 ), we transform the n \u2217 n matrixinto an n\u22172 one (Fig. 4-B). Each row represents a category of this factorand two columns represent the p-value and coef\ufb01cient respectively.Users can align a category to study its time-varying impact.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 433,
    "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers",
    "pub_year": 2022,
    "domain": "academic career",
    "requirement": {
      "requirement_text": "T4 How do the categories within a factor differ from affecting ca-reer success? For each factor, different categories may have differ-ent sizes of impacts. Figuring out those with high impacts can helpexplain and provide guidelines for academic career development.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].",
      "data_code": {
        "tables": 1,
        "textual": 1,
        "ordinal": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).",
        "solution_category": "data_manipulation",
        "solution_axial": "UserInput,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["UserInput", "Clustering&Grouping"],
        "componenet_code": ["user_input", "clustering_and_grouping"]
      },
      {
        "solution_text": "The Cluster View (Fig. 3-B) presents a list of categories (i.e., sequence clusters) chosen from the Impact Timeline. Three glyphs (Fig. 5-A, B, C) are designed to summarize the cluster of different sequences (T4). Description: Each row shows a sequence cluster of a time window with a title listing the window, the cluster label, and the number of researchers within the cluster. Each glyph of the row summarizes the distribution of researchers within the cluster at one year. We have designed three glyphs to show the cluster summary of three types of sequences (Fig. 5-A, B, C). The career glyph uses a sunburst struc_x0002_ture with two levels of hierarchy to show the career sequence event distribution. The inner ring shows three title ranks (i.e., junior, interme_x0002_diate, and senior) with purple in three saturation categories (Fig. 3-C). The outer ring encodes five citation ranks in black at five saturation categories. The domain glyph is a doughnut chart that records the dis_x0002_tribution of individuals\u2019 most frequently published paper venue types in the cluster. We highlight the visualization venues as pink and others as grey to show visualization researchers\u2019 domain diversity (Fig. 3-C). The sector glyph is also a doughnut chart showing the social sector (i.e., academia, industry, and government) distribution: green for the industry, blue for the academia, and yellow for the government agency (Fig. 3-C). The histogram on the right shows the citation distribution of the following year (i.e., DVs) of the sequence window. Users can compare the distribution of different clusters using the sequence list and choose a cluster of interest for further analysis.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph+area",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 434,
    "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers",
    "pub_year": 2022,
    "domain": "academic career",
    "requirement": {
      "requirement_text": "T5 How does a researcher\u2019s career path change over time? Theexpert wishes to identify different individuals from the data andstudy the multi-factor effects on career success from a micro per-spective. Revealing the career changes of a researcher over timecan help understand the different academic stages he goes over.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].",
      "data_code": {
        "tables": 1,
        "textual": 1,
        "ordinal": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).",
        "solution_category": "data_manipulation",
        "solution_axial": "UserInput,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["UserInput", "Clustering&Grouping"],
        "componenet_code": ["user_input", "clustering_and_grouping"]
      },
      {
        "solution_text": "The Cluster View (Fig. 3-B) presents a list of categories (i.e., sequence clusters) chosen from the Impact Timeline. Three glyphs (Fig. 5-A, B, C) are designed to summarize the cluster of different sequences (T4). Description: Each row shows a sequence cluster of a time window with a title listing the window, the cluster label, and the number of researchers within the cluster. Each glyph of the row summarizes the distribution of researchers within the cluster at one year. We have designed three glyphs to show the cluster summary of three types of sequences (Fig. 5-A, B, C). The career glyph uses a sunburst struc_x0002_ture with two levels of hierarchy to show the career sequence event distribution. The inner ring shows three title ranks (i.e., junior, interme_x0002_diate, and senior) with purple in three saturation categories (Fig. 3-C). The outer ring encodes five citation ranks in black at five saturation categories. The domain glyph is a doughnut chart that records the dis_x0002_tribution of individuals\u2019 most frequently published paper venue types in the cluster. We highlight the visualization venues as pink and others as grey to show visualization researchers\u2019 domain diversity (Fig. 3-C). The sector glyph is also a doughnut chart showing the social sector (i.e., academia, industry, and government) distribution: green for the industry, blue for the academia, and yellow for the government agency (Fig. 3-C). The histogram on the right shows the citation distribution of the following year (i.e., DVs) of the sequence window. Users can compare the distribution of different clusters using the sequence list and choose a cluster of interest for further analysis.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph+area",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph", "area"]
      },
      {
        "solution_text": "After choosing a cluster from the Cluster View, we use a CareerLine tovisualize an individual\u2019s careers and the effects of factors with foldedand unfolded modes in the Person View (Fig. 3-C, T5, T6).",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      },
      {
        "solution_text": "Description: In the folded mode (Fig. 6-A), the color of each circleshows the career title rank and the middle strip of the CareerLine represents the social sectors of the researcher, all with the same encoding inglyphs. Two flat gray areas distributed above and below the sector stripdepict the collaborator and domain diversity scores of the researcher,respectively. The outer light blue area shows the predicted citationsof the researcher based on the regression model. When hovering on acircle, a tooltip summarizing the researcher\u2019s domain diversity is shown(Fig. 3-C4). The outer ring is a doughnut chart summarizing the paperdistribution in different domains. The inner word cloud is generatedbased on the paper venue names from the bibliographic data. We breakthe venue names into separate words and count the frequency statisticsof each word to reflect the research topics. The word size encodes thenumber of papers.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "line+glyph+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "line", "text"]
      },
      {
        "solution_text": "Users can unfold the area of collaborator diversityscore to study details about the researcher\u2019s collaborators (Fig. 6-B). Amulti-line graph shows the collaborators\u2019 different diversity scores (i.e.,career, domain, and sector). They can click one of the lines (i.e., diversi-ties) to show the summary of the collaborators\u2019 population distributionvia corresponding glyphs (Fig. 5-A, B, C). We have provided sortingand filtering for users to choose researchers of interest. Users can rankthem by average predicted citations or the collaborator diversity scores.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore,Reconfigure,Filtering",
        "solution_compoent": "",
        "axial_code": ["Reconfigure", "Filtering", "OverviewandExplore"],
        "componenet_code": ["reconfigure", "filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 435,
    "paper_title": "Seek for Success: A Visualization Approach for Understanding the Dynamics of Academic Careers",
    "pub_year": 2022,
    "domain": "academic career",
    "requirement": {
      "requirement_text": "T6 How do the different factors of a researcher change over time?Showing the evolution of career factors over time is essential tointerpret the career development of a researcher and further validateexisting rules or generate new hypotheses.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "We manually check and filter out those from other fields (about 90 researchers) and finally obtain over 1,100 VIS researchers. We use researchers\u2019 names as inputs to search different data sources below, which are transformed into multiple sequences by each researcher for in-depth analysis. \u2022 Career data of researchers record the job-related attributes such as the institutions and titles. We collected it from LinkedIn [5], researchers\u2019 personal websites, and their institutional webpages. \u2022 Bibliographic data is directly gathered from Aminer [54], which includes over 21,600 papers in total based on these researchers. It includes all the publication metadata of a researcher (e.g., authors, year, venue, title, and abstract) by year. \u2022 Citation data (by year) is crawled from Google Scholar [4] as a measure of career success [21].",
      "data_code": {
        "tables": 1,
        "textual": 1,
        "ordinal": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "After collecting multiple data sources (Section 3.2), we preprocessed the data in a semi-automatic way. For the career data, we organized each job into an event with a timestamp (by year) and an institution. We manually tagged the job titles and sectors in career data. Job titles were tagged into three ranks (i.e., junior, intermediate, and senior) based on researchers\u2019 tenure in academic research (Fig. 2-B1). We also tagged three sectors: academia, industry, and government agency, based on the institutions (Fig. 2-B3). From the bibliographic data, we extracted the paper venues by year and classified them into twelve categories to represent different research domains based on [1] (Fig. 2-B2). We also extracted all the collaborators of a researcher by year to construct his ego-networks. For the citation data, we used Quartile [6] to divide each year\u2019s citations into four ranks of equal size (Fig. 2-B1). In addition, we separated the top 3% citation researchers from the top 25% to inspect the pioneers in the visualization field; Sequence History Analysis (SHA) [48] is an innovative approach to preserve more complex sequential information in two steps. First, Sequence Analysis [46] is applied to identify representative pat_x0002_terns over the historical sequences. A distance matrix is constructed to document the pairwise distances between raw sequences. Using this matrix, the sequences are clustered into groups (i.e., categories) based on clustering algorithms (e.g., k-means). It retains the most representa_x0002_tive sequential patterns over raw sequences and substantially reduces the computational demand in the subsequent multivariate analysis. Sec_x0002_ond, Event History Analysis [28] is used to analyze how these historical sequential patterns will affect the upcoming event. Different regression models will be applied to obtain the estimation of the effects; We have worked with our domain expert to enhance the SHA approach to support dynamic analysis of the impacts of multiple factors on academic careers over time. The whole framework consists of four components: sequence slicing, sequence clustering, multivariate linear regression, and cluster alignment (Fig. 2-D, E, F, G).",
        "solution_category": "data_manipulation",
        "solution_axial": "UserInput,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["UserInput", "Clustering&Grouping"],
        "componenet_code": ["user_input", "clustering_and_grouping"]
      },
      {
        "solution_text": "The Cluster View (Fig. 3-B) presents a list of categories (i.e., sequence clusters) chosen from the Impact Timeline. Three glyphs (Fig. 5-A, B, C) are designed to summarize the cluster of different sequences (T4). Description: Each row shows a sequence cluster of a time window with a title listing the window, the cluster label, and the number of researchers within the cluster. Each glyph of the row summarizes the distribution of researchers within the cluster at one year. We have designed three glyphs to show the cluster summary of three types of sequences (Fig. 5-A, B, C). The career glyph uses a sunburst struc_x0002_ture with two levels of hierarchy to show the career sequence event distribution. The inner ring shows three title ranks (i.e., junior, interme_x0002_diate, and senior) with purple in three saturation categories (Fig. 3-C). The outer ring encodes five citation ranks in black at five saturation categories. The domain glyph is a doughnut chart that records the dis_x0002_tribution of individuals\u2019 most frequently published paper venue types in the cluster. We highlight the visualization venues as pink and others as grey to show visualization researchers\u2019 domain diversity (Fig. 3-C). The sector glyph is also a doughnut chart showing the social sector (i.e., academia, industry, and government) distribution: green for the industry, blue for the academia, and yellow for the government agency (Fig. 3-C). The histogram on the right shows the citation distribution of the following year (i.e., DVs) of the sequence window. Users can compare the distribution of different clusters using the sequence list and choose a cluster of interest for further analysis.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph+area",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph", "area"]
      },
      {
        "solution_text": "After choosing a cluster from the Cluster View, we use a CareerLine tovisualize an individual\u2019s careers and the effects of factors with foldedand unfolded modes in the Person View (Fig. 3-C, T5, T6).",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      },
      {
        "solution_text": "Description: In the folded mode (Fig. 6-A), the color of each circleshows the career title rank and the middle strip of the CareerLine represents the social sectors of the researcher, all with the same encoding inglyphs. Two flat gray areas distributed above and below the sector stripdepict the collaborator and domain diversity scores of the researcher,respectively. The outer light blue area shows the predicted citationsof the researcher based on the regression model. When hovering on acircle, a tooltip summarizing the researcher\u2019s domain diversity is shown(Fig. 3-C4). The outer ring is a doughnut chart summarizing the paperdistribution in different domains. The inner word cloud is generatedbased on the paper venue names from the bibliographic data. We breakthe venue names into separate words and count the frequency statisticsof each word to reflect the research topics. The word size encodes thenumber of papers.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "line+glyph+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "line", "text"]
      },
      {
        "solution_text": "Users can unfold the area of collaborator diversityscore to study details about the researcher\u2019s collaborators (Fig. 6-B). Amulti-line graph shows the collaborators\u2019 different diversity scores (i.e.,career, domain, and sector). They can click one of the lines (i.e., diversi-ties) to show the summary of the collaborators\u2019 population distributionvia corresponding glyphs (Fig. 5-A, B, C). We have provided sortingand filtering for users to choose researchers of interest. Users can rankthem by average predicted citations or the collaborator diversity scores.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore,Reconfigure,Filtering",
        "solution_compoent": "",
        "axial_code": ["Reconfigure", "Filtering", "OverviewandExplore"],
        "componenet_code": ["reconfigure", "filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 436,
    "paper_title": "Real-Time Visual Analysis of High-Volume Social Media Posts",
    "pub_year": 2022,
    "domain": "Social media",
    "requirement": {
      "requirement_text": "Overview: Gain a continuous overview of major themes people currently talk about on social media.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The 20 Newsgroups data set. It contains nearly 20,000 posts spread across 20 different newsgroups, and the corresponding newsgroup of each post serves as a class label.",
      "data_code": {
        "ordinal": 1,
        "clusters_and_sets_and_lists": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "We establish two parallel and independent clustering processes with different levels of granularity (i.e., different thresholds for the maximum number of clusters). By default, the first, coarse-grained clustering does not extract more than 10 clusters to provide analysts with an interactive topical overview. The second process powers the diverse stream of representative posts with not more than 100 clusters per default. We set an upper limit of 10 for the number of main clusters so that we do not exceed the usual capacity of the analyst\u2019s short term memory, but both thresholds are adjustable.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Parallelclusterpoststogetdifferentlevelsofgranularity.",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 437,
    "paper_title": "Real-Time Visual Analysis of High-Volume Social Media Posts",
    "pub_year": 2022,
    "domain": "Social media",
    "requirement": {
      "requirement_text": "Details: Learn more about specific interesting themes",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The 20 Newsgroups data set. It contains nearly 20,000 posts spread across 20 different newsgroups, and the corresponding newsgroup of each post serves as a class label.",
      "data_code": {
        "ordinal": 1,
        "clusters_and_sets_and_lists": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "We also call the coarse-grained clusters topics and the more finegrained ones subtopics. It should be noted, however, that topics and subtopics do not form a classical hierarchy since both clustering processes are independent from each other. For each subtopic, we find its representative item, that is, the post closest to the respective centroid. Each post, therefore, has two cluster associations, one fine- and one coarse-grained, so each extracted representative item is also associated with exactly one topic. Analysts can select one or more topics to retrieve additional details, including a stream of representative posts that are associated with the selection and extracted relevant keyphrases. Such a selection of topics can be added as a new filter, which will create a new session layer that operates on the filtered stream. Hence, with our layered approach analysts can interactively increase the resolution and adapt the specificity of their analysis.",
        "solution_category": "data_manipulation",
        "solution_axial": "Retrieval",
        "solution_compoent": "Layeredapproachwithdifferentlevelsofgranularityincreaseresolution.",
        "axial_code": ["Retrieval"],
        "componenet_code": ["retrieval"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 438,
    "paper_title": "Real-Time Visual Analysis of High-Volume Social Media Posts",
    "pub_year": 2022,
    "domain": "Social media",
    "requirement": {
      "requirement_text": "Monitoring: Constantly monitor specific themes to keep track of new developments",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The 20 Newsgroups data set. It contains nearly 20,000 posts spread across 20 different newsgroups, and the corresponding newsgroup of each post serves as a class label.",
      "data_code": {
        "ordinal": 1,
        "clusters_and_sets_and_lists": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "On every update, the frequent phrases will be updated and new representative posts may be added. If a new subtopic appears or the representative item of a subtopic changes and is sufficiently different, the post will be added to the stream of representative items. The number of new items per update is limited because it correlates with the total number of subtopics. These items offer a diverse view of what is currently being posted since they originated from different clusters.",
        "solution_category": "data_manipulation",
        "solution_axial": "Real-TimeInput",
        "solution_compoent": "UpdatesforRepresentativePosts",
        "axial_code": ["Real-TimeInput"],
        "componenet_code": ["real_time_input"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 439,
    "paper_title": "Real-Time Visual Analysis of High-Volume Social Media Posts",
    "pub_year": 2022,
    "domain": "Social media",
    "requirement": {
      "requirement_text": "Dive-in: Make specific themes the center of the analysis and increase resolution",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The 20 Newsgroups data set. It contains nearly 20,000 posts spread across 20 different newsgroups, and the corresponding newsgroup of each post serves as a class label.",
      "data_code": {
        "ordinal": 1,
        "clusters_and_sets_and_lists": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "We also call the coarse-grained clusters topics and the more finegrained ones subtopics. It should be noted, however, that topics and subtopics do not form a classical hierarchy since both clustering processes are independent from each other. For each subtopic, we find its representative item, that is, the post closest to the respective centroid. Each post, therefore, has two cluster associations, one fine- and one coarse-grained, so each extracted representative item is also associated with exactly one topic. Analysts can select one or more topics to retrieve additional details, including a stream of representative posts that are associated with the selection and extracted relevant keyphrases. Such a selection of topics can be added as a new filter, which will create a new session layer that operates on the filtered stream. Hence, with our layered approach analysts can interactively increase the resolution and adapt the specificity of their analysis.",
        "solution_category": "data_manipulation",
        "solution_axial": "Retrieval",
        "solution_compoent": "Layeredapproachwithdifferentlevelsofgranularityincreaseresolution.",
        "axial_code": ["Retrieval"],
        "componenet_code": ["retrieval"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 450,
    "paper_title": "Interactive Visual Pattern Search on Graph Data via Graph Representation Learning",
    "pub_year": 2022,
    "domain": "pattern in Graphs ",
    "requirement": {
      "requirement_text": "T1 Browse/search the graph database. To start the query process,the user needs to be able to select from hundreds to thousandsof graphs. Therefore, the system should provide graph searchand filtering functionalities based on the category, the name, orgraph statistics such as the number of nodes/links. Besides that,a visualization showing an overview of all graphs in the databasewill be useful to help locate interesting graphs or clusters.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We evaluate the performance of the proposed system on 4 graph datasets in various domains: program workflow dataset (vehicle diagnostic), MSRC-21 (image processing), COX2 (chemistry) and Enzymes (biology). The workflow dataset contains \u223c500 individual workflow graphs with the number of nodes ranging from 5 to 150\u223c20 different types of nodes correspond to different diagnostic procedures. MSRC-21 [62] contains natural scene images with 21 object semantic labels. After the super-pixel extraction and processing steps as described in Section 5.2 and Fig. 7, the resulting graph dataset includes 544 graphs with 11 to 31 nodes. COX2 [46, 66] consists of 467 chemical molecule graphs with the number of nodes ranging from 32 to 56. Enzymes dataset [46, 58] contains 600 graphs of protein tertiary structure with 3 to 96 nodes. The last 3 datasets are public.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "We first define the subgraph matching problem and describe our overall framework to resolve it. We then describe NeuroMatch and NeuroAlign, the two GNNs as the core components of the framework. Finally, we introduce an improved inference method and a simple extension to support approximate query matching",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["Modeling", "SimilarityCalculation"],
        "componenet_code": ["modeling", "similarity_calculation"]
      },
      {
        "solution_text": "Overview and filters. In the overview panel the system displays the distribution of key graph statistics such as the number of the nodes/edges as well as domain-specific attributes such as the category of the graph. Both univariate distributions and bivariate distributions can be displayed as histograms or scatterplots.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "scatter+bar+table",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "scatter", "bar"]
      },
      {
        "solution_text": "Users can brush the charts and select a subset of graphs to create example-based query patterns. To provide an overview of the graph structural information and help users navigate and select a graph to start the query, we further precom- pute the graph editing distance [23] which roughly captures the struc- tural similarities between all pairs of graphs. A 2-D projection coordi- nates of the graph can then be precomputed using t-SNE [73] based on the distance matrix and stored as additional graph attributes.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 451,
    "paper_title": "Interactive Visual Pattern Search on Graph Data via Graph Representation Learning",
    "pub_year": 2022,
    "domain": "pattern in Graphs ",
    "requirement": {
      "requirement_text": "T2 Interactively construct the query pattern by selecting on agraph visualization. To minimize user effort, the system shouldsupport both bulk selection mechanisms such as brushing thegraph regions as well as query refinement methods to add/deleteindividual nodes/edges from the pattern.",
      "requirement_code": { "interactivity": 1, "data_filtering": 1 }
    },
    "data": {
      "data_text": "We evaluate the performance of the proposed system on 4 graph datasets in various domains: program workflow dataset (vehicle diagnostic), MSRC-21 (image processing), COX2 (chemistry) and Enzymes (biology). The workflow dataset contains \u223c500 individual workflow graphs with the number of nodes ranging from 5 to 150\u223c20 different types of nodes correspond to different diagnostic procedures. MSRC-21 [62] contains natural scene images with 21 object semantic labels. After the super-pixel extraction and processing steps as described in Section 5.2 and Fig. 7, the resulting graph dataset includes 544 graphs with 11 to 31 nodes. COX2 [46, 66] consists of 467 chemical molecule graphs with the number of nodes ranging from 32 to 56. Enzymes dataset [46, 58] contains 600 graphs of protein tertiary structure with 3 to 96 nodes. The last 3 datasets are public.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "We first define the subgraph matching problem and describe our overall framework to resolve it. We then describe NeuroMatch and NeuroAlign, the two GNNs as the core components of the framework. Finally, we introduce an improved inference method and a simple extension to support approximate query matching",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["Modeling", "SimilarityCalculation"],
        "componenet_code": ["modeling", "similarity_calculation"]
      },
      {
        "solution_text": "Graph query panel. In the graph query panel, the user can interactively select from a graph instance to construct the query pattern. The color of the nodes encodes the key node attribute to be matched in the subgraph pattern query. The system currently supports categorical node attributes. This can be extended to numerical attributes by quantizing the values. Additional node attributes are displayed in attachment to the nodes or in tooltips. We need to support fast, interactive query construction. In this panel, the user can quickly select a group of nodes and the subgraph they induce by brushing a rectangular area on the visualization. They can also construct the pattern in a more precise manner by clicking the + and - button on the top right corner of each node. A minimap on the bottom right of the panel allows the user to easily navigate and explore graphs of larger size. The layout of the graph is computed with existing layout algorithms, such as the algorithm described in [22] for directed graphs. When the nodes have inherent spatial locations, they are used directly for display.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "network+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["network", "text"]
      },
      {
        "solution_text": "Graph query panel. In the graph query panel, the user can interactively select from a graph instance to construct the query pattern. The color of the nodes encodes the key node attribute to be matched in the subgraph pattern query. The system currently supports categorical node attributes. This can be extended to numerical attributes by quantizing the values. Additional node attributes are displayed in attachment to the nodes or in tooltips. We need to support fast, interactive query construction. In this panel, the user can quickly select a group of nodes and the subgraph they induce by brushing a rectangular area on the visualization. They can also construct the pattern in a more precise manner by clicking the + and - button on the top right corner of each node. A minimap on the bottom right of the panel allows the user to easily navigate and explore graphs of larger size. The layout of the graph is computed with existing layout algorithms, such as the algorithm described in [22] for directed graphs. When the nodes have inherent spatial locations, they are used directly for display.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 452,
    "paper_title": "Interactive Visual Pattern Search on Graph Data via Graph Representation Learning",
    "pub_year": 2022,
    "domain": "pattern in Graphs ",
    "requirement": {
      "requirement_text": "T3 Interpret and validate the matched graphs via highlightedsimilarities and differences. To help users interpret the matchingresults, the node correspondences, as well as differences inthe query results, should be highlighted. Furthermore, sincethe subgraph matching and node correspondence calculationalgorithms are not 100% accurate, the results need to be presentedin a meaningful way for easy verification.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "We evaluate the performance of the proposed system on 4 graph datasets in various domains: program workflow dataset (vehicle diagnostic), MSRC-21 (image processing), COX2 (chemistry) and Enzymes (biology). The workflow dataset contains \u223c500 individual workflow graphs with the number of nodes ranging from 5 to 150\u223c20 different types of nodes correspond to different diagnostic procedures. MSRC-21 [62] contains natural scene images with 21 object semantic labels. After the super-pixel extraction and processing steps as described in Section 5.2 and Fig. 7, the resulting graph dataset includes 544 graphs with 11 to 31 nodes. COX2 [46, 66] consists of 467 chemical molecule graphs with the number of nodes ranging from 32 to 56. Enzymes dataset [46, 58] contains 600 graphs of protein tertiary structure with 3 to 96 nodes. The last 3 datasets are public.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "We first define the subgraph matching problem and describe our overall framework to resolve it. We then describe NeuroMatch and NeuroAlign, the two GNNs as the core components of the framework. Finally, we introduce an improved inference method and a simple extension to support approximate query matching",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["Modeling", "SimilarityCalculation"],
        "componenet_code": ["modeling", "similarity_calculation"]
      },
      {
        "solution_text": "Graph query panel. In the graph query panel, the user can interactively select from a graph instance to construct the query pattern. The color of the nodes encodes the key node attribute to be matched in the subgraph pattern query. The system currently supports categorical node attributes. This can be extended to numerical attributes by quantizing the values. Additional node attributes are displayed in attachment to the nodes or in tooltips. We need to support fast, interactive query construction. In this panel, the user can quickly select a group of nodes and the subgraph they induce by brushing a rectangular area on the visualization. They can also construct the pattern in a more precise manner by clicking the + and - button on the top right corner of each node. A minimap on the bottom right of the panel allows the user to easily navigate and explore graphs of larger size. The layout of the graph is computed with existing layout algorithms, such as the algorithm described in [22] for directed graphs. When the nodes have inherent spatial locations, they are used directly for display.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "network+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["network", "text"]
      },
      {
        "solution_text": "Graph query panel. In the graph query panel, the user can interactively select from a graph instance to construct the query pattern. The color of the nodes encodes the key node attribute to be matched in the subgraph pattern query. The system currently supports categorical node attributes. This can be extended to numerical attributes by quantizing the values. Additional node attributes are displayed in attachment to the nodes or in tooltips. We need to support fast, interactive query construction. In this panel, the user can quickly select a group of nodes and the subgraph they induce by brushing a rectangular area on the visualization. They can also construct the pattern in a more precise manner by clicking the + and - button on the top right corner of each node. A minimap on the bottom right of the panel allows the user to easily navigate and explore graphs of larger size. The layout of the graph is computed with existing layout algorithms, such as the algorithm described in [22] for directed graphs. When the nodes have inherent spatial locations, they are used directly for display.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      },
      {
        "solution_text": "Query results. After the sub-graph pattern matching results are returned, the query results panel will be updated to display all the matching graphs as a small multiples display. Since the number of returned results could be large, the system sup- ports sorting the returned graphs with graph attribute values such as the number of nodes. To support requirement, the matching nodes are highlighted based on the results returned by the node alignment module. The graphs can be displayed either in a node-link diagram with the same layout as the graph in the query panel or in a thumbnail visualization designed to display the graph in a more compact manner. In particular, we use topological sorting of the nodes for directed acyclic graphs to order the nodes, layout them vertically, and route the links on the right to obtain a compact view.",
        "solution_category": "data_manipulation",
        "solution_axial": "Retrieval",
        "solution_compoent": "",
        "axial_code": ["Retrieval"],
        "componenet_code": ["retrieval"]
      },
      {
        "solution_text": "Query results. After the sub-graph pattern matching results are returned, the query results panel will be updated to display all the matching graphs as a small multiples display. Since the number of returned results could be large, the system sup- ports sorting the returned graphs with graph attribute values such as the number of nodes. To support requirement, the matching nodes are highlighted based on the results returned by the node alignment module. The graphs can be displayed either in a node-link diagram with the same layout as the graph in the query panel or in a thumbnail visualization designed to display the graph in a more compact manner. In particular, we use topological sorting of the nodes for directed acyclic graphs to order the nodes, layout them vertically, and route the links on the right to obtain a compact view.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "network",
        "axial_code": ["Repetition"],
        "componenet_code": ["network"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 453,
    "paper_title": "Interactive Visual Pattern Search on Graph Data via Graph Representation Learning",
    "pub_year": 2022,
    "domain": "pattern in Graphs ",
    "requirement": {
      "requirement_text": "T3 Interpret and validate the matched graphs via highlightedsimilarities and differences. To help users interpret the matchingresults, the node correspondences, as well as differences inthe query results, should be highlighted. Furthermore, sincethe subgraph matching and node correspondence calculationalgorithms are not 100% accurate, the results need to be presentedin a meaningful way for easy verification.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "We evaluate the performance of the proposed system on 4 graph datasets in various domains: program workflow dataset (vehicle diagnostic), MSRC-21 (image processing), COX2 (chemistry) and Enzymes (biology). The workflow dataset contains \u223c500 individual workflow graphs with the number of nodes ranging from 5 to 150\u223c20 different types of nodes correspond to different diagnostic procedures. MSRC-21 [62] contains natural scene images with 21 object semantic labels. After the super-pixel extraction and processing steps as described in Section 5.2 and Fig. 7, the resulting graph dataset includes 544 graphs with 11 to 31 nodes. COX2 [46, 66] consists of 467 chemical molecule graphs with the number of nodes ranging from 32 to 56. Enzymes dataset [46, 58] contains 600 graphs of protein tertiary structure with 3 to 96 nodes. The last 3 datasets are public.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "We first define the subgraph matching problem and describe our overall framework to resolve it. We then describe NeuroMatch and NeuroAlign, the two GNNs as the core components of the framework. Finally, we introduce an improved inference method and a simple extension to support approximate query matching",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["Modeling", "SimilarityCalculation"],
        "componenet_code": ["modeling", "similarity_calculation"]
      },
      {
        "solution_text": "Comparison view. To support requirement, we further visualize the query and selected matching graphs side-by-side in a popup window. The user can click on the zoom-in button on each small multiple to bring out the comparison view and review each matching graph in detail. The matched nodes are highlighted for verification.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "network",
        "axial_code": ["Repetition"],
        "componenet_code": ["network"]
      },
      {
        "solution_text": "The user can click on the zoom-in button on each small multiple to bring out the comparison view and review each matching graph in detail. The matched nodes are highlighted for verification.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore,Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering", "OverviewandExplore"],
        "componenet_code": ["filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 454,
    "paper_title": "Interactive Visual Pattern Search on Graph Data via Graph Representation Learning",
    "pub_year": 2022,
    "domain": "pattern in Graphs ",
    "requirement": {
      "requirement_text": "T4 Explore the distribution of the matching instances. After thematched graphs are returned, the system should indicate howfrequently the query pattern occurs in the entire database, andprovide the distribution of the pattern among different categoriesof graphs in the database.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "We evaluate the performance of the proposed system on 4 graph datasets in various domains: program workflow dataset (vehicle diagnostic), MSRC-21 (image processing), COX2 (chemistry) and Enzymes (biology). The workflow dataset contains \u223c500 individual workflow graphs with the number of nodes ranging from 5 to 150\u223c20 different types of nodes correspond to different diagnostic procedures. MSRC-21 [62] contains natural scene images with 21 object semantic labels. After the super-pixel extraction and processing steps as described in Section 5.2 and Fig. 7, the resulting graph dataset includes 544 graphs with 11 to 31 nodes. COX2 [46, 66] consists of 467 chemical molecule graphs with the number of nodes ranging from 32 to 56. Enzymes dataset [46, 58] contains 600 graphs of protein tertiary structure with 3 to 96 nodes. The last 3 datasets are public.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "We first define the subgraph matching problem and describe our overall framework to resolve it. We then describe NeuroMatch and NeuroAlign, the two GNNs as the core components of the framework. Finally, we introduce an improved inference method and a simple extension to support approximate query matching",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["Modeling", "SimilarityCalculation"],
        "componenet_code": ["modeling", "similarity_calculation"]
      },
      {
        "solution_text": "After the query result is obtained, the charts will be updated to provide a contextual view of how the subgraph pattern occurs in the database. For example, the user can observe whether the pattern occur- rence concentrate on a small subset of graph categories or it is a generic pattern that appears in many different categories",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 455,
    "paper_title": "Interactive Visual Pattern Search on Graph Data via Graph Representation Learning",
    "pub_year": 2022,
    "domain": "pattern in Graphs ",
    "requirement": {
      "requirement_text": "T5 Refine query results. A flexible query system should furthersupport query refinement mechanism where the users can applytheir domain knowledge to filter the results with additionalconstraints, such as matching additional node attributes or limitingthe results to a certain category of graphs.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "We evaluate the performance of the proposed system on 4 graph datasets in various domains: program workflow dataset (vehicle diagnostic), MSRC-21 (image processing), COX2 (chemistry) and Enzymes (biology). The workflow dataset contains \u223c500 individual workflow graphs with the number of nodes ranging from 5 to 150\u223c20 different types of nodes correspond to different diagnostic procedures. MSRC-21 [62] contains natural scene images with 21 object semantic labels. After the super-pixel extraction and processing steps as described in Section 5.2 and Fig. 7, the resulting graph dataset includes 544 graphs with 11 to 31 nodes. COX2 [46, 66] consists of 467 chemical molecule graphs with the number of nodes ranging from 32 to 56. Enzymes dataset [46, 58] contains 600 graphs of protein tertiary structure with 3 to 96 nodes. The last 3 datasets are public.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "textual": 1
      }
    },
    "solution": [
      {
        "solution_text": "We first define the subgraph matching problem and describe our overall framework to resolve it. We then describe NeuroMatch and NeuroAlign, the two GNNs as the core components of the framework. Finally, we introduce an improved inference method and a simple extension to support approximate query matching",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling,SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["Modeling", "SimilarityCalculation"],
        "componenet_code": ["modeling", "similarity_calculation"]
      },
      {
        "solution_text": "Comparison view. To support requirement, we further visualize the query and selected matching graphs side-by-side in a popup window. The user can click on the zoom-in button on each small multiple to bring out the comparison view and review each matching graph in detail. The matched nodes are highlighted for verification.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "network",
        "axial_code": ["Repetition"],
        "componenet_code": ["network"]
      },
      {
        "solution_text": "The user can click on the zoom-in button on each small multiple to bring out the comparison view and review each matching graph in detail. The matched nodes are highlighted for verification.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore,Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering", "OverviewandExplore"],
        "componenet_code": ["filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 456,
    "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines",
    "pub_year": 2022,
    "domain": "manufacturing",
    "requirement": {
      "requirement_text": "T1 Gain overview of engines: By taking the role of explorers, en-gineers need a structured overview of the data, in our case of enginesrepresented by their acoustic signals. Grouping engines by the similar-ity of acoustic signals using clustering algorithms would be desirable.In addition, interactive grouping to adapt to the information need ofindividual engineers would be useful, e.g., based on different steeringparameters (such as the number of groups) or filtered subsets of engines.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "In (A), all clusters of similar Hypermatrices are displayed. Each Hypermtarix in the grid is represented by the mean aggregation of all Hypermatrices in a cluster of engines. The grid view serves to get an overview over all clusters and their individual properties and to support the decision of which cluster to select.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "matrix+pie+glyph",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph", "pie", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 457,
    "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines",
    "pub_year": 2022,
    "domain": "manufacturing",
    "requirement": {
      "requirement_text": "T1 Gain overview of engines: By taking the role of explorers, en-gineers need a structured overview of the data, in our case of enginesrepresented by their acoustic signals. Grouping engines by the similar-ity of acoustic signals using clustering algorithms would be desirable.In addition, interactive grouping to adapt to the information need ofindividual engineers would be useful, e.g., based on different steeringparameters (such as the number of groups) or filtered subsets of engines.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "First, they give an additional overview over groups of engines (T1 ), because clusterswhich are already completely labeled are less interesting for an analysis.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 458,
    "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines",
    "pub_year": 2022,
    "domain": "manufacturing",
    "requirement": {
      "requirement_text": "T2 Drill-down to engines: Engineers exploring large numbers ofengines need support for the drill-down to engines of interest. Theinformation need may differ between clusters of engines, the labelingstatus, or the degree of the anomaly of engines.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "In (A), all clusters of similar Hypermatrices are displayed. Each Hypermtarix in the grid is represented by the mean aggregation of all Hypermatrices in a cluster of engines. The grid view serves to get an overview over all clusters and their individual properties and to support the decision of which cluster to select.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "matrix+pie+glyph",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph", "pie", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 459,
    "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines",
    "pub_year": 2022,
    "domain": "manufacturing",
    "requirement": {
      "requirement_text": "T3 Identify engine of interest: The work\ufb02ow of both explorers andcon\ufb01rmers includes the identi\ufb01cation of single engines as an entry pointinto an in-depth analysis. Identi\ufb01cation may be through knowledgeabout a particular engine, by special or even unique acoustic signatures,or by traversing several (similar) engines in the exploration process.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "(B) shows engines in a cluster, the Hypermatrix of each engine as small multiple, and the aggregated deviation of the signature of a single engine to all other engines. This view is designed to get an overview of the engine\u2019s properties and compare them to other engines in the cluster. Thus, it supports the user in selecting an engine.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "matrix+text+glyph",
        "axial_code": ["Repetition"],
        "componenet_code": ["text", "glyph", "matrix"]
      },
      {
        "solution_text": "This view is designed to get an overview of the engine\u2019s properties and compare them to other engines in the cluster. Thus, it supports the user in selecting an engine.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 460,
    "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines",
    "pub_year": 2022,
    "domain": "manufacturing",
    "requirement": {
      "requirement_text": "T3 Identify engine of interest: The work\ufb02ow of both explorers andcon\ufb01rmers includes the identi\ufb01cation of single engines as an entry pointinto an in-depth analysis. Identi\ufb01cation may be through knowledgeabout a particular engine, by special or even unique acoustic signatures,or by traversing several (similar) engines in the exploration process.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Second, they support the selection of an engine (T3 ), since engines thatcontain a label are also less probable to be selected",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 461,
    "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines",
    "pub_year": 2022,
    "domain": "manufacturing",
    "requirement": {
      "requirement_text": "T4 Analyze single engine: Engineers need to analyze individualengines in detail for being able to assign labels on a profound basis.Single engines are both represented by their acoustic signature and theraw acoustic measurements. The analysis is also supported by storeddomain knowledge from the systems knowledge base.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "In (C), the detailed Hypermatrix of a selected engine is shown. Additional information about single selected cells in the matrix is displayed in the upper left triangle. This view is designed to support the user in the analysis of a selected engine. A matrix representation is adequate to represent the relation between pairs of sub-components (e.g. Gear and rotor shaft). The regions of sub-components in the Hypermatrix are marked with additional black lines. The same color scheme as in the cluster view for Hypermatrices is applied. The selection of a cell in the Hypermatrix is supported by additional lines and triangles (dark and light grey) for each axis.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "matrix",
        "axial_code": ["Repetition"],
        "componenet_code": ["matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 462,
    "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines",
    "pub_year": 2022,
    "domain": "manufacturing",
    "requirement": {
      "requirement_text": "T4 Analyze single engine: Engineers need to analyze individualengines in detail for being able to assign labels on a profound basis.Single engines are both represented by their acoustic signature and theraw acoustic measurements. The analysis is also supported by storeddomain knowledge from the systems knowledge base.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "(D) visualizes a spectrogram of the selected engine. This type of visualization is the same one, engineers analyze during their daily routines and is used to support the analysis of a single engine. Here, a diverging color scale is used. Blueish colors represent acoustic measurements, which are more quiet compared to all other engines, and reddish colors louder ones. This kind of color scale is appropriate since all values spread around 0 and deviate in different directions.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "matrix",
        "axial_code": ["Repetition"],
        "componenet_code": ["matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 463,
    "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines",
    "pub_year": 2022,
    "domain": "manufacturing",
    "requirement": {
      "requirement_text": "T4 Analyze single engine: Engineers need to analyze individualengines in detail for being able to assign labels on a profound basis.Single engines are both represented by their acoustic signature and theraw acoustic measurements. The analysis is also supported by storeddomain knowledge from the systems knowledge base.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "(F) shows three different views. First, the line chart displays two orders from the spectrogram across their rpm values. Second, the scatterplot shows the correlation of the selected order pair. Third, the bar chart shows the aggregated deviations for a region in the Hypermatrix to all other regions as indicated in Figure 6. These views are designed to facilitate the analysis of an engine, where their input data are displayed when hovering over a cell in (C). In our application domain, deviations above and below three tend to be reasons for errors in the selected part. Thus, lines above and below this limit are marked as red for (+3) and blue for (-3). The purpose of the scatterplot is to provide additional information on how the two selected order lines correlate with each other. An additional overview about the five most deviating sub-component pairs in (B) is provided as a bar chart view in (E). To be consistent with our use of colors, red bars represent aggregated deviations greater than zero and blue ones lower zero.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "line+scatter+bar",
        "axial_code": ["Stack"],
        "componenet_code": ["line", "scatter", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 464,
    "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines",
    "pub_year": 2022,
    "domain": "manufacturing",
    "requirement": {
      "requirement_text": "T4 Analyze single engine: Engineers need to analyze individualengines in detail for being able to assign labels on a profound basis.Single engines are both represented by their acoustic signature and theraw acoustic measurements. The analysis is also supported by storeddomain knowledge from the systems knowledge base.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Third, they help inthe analysis of a selected engine (T4 ). This is because they are immediately displayed in the engine list view in (B) and thus give hints on the probability of a label for the selected engine",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 465,
    "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines",
    "pub_year": 2022,
    "domain": "manufacturing",
    "requirement": {
      "requirement_text": "T4 Analyze single engine: Engineers need to analyze individualengines in detail for being able to assign labels on a profound basis.Single engines are both represented by their acoustic signature and theraw acoustic measurements. The analysis is also supported by storeddomain knowledge from the systems knowledge base.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "First, users are able to review annotations from previous analysesof other engineers. By selecting a label from the bar chart view inFigure 1-E all other annotations for the same label are displayed in thespectrogram. This supports the user in the analysis of an engine (T4 ).",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 466,
    "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines",
    "pub_year": 2022,
    "domain": "manufacturing",
    "requirement": {
      "requirement_text": "T5 Assign label to engine: Engineers need to assign labels of errorcategories. In close collaboration, we formed a default alphabet oflabel categories as follows: Error/Electromagnetic-Field, Error/FirstGear, Error/Second Gear, Error/A-Bearing, Error/B-Bearing,Error/C-Bearing, No error. In addition, engineers also expressed theneed to leave the label alphabet open for modi\ufb01cations, as knowledgeabout error variations will constantly grow as IRVINE is used. Finally,to enhance ef\ufb01ciency in combination with T2 , it would be desirableto label single engines but also multiple similar engines at a glance.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "(E) shows the distribution of already assigned labels as a bar chart. The main purpose of the view is to support the labeling of (selections of) engines, as outlined in detail in Section 6.3. To show label distributions, a bar chart is an obvious choice. It would have been possible to use a pie chart, but for the labels that would have broken the guideline that there should be no more than six segments [24]. In the example in figure, all but one engine in the cluster are labeled as B-Bearing error.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "bar+table",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 467,
    "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines",
    "pub_year": 2022,
    "domain": "manufacturing",
    "requirement": {
      "requirement_text": "T5 Assign label to engine: Engineers need to assign labels of errorcategories. In close collaboration, we formed a default alphabet oflabel categories as follows: Error/Electromagnetic-Field, Error/FirstGear, Error/Second Gear, Error/A-Bearing, Error/B-Bearing,Error/C-Bearing, No error. In addition, engineers also expressed theneed to leave the label alphabet open for modi\ufb01cations, as knowledgeabout error variations will constantly grow as IRVINE is used. Finally,to enhance ef\ufb01ciency in combination with T2 , it would be desirableto label single engines but also multiple similar engines at a glance.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "(E) shows the distribution of already assigned labels as a bar chart. The main purpose of the view is to support the labeling of (selections of) engines, as outlined in detail in Section 6.3. To show label distributions, a bar chart is an obvious choice. It would have been possible to use a pie chart, but for the labels that would have broken the guideline that there should be no more than six segments [24]. In the example in figure, all but one engine in the cluster are labeled as B-Bearing error.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "bar+table",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "bar"]
      },
      {
        "solution_text": "After an analysis is complete, the user can assign a label to the com-ponent in (E) with the two list views (T5 ).",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 468,
    "paper_title": "IRVINE: A Design Study on Analyzing Correlation Patterns of Electrical Engines",
    "pub_year": 2022,
    "domain": "manufacturing",
    "requirement": {
      "requirement_text": "T6 Annotate acoustic measurements: The cause of a labeled enginecan be annotated by marking the respective region inside of the acousticraw data of an engine (the spectrogram) serving two purposes. First,annotations can be used to review how similar labels were annotated bydifferent users. Second, a suf\ufb01cient amount of annotations for a givenlabel will allow building thresholds for semi-automatic error detection.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "For the first round of interviews, we used acoustic data from 434 randomly selected engines over a period of six months. The data did not contain any labels nor annotations. For the second round of interviews, acoustic data from 308 completely new engines from a period of four months were uploaded to the system; Engineers record their data inside a test bench with a sensor for mea_x0002_suring the noise of the engine. For that purpose, the engine\u2019s rpm is steered under controlled conditions to analyze the behavior of engines. Measuring the noise allows the evaluation of the acoustic properties of components and their technical condition. The result of such acous_x0002_tic measurements is a three-dimensional data structure consisting of loudness measured across all possible combinations of rpm and orders. To shift focus on anomalies, the loudness values are often replaced by residuals, e.g., by the deviation from measured values to mean values \u00b5 of an ensemble of engines (expected value), divided by the standard deviation \u03c3 (see also eq. (1)) A 2D pixel-based visualization of these measurements often used by engineers is the spectrogram, as shown in Figure 3. The rpm value is displayed on the y-axis, describing the acceleration of the engine up to a maximum speed. The order is displayed on the x-axis, which is the relation between a measured frequency and the speed of the engine during measurement, consequently describing how often an excitation occurs per revolution. Orders are beneficial, as they allow the fine-grained analysis of sub-components through an expert\u2019s eye. By analyzing the geometry of the rotating engine, engineers are able to derive the measured sub-component of an engine up to a very detailed level (e.g. 24th teeth of a gear). Residuals are shown on a color scale, where brighter colors represent particularly loud orders. According to the engineers, in serial manufacturing regions of loud frequencies are a good indicator for different types of errors. The three-dimensional measurement data forms the basic representation of an engine through abstract data, providing the raw data for any analysis scenario on engine errors in IRVINE. Figure 3 shows a residual order-spectrogram. Specifically, 512 order lines (each column in Figure 3) ranging from 2500 - 14000 rpm are recorded for each engine. To identify signatures in order combinations that might result in a faulty engine, theoretically every possible order combination has to be analyzed manually for each engine. For engineers, this would result in 262,144 possible combinations, which is not feasible for a manual analysis. Hence, the engineers narrowed down the orders to the 41 most relevant ones. Based on the informed selection of orders, they can be connected to the seven main sub-components of an engine (rotor-shaft-, electromagnetic-, first gear-, second gear-, A-bearing-, B-bearing-, and C-bearing orders). Knowing the connected sub-components for individual orders allows the fine_x0002_grained analysis and labeling of engine errors at a sub-component level.",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "As a first step, we calculate the mean and standard deviation for the ensemble of engines at hand. Consequently, the relative deviation of the i-th engine to the mean \u00b5 for each (rpm, order)-tuple is then expressed in units of the standard deviation \u03c3. An example of the resulting residual spectrogram described in Section 4.3 is given in (A). Next, we extract two measurements A and B (being 1D- curves each), and calculate the outer product for the pair, resulting in a 2D-matrix (see (B)). The choice of possible pairs is restricted to 41 relevant orders from the data abstraction, which are used to derive relevant order combinations. Calculating the mean value of the 2D-matrices over all engines for each entry (C) results in the correlation matrix (D). This correlation matrix effectively consists of Pearson correlations for pairs of rpm-values of two extracted orders. Therefore, the resolution regarding different engine speeds and the corresponding orders is still retained. To extract the difference in the correlation, we subtract the correlation matrix from each outer product resulting in a matrix describing correlations inside each measurement (E). This matrix is then reduced onto its cumulated deviation using the Frobenius-norm. Consequently, each cumulated deviation for each pair of orders is then ordered into the reduced-Difference-Correlation-Matrix, which we call Hypermatrix; To group similar acoustic signatures, we extract features from each en_x0002_gine\u2019s Hypermatrix. The feature extraction process is shown in Figure 6. The combination of all seven sub-components results in 28 possible combinations and is depicted as Region (R) in Figure 6. From each R, the sub-matrix is extracted. After experimenting with different feature sets, we received the best clustering results extracting the maximum of each of the 28 resulting matrices. This results in a 28x1 feature vector. To cluster similar Hypermatrices, the 28x1 feature vector is used as input for a Self-Organizing Map (SOM) [32], as it nicely combines clustering with dimensionality reduction functionality. For the computation of the SOM, we follow a simplified version of the standard training process described by Kohonen [31]. We set the SOM grid size such as to expect at least one data vector per node, which accounts for very specific error types. We initialize the SOM prototype vector dimensions with random numbers between 0 and 1 and train the SOM by iterating over the input data vectors and adjusting the SOM nodes. Specifically, we find for each input data vector the best matching SOM prototype unit (BMU) according to Euclidean distance. We then adjust the BMU and its neighborhood according to a linearly decreasing learning rate and circular neighborhood kernel. This configuration comprises the initial implementation of our SOM and can be changed by the user as described in detail in Section 6.2. We note that our heuristic setting of parameters already gave us robust results for our application, hence, we did not see the need for parameter optimizations. In principle, also other visual clustering techniques may be applied besides SOM. We particularly chose SOM because of its robustness in our application domain, and as it gives an overlap-free rectangular layout that well supports visual comparison tasks.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "In (C), the detailed Hypermatrix of a selected engine is shown. Additional information about single selected cells in the matrix is displayed in the upper left triangle. This view is designed to support the user in the analysis of a selected engine. A matrix representation is adequate to represent the relation between pairs of sub-components (e.g. Gear and rotor shaft). The regions of sub-components in the Hypermatrix are marked with additional black lines. The same color scheme as in the cluster view for Hypermatrices is applied. The selection of a cell in the Hypermatrix is supported by additional lines and triangles (dark and light grey) for each axis.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "matrix",
        "axial_code": ["Repetition"],
        "componenet_code": ["matrix"]
      },
      {
        "solution_text": "By hovering over a cell in (C) the according orders of a pair of sub-components are displayed in the spectrogram. In the example in Figure 1, the sub-component pair (B-Bearing and A-Bearing) is selected. The former is shown with a triangle in dark grey and the latter with light grey. An engine can be annotated by clicking on \u201cAdd Annotation\u201d.",
        "solution_category": "interaction",
        "solution_axial": "Filtering,Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration", "Filtering"],
        "componenet_code": ["participation/collaboration", "filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 481,
    "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
    "pub_year": 2022,
    "domain": "Life Sciences",
    "requirement": {
      "requirement_text": "For each approach, show similar patients, based on symptom severity at a specific time point.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.",
      "data_code": {
        "temporal": 1,
        "tables": 1,
        "quantitative": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "We first organized the symptom ratings into a patient-symptom matrix for the selected time point, where each element (i, j) corresponds to the score given to symptom j by patient i at that time point. Prior research in symptom cluster for HNC [34] had applied hierarchical clustering using Ward\u2019s method [48] with Euclidean distance on the patient-symptom matrix to group patients based on their raw symptom ratings. After alternative clustering with complete and average linkages, we found that Ward\u2019s method generated larger, more informative groups of high symptom patients, which made sense to the clinicians. We identified two patient groups with high and low symptom burden (T1.1). This two-group clustering was preferred by clinicians, who found it easier to compare two groups instead of more. The axes of the scatterplot correspond to the first two components obtained by applying PCA to the patient-symptom matrix. Clusters for a specific time point are extracted and displayed, while clusters for different timepoints can be investigated via the time slider, which will update the scatterplot.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Clustering&Grouping"],
        "componenet_code": [
          "dimensionality_reduction",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "The scatterplot was customized to separately capture acute and late symptom burden distribution as identified by the symptom clusters, and to reflect via marker color, shape, and size the therapeutic combination administered to each patient, their gender, and their disease stage.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "scatter+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "scatter"]
      },
      {
        "solution_text": "The data can be filtered by attributes, and filtering operations update the other views. A filtering control panel serves double duty, by also providing the plot legend. This customized scatterplot encoding effectively captured the symptom distribution across the patient population, patient outliers, and therapeutic distribution across the data.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 482,
    "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
    "pub_year": 2022,
    "domain": "Life Sciences",
    "requirement": {
      "requirement_text": "For each approach, detect correlations among symptoms, during and after treatment.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.",
      "data_code": { "tables": 1, "quantitative": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "We applied ARM to each of the acute stage and the late stage, and empirically chose to illustrate the top 20 rules yielded by this approach, because only a small number of rules were of clinical interest. We chose minimum values for the support and lift metrics that were suitable for frequent and interdependent symptoms.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "From the many possible encodings of ARMs [38], we selected a node-link representation (Fig. 1.A), which was deemed by clinicians to be more friendly to broader audiences (A3), and a good fit for the rela_x0002_tively small number of nodes. Graphs are laid out using a force-directed layout algorithm based on statistical multidimensional scaling [39, 79], which results in nodes with high degree being placed centrally.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "Network+bubble+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["text", "bubble", "Network"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 483,
    "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
    "pub_year": 2022,
    "domain": "Life Sciences",
    "requirement": {
      "requirement_text": "For each approach, detect correlations among symptoms, during and after treatment.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.",
      "data_code": {
        "temporal": 1,
        "tables": 1,
        "quantitative": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Designing an appropriate encoding for the symptom longitudinal data(A2) turned out to be particularly challenging, primarily due to thenature and richness of the temporal data, the acknowledged variabilityin ratings across patients, and the missing or uneven time points, whichwere expected in this context. The design process explored a widerange of possible temporal encodings, many of which suffered fromscalability issues and, after several sessions, focused on a promising encoding called a \u201dtendril plot\u201d [51]. A tendril plot is a visual summaryof the incidence, significance, and temporal aspects of adverse eventsin clinical trials, in which individual temporal threads, one per eachpatient, emanate from a common root and shoot upwards and curl eitherto the left or to the right depending on whether the next event in thetimeline was adverse or an improvement. For clinical trial data, tendrilswere shown to create beautiful, compact, naturally clustering pathlinesillustrating the positive or negative evolution of each group of patients.The clinicians had also seen this representation and thought it couldwork (T1.2, T2.2). Whereas promising on paper, unfortunately, thetendril implementation did not yield similarly clean illustrations forthe symptom data, because of the much smaller number of time points,the variability in therapeutic sequences, and the variability in patientoutcomes, which are not typical of clinical trials.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 484,
    "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
    "pub_year": 2022,
    "domain": "Life Sciences",
    "requirement": {
      "requirement_text": "For each approach, detect correlations among symptoms, during and after treatment.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.",
      "data_code": {
        "temporal": 1,
        "tables": 1,
        "quantitative": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The percentile heatmap (Fig. 1.D) is a custom representation show_x0002_ing the rating distribution of individual symptoms over time, for the entire patient cohort (T2.3). We arrived at this representation after exploring a variety of alternatives such as stacked line plots, parallel coordinates plots, and radar charts, guided by feedback from collabora_x0002_tors. We settled on a matrix-based layout due to its compactness and to its ability to support small multiple plots. Each row corresponds to a symptom, with rows grouped by symptom category, and each column corresponds to a time point. Each cell in this matrix is a horizontal bar graph showing via shade the percentage of patients reporting within a specific range (0, 1-5, 6-9, or 10) for that symptom, at that time point. The bar height maps the percentage of individuals from the entire co_x0002_hort who reported the symptom ratings at that time point. The current patient is indicated in this heatmap by cross markers (Fig. 4.C) (T3.1). This encoding proved to be an intuitive way of showing what symptoms produce a higher burden for patients, and when, as well as to indicate how many patients were affected by these symptoms from the entire cohort (T1.2, T2.3)",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "matrix+bar+heatmap",
        "axial_code": ["Nesting"],
        "componenet_code": ["heatmap", "bar", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 485,
    "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
    "pub_year": 2022,
    "domain": "Life Sciences",
    "requirement": {
      "requirement_text": "For each approach, detect correlations among symptoms, during and after treatment.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.",
      "data_code": {
        "textual": 1,
        "temporal": 1,
        "tables": 1,
        "quantitative": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Additionally, a compact correlation matrix along with the percentile heatmap, supports requirement, by showing the strength of the correlation between a selected symptom and all other symptoms, with circles encoding Spearman\u2019s coefficient via color and size.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "matrix+text+circle",
        "axial_code": ["Stack"],
        "componenet_code": ["text", "circle", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 486,
    "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
    "pub_year": 2022,
    "domain": "Life Sciences",
    "requirement": {
      "requirement_text": "For each approach, detect patient outliers and trends.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.",
      "data_code": {
        "temporal": 1,
        "tables": 1,
        "quantitative": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "We first organized the symptom ratings into a patient-symptom matrix for the selected time point, where each element (i, j) corresponds to the score given to symptom j by patient i at that time point. Prior research in symptom cluster for HNC [34] had applied hierarchical clustering using Ward\u2019s method [48] with Euclidean distance on the patient-symptom matrix to group patients based on their raw symptom ratings. After alternative clustering with complete and average linkages, we found that Ward\u2019s method generated larger, more informative groups of high symptom patients, which made sense to the clinicians. We identified two patient groups with high and low symptom burden (T1.1). This two-group clustering was preferred by clinicians, who found it easier to compare two groups instead of more. The axes of the scatterplot correspond to the first two components obtained by applying PCA to the patient-symptom matrix. Clusters for a specific time point are extracted and displayed, while clusters for different timepoints can be investigated via the time slider, which will update the scatterplot.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Clustering&Grouping"],
        "componenet_code": [
          "dimensionality_reduction",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "The scatterplot was customized to separately capture acute and late symptom burden distribution as identified by the symptom clusters, and to reflect via marker color, shape, and size the therapeutic combination administered to each patient, their gender, and their disease stage.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "scatter+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "scatter"]
      },
      {
        "solution_text": "The data can be filtered by attributes, and filtering operations update the other views. A filtering control panel serves double duty, by also providing the plot legend. This customized scatterplot encoding effectively captured the symptom distribution across the patient population, patient outliers, and therapeutic distribution across the data.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 487,
    "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
    "pub_year": 2022,
    "domain": "Life Sciences",
    "requirement": {
      "requirement_text": "Analyze the patient symptom trajectories as a whole, by therapy type, and by stage.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.",
      "data_code": {
        "temporal": 1,
        "tables": 1,
        "quantitative": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "In order to better support activities A1 and A2, an additional option uses the same filament encoding, this time with the color mapped to the therapy type, to capture the mean trajectory per each therapeutic combination. Since in the therapy case the symptom mean ratings across the population bear meaning, the filaments are spread out according to the mean ratings per therapy. This therapy-analysis option helps estimate what treatment plans are less symptomatic, or on the contrary, conduct to high symptom burden.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 488,
    "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
    "pub_year": 2022,
    "domain": "Life Sciences",
    "requirement": {
      "requirement_text": "T2.2. Compare symptom trajectories by therapy type",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.",
      "data_code": {
        "temporal": 1,
        "tables": 1,
        "quantitative": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Designing an appropriate encoding for the symptom longitudinal data(A2) turned out to be particularly challenging, primarily due to thenature and richness of the temporal data, the acknowledged variabilityin ratings across patients, and the missing or uneven time points, whichwere expected in this context. The design process explored a widerange of possible temporal encodings, many of which suffered fromscalability issues and, after several sessions, focused on a promising encoding called a \u201dtendril plot\u201d [51]. A tendril plot is a visual summaryof the incidence, significance, and temporal aspects of adverse eventsin clinical trials, in which individual temporal threads, one per eachpatient, emanate from a common root and shoot upwards and curl eitherto the left or to the right depending on whether the next event in thetimeline was adverse or an improvement. For clinical trial data, tendrilswere shown to create beautiful, compact, naturally clustering pathlinesillustrating the positive or negative evolution of each group of patients.The clinicians had also seen this representation and thought it couldwork (T1.2, T2.2). Whereas promising on paper, unfortunately, thetendril implementation did not yield similarly clean illustrations forthe symptom data, because of the much smaller number of time points,the variability in therapeutic sequences, and the variability in patientoutcomes, which are not typical of clinical trials.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 489,
    "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
    "pub_year": 2022,
    "domain": "Life Sciences",
    "requirement": {
      "requirement_text": "Summarize symptom ratings for the entire cohort, by stage",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.",
      "data_code": {
        "temporal": 1,
        "tables": 1,
        "quantitative": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "We first organized the symptom ratings into a patient-symptom matrix for the selected time point, where each element (i, j) corresponds to the score given to symptom j by patient i at that time point. Prior research in symptom cluster for HNC [34] had applied hierarchical clustering using Ward\u2019s method [48] with Euclidean distance on the patient-symptom matrix to group patients based on their raw symptom ratings. After alternative clustering with complete and average linkages, we found that Ward\u2019s method generated larger, more informative groups of high symptom patients, which made sense to the clinicians. We identified two patient groups with high and low symptom burden (T1.1). This two-group clustering was preferred by clinicians, who found it easier to compare two groups instead of more. The axes of the scatterplot correspond to the first two components obtained by applying PCA to the patient-symptom matrix. Clusters for a specific time point are extracted and displayed, while clusters for different timepoints can be investigated via the time slider, which will update the scatterplot.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Clustering&Grouping"],
        "componenet_code": [
          "dimensionality_reduction",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "The scatterplot was customized to separately capture acute and late symptom burden distribution as identified by the symptom clusters, and to reflect via marker color, shape, and size the therapeutic combination administered to each patient, their gender, and their disease stage.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "scatter+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "scatter"]
      },
      {
        "solution_text": "The data can be filtered by attributes, and filtering operations update the other views. A filtering control panel serves double duty, by also providing the plot legend. This customized scatterplot encoding effectively captured the symptom distribution across the patient population, patient outliers, and therapeutic distribution across the data.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 490,
    "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
    "pub_year": 2022,
    "domain": "Life Sciences",
    "requirement": {
      "requirement_text": "Summarize symptom ratings for the entire cohort, by stage",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.",
      "data_code": {
        "temporal": 1,
        "tables": 1,
        "quantitative": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The percentile heatmap is a custom representation showing the rating distribution of individual symptoms over time, for the entire patient cohort. We arrived at this representation after exploring a variety of alternatives such as stacked line plots, parallel coordinates plots, and radars, guided by feedback from collaborators.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "matrix+bar+heatmap",
        "axial_code": ["Nesting"],
        "componenet_code": ["heatmap", "bar", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 491,
    "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
    "pub_year": 2022,
    "domain": "Life Sciences",
    "requirement": {
      "requirement_text": "Show an individual patient in the context of the cohort",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.",
      "data_code": {
        "textual": 1,
        "temporal": 1,
        "tables": 1,
        "quantitative": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "For missing data during the observation period, the associated pointsare not represented, and we consider no rating change from the previoustime points; the surveillance period is represented on each filamentuntil the last recorded time point for each patient. We account for thetime ratio between the acute (1 week) and late (months) stages, so thedistances illustrated for the acute time points are smaller as opposedto the late time points. Hovering over a filament greys out all theother filaments in the plot. This interaction helps in the comparison ofsymptom trajectories for the same patient, and via brushing and linkingwith the other views, in highlighting the additional patient data (T3.1).",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 492,
    "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
    "pub_year": 2022,
    "domain": "Life Sciences",
    "requirement": {
      "requirement_text": "Show an individual patient in the context of the cohort",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.",
      "data_code": {
        "temporal": 1,
        "tables": 1,
        "quantitative": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The percentile heatmap (Fig. 1.D) is a custom representation show_x0002_ing the rating distribution of individual symptoms over time, for the entire patient cohort (T2.3). We arrived at this representation after exploring a variety of alternatives such as stacked line plots, parallel coordinates plots, and radar charts, guided by feedback from collabora_x0002_tors. We settled on a matrix-based layout due to its compactness and to its ability to support small multiple plots. Each row corresponds to a symptom, with rows grouped by symptom category, and each column corresponds to a time point. Each cell in this matrix is a horizontal bar graph showing via shade the percentage of patients reporting within a specific range (0, 1-5, 6-9, or 10) for that symptom, at that time point. The bar height maps the percentage of individuals from the entire co_x0002_hort who reported the symptom ratings at that time point. The current patient is indicated in this heatmap by cross markers (Fig. 4.C) (T3.1). This encoding proved to be an intuitive way of showing what symptoms produce a higher burden for patients, and when, as well as to indicate how many patients were affected by these symptoms from the entire cohort (T1.2, T2.3)",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "matrix+bar+heatmap",
        "axial_code": ["Nesting"],
        "componenet_code": ["heatmap", "bar", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 493,
    "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
    "pub_year": 2022,
    "domain": "Life Sciences",
    "requirement": {
      "requirement_text": "Display demographic and diagnostic patient data, and indicate patients with similar diagnostic attributes",
      "requirement_code": { "discover_observation": 1, "compare_entities": 1 }
    },
    "data": {
      "data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.",
      "data_code": {
        "temporal": 1,
        "tables": 1,
        "quantitative": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "We first organized the symptom ratings into a patient-symptom matrix for the selected time point, where each element (i, j) corresponds to the score given to symptom j by patient i at that time point. Prior research in symptom cluster for HNC [34] had applied hierarchical clustering using Ward\u2019s method [48] with Euclidean distance on the patient-symptom matrix to group patients based on their raw symptom ratings. After alternative clustering with complete and average linkages, we found that Ward\u2019s method generated larger, more informative groups of high symptom patients, which made sense to the clinicians. We identified two patient groups with high and low symptom burden (T1.1). This two-group clustering was preferred by clinicians, who found it easier to compare two groups instead of more. The axes of the scatterplot correspond to the first two components obtained by applying PCA to the patient-symptom matrix. Clusters for a specific time point are extracted and displayed, while clusters for different timepoints can be investigated via the time slider, which will update the scatterplot.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Clustering&Grouping"],
        "componenet_code": [
          "dimensionality_reduction",
          "clustering_and_grouping"
        ]
      },
      {
        "solution_text": "The scatterplot was customized to separately capture acute and late symptom burden distribution as identified by the symptom clusters, and to reflect via marker color, shape, and size the therapeutic combination administered to each patient, their gender, and their disease stage.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "scatter+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "scatter"]
      },
      {
        "solution_text": "The data can be filtered by attributes, and filtering operations update the other views. A filtering control panel serves double duty, by also providing the plot legend. This customized scatterplot encoding effectively captured the symptom distribution across the patient population, patient outliers, and therapeutic distribution across the data.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 494,
    "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
    "pub_year": 2022,
    "domain": "Life Sciences",
    "requirement": {
      "requirement_text": "Display the anatomical locations affected by a symptom",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.",
      "data_code": { "tables": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "Finally, because a discussion of task revealed that patients tend to point to the location of their symptoms, an anatomical sketch supports visual anchoring based on anatomy. Regions in the head and neck affected by the selected symptoms are highlighted in this sketch.",
        "solution_category": "visualization",
        "solution_axial": "Basic",
        "solution_compoent": "glyph",
        "axial_code": ["Basic"],
        "componenet_code": ["glyph"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 495,
    "paper_title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy",
    "pub_year": 2022,
    "domain": "Life Sciences",
    "requirement": {
      "requirement_text": "Filter a patient\u2019s symptoms by association rule",
      "requirement_code": { "data_filtering": 1 }
    },
    "data": {
      "data_text": "In accordance with the ACD paradigm for data visualization [68], the project requirements were based on a starter dataset, which was then expanded during the duration of the project. Patients who had completed fewer than two questionnaires were not included in the analysis. The final dataset included 699 HNC patients. For each patient, two types of information were recorded: 1) Patient demographics and diagnostic data, which covered three attribute types: quantitative data (e.g., age, weight, or the total radiation dose); ordinal data (disease stage), and nominal data (e.g., therapeutic combination); and 2) Longitudinal symptom data, as time-series attributes with quan_x0002_titative values (ratings for 28 symptoms) over a maximum of 12 time points. The symptoms were further grouped in three categories: core symptoms common for all cancer types (fatigue, disturbed sleep, dis_x0002_tress, pain, drowsiness, sadness, memory, numbness, dry mouth, lack of appetite, shortness of breath, nausea, and vomiting), HNC specific symptoms (difficulty swallowing, difficulty speaking, mucus in throat, difficulty tasting food, constipation, teeth/gum issues, mouth/throat sores, choking, and skin pain), and ratings of symptoms\u2019 interference with daily life (work, enjoyment, general activity, mood, walking, rela_x0002_tionships). The symptoms were rated on a 0-to-10 scale ranging from \u201dnot present\u201d (0) to \u201das bad as you can imagine\u201d (10) for the core and HNC specific items, and from \u201ddid not interfere\u201d (0) to \u201dinterfered completely\u201d (10) for the interference items. Each patient rated all 28 symptoms during a questionnaire completion (time point). The dataset included a total of 12 time points. Because of the desired longitudinal aspect of the analysis, we separated these points into three categories: baseline (week 0), acute stage (on-treatment period), and late stage (>= 6 weeks after treatment). For acute time points during treatment, data was collected every week (at most 7 weeks), while after treatment, time points data was collected at lower granularity, at 6-weeks, and 6-, 12-, or 18-months post-treatment. Previous timepoint values were substituted for missing values; missing baseline values (i.e., for the first timepoint) were marked with 0. Patients with no symptoms recorded during the acute or late phases were not included in the analysis for that time frame.",
      "data_code": { "tables": 1, "quantitative": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "We applied ARM to each of the acute stage and the late stage, and empirically chose to illustrate the top 20 rules yielded by this approach, because only a small number of rules were of clinical interest. We chose minimum values for the support and lift metrics that were suitable for frequent and interdependent symptoms.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "From the many possible encodings of ARMs [38], we selected a node-link representation (Fig. 1.A), which was deemed by clinicians to be more friendly to broader audiences (A3), and a good fit for the rela_x0002_tively small number of nodes. Graphs are laid out using a force-directed layout algorithm based on statistical multidimensional scaling [39, 79], which results in nodes with high degree being placed centrally.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "Network+bubble+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["text", "bubble", "Network"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 496,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.",
      "requirement_code": {
        "describe_observation_item": 1,
        "identify_main_cause_item": 1
      }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "geometry": 1, "media": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "A geographical analysis of factors is provided in view (D). By clicking a country on the map, the speakers belonging to the country will be highlighted, so users can analyze the regional difference between countries.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "image+map+text",
        "axial_code": ["Stack"],
        "componenet_code": ["text", "image", "map"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 497,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.",
      "requirement_code": {
        "describe_observation_item": 1,
        "identify_main_cause_item": 1
      }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27].",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection", "Modeling"],
        "componenet_code": ["feature_selection", "modeling"]
      },
      {
        "solution_text": "How speech script content relates to key speech delivery information is the subject of much literature on speech giving. E-script allows fine grained understanding of how multi-modal speech emotion, word speed, and pauses relate to the timing of each word of a speech.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "text+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 498,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.",
      "requirement_code": {
        "describe_observation_item": 1,
        "identify_main_cause_item": 1
      }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "As discussed in the interview section, an open question for speech experts is the role of the categories of emotions in speeches, as well as the temporal distribution of emotions. E-type allows continuous valence and arousal data to be compared to discrete emotional type data across time in speeches. We supposed that a linear visualization might be more intuitively understandable to non-expert audiences.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "bar+line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 499,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.",
      "requirement_code": {
        "describe_observation_item": 1,
        "identify_main_cause_item": 1
      }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Upon selection of a factor in (A) the user is presented with many dots, with each dot positioned horizontally according to the cumulative amount of the factor in a speech. The speeches are sorted vertically by the level of the speech.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 500,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.",
      "requirement_code": {
        "describe_observation_item": 1,
        "identify_main_cause_item": 1
      }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "As shown in figure, the light blue rectangle for each level covers the middle 50% distribution of the speeches and the dark blue line indicates the median of each level.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "scatter+rectangle",
        "axial_code": ["Coordinate"],
        "componenet_code": ["scatter", "rectangle"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 501,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.",
      "requirement_code": {
        "describe_observation_item": 1,
        "identify_main_cause_item": 1
      }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "DimensionalityReduction",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "dimensionality_reduction",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "radar",
        "axial_code": ["Co-axis"],
        "componenet_code": ["radar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 502,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.",
      "requirement_code": {
        "describe_observation_item": 1,
        "identify_main_cause_item": 1
      }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "We therefore created a new form of spiral that shows the emotional twists and turns in a visually dramatic way. Clockwise and counterclockwise turns in the spiral indicate shifting negative and positive emotions, with sharp angles of the visualization showing the emotional turning points. Due to the compact structure, large scale comparison is possible, supporting comparison and navigation between speeches and within a speech.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bubble",
        "axial_code": ["Repetition"],
        "componenet_code": ["bubble"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 503,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.",
      "requirement_code": {
        "describe_observation_item": 1,
        "identify_main_cause_item": 1
      }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "DimensionalityReduction",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "dimensionality_reduction",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      },
      {
        "solution_text": "In order to better understand this relation, a radar displays the five most significant factors, and a given speech\u2019s estimated level based on the amount of each of the factors",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "radar",
        "axial_code": ["Co-axis"],
        "componenet_code": ["radar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 504,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G1: To understand the relation between speech effectivenes and various speech factors. The relative importance of different speech factors and the role of factors on effectiveness in the contest are critical for users.",
      "requirement_code": {
        "describe_observation_item": 1,
        "identify_main_cause_item": 1
      }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "We designed E-distribution in order to show how the effect of each factor changes the calculated probability of each level, which can be interpreted as a metric of effectiveness. We obtained the probability of the five levels with respect to each factor. The five lines in the graph represent the distribution of probability of the five levels of the contest, with the same color encoding used as in E-Similarity. For example for the factor arousal mean we can observe larger values to the right of E-distribution result in higher probabilities of the darker line, or final level of the contest.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "table+text+area",
        "axial_code": ["Nesting"],
        "componenet_code": ["table", "area", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 505,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "geometry": 1, "media": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "A geographical analysis of factors is provided in view (D). By clicking a country on the map, the speakers belonging to the country will be highlighted, so users can analyze the regional difference between countries.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "image+map+text",
        "axial_code": ["Stack"],
        "componenet_code": ["text", "image", "map"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 506,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27].",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection", "Modeling"],
        "componenet_code": ["feature_selection", "modeling"]
      },
      {
        "solution_text": "How speech script content relates to key speech delivery information is the subject of much literature on speech giving. E-script allows fine grained understanding of how multi-modal speech emotion, word speed, and pauses relate to the timing of each word of a speech.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "text+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 507,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "As discussed in the interview section, an open question for speech experts is the role of the categories of emotions in speeches, as well as the temporal distribution of emotions. E-type allows continuous valence and arousal data to be compared to discrete emotional type data across time in speeches. We supposed that a linear visualization might be more intuitively understandable to non-expert audiences.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "bar+line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 508,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Upon selection of a factor in (A) the user is presented with many dots, with each dot positioned horizontally according to the cumulative amount of the factor in a speech. The speeches are sorted vertically by the level of the speech.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 509,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "As shown in figure, the light blue rectangle for each level covers the middle 50% distribution of the speeches and the dark blue line indicates the median of each level.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "scatter+rectangle",
        "axial_code": ["Coordinate"],
        "componenet_code": ["scatter", "rectangle"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 510,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "DimensionalityReduction",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "dimensionality_reduction",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "radar",
        "axial_code": ["Co-axis"],
        "componenet_code": ["radar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 511,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "With the help of E-spiral, as shown in figure, we can clearly see the changes of emotion during the speech via the turning spiral. Interaction of rapidly skipping to the video frame of the selected speech by clicking the circle on the spiral supports rapid browsing with emotional cues.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bubble",
        "axial_code": ["Repetition"],
        "componenet_code": ["bubble"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 512,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27].",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection", "Modeling"],
        "componenet_code": ["feature_selection", "modeling"]
      },
      {
        "solution_text": "How speech script content relates to key speech delivery information is the subject of much literature on speech giving. E-script allows fine grained understanding of how multi-modal speech emotion, word speed, and pauses relate to the timing of each word of a speech.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "text+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 513,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G2: To understand the spatio-temporal distribution of factors across multiple speeches. Referenced work showed that experts be_x0002_lieved certain factors were more important at different moments of speeches or that the time order of certain factors was important. The ge_x0002_ographical distribution was also considered important by some domain experts.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "We therefore created a new form of spiral that shows the emotional twists and turns in a visually dramatic way. Clockwise and counterclockwise turns in the spiral indicate shifting negative and positive emotions, with sharp angles of the visualization showing the emotional turning points. Due to the compact structure, large scale comparison is possible, supporting comparison and navigation between speeches and within a speech.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bubble",
        "axial_code": ["Repetition"],
        "componenet_code": ["bubble"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 514,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "geometry": 1, "media": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "A geographical analysis of factors is provided in view (D). By clicking a country on the map, the speakers belonging to the country will be highlighted, so users can analyze the regional difference between countries.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "image+map+text",
        "axial_code": ["Stack"],
        "componenet_code": ["text", "image", "map"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 515,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27].",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection", "Modeling"],
        "componenet_code": ["feature_selection", "modeling"]
      },
      {
        "solution_text": "How speech script content relates to key speech delivery information is the subject of much literature on speech giving. E-script allows fine grained understanding of how multi-modal speech emotion, word speed, and pauses relate to the timing of each word of a speech.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "text+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 516,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "As discussed in the interview section, an open question for speech experts is the role of the categories of emotions in speeches, as well as the temporal distribution of emotions. E-type allows continuous valence and arousal data to be compared to discrete emotional type data across time in speeches. We supposed that a linear visualization might be more intuitively understandable to non-expert audiences.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "bar+line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 517,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Upon selection of a factor in (A) the user is presented with many dots, with each dot positioned horizontally according to the cumulative amount of the factor in a speech. The speeches are sorted vertically by the level of the speech.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 518,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "With the help of E-spiral, as shown in figure, we can clearly see the changes of emotion during the speech via the turning spiral. Interaction of rapidly skipping to the video frame of the selected speech by clicking the circle on the spiral supports rapid browsing with emotional cues.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bubble",
        "axial_code": ["Repetition"],
        "componenet_code": ["bubble"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 519,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27].",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection", "Modeling"],
        "componenet_code": ["feature_selection", "modeling"]
      },
      {
        "solution_text": "How speech script content relates to key speech delivery information is the subject of much literature on speech giving. E-script allows fine grained understanding of how multi-modal speech emotion, word speed, and pauses relate to the timing of each word of a speech.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "text+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 520,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "We therefore created a new form of spiral that shows the emotional twists and turns in a visually dramatic way. Clockwise and counterclockwise turns in the spiral indicate shifting negative and positive emotions, with sharp angles of the visualization showing the emotional turning points. Due to the compact structure, large scale comparison is possible, supporting comparison and navigation between speeches and within a speech.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bubble",
        "axial_code": ["Repetition"],
        "componenet_code": ["bubble"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 521,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "DimensionalityReduction",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "dimensionality_reduction",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      },
      {
        "solution_text": "In order to better understand this relation, a radar displays the five most significant factors, and a given speech\u2019s estimated level based on the amount of each of the factors",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "radar",
        "axial_code": ["Co-axis"],
        "componenet_code": ["radar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 522,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G3: To understand the effectiveness of individual speeches in context. Speech experts in preliminary interviews expressed interest in wanting to understand the patterns of an individual speech. Furthermore how these factors in one speech relate to the factors and effectiveness metrics of all speeches.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "We designed E-distribution in order to show how the effect of each factor changes the calculated probability of each level, which can be interpreted as a metric of effectiveness. We obtained the probability of the five levels with respect to each factor. The five lines in the graph represent the distribution of probability of the five levels of the contest, with the same color encoding used as in E-Similarity. For example for the factor arousal mean we can observe larger values to the right of E-distribution result in higher probabilities of the darker line, or final level of the contest.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "table+text+area",
        "axial_code": ["Nesting"],
        "componenet_code": ["table", "area", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 523,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "geometry": 1, "media": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "A geographical analysis of factors is provided in view (D). By clicking a country on the map, the speakers belonging to the country will be highlighted, so users can analyze the regional difference between countries.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "image+map+text",
        "axial_code": ["Stack"],
        "componenet_code": ["text", "image", "map"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 524,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27].",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection", "Modeling"],
        "componenet_code": ["feature_selection", "modeling"]
      },
      {
        "solution_text": "How speech script content relates to key speech delivery information is the subject of much literature on speech giving. E-script allows fine grained understanding of how multi-modal speech emotion, word speed, and pauses relate to the timing of each word of a speech.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "text+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 525,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "As discussed in the interview section, an open question for speech experts is the role of the categories of emotions in speeches, as well as the temporal distribution of emotions. E-type allows continuous valence and arousal data to be compared to discrete emotional type data across time in speeches. We supposed that a linear visualization might be more intuitively understandable to non-expert audiences.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "bar+line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 526,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "As shown in figure, the light blue rectangle for each level covers the middle 50% distribution of the speeches and the dark blue line indicates the median of each level.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "scatter+rectangle",
        "axial_code": ["Coordinate"],
        "componenet_code": ["scatter", "rectangle"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 527,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "DimensionalityReduction",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "dimensionality_reduction",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "radar",
        "axial_code": ["Co-axis"],
        "componenet_code": ["radar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 528,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "DimensionalityReduction",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "dimensionality_reduction",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches. To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. The closer two speeches are to each other, the more similar the two speeches are.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 529,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "We therefore created a new form of spiral that shows the emotional twists and turns in a visually dramatic way. Clockwise and counterclockwise turns in the spiral indicate shifting negative and positive emotions, with sharp angles of the visualization showing the emotional turning points. Due to the compact structure, large scale comparison is possible, supporting comparison and navigation between speeches and within a speech.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bubble",
        "axial_code": ["Repetition"],
        "componenet_code": ["bubble"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 530,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "DimensionalityReduction",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "dimensionality_reduction",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      },
      {
        "solution_text": "In order to better understand this relation, a radar displays the five most significant factors, and a given speech\u2019s estimated level based on the amount of each of the factors",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "radar",
        "axial_code": ["Co-axis"],
        "componenet_code": ["radar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 531,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G4: To compare between speeches on various speech factors. Observing similarity and differences of speeches can allow effective_x0002_ness metrics to be connected with speaking styles.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "We designed E-distribution in order to show how the effect of each factor changes the calculated probability of each level, which can be interpreted as a metric of effectiveness. We obtained the probability of the five levels with respect to each factor. The five lines in the graph represent the distribution of probability of the five levels of the contest, with the same color encoding used as in E-Similarity. For example for the factor arousal mean we can observe larger values to the right of E-distribution result in higher probabilities of the darker line, or final level of the contest.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "table+text+area",
        "axial_code": ["Nesting"],
        "componenet_code": ["table", "area", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 532,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "geometry": 1, "media": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "A geographical analysis of factors is provided in view (D). By clicking a country on the map, the speakers belonging to the country will be highlighted, so users can analyze the regional difference between countries.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "image+map+text",
        "axial_code": ["Stack"],
        "componenet_code": ["text", "image", "map"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 533,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27].",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection", "Modeling"],
        "componenet_code": ["feature_selection", "modeling"]
      },
      {
        "solution_text": "How speech script content relates to key speech delivery information is the subject of much literature on speech giving. E-script allows fine grained understanding of how multi-modal speech emotion, word speed, and pauses relate to the timing of each word of a speech.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "text+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 534,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "As discussed in the interview section, an open question for speech experts is the role of the categories of emotions in speeches, as well as the temporal distribution of emotions. E-type allows continuous valence and arousal data to be compared to discrete emotional type data across time in speeches. We supposed that a linear visualization might be more intuitively understandable to non-expert audiences.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "bar+line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 535,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "As shown in figure, the light blue rectangle for each level covers the middle 50% distribution of the speeches and the dark blue line indicates the median of each level.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "scatter+rectangle",
        "axial_code": ["Coordinate"],
        "componenet_code": ["scatter", "rectangle"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 536,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "DimensionalityReduction",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "dimensionality_reduction",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "radar",
        "axial_code": ["Co-axis"],
        "componenet_code": ["radar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 537,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "DimensionalityReduction",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "dimensionality_reduction",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches. To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. The closer two speeches are to each other, the more similar the two speeches are.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 538,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "With the help of E-spiral, as shown in figure, we can clearly see the changes of emotion during the speech via the turning spiral. Interaction of rapidly skipping to the video frame of the selected speech by clicking the circle on the spiral supports rapid browsing with emotional cues.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bubble",
        "axial_code": ["Repetition"],
        "componenet_code": ["bubble"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 539,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27].",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection", "Modeling"],
        "componenet_code": ["feature_selection", "modeling"]
      },
      {
        "solution_text": "How speech script content relates to key speech delivery information is the subject of much literature on speech giving. E-script allows fine grained understanding of how multi-modal speech emotion, word speed, and pauses relate to the timing of each word of a speech.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "text+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 540,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "We therefore created a new form of spiral that shows the emotional twists and turns in a visually dramatic way. Clockwise and counterclockwise turns in the spiral indicate shifting negative and positive emotions, with sharp angles of the visualization showing the emotional turning points. Due to the compact structure, large scale comparison is possible, supporting comparison and navigation between speeches and within a speech.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bubble",
        "axial_code": ["Repetition"],
        "componenet_code": ["bubble"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 541,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value; According to the experts\u2019 feedback, we found that they desired to compare the similarity between speeches (T4). To allow this comparison we chose the five most significant factors as the speech\u2019s feature vectors, and used t-SNE [28] to reduce the dimensionality of feature vectors to display all speeches on a two-dimensional map. ",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation,DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "DimensionalityReduction",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "dimensionality_reduction",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      },
      {
        "solution_text": "As shown in figure, clicking a dot representing an individual speech in the scatter plot brings up a radar. Dots are color-encoded, allowing for rapid comparison of a speech\u2019s estimated level with its true value.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      },
      {
        "solution_text": "In order to better understand this relation, a radar displays the five most significant factors, and a given speech\u2019s estimated level based on the amount of each of the factors",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "radar",
        "axial_code": ["Co-axis"],
        "componenet_code": ["radar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 542,
    "paper_title": "E-ffective: A Visual Analytic System for Exploring the Emotion and Effectiveness of Inspirational Speeches",
    "pub_year": 2022,
    "domain": "speaking",
    "requirement": {
      "requirement_text": "G5: To understand speaking strategies among speech factors. As revealed in our literature survey, there are different opinions about how different factors are effectively used. These theories could be evaluated.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The entire database includes 203 videos from the World Championship of Public Speaking published online, including YouTube and WeChat channels. We collected the videos and metadata manually. The contest levels of the speech videos were recorded as a measurement of effec_x0002_tiveness: area, division, district, semi-final and final. The amount of videos for each level is approximately balanced, and we ensure that all collected videos are of good quality; Each speech consists of: 1) the original video; 2) the scripts; 3) metadata such as the start and ending of the speech in the video; 4) information about the speech, including contest region, year, level and rank;",
      "data_code": { "sequential": 1, "media": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In order to acquire the previously mentioned factors, we extracted image frames, voice and text from the original video. The voice and text are aligned at the sentence level while the images remain at the frame level; Facial emotional data: We recognized discrete emotion types, valence and arousal of the speaker from the frames of video. Faces in frames and their positions are detected by face recognition [10]. The faces are further clustered by DBSCAN [8] with the facial features extracted during detection to identify the faces of each speaker without the interference from others\u2019 faces in the video. AffectNet [22] is a widely used database and baseline method for facial expression, valence and arousal computing in the wild. We used AffectNet to extract the facial emotion valence and arousal data. We extracted the facial emotion types using an emotion classification convolutional neural network by Arriaga et al. [2]. Textual emotional data: We applied Azure Cognitive Speech to Text Service1 to convert spoken audio to script text with timestamps of each sentence and word. The method to extract the textual valence and arousal data came from the work of Wang et al. [29], which uses a regional CNN-LSTM model to analyze dimensional sentiment. Vocal emotional data: With the timestamps of sentences, we split the audio into sentence-level clips and applied an open-source toolbox for multimodal emotion analysis developed by Buitelaar et al. [6] to obtain the vocal valence and arousal data of the clips. Non-emotional data: We considered two non-emotional factors. The pauses between words and sentences as well as the words spoken per minute were calculated with the timestamps. The vocabulary level was measured with the Dale-Chall measure [30] calculated with an open-source tool for assessing the readability level of a given text [27]; For calculating final emotion, diversity and type ratio, we only select facial data as input, since textual data and vocal data are much sparser than facial data over time. For example, we could only extract an item of textual or vocal data from a complete sentence, while we can extract an item of facial data from each video frame. So for the same video, the amount of textual/vocal data is much sparser. We found processed textual/vocal results of these factors less convincing; According to G1, we want to find out the relation between speech factors and effectiveness. Contest speech levels can be regarded as ordinal variables, whose relative ordering is significant. For example, the grades of 1-5 may respectively represent 5 levels \u201carea\u201d, \u201cdivision\u201d, \u201cdistrict\u201d, \u201csemi final\u201d and \u201cworld final\u201d. In the World Championship of Public Speaking, only those who rank at the top of a level will advance to a higher level. So we hypothesize that the higher the level, the more effective the speech is. Given the factors of a certain speech, the problem of predicting its level can be considered as an intermediate problem between regression and classification [12]. We first conducted the test of parallel lines and found the p-value is smaller than 0.05, proving that the level prediction problem is suitable to be solved by multi-class ordinal regression. Then we split this prediction problem into four sub-problems as shown in Fig. 2 IV. In each sub-problem, we performed logistic regression on the odds ratio of each factor. Finally, we obtained the p-value of each factor in Table 1, where we indicate factors calculated as significant relating to effectiveness. The result of our factor effectiveness analysis shows that the average of facial arousal, the average of vocal arousal and valence, the volatility of facial arousal and valence, the final facial arousal, and the ratio of facial happy expressions all have a significant correlation with speech effectiveness. Taking experts\u2019 advice into consideration, we selected typical factors and embedded them into our system. According to the result of the four sub-problems, we calculated the probability of the five levels as the factors change value.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection,Modeling,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": [
          "AlgorithmicCalculation",
          "FeatureSelection",
          "Modeling"
        ],
        "componenet_code": [
          "algorithmic_calculation",
          "feature_selection",
          "modeling"
        ]
      },
      {
        "solution_text": "We designed E-distribution in order to show how the effect of each factor changes the calculated probability of each level, which can be interpreted as a metric of effectiveness. We obtained the probability of the five levels with respect to each factor. The five lines in the graph represent the distribution of probability of the five levels of the contest, with the same color encoding used as in E-Similarity. For example for the factor arousal mean we can observe larger values to the right of E-distribution result in higher probabilities of the darker line, or final level of the contest.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "table+text+area",
        "axial_code": ["Nesting"],
        "componenet_code": ["table", "area", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 544,
    "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
    "pub_year": 2022,
    "domain": "Bitcoin",
    "requirement": {
      "requirement_text": "AF1: Bitcoin regulation and the evolution of mining pools. An excessive concentration of pool market shares is dangerous to Bitcoin\u2019s operations and raises questions about potential mining regulations. The economist wanted to analyze factors that pose a risk to pool concentra_x0002_tion and detect critical periods in Bitcoin mining. Periods characterized by significant variations of bitcoin values (e.g., halving days, peak in bitcoin value) impact mining pools and should be analyzed. Further more, external data such as news about Bitcoin might give insights to better understand the impact of regulatory changes in some countries.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "D1: The mining pool distribution dataset gives information about the evolution of mining pools over time. It was derived from identifying mining pools that received the reward for each individual block in the Bitcoin blockchain. We extended the initial dataset compiled by Romiti et al. [49] (data up to: 2018\u201312) to extract the mining pool that mined each block. Blocks that could not be attributed to any known mining pool were marked as \u201cunknown\u201d. Our final dataset at the time of writing includes data until 2020-09 and contains the mining power of each mining pool for each month. We computed multiple quantitative measures related to mining power: hash rate, market share (normalized hash rate), and total reward received from mining (in BTC and USD). Additionally, we calculated the estimated electricity consumption of each mining pool based on the CBECI index [4] to allow to judge the externality of mining on the environment.",
      "data_code": { "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Visual Encoding: We chose a temporal ribbon chart that allows analysts to see both the changing rank and a quantitative metric related to each mining pool. The ribbon chart is filtered to the time period selected in V1 and users can choose among multiple statistical measures and how to group and color the pools. The chart\u2019s multiple stacked bars sorted by rank show data aggregated and displayed by months. By default mining pools are sorted from the highest value at the top of the stack to the lowest one at the bottom per month. The same mining pool is connected across months with a ribbon to highlight its rank changing. We encode the top-10 mining pools for the selected time in distinct colors, while the remaining pools are colored in grey. Fig. 2 shows examples of different configurations of the ribbon chart. Analysts can select the mining power measure, e .g., market share, hash rate, total reward, and transaction fees. They can also chose mining pool characteristics to display in different color hue scales, i .e., the name of the pool, its payout scheme, and location. Using the coloring mechanism, analysts can see patterns for the characteristics of the top mining pool. Furthermore, analysts can group those mining pools by the same characteristics to see if any characteristics correlate with the mining growth and domination in the market. Within each group, mining pools are sorted by the selected measure.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "sankey",
        "axial_code": ["Co-axis"],
        "componenet_code": ["sankey"]
      },
      {
        "solution_text": "Interaction: Besides the measures, group-by, and color-by selectors, we provide two ways to highlight mining pools or their characteristics. First, analysts can click on the left side labels which will increase transparency of the unselected pools and consequently highlight those that fall into the selection. The selection also affects and filters the mining pools displayed on the mining pool details view (V3). Second, analysts can draw a brush on the ribbon chart to filter both mining pools and highlight a specific time range. The highlighted timeframe is also represented in views V3\u2013V6. Finally, like in all other views, detail-on_x0002_demand is available on hover via tooltips that show the mining pool name and exact value of the selected measure for this pool.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 545,
    "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
    "pub_year": 2022,
    "domain": "Bitcoin",
    "requirement": {
      "requirement_text": "AF1: Bitcoin regulation and the evolution of mining pools. An excessive concentration of pool market shares is dangerous to Bitcoin\u2019s operations and raises questions about potential mining regulations. The economist wanted to analyze factors that pose a risk to pool concentra_x0002_tion and detect critical periods in Bitcoin mining. Periods characterized by significant variations of bitcoin values (e.g., halving days, peak in bitcoin value) impact mining pools and should be analyzed. Further more, external data such as news about Bitcoin might give insights to better understand the impact of regulatory changes in some countries.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "D1: The mining pool distribution dataset gives information about the evolution of mining pools over time. It was derived from identifying mining pools that received the reward for each individual block in the Bitcoin blockchain. We extended the initial dataset compiled by Romiti et al. [49] (data up to: 2018\u201312) to extract the mining pool that mined each block. Blocks that could not be attributed to any known mining pool were marked as \u201cunknown\u201d. Our final dataset at the time of writing includes data until 2020-09 and contains the mining power of each mining pool for each month. We computed multiple quantitative measures related to mining power: hash rate, market share (normalized hash rate), and total reward received from mining (in BTC and USD). Additionally, we calculated the estimated electricity consumption of each mining pool based on the CBECI index [4] to allow to judge the externality of mining on the environment; D2: The mining pool characteristics dataset includes external data about mining pools that we obtained from public sources [6, 7] and manually cleaned. The attributes include the primary location of the pool (nominal), payout scheme (nominal), pool fee (quantitative), and whether the pool kept transaction fees or shared it to miners (binary). We also extracted the wiki history to track attribute changes as mining pools adapt their payout policies to compete with other pools; D3: The Bitcoin network statistics dataset contains multiple quan_x0002_titative measurements of Bitcoin network\u2019s state over time. We col_x0002_lected daily statistics from Blockchain.info [10] and averaged those measures to a monthly time window. Examples of network statistics for each month include the market price, total hash rate, total block reward, total transaction fees, mining difficulty, the number of transactions, electricity consumption [4], and the global energy price index [27].",
      "data_code": {
        "sequential": 1,
        "temporal": 1,
        "geometry": 1,
        "categorical": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "Visual Encoding: The time series shows a selected network statis_x0002_tic measure for the entire history of Bitcoin mining. By default, the timeline shows the mining pool concentration index as an indication to which extent the mining pool distribution risks being dominated by just a few pools [60]. Analysts can select other Bitcoin statistics measures (e.g., total reward, market price, mining difficulty) from a dropdown menu and switch to a log scale. The time series highlights halving days as important events related to mining rewards.",
        "solution_category": "visualization",
        "solution_axial": "Basic",
        "solution_compoent": "line",
        "axial_code": ["Basic"],
        "componenet_code": ["line"]
      },
      {
        "solution_text": "Interaction: Analysts can filter all remaining views (V2\u2013V6) to a specific time interval by brushing on the time axis or specifying a range with the calendar inputs. The selection will then trigger the other views to filter the information to the specified time interval.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 546,
    "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
    "pub_year": 2022,
    "domain": "Bitcoin",
    "requirement": {
      "requirement_text": "AF1: Bitcoin regulation and the evolution of mining pools. An excessive concentration of pool market shares is dangerous to Bitcoin\u2019s operations and raises questions about potential mining regulations. The economist wanted to analyze factors that pose a risk to pool concentra_x0002_tion and detect critical periods in Bitcoin mining. Periods characterized by significant variations of bitcoin values (e.g., halving days, peak in bitcoin value) impact mining pools and should be analyzed. Further more, external data such as news about Bitcoin might give insights to better understand the impact of regulatory changes in some countries.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "D1: The mining pool distribution dataset gives information about the evolution of mining pools over time. It was derived from identifying mining pools that received the reward for each individual block in the Bitcoin blockchain. We extended the initial dataset compiled by Romiti et al. [49] (data up to: 2018\u201312) to extract the mining pool that mined each block. Blocks that could not be attributed to any known mining pool were marked as \u201cunknown\u201d. Our final dataset at the time of writing includes data until 2020-09 and contains the mining power of each mining pool for each month. We computed multiple quantitative measures related to mining power: hash rate, market share (normalized hash rate), and total reward received from mining (in BTC and USD). Additionally, we calculated the estimated electricity consumption of each mining pool based on the CBECI index [4] to allow to judge the externality of mining on the environment; D2: The mining pool characteristics dataset includes external data about mining pools that we obtained from public sources [6, 7] and manually cleaned. The attributes include the primary location of the pool (nominal), payout scheme (nominal), pool fee (quantitative), and whether the pool kept transaction fees or shared it to miners (binary). We also extracted the wiki history to track attribute changes as mining pools adapt their payout policies to compete with other pools; D3: The Bitcoin network statistics dataset contains multiple quan_x0002_titative measurements of Bitcoin network\u2019s state over time. We col_x0002_lected daily statistics from Blockchain.info [10] and averaged those measures to a monthly time window. Examples of network statistics for each month include the market price, total hash rate, total block reward, total transaction fees, mining difficulty, the number of transactions, electricity consumption [4], and the global energy price index [27].",
      "data_code": {
        "sequential": 1,
        "temporal": 1,
        "geometry": 1,
        "categorical": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "Visual Encoding: We chose a temporal ribbon chart that allows analysts to see both the changing rank and a quantitative metric related to each mining pool. The ribbon chart is filtered to the time period selected in V1 and users can choose among multiple statistical measures and how to group and color the pools. The chart\u2019s multiple stacked bars sorted by rank show data aggregated and displayed by months. By default mining pools are sorted from the highest value at the top of the stack to the lowest one at the bottom per month. The same mining pool is connected across months with a ribbon to highlight its rank changing. We encode the top-10 mining pools for the selected time in distinct colors, while the remaining pools are colored in grey. Fig. 2 shows examples of different configurations of the ribbon chart. Analysts can select the mining power measure, e .g., market share, hash rate, total reward, and transaction fees. They can also chose mining pool characteristics to display in different color hue scales, i .e., the name of the pool, its payout scheme, and location. Using the coloring mechanism, analysts can see patterns for the characteristics of the top mining pool. Furthermore, analysts can group those mining pools by the same characteristics to see if any characteristics correlate with the mining growth and domination in the market. Within each group, mining pools are sorted by the selected measure.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "sankey",
        "axial_code": ["Co-axis"],
        "componenet_code": ["sankey"]
      },
      {
        "solution_text": "Interaction: Besides the measures, group-by, and color-by selectors, we provide two ways to highlight mining pools or their characteristics. First, analysts can click on the left side labels which will increase transparency of the unselected pools and consequently highlight those that fall into the selection. The selection also affects and filters the mining pools displayed on the mining pool details view (V3). Second, analysts can draw a brush on the ribbon chart to filter both mining pools and highlight a specific time range. The highlighted timeframe is also represented in views V3\u2013V6. Finally, like in all other views, detail-on_x0002_demand is available on hover via tooltips that show the mining pool name and exact value of the selected measure for this pool.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 547,
    "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
    "pub_year": 2022,
    "domain": "Bitcoin",
    "requirement": {
      "requirement_text": "AF1: Bitcoin regulation and the evolution of mining pools. An excessive concentration of pool market shares is dangerous to Bitcoin\u2019s operations and raises questions about potential mining regulations. The economist wanted to analyze factors that pose a risk to pool concentra_x0002_tion and detect critical periods in Bitcoin mining. Periods characterized by significant variations of bitcoin values (e.g., halving days, peak in bitcoin value) impact mining pools and should be analyzed. Further more, external data such as news about Bitcoin might give insights to better understand the impact of regulatory changes in some countries.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "D2: The mining pool characteristics dataset includes external data about mining pools that we obtained from public sources [6, 7] and manually cleaned. The attributes include the primary location of the pool (nominal), payout scheme (nominal), pool fee (quantitative), and whether the pool kept transaction fees or shared it to miners (binary). We also extracted the wiki history to track attribute changes as mining pools adapt their payout policies to compete with other pools.",
      "data_code": { "geometry": 1, "categorical": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Visual Encoding: The Bitcoin statistics view (Fig. 4) is located in the same position as V3 reachable via a toggle bar. We encode 17 different factors each as a gray temporal area chart including Bitcoin_x0002_internal statistics such as the total number of blocks mined, the total hash rate, the median confirmation time for a block, total rewards paid out, or the total transaction fees. In addition, we calculated statistics with external data such as fees converted to USD according to the current market price or the trade volume in USD. As such, the list of Bitcoin statistics provides information about the status of the Bitcoin network. For example, the number of transactions implies the demand of users; the amount of Bitcoins in transactions means the supply of currency circulating in the market; and the average waiting time indicates the network capacity to verify transactions.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "area",
        "axial_code": ["Repetition"],
        "componenet_code": ["area"]
      },
      {
        "solution_text": "Interaction: The view offers details-on-demand via a tooltip.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 548,
    "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
    "pub_year": 2022,
    "domain": "Bitcoin",
    "requirement": {
      "requirement_text": "AF1: Bitcoin regulation and the evolution of mining pools. An excessive concentration of pool market shares is dangerous to Bitcoin\u2019s operations and raises questions about potential mining regulations. The economist wanted to analyze factors that pose a risk to pool concentra_x0002_tion and detect critical periods in Bitcoin mining. Periods characterized by significant variations of bitcoin values (e.g., halving days, peak in bitcoin value) impact mining pools and should be analyzed. Further more, external data such as news about Bitcoin might give insights to better understand the impact of regulatory changes in some countries.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "D4: The Bitcoin news dataset lists headlines from the Press forum in Bitcointalk.org [8], where users posted links to news articles related to Bitcoin. Each news contains the date when the news was published by its source, the news headline, and the number of replies and views. We chose to use the number of views and replies as an attractiveness indicator of the news from the Bitcointalk\u2019s members. We derived an importance score of each news items as views\u00d7(replies+1).",
      "data_code": { "categorical": 1, "textual": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The news we collected covered various Bitcoin-related topics. To group related articles, we used the Word Network Topic Model method [71] that proved to be simple and effective for short texts. We extracted 15 topics and the top-10 keywords for each topic.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Visual Encoding: We use a swarm plot to display each individual news item in a compact fashion across a timeline. It trades off accurate position across the timeline for an overlap-free layout. Each circle represents one news article and its size corresponds to the calculated importance score. The larger the size, the more frequently the posting was read or commented on. We use color to indicate topic membership. The news topics are encoded in different colors, and the list of keywords listed on the left. The numbers behind the topic label indicate the number of news shown in the swam plot versus the total news.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "bubble+bar",
        "axial_code": ["Co-axis"],
        "componenet_code": ["bubble", "bar"]
      },
      {
        "solution_text": "Interaction: We demonstrate an example use case for the view in Fig. 5. Analysts can browse the news by hovering circles to see tooltips with news headlines on the top-left of the chart. When the analyst clicks on the circle, it will open a new tab to the news source. On the top panel, the list of news can also be narrowed down by keyword search. Analysts can use a slider to specify the number of news showing in the chart. The panel also shows the number of total news and the percentage of news that the chart currently displays. Finally, analysts can use the topics panel on the left to filter news by topic.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore,Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering", "OverviewandExplore"],
        "componenet_code": ["filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 549,
    "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
    "pub_year": 2022,
    "domain": "Bitcoin",
    "requirement": {
      "requirement_text": "AF2: Pool managers\u2019 behavior and the competition to attract miners. Mining pools have different payout schemes [50] that define which and how much miners are paid. Payouts are reduced by pool fees [5] a pool may keep to pay for operating costs. We would expect that pools with lower fees pay more constant income to miners for a given payout scheme. These pools may therefore attract miners and exhibit increasing mining power. Pool competition might be visible by convergence toward a limited number of (best) payout schemes and a decrease in pool fees. Such information is crucial for economic debates around pool viability as these fees correspond to a significant part of their income. An analysis of the data might provide additional evidence of the importance of payout schemes and pool fees in attracting miners.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "D1: The mining pool distribution dataset gives information about the evolution of mining pools over time. It was derived from identifying mining pools that received the reward for each individual block in the Bitcoin blockchain. We extended the initial dataset compiled by Romiti et al. [49] (data up to: 2018\u201312) to extract the mining pool that mined each block. Blocks that could not be attributed to any known mining pool were marked as \u201cunknown\u201d. Our final dataset at the time of writing includes data until 2020-09 and contains the mining power of each mining pool for each month. We computed multiple quantitative measures related to mining power: hash rate, market share (normalized hash rate), and total reward received from mining (in BTC and USD). Additionally, we calculated the estimated electricity consumption of each mining pool based on the CBECI index [4] to allow to judge the externality of mining on the environment; D3: The Bitcoin network statistics dataset contains multiple quan_x0002_titative measurements of Bitcoin network\u2019s state over time. We col_x0002_lected daily statistics from Blockchain.info [10] and averaged those measures to a monthly time window. Examples of network statistics for each month include the market price, total hash rate, total block reward, total transaction fees, mining difficulty, the number of transactions, electricity consumption [4], and the global energy price index [27].",
      "data_code": {
        "sequential": 1,
        "temporal": 1,
        "categorical": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "Visual Encoding: We chose a temporal ribbon chart that allows analysts to see both the changing rank and a quantitative metric related to each mining pool. The ribbon chart is filtered to the time period selected in V1 and users can choose among multiple statistical measures and how to group and color the pools. The chart\u2019s multiple stacked bars sorted by rank show data aggregated and displayed by months. By default mining pools are sorted from the highest value at the top of the stack to the lowest one at the bottom per month. The same mining pool is connected across months with a ribbon to highlight its rank changing. We encode the top-10 mining pools for the selected time in distinct colors, while the remaining pools are colored in grey. Fig. 2 shows examples of different configurations of the ribbon chart. Analysts can select the mining power measure, e .g., market share, hash rate, total reward, and transaction fees. They can also chose mining pool characteristics to display in different color hue scales, i .e., the name of the pool, its payout scheme, and location. Using the coloring mechanism, analysts can see patterns for the characteristics of the top mining pool. Furthermore, analysts can group those mining pools by the same characteristics to see if any characteristics correlate with the mining growth and domination in the market. Within each group, mining pools are sorted by the selected measure.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "sankey",
        "axial_code": ["Co-axis"],
        "componenet_code": ["sankey"]
      },
      {
        "solution_text": "Interaction: Besides the measures, group-by, and color-by selectors, we provide two ways to highlight mining pools or their characteristics. First, analysts can click on the left side labels which will increase transparency of the unselected pools and consequently highlight those that fall into the selection. The selection also affects and filters the mining pools displayed on the mining pool details view (V3). Second, analysts can draw a brush on the ribbon chart to filter both mining pools and highlight a specific time range. The highlighted timeframe is also represented in views V3\u2013V6. Finally, like in all other views, detail-on_x0002_demand is available on hover via tooltips that show the mining pool name and exact value of the selected measure for this pool.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 550,
    "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
    "pub_year": 2022,
    "domain": "Bitcoin",
    "requirement": {
      "requirement_text": "AF2: Pool managers\u2019 behavior and the competition to attract miners. Mining pools have different payout schemes [50] that define which and how much miners are paid. Payouts are reduced by pool fees [5] a pool may keep to pay for operating costs. We would expect that pools with lower fees pay more constant income to miners for a given payout scheme. These pools may therefore attract miners and exhibit increasing mining power. Pool competition might be visible by convergence toward a limited number of (best) payout schemes and a decrease in pool fees. Such information is crucial for economic debates around pool viability as these fees correspond to a significant part of their income. An analysis of the data might provide additional evidence of the importance of payout schemes and pool fees in attracting miners.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "D1: The mining pool distribution dataset gives information about the evolution of mining pools over time. It was derived from identifying mining pools that received the reward for each individual block in the Bitcoin blockchain. We extended the initial dataset compiled by Romiti et al. [49] (data up to: 2018\u201312) to extract the mining pool that mined each block. Blocks that could not be attributed to any known mining pool were marked as \u201cunknown\u201d. Our final dataset at the time of writing includes data until 2020-09 and contains the mining power of each mining pool for each month. We computed multiple quantitative measures related to mining power: hash rate, market share (normalized hash rate), and total reward received from mining (in BTC and USD). Additionally, we calculated the estimated electricity consumption of each mining pool based on the CBECI index [4] to allow to judge the externality of mining on the environment; D3: The Bitcoin network statistics dataset contains multiple quan_x0002_titative measurements of Bitcoin network\u2019s state over time. We col_x0002_lected daily statistics from Blockchain.info [10] and averaged those measures to a monthly time window. Examples of network statistics for each month include the market price, total hash rate, total block reward, total transaction fees, mining difficulty, the number of transactions, electricity consumption [4], and the global energy price index [27].",
      "data_code": {
        "sequential": 1,
        "temporal": 1,
        "categorical": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "Visual Encoding: We use a temporal bar chart to encode aggregated (per month) mining pool measures (Fig. 3 (B)). Each bar chart is nor_x0002_malized to the maximum measure of each pool instead of a maximum across all pools. This helps to see the mining power of small pools more clearly. The color of each bar corresponds to the one in V2 to help cross-comparison of these two views. In order to address T2.3, we added two additional visual encodings to the bar chart: the pool fees kept by the pool (quantitative), and whether the mining pool shares the transaction fee to its miners (nominal). Due to limited screen space we chose a dual-axis encoding instead of an additional chart and show the pool fee as a line chart (Fig. 3 (C)). As the pool fee may vary according to the payout scheme, we use different line colors for different schemes. This dual-axis allows analysts to estimate a possible correlation between pool measures and pool fees. The information about the share of the transaction fee (binary) is encoded as the background color (Fig. 3 (D)). It helps to see when a mining pool changed its policy.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "bar+line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line", "bar"]
      },
      {
        "solution_text": "Interaction: The charts in V3 are mostly controlled by selections made in V2 as they are meant as accompanying detail. Analysts can click the info icon for a text description of the pool\u2019s characteristics (Fig. 3 (A)). Additional interactions are tooltips for detail-in-demand.",
        "solution_category": "interaction",
        "solution_axial": "Filtering,OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["Filtering", "OverviewandExplore"],
        "componenet_code": ["filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 551,
    "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
    "pub_year": 2022,
    "domain": "Bitcoin",
    "requirement": {
      "requirement_text": "AF2: Pool managers\u2019 behavior and the competition to attract miners. Mining pools have different payout schemes [50] that define which and how much miners are paid. Payouts are reduced by pool fees [5] a pool may keep to pay for operating costs. We would expect that pools with lower fees pay more constant income to miners for a given payout scheme. These pools may therefore attract miners and exhibit increasing mining power. Pool competition might be visible by convergence toward a limited number of (best) payout schemes and a decrease in pool fees. Such information is crucial for economic debates around pool viability as these fees correspond to a significant part of their income. An analysis of the data might provide additional evidence of the importance of payout schemes and pool fees in attracting miners.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "D2: The mining pool characteristics dataset includes external data about mining pools that we obtained from public sources [6, 7] and manually cleaned. The attributes include the primary location of the pool (nominal), payout scheme (nominal), pool fee (quantitative), and whether the pool kept transaction fees or shared it to miners (binary). We also extracted the wiki history to track attribute changes as mining pools adapt their payout policies to compete with other pools; D3: The Bitcoin network statistics dataset contains multiple quantitative measurements of Bitcoin network\u2019s state over time. We collected daily statistics from Blockchain.info [10] and averaged those measures to a monthly time window. Examples of network statistics for each month include the market price, total hash rate, total block reward, total transaction fees, mining difficulty, the number of transactions, electricity consumption [4], and the global energy price index [27].",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "tables": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Visual Encoding: We chose a temporal ribbon chart that allows analysts to see both the changing rank and a quantitative metric related to each mining pool. The ribbon chart is filtered to the time period selected in V1 and users can choose among multiple statistical measures and how to group and color the pools. The chart\u2019s multiple stacked bars sorted by rank show data aggregated and displayed by months. By default mining pools are sorted from the highest value at the top of the stack to the lowest one at the bottom per month. The same mining pool is connected across months with a ribbon to highlight its rank changing. We encode the top-10 mining pools for the selected time in distinct colors, while the remaining pools are colored in grey. Fig. 2 shows examples of different configurations of the ribbon chart. Analysts can select the mining power measure, e .g., market share, hash rate, total reward, and transaction fees. They can also chose mining pool characteristics to display in different color hue scales, i .e., the name of the pool, its payout scheme, and location. Using the coloring mechanism, analysts can see patterns for the characteristics of the top mining pool. Furthermore, analysts can group those mining pools by the same characteristics to see if any characteristics correlate with the mining growth and domination in the market. Within each group, mining pools are sorted by the selected measure.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "sankey",
        "axial_code": ["Co-axis"],
        "componenet_code": ["sankey"]
      },
      {
        "solution_text": "Interaction: Besides the measures, group-by, and color-by selectors, we provide two ways to highlight mining pools or their characteristics. First, analysts can click on the left side labels which will increase transparency of the unselected pools and consequently highlight those that fall into the selection. The selection also affects and filters the mining pools displayed on the mining pool details view (V3). Second, analysts can draw a brush on the ribbon chart to filter both mining pools and highlight a specific time range. The highlighted timeframe is also represented in views V3\u2013V6. Finally, like in all other views, detail-on_x0002_demand is available on hover via tooltips that show the mining pool name and exact value of the selected measure for this pool.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 552,
    "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
    "pub_year": 2022,
    "domain": "Bitcoin",
    "requirement": {
      "requirement_text": "AF2: Pool managers\u2019 behavior and the competition to attract miners. Mining pools have different payout schemes [50] that define which and how much miners are paid. Payouts are reduced by pool fees [5] a pool may keep to pay for operating costs. We would expect that pools with lower fees pay more constant income to miners for a given payout scheme. These pools may therefore attract miners and exhibit increasing mining power. Pool competition might be visible by convergence toward a limited number of (best) payout schemes and a decrease in pool fees. Such information is crucial for economic debates around pool viability as these fees correspond to a significant part of their income. An analysis of the data might provide additional evidence of the importance of payout schemes and pool fees in attracting miners.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "D2: The mining pool characteristics dataset includes external data about mining pools that we obtained from public sources [6, 7] and manually cleaned. The attributes include the primary location of the pool (nominal), payout scheme (nominal), pool fee (quantitative), and whether the pool kept transaction fees or shared it to miners (binary). We also extracted the wiki history to track attribute changes as mining pools adapt their payout policies to compete with other pools; D3: The Bitcoin network statistics dataset contains multiple quantitative measurements of Bitcoin network\u2019s state over time. We collected daily statistics from Blockchain.info [10] and averaged those measures to a monthly time window. Examples of network statistics for each month include the market price, total hash rate, total block reward, total transaction fees, mining difficulty, the number of transactions, electricity consumption [4], and the global energy price index [27].",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "tables": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Visual Encoding: We use a temporal bar chart to encode aggregated (per month) mining pool measures (Fig. 3 (B)). Each bar chart is nor_x0002_malized to the maximum measure of each pool instead of a maximum across all pools. This helps to see the mining power of small pools more clearly. The color of each bar corresponds to the one in V2 to help cross-comparison of these two views. In order to address T2.3, we added two additional visual encodings to the bar chart: the pool fees kept by the pool (quantitative), and whether the mining pool shares the transaction fee to its miners (nominal). Due to limited screen space we chose a dual-axis encoding instead of an additional chart and show the pool fee as a line chart (Fig. 3 (C)). As the pool fee may vary according to the payout scheme, we use different line colors for different schemes. This dual-axis allows analysts to estimate a possible correlation between pool measures and pool fees. The information about the share of the transaction fee (binary) is encoded as the background color (Fig. 3 (D)). It helps to see when a mining pool changed its policy.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "bar+line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line", "bar"]
      },
      {
        "solution_text": "Interaction: The charts in V3 are mostly controlled by selections made in V2 as they are meant as accompanying detail. Analysts can click the info icon for a text description of the pool\u2019s characteristics (Fig. 3 (A)). Additional interactions are tooltips for detail-in-demand.",
        "solution_category": "interaction",
        "solution_axial": "Filtering,OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["Filtering", "OverviewandExplore"],
        "componenet_code": ["filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 553,
    "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
    "pub_year": 2022,
    "domain": "Bitcoin",
    "requirement": {
      "requirement_text": "AF2: Pool managers\u2019 behavior and the competition to attract miners. Mining pools have different payout schemes [50] that define which and how much miners are paid. Payouts are reduced by pool fees [5] a pool may keep to pay for operating costs. We would expect that pools with lower fees pay more constant income to miners for a given payout scheme. These pools may therefore attract miners and exhibit increasing mining power. Pool competition might be visible by convergence toward a limited number of (best) payout schemes and a decrease in pool fees. Such information is crucial for economic debates around pool viability as these fees correspond to a significant part of their income. An analysis of the data might provide additional evidence of the importance of payout schemes and pool fees in attracting miners.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "D2: The mining pool characteristics dataset includes external data about mining pools that we obtained from public sources [6, 7] and manually cleaned. The attributes include the primary location of the pool (nominal), payout scheme (nominal), pool fee (quantitative), and whether the pool kept transaction fees or shared it to miners (binary). We also extracted the wiki history to track attribute changes as mining pools adapt their payout policies to compete with other pools; D3: The Bitcoin network statistics dataset contains multiple quantitative measurements of Bitcoin network\u2019s state over time. We collected daily statistics from Blockchain.info [10] and averaged those measures to a monthly time window. Examples of network statistics for each month include the market price, total hash rate, total block reward, total transaction fees, mining difficulty, the number of transactions, electricity consumption [4], and the global energy price index [27].",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "tables": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Visual Encoding: The Bitcoin statistics view (Fig. 4) is located in the same position as V3 reachable via a toggle bar. We encode 17 different factors each as a gray temporal area chart including Bitcoin_x0002_internal statistics such as the total number of blocks mined, the total hash rate, the median confirmation time for a block, total rewards paid out, or the total transaction fees. In addition, we calculated statistics with external data such as fees converted to USD according to the current market price or the trade volume in USD. As such, the list of Bitcoin statistics provides information about the status of the Bitcoin network. For example, the number of transactions implies the demand of users; the amount of Bitcoins in transactions means the supply of currency circulating in the market; and the average waiting time indicates the network capacity to verify transactions.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "area",
        "axial_code": ["Repetition"],
        "componenet_code": ["area"]
      },
      {
        "solution_text": "Interaction: The view offers details-on-demand via a tooltip.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 554,
    "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
    "pub_year": 2022,
    "domain": "Bitcoin",
    "requirement": {
      "requirement_text": "AF2: Pool managers\u2019 behavior and the competition to attract miners. Mining pools have different payout schemes [50] that define which and how much miners are paid. Payouts are reduced by pool fees [5] a pool may keep to pay for operating costs. We would expect that pools with lower fees pay more constant income to miners for a given payout scheme. These pools may therefore attract miners and exhibit increasing mining power. Pool competition might be visible by convergence toward a limited number of (best) payout schemes and a decrease in pool fees. Such information is crucial for economic debates around pool viability as these fees correspond to a significant part of their income. An analysis of the data might provide additional evidence of the importance of payout schemes and pool fees in attracting miners.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "D3: The Bitcoin network statistics dataset contains multiple quan_x0002_titative measurements of Bitcoin network\u2019s state over time. We col_x0002_lected daily statistics from Blockchain.info [10] and averaged those measures to a monthly time window. Examples of network statistics for each month include the market price, total hash rate, total block reward, total transaction fees, mining difficulty, the number of transactions, electricity consumption [4], and the global energy price index [27].",
      "data_code": {
        "categorical": 1,
        "tables": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Visual Encoding: We use a temporal bar chart to encode aggregated (per month) mining pool measures (Fig. 3 (B)). Each bar chart is nor_x0002_malized to the maximum measure of each pool instead of a maximum across all pools. This helps to see the mining power of small pools more clearly. The color of each bar corresponds to the one in V2 to help cross-comparison of these two views. In order to address T2.3, we added two additional visual encodings to the bar chart: the pool fees kept by the pool (quantitative), and whether the mining pool shares the transaction fee to its miners (nominal). Due to limited screen space we chose a dual-axis encoding instead of an additional chart and show the pool fee as a line chart (Fig. 3 (C)). As the pool fee may vary according to the payout scheme, we use different line colors for different schemes. This dual-axis allows analysts to estimate a possible correlation between pool measures and pool fees. The information about the share of the transaction fee (binary) is encoded as the background color (Fig. 3 (D)). It helps to see when a mining pool changed its policy.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "bar+line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line", "bar"]
      },
      {
        "solution_text": "Interaction: The charts in V3 are mostly controlled by selections made in V2 as they are meant as accompanying detail. Analysts can click the info icon for a text description of the pool\u2019s characteristics (Fig. 3 (A)). Additional interactions are tooltips for detail-in-demand.",
        "solution_category": "interaction",
        "solution_axial": "Filtering,OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["Filtering", "OverviewandExplore"],
        "componenet_code": ["filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 555,
    "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
    "pub_year": 2022,
    "domain": "Bitcoin",
    "requirement": {
      "requirement_text": "AF3: Miners\u2019 decision to join or leave a pool and its impact on mining pool market shares. Miners are economic agents who con_x0002_sider the cost and benefit of mining. In this respect, Bitcoin value, pay_x0002_out schemes, and pool fees are major determinants of miners\u2019 expected income. Miners\u2019 migration data (i.e., pool hopping and cross-pooling) helps to test assumptions and better understand the drivers of miners\u2019 pool choice. Such analyses are critical to understand a pool\u2019s growth or decline and to which extent miners behave as rational economic agents.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "D5: The miners\u2019 migration dataset describes miners\u2019 migration between mining pools over months. We detect miners who participate in more than one pool (cross-pooling) within a month and calculate the number of miners\u2019 addresses and the total reward those miners received from each pool. Besides, we also detect the flow of miners that join a mining pool for the first time (enter), move from a pool to another (pool hopping), and quit the mining pool (exit) between months. The process of obtaining this dataset is complex and covered in one of our previous articles focused on pool hopping [62].",
      "data_code": { "geometry": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Visual Encoding: The cross-pooling miners during a selected time interval can be considered as a flow of miners between mining pools. Fig. 5 (B) shows the visual encoding of the view. We used a chord dia_x0002_gram to display a metric related to miners crossing between pools; the total amount of miners\u2019 rewards (default) or the total number of miner addresses. The diagram shows the metric encoded as the outside arcs\u2019 length. The flow between mining pools represents the total amount of the metric for those miners who cross-pooled. We represent the average percentage of the miner\u2019s migration statistics per month (i.e., new, exit, hopping in, hopping out, cross pooling) as stacked bar charts around the outer circle. For each pool, three rows represent the percentage of miners incoming (new and hopping in), outgoing (dropout and hopping out), and cross-pooling with the mining pool, respectively.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "chord+bar",
        "axial_code": ["Stack"],
        "componenet_code": ["chord", "bar"]
      },
      {
        "solution_text": "Interaction: Analysts can hover over the flow or stacked bars to see the exact value. They can also change the metric from the total reward to the number of miner addresses. The total reward is a weighted average that considers the impact of large players in the pool. The measure is more robust than just the number of miners.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore,Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering", "OverviewandExplore"],
        "componenet_code": ["filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 556,
    "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
    "pub_year": 2022,
    "domain": "Bitcoin",
    "requirement": {
      "requirement_text": "AF3: Miners\u2019 decision to join or leave a pool and its impact on mining pool market shares. Miners are economic agents who con_x0002_sider the cost and benefit of mining. In this respect, Bitcoin value, pay_x0002_out schemes, and pool fees are major determinants of miners\u2019 expected income. Miners\u2019 migration data (i.e., pool hopping and cross-pooling) helps to test assumptions and better understand the drivers of miners\u2019 pool choice. Such analyses are critical to understand a pool\u2019s growth or decline and to which extent miners behave as rational economic agents.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "D2: The mining pool characteristics dataset includes external data about mining pools that we obtained from public sources [6, 7] and manually cleaned. The attributes include the primary location of the pool (nominal), payout scheme (nominal), pool fee (quantitative), and whether the pool kept transaction fees or shared it to miners (binary). We also extracted the wiki history to track attribute changes as mining pools adapt their payout policies to compete with other pools; D5: The miners\u2019 migration dataset describes miners\u2019 migration between mining pools over months. We detect miners who participate in more than one pool (cross-pooling) within a month and calculate the number of miners\u2019 addresses and the total reward those miners received from each pool. Besides, we also detect the flow of miners that join a mining pool for the first time (enter), move from a pool to another (pool hopping), and quit the mining pool (exit) between months. The process of obtaining this dataset is complex and covered in one of our previous articles focused on pool hopping [62].",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "tables": 1,
        "sequential": 1,
        "temporal": 1
      }
    },
    "solution": [
      {
        "solution_text": "Visual Encoding: We use a temporal bar chart to encode aggregated (per month) mining pool measures (Fig. 3 (B)). Each bar chart is nor_x0002_malized to the maximum measure of each pool instead of a maximum across all pools. This helps to see the mining power of small pools more clearly. The color of each bar corresponds to the one in V2 to help cross-comparison of these two views. In order to address T2.3, we added two additional visual encodings to the bar chart: the pool fees kept by the pool (quantitative), and whether the mining pool shares the transaction fee to its miners (nominal). Due to limited screen space we chose a dual-axis encoding instead of an additional chart and show the pool fee as a line chart (Fig. 3 (C)). As the pool fee may vary according to the payout scheme, we use different line colors for different schemes. This dual-axis allows analysts to estimate a possible correlation between pool measures and pool fees. The information about the share of the transaction fee (binary) is encoded as the background color (Fig. 3 (D)). It helps to see when a mining pool changed its policy.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "bar+line",
        "axial_code": ["Co-axis"],
        "componenet_code": ["line", "bar"]
      },
      {
        "solution_text": "Interaction: The charts in V3 are mostly controlled by selections made in V2 as they are meant as accompanying detail. Analysts can click the info icon for a text description of the pool\u2019s characteristics (Fig. 3 (A)). Additional interactions are tooltips for detail-in-demand.",
        "solution_category": "interaction",
        "solution_axial": "Filtering,OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["Filtering", "OverviewandExplore"],
        "componenet_code": ["filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 557,
    "paper_title": "MiningVis: Visual Analytics of the Bitcoin Mining Economy",
    "pub_year": 2022,
    "domain": "Bitcoin",
    "requirement": {
      "requirement_text": "AF3: Miners\u2019 decision to join or leave a pool and its impact on mining pool market shares. Miners are economic agents who con_x0002_sider the cost and benefit of mining. In this respect, Bitcoin value, pay_x0002_out schemes, and pool fees are major determinants of miners\u2019 expected income. Miners\u2019 migration data (i.e., pool hopping and cross-pooling) helps to test assumptions and better understand the drivers of miners\u2019 pool choice. Such analyses are critical to understand a pool\u2019s growth or decline and to which extent miners behave as rational economic agents.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "D2: The mining pool characteristics dataset includes external data about mining pools that we obtained from public sources [6, 7] and manually cleaned. The attributes include the primary location of the pool (nominal), payout scheme (nominal), pool fee (quantitative), and whether the pool kept transaction fees or shared it to miners (binary). We also extracted the wiki history to track attribute changes as mining pools adapt their payout policies to compete with other pools; D5: The miners\u2019 migration dataset describes miners\u2019 migration between mining pools over months. We detect miners who participate in more than one pool (cross-pooling) within a month and calculate the number of miners\u2019 addresses and the total reward those miners received from each pool. Besides, we also detect the flow of miners that join a mining pool for the first time (enter), move from a pool to another (pool hopping), and quit the mining pool (exit) between months. The process of obtaining this dataset is complex and covered in one of our previous articles focused on pool hopping [62].",
      "data_code": {
        "geometry": 1,
        "categorical": 1,
        "tables": 1,
        "sequential": 1,
        "temporal": 1
      }
    },
    "solution": [
      {
        "solution_text": "Visual Encoding: The cross-pooling miners during a selected time interval can be considered as a flow of miners between mining pools. Fig. 5 (B) shows the visual encoding of the view. We used a chord dia_x0002_gram to display a metric related to miners crossing between pools; the total amount of miners\u2019 rewards (default) or the total number of miner addresses. The diagram shows the metric encoded as the outside arcs\u2019 length. The flow between mining pools represents the total amount of the metric for those miners who cross-pooled. We represent the average percentage of the miner\u2019s migration statistics per month (i.e., new, exit, hopping in, hopping out, cross pooling) as stacked bar charts around the outer circle. For each pool, three rows represent the percentage of miners incoming (new and hopping in), outgoing (dropout and hopping out), and cross-pooling with the mining pool, respectively.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "chord+bar",
        "axial_code": ["Stack"],
        "componenet_code": ["chord", "bar"]
      },
      {
        "solution_text": "Interaction: Analysts can hover over the flow or stacked bars to see the exact value. They can also change the metric from the total reward to the number of miner addresses. The total reward is a weighted average that considers the impact of large players in the pool. The measure is more robust than just the number of miners.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore,Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering", "OverviewandExplore"],
        "componenet_code": ["filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 558,
    "paper_title": "Compass: Towards Better Causal Analysis of Urban Time Series",
    "pub_year": 2022,
    "domain": "urban causality",
    "requirement": {
      "requirement_text": "R1 Summarize causal graphs across the time (WA). First, the ex-perts require the system to summarize all causal graphs detected indifferent time windows and thereby grasp the brief patterns. Forexample, which sensors have strong causal relations across thewhole period? What are the causal directions between them?",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The air pollution dataset we used comprises the hourly readings of the PM2.5 2 concentration from 448 major air quality sensors in China between January 8 and March 23, 2018. Each sensor comprises a time series with 1800 recorded values. This dataset has a total of 806,400 records and a size of 2.99 MB. We constructed a neighbor index based on the sensors\u2019 spatial proximity (Sec. 4.2.1); The traffic dataset we used was from the released Q_x0002_Traffic dataset [39]. This dataset includes subparts of the road network in Beijing and the traffic speed for every 15 minutes for every road segment. We first sampled 511 high-level roads, such as highways. Each road segment is regarded as a road sensor. We also sampled the readings from May 9, 2017 to May 18, 2017. Finally, this dataset totally comprises 490,560 records and has a size of 1.89 MB.",
      "data_code": { "geometry": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "First, we pre-process the data in space and time dimensions. Indexing neighbors. A sensor has causal relations only with its neighbor sensors. This neighbor relationship depends on specific ap_x0002_plication scenarios. Two concrete examples are provided below. A road sensor can be influenced only by those sensors encountered first along forward the traffic direction because traffic congestions propagate backward. An air quality sensor has causal relations only with those closest sensors around it because air pollutants spread continuously in space. For every sensor, we index its neighbors with potential causality and categorize them into downstream and upstream neighbors based on actual applications. The center sensor is denoted as an ego sensor se. The downstream and upstream sensors are denoted as Egod(se) and Egou(se), respectively. Fig. 4a illustrates Egod(sx) and Fig. 4b illustrates Egou(sy). Partitioning time. Two strategies for time partitioning can be used based on whether the time series is periodic. For periodic time series, they can be directly partitioned by their period. For example, traffic time series can be naturally partitioned by 24 hours. Otherwise, the peaks of the time series will be extracted automatically, and thereby the time windows can be identified based on these peaks. This proce_x0002_dure is accomplished using Python find peaks imported from a well known package scipy.signal [32]. The find peaks has four important parameters: height, prominence, wlen, and distance. The height and prominence limit peaks\u2019 minimum heights and prominences. The wlen indicates the maximum duration of windows. The distance means the minimum horizon distance between peaks. Given these parameters and a time series, find peaks returns the intervals where peaks are located.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Retrieval",
        "solution_compoent": "",
        "axial_code": ["Retrieval", "AlgorithmicCalculation"],
        "componenet_code": ["retrieval", "algorithmic_calculation"]
      },
      {
        "solution_text": "Visualizing spatiotemporal causal graph (5f). The causal graphs across the windows can be summarized as a spatiotemporal causal graph on the map. The causal directions across multiple graphs are first aggregated according to the edge. The aggregation is then encoded with a revisited compass glyph (5e). For example, 5e summarizes the causal directions in the bottom-right edge across the three causal graphs. In a revisited compass glyph, arrows still convey the causal directions within a spatial context. The glyph is revised to incorporate the temporal information as follows. The arrow\u2019s size encodes the frequency of the causal links with this direction. An offset between the two opposite arrows can be observed if bi-directional relations exist. The overlapping part of the two arrows encodes the frequency of bi-directional relations.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "network+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "network"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 559,
    "paper_title": "Compass: Towards Better Causal Analysis of Urban Time Series",
    "pub_year": 2022,
    "domain": "urban causality",
    "requirement": {
      "requirement_text": "R2 Explore causal graphs along the time (WA). Multiple graphsin different time windows constitute a dynamic graph where thestructure can change over time [8]. The experts need to relate thecausal graphs to the time for effective time-oriented exploration [69,70,77]. They also want to learn the temporal variations of the causalstructures, such as periodicity and stability. Therefore, the systemneeds to couple a timeline-based organization with a structure-aware representation to visualize these causal graphs.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The air pollution dataset we used comprises the hourly readings of the PM2.5 2 concentration from 448 major air quality sensors in China between January 8 and March 23, 2018. Each sensor comprises a time series with 1800 recorded values. This dataset has a total of 806,400 records and a size of 2.99 MB. We constructed a neighbor index based on the sensors\u2019 spatial proximity (Sec. 4.2.1); The traffic dataset we used was from the released Q_x0002_Traffic dataset [39]. This dataset includes subparts of the road network in Beijing and the traffic speed for every 15 minutes for every road segment. We first sampled 511 high-level roads, such as highways. Each road segment is regarded as a road sensor. We also sampled the readings from May 9, 2017 to May 18, 2017. Finally, this dataset totally comprises 490,560 records and has a size of 1.89 MB.",
      "data_code": { "geometry": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "First, we pre-process the data in space and time dimensions. Indexing neighbors. A sensor has causal relations only with its neighbor sensors. This neighbor relationship depends on specific ap_x0002_plication scenarios. Two concrete examples are provided below. A road sensor can be influenced only by those sensors encountered first along forward the traffic direction because traffic congestions propagate backward. An air quality sensor has causal relations only with those closest sensors around it because air pollutants spread continuously in space. For every sensor, we index its neighbors with potential causality and categorize them into downstream and upstream neighbors based on actual applications. The center sensor is denoted as an ego sensor se. The downstream and upstream sensors are denoted as Egod(se) and Egou(se), respectively. Fig. 4a illustrates Egod(sx) and Fig. 4b illustrates Egou(sy). Partitioning time. Two strategies for time partitioning can be used based on whether the time series is periodic. For periodic time series, they can be directly partitioned by their period. For example, traffic time series can be naturally partitioned by 24 hours. Otherwise, the peaks of the time series will be extracted automatically, and thereby the time windows can be identified based on these peaks. This proce_x0002_dure is accomplished using Python find peaks imported from a well known package scipy.signal [32]. The find peaks has four important parameters: height, prominence, wlen, and distance. The height and prominence limit peaks\u2019 minimum heights and prominences. The wlen indicates the maximum duration of windows. The distance means the minimum horizon distance between peaks. Given these parameters and a time series, find peaks returns the intervals where peaks are located.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Retrieval",
        "solution_compoent": "",
        "axial_code": ["Retrieval", "AlgorithmicCalculation"],
        "componenet_code": ["retrieval", "algorithmic_calculation"]
      },
      {
        "solution_text": "Visualizing multiple causal graphs. The system should expose the temporal variation of causalities and support the time-oriented exploration. Thus, all graph bands are placed along the time view according to the time window in which the graph is detected (5d). The bands repel each other to avoid occlusion while keeping their positions along the timeline as much as possible. To further enhance the spatial context, we add minimaps on the left side of this view. Each minimap (5d-2) contains the same ego-graph structure as in the map view but removes detailed information such as geographic background, colors, and directions. The bold edge indicates where the glyphs on the right are located. This visualization presents time-varying causal relations in an unobstructed way. Take figure as an example. The causal relations of the bottom left edge exist in all time windows. These existences may be hidden by the scattered arrows in 5c but are clearly revealed in the third row in 5d.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "network+glyph",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph", "network"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 560,
    "paper_title": "Compass: Towards Better Causal Analysis of Urban Time Series",
    "pub_year": 2022,
    "domain": "urban causality",
    "requirement": {
      "requirement_text": "R3 Learn influence propagation via causal graphs (WA). The ex-perts also need to drill down individual graphs and learn the influ-ence propagation during a specific period. Specifically, they aim toestablish where the stimulus is from and how it influences the urbanspace. Therefore, in the system, every individual graph should beaccessible in the spatial context.",
      "requirement_code": {
        "describe_observation_item": 1,
        "identify_main_cause_item": 1
      }
    },
    "data": {
      "data_text": "The air pollution dataset we used comprises the hourly readings of the PM2.5 2 concentration from 448 major air quality sensors in China between January 8 and March 23, 2018. Each sensor comprises a time series with 1800 recorded values. This dataset has a total of 806,400 records and a size of 2.99 MB. We constructed a neighbor index based on the sensors\u2019 spatial proximity (Sec. 4.2.1); The traffic dataset we used was from the released Q_x0002_Traffic dataset [39]. This dataset includes subparts of the road network in Beijing and the traffic speed for every 15 minutes for every road segment. We first sampled 511 high-level roads, such as highways. Each road segment is regarded as a road sensor. We also sampled the readings from May 9, 2017 to May 18, 2017. Finally, this dataset totally comprises 490,560 records and has a size of 1.89 MB.",
      "data_code": { "geometry": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "First, we pre-process the data in space and time dimensions. Indexing neighbors. A sensor has causal relations only with its neighbor sensors. This neighbor relationship depends on specific ap_x0002_plication scenarios. Two concrete examples are provided below. A road sensor can be influenced only by those sensors encountered first along forward the traffic direction because traffic congestions propagate backward. An air quality sensor has causal relations only with those closest sensors around it because air pollutants spread continuously in space. For every sensor, we index its neighbors with potential causality and categorize them into downstream and upstream neighbors based on actual applications. The center sensor is denoted as an ego sensor se. The downstream and upstream sensors are denoted as Egod(se) and Egou(se), respectively. Fig. 4a illustrates Egod(sx) and Fig. 4b illustrates Egou(sy). Partitioning time. Two strategies for time partitioning can be used based on whether the time series is periodic. For periodic time series, they can be directly partitioned by their period. For example, traffic time series can be naturally partitioned by 24 hours. Otherwise, the peaks of the time series will be extracted automatically, and thereby the time windows can be identified based on these peaks. This proce_x0002_dure is accomplished using Python find peaks imported from a well known package scipy.signal [32]. The find peaks has four important parameters: height, prominence, wlen, and distance. The height and prominence limit peaks\u2019 minimum heights and prominences. The wlen indicates the maximum duration of windows. The distance means the minimum horizon distance between peaks. Given these parameters and a time series, find peaks returns the intervals where peaks are located.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Retrieval",
        "solution_compoent": "",
        "axial_code": ["Retrieval", "AlgorithmicCalculation"],
        "componenet_code": ["retrieval", "algorithmic_calculation"]
      },
      {
        "solution_text": "Visualizing spatial causal graph (5c-1). Each causal graph detected in a time window can be displayed on the map as a spatial causal graph. The question mark will be replaced with the arrows that denote the causal relations between sensors. Each arrow represents a causal link and points from the cause sensor to the effect sensor. The arrows are colored according to the cause sensor. The arrow opacity encodes the causal strength. Moreover, we follow a compass metaphor to place the arrows (5b). We call such a design compass glyph. Each glyph denotes a causal relation. An edge is colored red, blue, or gray according to its direction indicated by the arrows. Sensors are removed to reduce clutter if they have no causality with the ego sensor.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "network+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "network"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 561,
    "paper_title": "Compass: Towards Better Causal Analysis of Urban Time Series",
    "pub_year": 2022,
    "domain": "urban causality",
    "requirement": {
      "requirement_text": "R3 Learn influence propagation via causal graphs (WA). The ex-perts also need to drill down individual graphs and learn the influ-ence propagation during a specific period. Specifically, they aim toestablish where the stimulus is from and how it influences the urbanspace. Therefore, in the system, every individual graph should beaccessible in the spatial context.",
      "requirement_code": {
        "describe_observation_item": 1,
        "identify_main_cause_item": 1
      }
    },
    "data": {
      "data_text": "The air pollution dataset we used comprises the hourly readings of the PM2.5 2 concentration from 448 major air quality sensors in China between January 8 and March 23, 2018. Each sensor comprises a time series with 1800 recorded values. This dataset has a total of 806,400 records and a size of 2.99 MB. We constructed a neighbor index based on the sensors\u2019 spatial proximity (Sec. 4.2.1); The traffic dataset we used was from the released Q_x0002_Traffic dataset [39]. This dataset includes subparts of the road network in Beijing and the traffic speed for every 15 minutes for every road segment. We first sampled 511 high-level roads, such as highways. Each road segment is regarded as a road sensor. We also sampled the readings from May 9, 2017 to May 18, 2017. Finally, this dataset totally comprises 490,560 records and has a size of 1.89 MB.",
      "data_code": { "geometry": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "First, we pre-process the data in space and time dimensions. Indexing neighbors. A sensor has causal relations only with its neighbor sensors. This neighbor relationship depends on specific ap_x0002_plication scenarios. Two concrete examples are provided below. A road sensor can be influenced only by those sensors encountered first along forward the traffic direction because traffic congestions propagate backward. An air quality sensor has causal relations only with those closest sensors around it because air pollutants spread continuously in space. For every sensor, we index its neighbors with potential causality and categorize them into downstream and upstream neighbors based on actual applications. The center sensor is denoted as an ego sensor se. The downstream and upstream sensors are denoted as Egod(se) and Egou(se), respectively. Fig. 4a illustrates Egod(sx) and Fig. 4b illustrates Egou(sy). Partitioning time. Two strategies for time partitioning can be used based on whether the time series is periodic. For periodic time series, they can be directly partitioned by their period. For example, traffic time series can be naturally partitioned by 24 hours. Otherwise, the peaks of the time series will be extracted automatically, and thereby the time windows can be identified based on these peaks. This proce_x0002_dure is accomplished using Python find peaks imported from a well known package scipy.signal [32]. The find peaks has four important parameters: height, prominence, wlen, and distance. The height and prominence limit peaks\u2019 minimum heights and prominences. The wlen indicates the maximum duration of windows. The distance means the minimum horizon distance between peaks. Given these parameters and a time series, find peaks returns the intervals where peaks are located.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Retrieval",
        "solution_compoent": "",
        "axial_code": ["Retrieval", "AlgorithmicCalculation"],
        "componenet_code": ["retrieval", "algorithmic_calculation"]
      },
      {
        "solution_text": "Visualizing single causal graph (R3). Simply presenting spatialcausal graphs results in low scalability because the maps are spaceineffective (Fig. 5c). To this end, we compact each causal graph (e.g.,Fig. 5c-1) into a graph band (e.g., Fig. 5d-1). The compass glyphsfrom top to bottom in a graph band correspond to that of the edgesin a clockwise order starting from the west direction. These compassglyphs preserve the spatial context for each causal relation.To further integrate the spatial context of causal graphs, heatmap-based spatial summaries for every causal graph are provided and placedabove the bands (Fig. 1c-1). The heatmap is generated from the spatialdistribution of the influences of the ego and neighbor sensors. Thedarker red (or blue) indicates that the area receives more influencesfrom the ego sensor (or the neighbor sensors). To do so, we firstgenerate a red heatmap based on the spatial distribution of the redcausal links. A blue heatmap is generated in the same way. We subtractthe blue heatmap from the red heatmap and finally derive the summary.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 562,
    "paper_title": "Compass: Towards Better Causal Analysis of Urban Time Series",
    "pub_year": 2022,
    "domain": "urban causality",
    "requirement": {
      "requirement_text": "R4 Interpret and validate causal relations (WA, WI). Causal inter-pretation and validation are required. The experts hope to interpretwhy there are causalities between sensors and learn what the ef-fects are based on the involved time series. Furthermore, validationquestions may be asked, for instance, are causal relations includ-ing their directions and time lags reasonable? So, causal relationsmust be fully encoded regarding the occurrence frequency, involvedtime series, and comprised multidimensional attributes.",
      "requirement_code": { "identify_main_cause_aggregate": 1 }
    },
    "data": {
      "data_text": "The air pollution dataset we used comprises the hourly readings of the PM2.5 2 concentration from 448 major air quality sensors in China between January 8 and March 23, 2018. Each sensor comprises a time series with 1800 recorded values. This dataset has a total of 806,400 records and a size of 2.99 MB. We constructed a neighbor index based on the sensors\u2019 spatial proximity (Sec. 4.2.1); The traffic dataset we used was from the released Q_x0002_Traffic dataset [39]. This dataset includes subparts of the road network in Beijing and the traffic speed for every 15 minutes for every road segment. We first sampled 511 high-level roads, such as highways. Each road segment is regarded as a road sensor. We also sampled the readings from May 9, 2017 to May 18, 2017. Finally, this dataset totally comprises 490,560 records and has a size of 1.89 MB.",
      "data_code": { "geometry": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "First, we pre-process the data in space and time dimensions. Indexing neighbors. A sensor has causal relations only with its neighbor sensors. This neighbor relationship depends on specific ap_x0002_plication scenarios. Two concrete examples are provided below. A road sensor can be influenced only by those sensors encountered first along forward the traffic direction because traffic congestions propagate backward. An air quality sensor has causal relations only with those closest sensors around it because air pollutants spread continuously in space. For every sensor, we index its neighbors with potential causality and categorize them into downstream and upstream neighbors based on actual applications. The center sensor is denoted as an ego sensor se. The downstream and upstream sensors are denoted as Egod(se) and Egou(se), respectively. Fig. 4a illustrates Egod(sx) and Fig. 4b illustrates Egou(sy). Partitioning time. Two strategies for time partitioning can be used based on whether the time series is periodic. For periodic time series, they can be directly partitioned by their period. For example, traffic time series can be naturally partitioned by 24 hours. Otherwise, the peaks of the time series will be extracted automatically, and thereby the time windows can be identified based on these peaks. This proce_x0002_dure is accomplished using Python find peaks imported from a well known package scipy.signal [32]. The find peaks has four important parameters: height, prominence, wlen, and distance. The height and prominence limit peaks\u2019 minimum heights and prominences. The wlen indicates the maximum duration of windows. The distance means the minimum horizon distance between peaks. Given these parameters and a time series, find peaks returns the intervals where peaks are located.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Retrieval",
        "solution_compoent": "",
        "axial_code": ["Retrieval", "AlgorithmicCalculation"],
        "componenet_code": ["retrieval", "algorithmic_calculation"]
      },
      {
        "solution_text": "Visualizing the causal relations of a graph. The graphs need to be unfolded to interpret and validate the causal relations further. Figure shows an unfolded graph band. First, the compass glyphs are shrunk, leaving a ring space to encode the lags with arcs. The arc behind an arrow corresponds to the causal link denoted by the arrow and has the same color as the arrow. The arc length encodes the lag of the causal link. Second, a canvas expands from the right. For every causal relation, the two involved time series are sliced according to the time window and displayed together in the same line chart on the glyph\u2019s right side. The lines are also colored according to the sensor. Users can easily compare the time series of all sensors involved in the graph because these glyphs are aligned vertically in a graph band.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph+line",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph", "line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 563,
    "paper_title": "Compass: Towards Better Causal Analysis of Urban Time Series",
    "pub_year": 2022,
    "domain": "urban causality",
    "requirement": {
      "requirement_text": "R4 Interpret and validate causal relations (WA, WI). Causal inter-pretation and validation are required. The experts hope to interpretwhy there are causalities between sensors and learn what the ef-fects are based on the involved time series. Furthermore, validationquestions may be asked, for instance, are causal relations includ-ing their directions and time lags reasonable? So, causal relationsmust be fully encoded regarding the occurrence frequency, involvedtime series, and comprised multidimensional attributes.",
      "requirement_code": { "identify_main_cause_aggregate": 1 }
    },
    "data": {
      "data_text": "The air pollution dataset we used comprises the hourly readings of the PM2.5 2 concentration from 448 major air quality sensors in China between January 8 and March 23, 2018. Each sensor comprises a time series with 1800 recorded values. This dataset has a total of 806,400 records and a size of 2.99 MB. We constructed a neighbor index based on the sensors\u2019 spatial proximity (Sec. 4.2.1); The traffic dataset we used was from the released Q_x0002_Traffic dataset [39]. This dataset includes subparts of the road network in Beijing and the traffic speed for every 15 minutes for every road segment. We first sampled 511 high-level roads, such as highways. Each road segment is regarded as a road sensor. We also sampled the readings from May 9, 2017 to May 18, 2017. Finally, this dataset totally comprises 490,560 records and has a size of 1.89 MB.",
      "data_code": { "geometry": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "First, we pre-process the data in space and time dimensions. Indexing neighbors. A sensor has causal relations only with its neighbor sensors. This neighbor relationship depends on specific ap_x0002_plication scenarios. Two concrete examples are provided below. A road sensor can be influenced only by those sensors encountered first along forward the traffic direction because traffic congestions propagate backward. An air quality sensor has causal relations only with those closest sensors around it because air pollutants spread continuously in space. For every sensor, we index its neighbors with potential causality and categorize them into downstream and upstream neighbors based on actual applications. The center sensor is denoted as an ego sensor se. The downstream and upstream sensors are denoted as Egod(se) and Egou(se), respectively. Fig. 4a illustrates Egod(sx) and Fig. 4b illustrates Egou(sy). Partitioning time. Two strategies for time partitioning can be used based on whether the time series is periodic. For periodic time series, they can be directly partitioned by their period. For example, traffic time series can be naturally partitioned by 24 hours. Otherwise, the peaks of the time series will be extracted automatically, and thereby the time windows can be identified based on these peaks. This proce_x0002_dure is accomplished using Python find peaks imported from a well known package scipy.signal [32]. The find peaks has four important parameters: height, prominence, wlen, and distance. The height and prominence limit peaks\u2019 minimum heights and prominences. The wlen indicates the maximum duration of windows. The distance means the minimum horizon distance between peaks. Given these parameters and a time series, find peaks returns the intervals where peaks are located.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Retrieval",
        "solution_compoent": "",
        "axial_code": ["Retrieval", "AlgorithmicCalculation"],
        "componenet_code": ["retrieval", "algorithmic_calculation"]
      },
      {
        "solution_text": "The relation view shows the multidimensional details of causal relations. Causality suspiciousness is visually encoded to help identify and correct suspicious causal links and relations.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "Network+bubble",
        "axial_code": ["Nesting"],
        "componenet_code": ["bubble", "Network"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 564,
    "paper_title": "Compass: Towards Better Causal Analysis of Urban Time Series",
    "pub_year": 2022,
    "domain": "urban causality",
    "requirement": {
      "requirement_text": "R5 Modify incorrect causal relations (WI). The experts commentedthat detection results are not completely reliable. Causal interpre-tation and validation can help identify incorrect causal relations.Afterward, the experts require modifying them interactively.",
      "requirement_code": { "ensuring_data_quality": 1 }
    },
    "data": {
      "data_text": "The air pollution dataset we used comprises the hourly readings of the PM2.5 2 concentration from 448 major air quality sensors in China between January 8 and March 23, 2018. Each sensor comprises a time series with 1800 recorded values. This dataset has a total of 806,400 records and a size of 2.99 MB. We constructed a neighbor index based on the sensors\u2019 spatial proximity (Sec. 4.2.1); The traffic dataset we used was from the released Q_x0002_Traffic dataset [39]. This dataset includes subparts of the road network in Beijing and the traffic speed for every 15 minutes for every road segment. We first sampled 511 high-level roads, such as highways. Each road segment is regarded as a road sensor. We also sampled the readings from May 9, 2017 to May 18, 2017. Finally, this dataset totally comprises 490,560 records and has a size of 1.89 MB.",
      "data_code": { "geometry": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "First, we pre-process the data in space and time dimensions. Indexing neighbors. A sensor has causal relations only with its neighbor sensors. This neighbor relationship depends on specific ap_x0002_plication scenarios. Two concrete examples are provided below. A road sensor can be influenced only by those sensors encountered first along forward the traffic direction because traffic congestions propagate backward. An air quality sensor has causal relations only with those closest sensors around it because air pollutants spread continuously in space. For every sensor, we index its neighbors with potential causality and categorize them into downstream and upstream neighbors based on actual applications. The center sensor is denoted as an ego sensor se. The downstream and upstream sensors are denoted as Egod(se) and Egou(se), respectively. Fig. 4a illustrates Egod(sx) and Fig. 4b illustrates Egou(sy). Partitioning time. Two strategies for time partitioning can be used based on whether the time series is periodic. For periodic time series, they can be directly partitioned by their period. For example, traffic time series can be naturally partitioned by 24 hours. Otherwise, the peaks of the time series will be extracted automatically, and thereby the time windows can be identified based on these peaks. This proce_x0002_dure is accomplished using Python find peaks imported from a well known package scipy.signal [32]. The find peaks has four important parameters: height, prominence, wlen, and distance. The height and prominence limit peaks\u2019 minimum heights and prominences. The wlen indicates the maximum duration of windows. The distance means the minimum horizon distance between peaks. Given these parameters and a time series, find peaks returns the intervals where peaks are located.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation,Retrieval",
        "solution_compoent": "",
        "axial_code": ["Retrieval", "AlgorithmicCalculation"],
        "componenet_code": ["retrieval", "algorithmic_calculation"]
      },
      {
        "solution_text": "The relation view shows the multidimensional details of causal relations. Causality suspiciousness is visually encoded to help identify and correct suspicious causal links and relations.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "Network+bubble",
        "axial_code": ["Nesting"],
        "componenet_code": ["bubble", "Network"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 565,
    "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse",
    "pub_year": 2022,
    "domain": "E-Commerce",
    "requirement": {
      "requirement_text": "M1 Facilitate the inspection of delayed orders. Orders that havebeen processed or hung up for a long time should be preliminarilyhighlighted for users to ef\ufb01ciently locate the possibly abnormalprocedure and delayed orders. Thus, the proposed system shouldbe able to highlight the possible delayed orders. Furthermore,the highlighted orders need to be more evident for a progressivewarning based on more cost duration.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.",
      "data_code": { "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "The monitoring view is motivated by the requirements for real-time delay detection and unpredictable data stream. This view presents the processing status of incoming online orders updated in real time.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 566,
    "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse",
    "pub_year": 2022,
    "domain": "E-Commerce",
    "requirement": {
      "requirement_text": "M1 Facilitate the inspection of delayed orders. Orders that havebeen processed or hung up for a long time should be preliminarilyhighlighted for users to ef\ufb01ciently locate the possibly abnormalprocedure and delayed orders. Thus, the proposed system shouldbe able to highlight the possible delayed orders. Furthermore,the highlighted orders need to be more evident for a progressivewarning based on more cost duration.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.",
      "data_code": { "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "The slots are set based on integer multiples of the threshold, which increase from left to right. There is a boundary between these slots, indicating the border of normal and delayed processed orders. Within each slot, the darkness of color encodes the time cost and the width of the bar represents the number of orders in the slot.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 567,
    "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse",
    "pub_year": 2022,
    "domain": "E-Commerce",
    "requirement": {
      "requirement_text": "M2 Scale with the unpredictable volumes of incoming orders.The system should be capable of an efficient scalability to presentthe incoming data, due to the uncertain volumes of streaming data.Moreover, the presentations of real-time data change need smoothtransitions to ensure readability.",
      "requirement_code": { "flexibility_and_scalability": 1 }
    },
    "data": {
      "data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.",
      "data_code": { "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "The monitoring view is motivated by the requirements for real-time delay detection and unpredictable data stream. This view presents the processing status of incoming online orders updated in real time.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 568,
    "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse",
    "pub_year": 2022,
    "domain": "E-Commerce",
    "requirement": {
      "requirement_text": "M3 Present parallel operation status. As a part of order processingdata in an e-commerce warehouse, the status of parallel taskoperators is crucial for managers to know the operation ef\ufb01ciencyand engagement. Thus, it is necessary to provide a design topresent these parallel task data in the system.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.",
      "data_code": { "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Considering that the order-processing data have a hierarchical struc-ture (M3) and parallel tasks (M4), we propose a tailored design forthese data features. Specially, for such procedures as Picking & Sortingand Packing, extra visual designs are integrated into the order sedimen-tation metaphor. In the Picking & Sorting procedure, the falling objectschange from individual orders to picking lists because the processed ob-jects have changed. Within this procedure, multiple tokens are attachedto represent orders in one picking list (Fig. 3 (d)). The attached designillustrates the hierarchical structure of aggregated individual orders andthe picking list item. As for the Packing & Weighting procedure, wepresent parallel tasks in the parallel drop charts (Fig. 3 (e)). Each dropchart represents one individual working operator or workstation, withinwhich order items fall and accumulate independently. In the proceduresof Preprocessing, Aggregating, and Weighting, the processed timesare nearly zero because operations in these procedures are executedby the computer system. Therefore, we only leave processed layersin these procedures. In the Outbound procedure, the processed ordersare outbound, such that no waiting or delay issues occur, which makesweighting the last procedure in the monitoring view.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 569,
    "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse",
    "pub_year": 2022,
    "domain": "E-Commerce",
    "requirement": {
      "requirement_text": "M4 Support the visualization of hierarchical order processingdata. To support a comprehensive presentation of the order pro-cessing data content, such as the picking list, the system shouldconsider the hierarchical data structures and enable the real-timepresentations of these data, including individual orders and pick-ing lists.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.",
      "data_code": { "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Considering that the order-processing data have a hierarchical struc-ture (M3) and parallel tasks (M4), we propose a tailored design forthese data features. Specially, for such procedures as Picking & Sortingand Packing, extra visual designs are integrated into the order sedimen-tation metaphor. In the Picking & Sorting procedure, the falling objectschange from individual orders to picking lists because the processed ob-jects have changed. Within this procedure, multiple tokens are attachedto represent orders in one picking list (Fig. 3 (d)). The attached designillustrates the hierarchical structure of aggregated individual orders andthe picking list item. As for the Packing & Weighting procedure, wepresent parallel tasks in the parallel drop charts (Fig. 3 (e)). Each dropchart represents one individual working operator or workstation, withinwhich order items fall and accumulate independently. In the proceduresof Preprocessing, Aggregating, and Weighting, the processed timesare nearly zero because operations in these procedures are executedby the computer system. Therefore, we only leave processed layersin these procedures. In the Outbound procedure, the processed ordersare outbound, such that no waiting or delay issues occur, which makesweighting the last procedure in the monitoring view.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 570,
    "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse",
    "pub_year": 2022,
    "domain": "E-Commerce",
    "requirement": {
      "requirement_text": "A1 Visualize the historical processing records of orders. Bychecking the processing timeline, users can judge whether anorder is delayed from the comprehensive historical overview,including the time cost on processing and waiting, and the rela-tionship of picking lists with the aggregated orders. Therefore,the system should support the visualization of these historicalrecords for in-depth analysis.",
      "requirement_code": {
        "describe_observation_aggregate": 1,
        "collect_evidence": 1
      }
    },
    "data": {
      "data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.",
      "data_code": { "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "The goal of the analyzing view is to allow users to check the historical processing record of delayed orders detected by time thresholds. This kind of checking enables users to know the source of delayed orders to solve the delay problems efficiently.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "gantt+marey\u2019sgraph",
        "axial_code": ["Nesting"],
        "componenet_code": ["marey\u2019sgraph", "gantt"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 571,
    "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse",
    "pub_year": 2022,
    "domain": "E-Commerce",
    "requirement": {
      "requirement_text": "A1 Visualize the historical processing records of orders. Bychecking the processing timeline, users can judge whether anorder is delayed from the comprehensive historical overview,including the time cost on processing and waiting, and the rela-tionship of picking lists with the aggregated orders. Therefore,the system should support the visualization of these historicalrecords for in-depth analysis.",
      "requirement_code": {
        "describe_observation_aggregate": 1,
        "collect_evidence": 1
      }
    },
    "data": {
      "data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.",
      "data_code": { "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Our final design, named order processing lifeline, is built on thecombination of Marey\u2019s graph and a Gantt chart (Fig. 4 (d)), with theadvantages of both. In each procedure, the horizontal axis encodestime, which increases from left to right. We employ a componentconsisting of a Gantt chart unit visualizing the processed time and aMarey\u2019s graph unit presenting the blocked time (A1). In the Gantt chart unit, we make each row represent parallel operators or workstations,where individual processing tasks are shown in a temporal sequence.However, this way is unscalable when many operators exist. Also, we\ufb01nd real-world data are sparse among individual operators. Therefore,we provide an overview of the parallel task status at the same timeby compressing the Gantt chart units in the dimension of individualoperators (Fig. 4 (c)). The overview is obtained through a dynamicprogramming algorithm, which makes the rectangles in the Gantt chartarranged in as few rows as possible. Through the overview, users canaccess the parallel task density in a speci\ufb01c time.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "gantt+marey\u2019sgraph",
        "axial_code": ["Nesting"],
        "componenet_code": ["marey\u2019sgraph", "gantt"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 572,
    "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse",
    "pub_year": 2022,
    "domain": "E-Commerce",
    "requirement": {
      "requirement_text": "A2 Provide related information about processing history. Re-lated information is necessary for analysis. For example, par-allel task density can assist warehouse managers in accessing theprocessing load degree. Statistical information such as each pro-cedures\u2019 time cost distribution can also make users aware of thenormal and abnormal time cost. The system should also provide\ufb02exible interactions such as selecting or \ufb01ltering speci\ufb01c ordersfor further analysis and priority evaluations.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.",
      "data_code": { "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "The goal of the analyzing view is to allow users to check the historical processing record of delayed orders detected by time thresholds. This kind of checking enables users to know the source of delayed orders to solve the delay problems efficiently.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "gantt+marey\u2019sgraph",
        "axial_code": ["Nesting"],
        "componenet_code": ["marey\u2019sgraph", "gantt"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 573,
    "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse",
    "pub_year": 2022,
    "domain": "E-Commerce",
    "requirement": {
      "requirement_text": "E1 Present handling priority ranking. Several factors decide thehandling priorities. For example, orders close to the delivery dead-line have a high priority to be handled. Thus, the system shouldintegrate a ranking method considering the trade-off between allkinds of delay-handling factors. Moreover, users should be al-lowed to adjust the weight of these factors for a proper rankingresult under practical real-time conditions.",
      "requirement_code": { "discover_observation": 1, "compare_entities": 1 }
    },
    "data": {
      "data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.",
      "data_code": { "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "To help users decide which delayed orders should be handled first, we provide an evaluating view. In this view, users can set a series of weights on the factors that determine the dealing priority of delayed orders and access details about each order, including operator information and goods details. Through the above information, users can identify whether orders should be handled.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "area+bar+table",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "bar", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 574,
    "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse",
    "pub_year": 2022,
    "domain": "E-Commerce",
    "requirement": {
      "requirement_text": "E2 Show order details. To facilitate the further exploration of se-lected orders, users need to know the details of the orders, suchas the goods SKUs, goods attributes, and order ids. So the systemshould present the detail information on users\u2019 demand.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.",
      "data_code": { "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "The evaluating view also contains detail panels that show detailedinformation about selected orders in the ranking component (E2). Bydiscussing with the domain experts, we select some key attributes thatthey concern about, including the SKUs, quantity, fragility, and retailerof the goods (Fig. 5 (f1-f4)). For example, if an order contains fragilegoods, the operators will do more packing operations than those for theorders with other goods. Moreover, within each order, a bar chart showsthe time distribution of the processed procedures (Fig. 5 (f5)). Fromthe details presented, managers can further judge the delaying issues onthe selected orders. Moreover, this panel also supports users to untagsome wrongly detected orders on demand (E3) through a button (Fig. 5(e1)). The untagged orders will be removed from the ranking list.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "bar"]
      },
      {
        "solution_text": "Moreover, this panel also supports users to untagsome wrongly detected orders on demand (E3) through a button (Fig. 5(e1)). The untagged orders will be removed from the ranking list.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 575,
    "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse",
    "pub_year": 2022,
    "domain": "E-Commerce",
    "requirement": {
      "requirement_text": "E2 Show order details. To facilitate the further exploration of se-lected orders, users need to know the details of the orders, suchas the goods SKUs, goods attributes, and order ids. So the systemshould present the detail information on users\u2019 demand.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.",
      "data_code": { "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "To help users decide which delayed orders should be handled first, we provide an evaluating view. In this view, users can set a series of weights on the factors that determine the dealing priority of delayed orders and access details about each order, including operator information and goods details. Through the above information, users can identify whether orders should be handled.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "area+bar+table",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "bar", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 576,
    "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse",
    "pub_year": 2022,
    "domain": "E-Commerce",
    "requirement": {
      "requirement_text": "E3 Enable interactive identi\ufb01cation of delayed orders. The initialdelay detection results according to the time interval thresholdsare rough, within which, experts may \ufb01nd several incorrectly de-tected delayed orders or potential delayed orders based on visualexploration and their domain knowledge, Under this condition,the system should support smooth user interactions to identifyand tag delayed orders.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.",
      "data_code": { "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "The evaluating view also contains detail panels that show detailedinformation about selected orders in the ranking component (E2). Bydiscussing with the domain experts, we select some key attributes thatthey concern about, including the SKUs, quantity, fragility, and retailerof the goods (Fig. 5 (f1-f4)). For example, if an order contains fragilegoods, the operators will do more packing operations than those for theorders with other goods. Moreover, within each order, a bar chart showsthe time distribution of the processed procedures (Fig. 5 (f5)). Fromthe details presented, managers can further judge the delaying issues onthe selected orders. Moreover, this panel also supports users to untagsome wrongly detected orders on demand (E3) through a button (Fig. 5(e1)). The untagged orders will be removed from the ranking list.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "bar"]
      },
      {
        "solution_text": "Moreover, this panel also supports users to untagsome wrongly detected orders on demand (E3) through a button (Fig. 5(e1)). The untagged orders will be removed from the ranking list.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 577,
    "paper_title": "A Visualization Approach for Monitoring Order Processing in E-Commerce Warehouse",
    "pub_year": 2022,
    "domain": "E-Commerce",
    "requirement": {
      "requirement_text": "E3 Enable interactive identi\ufb01cation of delayed orders. The initialdelay detection results according to the time interval thresholdsare rough, within which, experts may \ufb01nd several incorrectly de-tected delayed orders or potential delayed orders based on visualexploration and their domain knowledge, Under this condition,the system should support smooth user interactions to identifyand tag delayed orders.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In our iterative design and develop_x0002_ment progress, we use one real world datasets from one e-commerce warehouse. The dataset include more than 500,000 order processing events, and above 94,000 orders in the warehouse in one day.",
      "data_code": { "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Temporal processing events. The whole order processing pipeline in the e-commerce warehouse is a kind of an assembly line [68], which can be regarded as a DAG (directed acyclic graph). Each order needs to pass through a series of procedures in the DAG to finish the pro_x0002_cessing. The start or end of a procedure is regarded as one event in order processing. Each event records the operator ID, through which we can access the operator information. The event sequence depicts the processing progress of the order. The start and end timestamp of each procedure Pi is recorded through the radio frequency identification (RFID) technique when operators use their portable RFID devices or the RFID scanners in work stations [34]. These data are synchronized to the cloud database. The temporal order processing event sequence consists of timestamps. Through these data and the current timestamp, we can calculate the processing time, waiting time, processed time, and blocked timen of each procedure. The processing time and waiting time are used for real-time monitor_x0002_ing, while the processed time and blocked time are used for historical record analysis. Specifically, the order that is being processed has the processing time, which is the time since the order starts being processed. The order that has been processed but not transferred to the following procedure has the waiting time. The waiting time refers to the time an order waits for being processed in the follow_x0002_ing procedure. Processed time is the total time one order was processed. Blocked time is the total time one processed order spent on waiting for being transferred to the following procedure. Warehouse managers can compare these time data with a series of time interval thresholds for initial delay detection, thereby warning of the possible delay issues. The threshold series in each procedure is defined in multiple templates for different situations due to the unpredictable order volumes and tight delivery schedules. Details of processed objects.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "To help users decide which delayed orders should be handled first, we provide an evaluating view. In this view, users can set a series of weights on the factors that determine the dealing priority of delayed orders and access details about each order, including operator information and goods details. Through the above information, users can identify whether orders should be handled.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "area+bar+table",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "bar", "area"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 578,
    "paper_title": "TIVEE: Visual Exploration and Explanation of Badminton Tactics in Immersive Visualizations",
    "pub_year": 2022,
    "domain": "sports",
    "requirement": {
      "requirement_text": "R1: Summarizing the usage of tactics. Badminton tactics are di-verse, and a player will use various tactics in a match. Experts areinterested in analyzing a player\u2019s commonly used tactics, which helps to outline his/her playing style. In addition, knowing thescoring rates of different tactics helps to deepen the understand-ing of the player\u2019s strength and provides clear guidance for thefollowing analysis.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The data of each badminton game is provided as hundreds of strokes. Each stroke has a variety of attributes. The detailed explanation of each stroke attribute is as follows: Splayer: The player giving the stroke. Stechnique: The technique used by the player to give the stroke. According to kinematic features, stroke techniques can be di_x0002_vided into three categories [60]: 1) offensive technique (including smash, net shot, and cut smash), 2) control technique (including clear, drop, chop, push, hook shot, and drive), and 3) defensive technique (including lob and block). Tplayer: The 2D movement trajectory of the player. For each stroke, we record two 2D positions of the player \u2014 the one when the opponent hit the shuttle at the previous stroke (Pstart) and the one when he/she hit the shuttle at the current stroke (Pend) \u2014 to reveal his/her movement to return the shuttle. Tshuttle: The 3D flying trajectory of the shuttle. For each stroke, we record three key points of the shuttle\u2019s flying trajectory \u2014 the start position (Pstart), the highest position (Phighest), and the end position (Pend) \u2014 to reconstruct the whole trajectory based on the shuttle\u2019s kinematic features [41]. Tfield: The fields where the shuttle comes from and falls into. Instead of analyzing the exact 3D positions, experts tend to divide the 3D court space into multiple fields and analyze the fields of the start/end position of the shuttle [37]. Following experts\u2019 requirements, we divide a half-court into 3\u00d72\u00d73 fields (Fig. 3). According to the distance to the net, the court can be divided into fore-court, middle-court, and back-court. The player can hit the shuttle in their forehand area or backhand area. The height can be divided into three levels: low-space (0-1.55m), middle-space (1.55-2.5m), and high-space (2.5-4m). According to the category of the technique used in the last stroke, we further consider three types of tactics: the offensive tactics, the control tactics, and the defensive tactics [58].",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Hence, the number of tactics could be large and it is hard to present all the tactics in the VR environment. To address this issue, we aggregate similar tactics into groups to reduce the number of visual items. Two tactics will be aggregated into one group if they have the same sequence of Stechnique.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "The tactic overview is provided to show the corresponding statistical data of badminton tactics and the relation between consecutive stroke sequences in tactics (R1, R2). Users are able to explore the scene in a third-person perspective with flying navigation mode. Considering the effectiveness of small multiples in VR environments [30], We design a multi-court layout (Fig. 5) to visually summarize the tactics and prevent the issue of visual overwhelming when the tactic number grows. Each column in the layout (Fig. 5) represents a tactic group that consists of multiple similar tactics. Each row (Fig. 5) in a group shows the strokes of tactics in this group using a virtual court. We rank the tactic group from left to right (Fig. 5) according to the tactic score (i.e., a summarization of the usage rate and the scoring rate of each tactic group). This can help users focus on the analysis of important tactics. Visualization of a tactic group. According to Figure 2, a tactic is characterized by a sequence of consecutive strokes. Hence, the number of tactics could be large and it is hard to present all the tactics in the VR environment. To address this issue, we aggregate similar tactics into groups to reduce the number of visual items. Two tactics will be aggregated into one group if they have the same sequence of Stechnique. In each tactic group, we use 2n\u22121 courts to visualize the strokes of tactics as well as the reaction strokes of opponents where n is the tactic length (n = 2 in Fig. 5). The first stroke of tactics is placed at the top court and the last stroke is placed at the bottom court (Fig. 5). Each court serves as a small multiple in three dimensions that displays the shuttle trajectories with the same stroke ordinal in one tactic. The thickness of strokes encodes the usage rate of the corresponding tactic (Rusage) and the color encodes the scoring rate (Rscoring) of the corresponding tactic. Rusage is computed as the ratio of the number of rallies with this tactic to the number of all rallies. Rscoring is computed as the ratio of the number of winning rallies with this tactic to the number of all rallies with this tactic. We use a discrete palette to encode the scoring rate of the strokes (Fig. 1a). Similar to Rusage and Rscoring, we further compute the usage rate and the scoring rate of each tactic group and use waffle charts at the top-back of each tactic group (Fig. 1a) to encode the two indicators. The left waffle chart shows the usage rate while the right one shows the scoring rate.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph"]
      },
      {
        "solution_text": "Interactions in the tactic overview are as follows. \u2022 Changing the viewing angle of small multiples. Experts can use the trackpad of the controller to get close to the tactic they are interested in or fly far away to observe as many tactics as possible at the same time. \u2022 Adjusting the order of small multiples. Experts can click the virtual function menu to see the slider of the weight of usage rates and scoring rates. They can further use raycasting to change the weights to recompute the tactic score and the order of small multiples (tactic groups) will be changed accordingly. \u2022 Decomposing a tactic group. Experts can use the trackpad to select a tactic group and decompose the tactic group into multiple subgroups according to the hit point and the drop point. For example, when using the hit point to decompose the tactic group, the tactic overview will be updated to show the subgroups and strokes in a subgroup will share the same field of the hit point.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore,Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration", "OverviewandExplore"],
        "componenet_code": [
          "participation/collaboration",
          "overview_and_explore"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 579,
    "paper_title": "TIVEE: Visual Exploration and Explanation of Badminton Tactics in Immersive Visualizations",
    "pub_year": 2022,
    "domain": "sports",
    "requirement": {
      "requirement_text": "R2: Presenting the similarity between tactics. Although tactics aredifferent in terms of the kinematic features, such as the 3D po-sition of the hit point and the technique used, a set of tacticswould be considered as similar to each other due to the similartactical aim (e.g., playing defensive tactics to wait for mistakes).Showing a category of tactics can help the experts learn differentoffensive/defensive tactics and conduct a detailed analysis.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The data of each badminton game is provided as hundreds of strokes. Each stroke has a variety of attributes. The detailed explanation of each stroke attribute is as follows: Splayer: The player giving the stroke. Stechnique: The technique used by the player to give the stroke. According to kinematic features, stroke techniques can be di_x0002_vided into three categories [60]: 1) offensive technique (including smash, net shot, and cut smash), 2) control technique (including clear, drop, chop, push, hook shot, and drive), and 3) defensive technique (including lob and block). Tplayer: The 2D movement trajectory of the player. For each stroke, we record two 2D positions of the player \u2014 the one when the opponent hit the shuttle at the previous stroke (Pstart) and the one when he/she hit the shuttle at the current stroke (Pend) \u2014 to reveal his/her movement to return the shuttle. Tshuttle: The 3D flying trajectory of the shuttle. For each stroke, we record three key points of the shuttle\u2019s flying trajectory \u2014 the start position (Pstart), the highest position (Phighest), and the end position (Pend) \u2014 to reconstruct the whole trajectory based on the shuttle\u2019s kinematic features [41]. Tfield: The fields where the shuttle comes from and falls into. Instead of analyzing the exact 3D positions, experts tend to divide the 3D court space into multiple fields and analyze the fields of the start/end position of the shuttle [37]. Following experts\u2019 requirements, we divide a half-court into 3\u00d72\u00d73 fields (Fig. 3). According to the distance to the net, the court can be divided into fore-court, middle-court, and back-court. The player can hit the shuttle in their forehand area or backhand area. The height can be divided into three levels: low-space (0-1.55m), middle-space (1.55-2.5m), and high-space (2.5-4m). According to the category of the technique used in the last stroke, we further consider three types of tactics: the offensive tactics, the control tactics, and the defensive tactics [58].",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Hence, the number of tactics could be large and it is hard to present all the tactics in the VR environment. To address this issue, we aggregate similar tactics into groups to reduce the number of visual items. Two tactics will be aggregated into one group if they have the same sequence of Stechnique.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "The tactic overview is provided to show the corresponding statistical data of badminton tactics and the relation between consecutive stroke sequences in tactics (R1, R2). Users are able to explore the scene in a third-person perspective with flying navigation mode. Considering the effectiveness of small multiples in VR environments [30], We design a multi-court layout (Fig. 5) to visually summarize the tactics and prevent the issue of visual overwhelming when the tactic number grows. Each column in the layout (Fig. 5) represents a tactic group that consists of multiple similar tactics. Each row (Fig. 5) in a group shows the strokes of tactics in this group using a virtual court. We rank the tactic group from left to right (Fig. 5) according to the tactic score (i.e., a summarization of the usage rate and the scoring rate of each tactic group). This can help users focus on the analysis of important tactics. Visualization of a tactic group. According to Figure 2, a tactic is characterized by a sequence of consecutive strokes. Hence, the number of tactics could be large and it is hard to present all the tactics in the VR environment. To address this issue, we aggregate similar tactics into groups to reduce the number of visual items. Two tactics will be aggregated into one group if they have the same sequence of Stechnique. In each tactic group, we use 2n\u22121 courts to visualize the strokes of tactics as well as the reaction strokes of opponents where n is the tactic length (n = 2 in Fig. 5). The first stroke of tactics is placed at the top court and the last stroke is placed at the bottom court (Fig. 5). Each court serves as a small multiple in three dimensions that displays the shuttle trajectories with the same stroke ordinal in one tactic. The thickness of strokes encodes the usage rate of the corresponding tactic (Rusage) and the color encodes the scoring rate (Rscoring) of the corresponding tactic. Rusage is computed as the ratio of the number of rallies with this tactic to the number of all rallies. Rscoring is computed as the ratio of the number of winning rallies with this tactic to the number of all rallies with this tactic. We use a discrete palette to encode the scoring rate of the strokes (Fig. 1a). Similar to Rusage and Rscoring, we further compute the usage rate and the scoring rate of each tactic group and use waffle charts at the top-back of each tactic group (Fig. 1a) to encode the two indicators. The left waffle chart shows the usage rate while the right one shows the scoring rate.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "glyph",
        "axial_code": ["Repetition"],
        "componenet_code": ["glyph"]
      },
      {
        "solution_text": "Interactions in the tactic overview are as follows. \u2022 Changing the viewing angle of small multiples. Experts can use the trackpad of the controller to get close to the tactic they are interested in or fly far away to observe as many tactics as possible at the same time. \u2022 Adjusting the order of small multiples. Experts can click the virtual function menu to see the slider of the weight of usage rates and scoring rates. They can further use raycasting to change the weights to recompute the tactic score and the order of small multiples (tactic groups) will be changed accordingly. \u2022 Decomposing a tactic group. Experts can use the trackpad to select a tactic group and decompose the tactic group into multiple subgroups according to the hit point and the drop point. For example, when using the hit point to decompose the tactic group, the tactic overview will be updated to show the subgroups and strokes in a subgroup will share the same field of the hit point.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore,Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration", "OverviewandExplore"],
        "componenet_code": [
          "participation/collaboration",
          "overview_and_explore"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 580,
    "paper_title": "TIVEE: Visual Exploration and Explanation of Badminton Tactics in Immersive Visualizations",
    "pub_year": 2022,
    "domain": "sports",
    "requirement": {
      "requirement_text": "R3: Identifying the relation between tactics and game situations.After obtaining an overall picture of a player\u2019s tactics, the expertsneed to know how the player uses different tactics to cope withdifferent game situations. Correctly applying a tactic in an ap-propriate situation is the key to increase the winning rate. Hence,knowing the usage of tactics under different situations can helpplayers identify valuable usages and existing weaknesses.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "The data of each badminton game is provided as hundreds of strokes. Each stroke has a variety of attributes. The detailed explanation of each stroke attribute is as follows: Splayer: The player giving the stroke. Stechnique: The technique used by the player to give the stroke. According to kinematic features, stroke techniques can be di_x0002_vided into three categories [60]: 1) offensive technique (including smash, net shot, and cut smash), 2) control technique (including clear, drop, chop, push, hook shot, and drive), and 3) defensive technique (including lob and block). Tplayer: The 2D movement trajectory of the player. For each stroke, we record two 2D positions of the player \u2014 the one when the opponent hit the shuttle at the previous stroke (Pstart) and the one when he/she hit the shuttle at the current stroke (Pend) \u2014 to reveal his/her movement to return the shuttle. Tshuttle: The 3D flying trajectory of the shuttle. For each stroke, we record three key points of the shuttle\u2019s flying trajectory \u2014 the start position (Pstart), the highest position (Phighest), and the end position (Pend) \u2014 to reconstruct the whole trajectory based on the shuttle\u2019s kinematic features [41]. Tfield: The fields where the shuttle comes from and falls into. Instead of analyzing the exact 3D positions, experts tend to divide the 3D court space into multiple fields and analyze the fields of the start/end position of the shuttle [37]. Following experts\u2019 requirements, we divide a half-court into 3\u00d72\u00d73 fields (Fig. 3). According to the distance to the net, the court can be divided into fore-court, middle-court, and back-court. The player can hit the shuttle in their forehand area or backhand area. The height can be divided into three levels: low-space (0-1.55m), middle-space (1.55-2.5m), and high-space (2.5-4m). According to the category of the technique used in the last stroke, we further consider three types of tactics: the offensive tactics, the control tactics, and the defensive tactics [58].",
      "data_code": { "tables": 1, "clusters_and_sets_and_lists": 1 }
    },
    "solution": [
      {
        "solution_text": "In addition to having an overview of general tactics, the expert wants to identify tactics that the player commonly uses in specific game situations to better tailor strategies. Users can use the virtual menu to jump to the tactic customization and set the context game scenario.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 581,
    "paper_title": "TIVEE: Visual Exploration and Explanation of Badminton Tactics in Immersive Visualizations",
    "pub_year": 2022,
    "domain": "sports",
    "requirement": {
      "requirement_text": "R4: Revealing the characteristics of a tactic. When focusing on an-alyzing a specific tactic, experts will develop an interest in howthe player performs such a tactic with strokes. Different executionstyles can reflect the athletic or physiological characteristics ofthe player; for example, different players may have various dis-tributions of height at the hit point when performing the tactic oflob-smash due to height issues. Revealing these details can helpexperts understand the efficient way of performing a tactic andestablish corresponding coping tactics.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The data of each badminton game is provided as hundreds of strokes. Each stroke has a variety of attributes. The detailed explanation of each stroke attribute is as follows: Splayer: The player giving the stroke. Stechnique: The technique used by the player to give the stroke. According to kinematic features, stroke techniques can be di_x0002_vided into three categories [60]: 1) offensive technique (including smash, net shot, and cut smash), 2) control technique (including clear, drop, chop, push, hook shot, and drive), and 3) defensive technique (including lob and block). Tplayer: The 2D movement trajectory of the player. For each stroke, we record two 2D positions of the player \u2014 the one when the opponent hit the shuttle at the previous stroke (Pstart) and the one when he/she hit the shuttle at the current stroke (Pend) \u2014 to reveal his/her movement to return the shuttle. Tshuttle: The 3D flying trajectory of the shuttle. For each stroke, we record three key points of the shuttle\u2019s flying trajectory \u2014 the start position (Pstart), the highest position (Phighest), and the end position (Pend) \u2014 to reconstruct the whole trajectory based on the shuttle\u2019s kinematic features [41]. Tfield: The fields where the shuttle comes from and falls into. Instead of analyzing the exact 3D positions, experts tend to divide the 3D court space into multiple fields and analyze the fields of the start/end position of the shuttle [37]. Following experts\u2019 requirements, we divide a half-court into 3\u00d72\u00d73 fields (Fig. 3). According to the distance to the net, the court can be divided into fore-court, middle-court, and back-court. The player can hit the shuttle in their forehand area or backhand area. The height can be divided into three levels: low-space (0-1.55m), middle-space (1.55-2.5m), and high-space (2.5-4m). According to the category of the technique used in the last stroke, we further consider three types of tactics: the offensive tactics, the control tactics, and the defensive tactics [58].",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "After rounds of decomposition of tactic groups in the tactic overview, users can select a tactic of interest and jump to the tactic explanation to perceive the tactic from a first-person perspective. With the first-person perspective, users can watch the animation of the selected tactic to replicate the real scenario. This can help users more clearly see the detailed kinematic characteristics of the tactic (which is important for justifying the performance) and more easily learn the tactical purpose. Specifically, a tactic can be summarized and demonstrated by many use cases that appeared in real games. Here we present the summary of a tactic and allow users to use several forms of visualizations to explain the tactic\u2019s practicality.",
        "solution_category": "interaction",
        "solution_axial": "Gamification",
        "solution_compoent": "",
        "axial_code": ["Gamification"],
        "componenet_code": ["gamification"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 582,
    "paper_title": "TIVEE: Visual Exploration and Explanation of Badminton Tactics in Immersive Visualizations",
    "pub_year": 2022,
    "domain": "sports",
    "requirement": {
      "requirement_text": "R5: Explaining the effect of a tactic. The effectiveness of a tactic isdetermined by multiple factors, including how the player deploysthis tactic and how the opponent responds to this tactic. The suc-cess of this tactic may be because the player found the opponent\u2019sgap or the player forced the opponent, whose weakness is a back-hand catch, to return the ball backhand. Therefore, the systemshould support the correlation analysis of different attributes andassist experts in \ufb01nding the key to success.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The data of each badminton game is provided as hundreds of strokes. Each stroke has a variety of attributes. The detailed explanation of each stroke attribute is as follows: Splayer: The player giving the stroke. Stechnique: The technique used by the player to give the stroke. According to kinematic features, stroke techniques can be di_x0002_vided into three categories [60]: 1) offensive technique (including smash, net shot, and cut smash), 2) control technique (including clear, drop, chop, push, hook shot, and drive), and 3) defensive technique (including lob and block). Tplayer: The 2D movement trajectory of the player. For each stroke, we record two 2D positions of the player \u2014 the one when the opponent hit the shuttle at the previous stroke (Pstart) and the one when he/she hit the shuttle at the current stroke (Pend) \u2014 to reveal his/her movement to return the shuttle. Tshuttle: The 3D flying trajectory of the shuttle. For each stroke, we record three key points of the shuttle\u2019s flying trajectory \u2014 the start position (Pstart), the highest position (Phighest), and the end position (Pend) \u2014 to reconstruct the whole trajectory based on the shuttle\u2019s kinematic features [41]. Tfield: The fields where the shuttle comes from and falls into. Instead of analyzing the exact 3D positions, experts tend to divide the 3D court space into multiple fields and analyze the fields of the start/end position of the shuttle [37]. Following experts\u2019 requirements, we divide a half-court into 3\u00d72\u00d73 fields (Fig. 3). According to the distance to the net, the court can be divided into fore-court, middle-court, and back-court. The player can hit the shuttle in their forehand area or backhand area. The height can be divided into three levels: low-space (0-1.55m), middle-space (1.55-2.5m), and high-space (2.5-4m). According to the category of the technique used in the last stroke, we further consider three types of tactics: the offensive tactics, the control tactics, and the defensive tactics [58].",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "After rounds of decomposition of tactic groups in the tactic overview, users can select a tactic of interest and jump to the tactic explanation to perceive the tactic from a first-person perspective. With the first-person perspective, users can watch the animation of the selected tactic to replicate the real scenario. This can help users more clearly see the detailed kinematic characteristics of the tactic (which is important for justifying the performance) and more easily learn the tactical purpose. Specifically, a tactic can be summarized and demonstrated by many use cases that appeared in real games. Here we present the summary of a tactic and allow users to use several forms of visualizations to explain the tactic\u2019s practicality.",
        "solution_category": "interaction",
        "solution_axial": "Gamification",
        "solution_compoent": "",
        "axial_code": ["Gamification"],
        "componenet_code": ["gamification"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 583,
    "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
    "pub_year": 2022,
    "domain": "Temporal event sequence",
    "requirement": {
      "requirement_text": "T1. Explore common and deviating pathways: help users to ex-plore and discover which clusterings summarize better the mostcommon (and deviating) pathways in the data. Clusters will groupsequences that share a set of event types, regardless of their order.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "The final height of a cluster is proportional to the number of records it contains, however, sometimes clusters might contain too few records in proportion to the whole dataset ending up not visible. In such cases the height is scaled up by a constant number of pixels and the cluster is surrounded by a dotted line, allowing users to identify deviating pathways.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "matrix+bar",
        "axial_code": ["Nesting"],
        "componenet_code": ["bar", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 584,
    "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
    "pub_year": 2022,
    "domain": "Temporal event sequence",
    "requirement": {
      "requirement_text": "T1. Explore common and deviating pathways: help users to ex-plore and discover which clusterings summarize better the mostcommon (and deviating) pathways in the data. Clusters will groupsequences that share a set of event types, regardless of their order.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Figure shows the two sliders used to transform the level-of-detail of the overview: the cluster slider and the information score slider. The cluster slider transforms the vertical level-of-detail by changing the number of clusters k in the range 1 \u2264 k \u2264 N, being N the number of input unique sequences. A combobox next to the cluster slider shows the current number of clusters and contains the list of alternative optimal number of clusters, to guide users in finding a set of pathways that best summarize the data. Alternatively, users can break down a selected cluster into its two child sub-clusters, and so on, until a cluster with a single sequence is reached. The information score slider transforms the horizontal level-of-detail by changing the information score threshold I\u03c4 in the range.",
        "solution_category": "interaction",
        "solution_axial": "Abstract/Elaborate,Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Abstract/Elaborate", "Participation/Collaboration"],
        "componenet_code": ["abstract/elaborate", "participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 585,
    "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
    "pub_year": 2022,
    "domain": "Temporal event sequence",
    "requirement": {
      "requirement_text": "T2. Interpret the sequences that constitute a cluster: the visual-ization should allow users to compare the most common eventorderings (and permutations) within and across clusters usingsequence alignment.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Figure shows how each of the elements in the representation matrix contains either one or multiple event types, where an element with multiple event types corresponds to the row wise merged sub-sequences in the Simplify step. Sub-sequences contained in a single element are represented using a box divided by colored bars, where each bar is colored by event type and ordered as per the sub-sequence. This visual encoding allows to derive the original sequences forming a cluster. To reduce visual clutter, when the number of events in the merged sub-sequence increases, bars can be ordered by event type to show proportion, or colored in gray to show the number of merged records.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "matrix+bar",
        "axial_code": ["Nesting"],
        "componenet_code": ["bar", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 586,
    "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
    "pub_year": 2022,
    "domain": "Temporal event sequence",
    "requirement": {
      "requirement_text": "T2. Interpret the sequences that constitute a cluster: the visual-ization should allow users to compare the most common eventorderings (and permutations) within and across clusters usingsequence alignment.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "This view shows the individual sequences in the selected clusters, grouped by unique sequence. The sequences are visually encoded as an ordered sequence of boxes arranged horizontally and colored by event type, along with their identifier and frequency. Sequences are shown without any simplification allowing the inspection of the full sequences in the selected clusters.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table",
        "axial_code": ["Repetition"],
        "componenet_code": ["table"]
      },
      {
        "solution_text": "These sequences can be sorted by frequency or similarity, or aligned by a selected event.",
        "solution_category": "interaction",
        "solution_axial": "Reconfigure",
        "solution_compoent": "",
        "axial_code": ["Reconfigure"],
        "componenet_code": ["reconfigure"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 587,
    "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
    "pub_year": 2022,
    "domain": "Temporal event sequence",
    "requirement": {
      "requirement_text": "T3. Focus the analysis on a selected set of records: allow queriesin the dataset to focus on sequences with specific characteristics.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "The distribution of a data attribute can be analysed for a selected set of records, or compared amongst clusters and unique sequences. This view shows one stacked bar chart per attribute in the dataset, where a chart contains one vertical bar per value, each bar is divided in sub-bars representing series, and series are identified by a unique color. Series can be interactively hidden to focus on only one or compare a reduced number of series. Three types of charts are provided: 1) Selected data: compares the selected data against the rest of the records in the dataset. For a given attribute, this type of bar chart shows one series colored in red for the records contained in the selected clusters or unique sequences, and another series (in grey) for the rest of data. 2) Sequence: it plots one series for each unique sequence shown in the unique sequence view. 3) Cluster: it compares all clusters in the overview, and assigns one series per cluster.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 588,
    "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
    "pub_year": 2022,
    "domain": "Temporal event sequence",
    "requirement": {
      "requirement_text": "T3. Focus the analysis on a selected set of records: allow queriesin the dataset to focus on sequences with specific characteristics.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Records can be removed from the overview by applying filters basedon data attributes, frequency, date range, event occurrence; includingfilters by day of the week, month, or year (T3). A filter is specifiedby an attribute, operator, and value. For example, the filter event = Atranslates to \u201cshow only sequences that contain event A at least once\u201d.Users can select sections of a cluster, such as events and sub-sequences,or sequences in the unique sequence view by drawing a square with themouse. These selections are added to the unique sequence view andindividual sequence view, and are plotted in the attribute analysis view(T4).",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 589,
    "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
    "pub_year": 2022,
    "domain": "Temporal event sequence",
    "requirement": {
      "requirement_text": "T4. Obtain details on demand: provide coordinated views so thatusers can request \ufb01ner details of interesting items in the overview.Users should be able to go from the highest level of aggregation(i.e. clusters), passing through sequences grouped by their uniquesequence, to individual sequences and their raw data includingevent timestamps and duration.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "This view shows the individual sequences in the selected clusters, grouped by unique sequence. The sequences are visually encoded as an ordered sequence of boxes arranged horizontally and colored by event type, along with their identifier and frequency. Sequences are shown without any simplification allowing the inspection of the full sequences in the selected clusters.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table",
        "axial_code": ["Repetition"],
        "componenet_code": ["table"]
      },
      {
        "solution_text": "These sequences can be sorted by frequency or similarity, or aligned by a selected event.",
        "solution_category": "interaction",
        "solution_axial": "Reconfigure",
        "solution_compoent": "",
        "axial_code": ["Reconfigure"],
        "componenet_code": ["reconfigure"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 590,
    "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
    "pub_year": 2022,
    "domain": "Temporal event sequence",
    "requirement": {
      "requirement_text": "T4. Obtain details on demand: provide coordinated views so thatusers can request \ufb01ner details of interesting items in the overview.Users should be able to go from the highest level of aggregation(i.e. clusters), passing through sequences grouped by their uniquesequence, to individual sequences and their raw data includingevent timestamps and duration.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "This view shows the individual sequences of the selections in the unique sequence view and the overview, along with their temporal information and raw data attributes. Following a Gantt chart approach, each individual sequence is visualized as a horizontal sequence of events, positioned along the horizontal axis according to their timestamp. A table of attributes is displayed next to the Gantt chart, where each column represents a data attribute at either individual sequence level or individual event level.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "text+gantt",
        "axial_code": ["Stack"],
        "componenet_code": ["gantt", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 591,
    "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
    "pub_year": 2022,
    "domain": "Temporal event sequence",
    "requirement": {
      "requirement_text": "T4. Obtain details on demand: provide coordinated views so thatusers can request \ufb01ner details of interesting items in the overview.Users should be able to go from the highest level of aggregation(i.e. clusters), passing through sequences grouped by their uniquesequence, to individual sequences and their raw data includingevent timestamps and duration.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "The distribution of a data attribute can be analysed for a selected set of records, or compared amongst clusters and unique sequences. This view shows one stacked bar chart per attribute in the dataset, where a chart contains one vertical bar per value, each bar is divided in sub-bars representing series, and series are identified by a unique color. Series can be interactively hidden to focus on only one or compare a reduced number of series. Three types of charts are provided: 1) Selected data: compares the selected data against the rest of the records in the dataset. For a given attribute, this type of bar chart shows one series colored in red for the records contained in the selected clusters or unique sequences, and another series (in grey) for the rest of data. 2) Sequence: it plots one series for each unique sequence shown in the unique sequence view. 3) Cluster: it compares all clusters in the overview, and assigns one series per cluster.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 592,
    "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
    "pub_year": 2022,
    "domain": "Temporal event sequence",
    "requirement": {
      "requirement_text": "T5. Aggregate and compare context information for selectedgroups of records: the system should allow to aggregate andcompare data attributes (e.g. age, gender, country) for selectedclusters, unique sequences, or individual sequences.",
      "requirement_code": {
        "compare_entities": 1,
        "describe_observation_aggregate": 1
      }
    },
    "data": {
      "data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "This view shows the individual sequences of the selections in the unique sequence view and the overview, along with their temporal information and raw data attributes. Following a Gantt chart approach, each individual sequence is visualized as a horizontal sequence of events, positioned along the horizontal axis according to their timestamp. A table of attributes is displayed next to the Gantt chart, where each column represents a data attribute at either individual sequence level or individual event level.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "text+gantt",
        "axial_code": ["Stack"],
        "componenet_code": ["gantt", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 593,
    "paper_title": "Sequen-C: A Multilevel Overview of Temporal Event Sequences",
    "pub_year": 2022,
    "domain": "Temporal event sequence",
    "requirement": {
      "requirement_text": "T5. Aggregate and compare context information for selectedgroups of records: the system should allow to aggregate andcompare data attributes (e.g. age, gender, country) for selectedclusters, unique sequences, or individual sequences.",
      "requirement_code": {
        "compare_entities": 1,
        "describe_observation_aggregate": 1
      }
    },
    "data": {
      "data_text": "The CUREd research database [21] contains timestamped events and demographic data related to telephone calls made to the emergency service (calls to 999 or 111), throughout Yorkshire and the Humber region. Calls can lead to different pathways, including ambulance conveyance to the Emergency department (ED) and admissions to inpatient facilities. A three month subset of the dataset was used, containing 25,243 calls relating to 21,805 unique patients, and 57 data attributes. The data were processed so that an individual sequence represents all the events of multiple calls and incidents for the same patient. We conducted an analysis session along with members of the Centre for Urgent and Emergency Care Research (CURE); The MIMIC-III database [19] contains data for 58,976 patient admis_x0002_sions to acute and critical care units at a tertiary hospital, organised in 26 tables containing demographic data and timestamped clinical events from admission to discharge. In this case study, an individual sequence represents all the events for a single admission, obtained from the admissions, transfers, and prescriptions tables. This case study was de_x0002_veloped in collaboration with a consultant cardiologist (i.e. the analyst). A query was added to show patients with a primary or secondary diag_x0002_nosis of Atrial Fibrillation (AF) (code 42731 in the DIAGNOSES ICD table). The subset data contained 1,425 patient admissions, and 448 event types, from which 438 are types of prescriptions.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We propose a technique to build and explore a multilevel overview of event sequences through hierarchical aggregation. A multilevel overview is a visual summary which can be interactively transformed from coarse to fine level-of-detail [9]. Our overview displays a number of sequence clusters retrieved from an aggregate tree; where each sequence cluster is represented with the steps Align-Score-Simplify. The overview can be interactively transformed vertically and hori_x0002_zontally. The vertical level-of-detail is controlled with the number of clusters retrieved from the tree. Fig. 2-C shows how the higher in the hierarchy (i.e. smaller number of clusters), the coarser the overview; whereas the lower in the hierarchy (i.e. larger number of clusters), the finer the details. The horizontal level-of-detail refers to the level of sim_x0002_plification of each cluster representation according to its information score (Fig. 3). The initial overview shows the best number of clusters according to the Average Silhouette Width metric [35], but we also offer a set of alternative values that might provide valuable overviews.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "The distribution of a data attribute can be analysed for a selected set of records, or compared amongst clusters and unique sequences. This view shows one stacked bar chart per attribute in the dataset, where a chart contains one vertical bar per value, each bar is divided in sub-bars representing series, and series are identified by a unique color. Series can be interactively hidden to focus on only one or compare a reduced number of series. Three types of charts are provided: 1) Selected data: compares the selected data against the rest of the records in the dataset. For a given attribute, this type of bar chart shows one series colored in red for the records contained in the selected clusters or unique sequences, and another series (in grey) for the rest of data. 2) Sequence: it plots one series for each unique sequence shown in the unique sequence view. 3) Cluster: it compares all clusters in the overview, and assigns one series per cluster.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 594,
    "paper_title": "Visual Evaluation for Autonomous Driving",
    "pub_year": 2022,
    "domain": "Autonomous Driving",
    "requirement": {
      "requirement_text": "R1: Module developers should be able to use the system to assess the overall performance of an autonomous driving system with a score as general feedback.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "Autonomous driving data can be obtained from the real car tests or simulation tests. Due to the limitations of the real testing environment, we use the autonomous driving data records obtained from the LGSVL simulator in this work. Based on the official classification [3] and the inputs from our collaborators, we divide our data into five mod_x0002_ules: perception, planning, prediction, control, and comfort. In each module, there are several measurement factors, i.e., the attributes captured by the sensors, that are related to its performance. The mea_x0002_surement factors of the perception module are from the perception component, and include signal detection accuracy, obstacle detection accuracy, as well as the accuracy of distance between vehicle and obstacles. With the perceived information, the autonomous driving sys_x0002_tem needs to predict the behaviors of obstacles and traffic participants. The prediction module from the decision making component can be measured with the accuracy of the predicted trajectory of obstacles. For the planning module with the understanding of the environment and obstacles, which is also from the decision making component, au_x0002_tonomous driving model developers are interested in such measures as the differences between predicted vehicle speed and the actual speed at a given time, the differences between predicted position and actual po_x0002_sition, etc. The control module from the control component includes those factors related to the control of throttle, brake, steering wheel, etc. The comfort module, which concerns the feelings of the driver, uses measures like the acceleration rate of the vehicle, turning angle, etc. Table 1 summarizes these modules, involved factors in each module, their descriptions, and their evaluation criteria.",
      "data_code": {
        "temporal": 1,
        "tables": 1,
        "quantitative": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "In our case, we divided the autonomous driving process into five modules and selected relevant evaluation factors in each module. We combine AHP and TOPSIS in calculating the overall evaluation score for autonomous driving, because there are no officially defined eval_x0002_uation criteria for autonomous driving. Therefore, with the help of TOPSIS, for a driving record over a period of time and a given eval_x0002_uation factor, a time series of evaluation results for the factor can be obtained. In this way, we can obtain the time series of evaluation re_x0002_sults for all factors involved in the evaluation of autonomous driving over a period of time. Because there are no standard rules for ranking the importance of the factors of autonomous driving, AHP is used to determine the weight of each evaluation factor. Initially the weights are determined by module developers to equalize every evaluation module weight. With our visual evaluation system, they can determine the importance of each factor for different needs by themselves and the system can calculate the parameters that meet their needs with the help of AHP; With the input of the autonomous driving data record, we first calculatethe scores for each factor based on our model described in Sect. 4.1.The total score, module score and factor score range from 0 to 1. Thescore of each module is averaged by the scores of all the factors fromthis module at each time period. The score of each time period isaveraged by the scores of all the modules at this time period. Thetotal score is calculated by the average of all the scores in every timestep.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Module developers first read the total score in Fig. 1-b (R1) andexamine the timeline visualization (R2, Fig. 1-c) to check the trendof the scores along the time. They can identify the outlier momentswith the low scores and check details in the radar view (Fig. 1-b). Wechoose the radar because we have five modules which is a suitablenumber of axes for choosing the radar. Users can easily identifythe good and bad performance scores in different modules.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "radar",
        "axial_code": ["Co-axis"],
        "componenet_code": ["radar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 595,
    "paper_title": "Visual Evaluation for Autonomous Driving",
    "pub_year": 2022,
    "domain": "Autonomous Driving",
    "requirement": {
      "requirement_text": "R2: The system should allow module developers to evaluate the performance of a system in different time periods of the whole driving process and to identify the time periods with bad performance.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Autonomous driving data can be obtained from the real car tests or simulation tests. Due to the limitations of the real testing environment, we use the autonomous driving data records obtained from the LGSVL simulator in this work. Based on the official classification [3] and the inputs from our collaborators, we divide our data into five mod_x0002_ules: perception, planning, prediction, control, and comfort. In each module, there are several measurement factors, i.e., the attributes captured by the sensors, that are related to its performance. The mea_x0002_surement factors of the perception module are from the perception component, and include signal detection accuracy, obstacle detection accuracy, as well as the accuracy of distance between vehicle and obstacles. With the perceived information, the autonomous driving sys_x0002_tem needs to predict the behaviors of obstacles and traffic participants. The prediction module from the decision making component can be measured with the accuracy of the predicted trajectory of obstacles. For the planning module with the understanding of the environment and obstacles, which is also from the decision making component, au_x0002_tonomous driving model developers are interested in such measures as the differences between predicted vehicle speed and the actual speed at a given time, the differences between predicted position and actual po_x0002_sition, etc. The control module from the control component includes those factors related to the control of throttle, brake, steering wheel, etc. The comfort module, which concerns the feelings of the driver, uses measures like the acceleration rate of the vehicle, turning angle, etc. Table 1 summarizes these modules, involved factors in each module, their descriptions, and their evaluation criteria.",
      "data_code": {
        "temporal": 1,
        "tables": 1,
        "quantitative": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "In our case, we divided the autonomous driving process into five modules and selected relevant evaluation factors in each module. We combine AHP and TOPSIS in calculating the overall evaluation score for autonomous driving, because there are no officially defined eval_x0002_uation criteria for autonomous driving. Therefore, with the help of TOPSIS, for a driving record over a period of time and a given eval_x0002_uation factor, a time series of evaluation results for the factor can be obtained. In this way, we can obtain the time series of evaluation re_x0002_sults for all factors involved in the evaluation of autonomous driving over a period of time. Because there are no standard rules for ranking the importance of the factors of autonomous driving, AHP is used to determine the weight of each evaluation factor. Initially the weights are determined by module developers to equalize every evaluation module weight. With our visual evaluation system, they can determine the importance of each factor for different needs by themselves and the system can calculate the parameters that meet their needs with the help of AHP; With the input of the autonomous driving data record, we first calculatethe scores for each factor based on our model described in Sect. 4.1.The total score, module score and factor score range from 0 to 1. Thescore of each module is averaged by the scores of all the factors fromthis module at each time period. The score of each time period isaveraged by the scores of all the modules at this time period. Thetotal score is calculated by the average of all the scores in every timestep.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Module developers first read the total score in Fig. 1-b (R1) andexamine the timeline visualization (R2, Fig. 1-c) to check the trendof the scores along the time. They can identify the outlier momentswith the low scores.",
        "solution_category": "visualization",
        "solution_axial": "Basic",
        "solution_compoent": "line",
        "axial_code": ["Basic"],
        "componenet_code": ["line"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 596,
    "paper_title": "Visual Evaluation for Autonomous Driving",
    "pub_year": 2022,
    "domain": "Autonomous Driving",
    "requirement": {
      "requirement_text": "R3: The system should provide methods for the evaluation of the five modules (perception, prediction, planning, control and comfort) so that module developers can tell the performances of individual modules.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "Autonomous driving data can be obtained from the real car tests or simulation tests. Due to the limitations of the real testing environment, we use the autonomous driving data records obtained from the LGSVL simulator in this work. Based on the official classification [3] and the inputs from our collaborators, we divide our data into five mod_x0002_ules: perception, planning, prediction, control, and comfort. In each module, there are several measurement factors, i.e., the attributes captured by the sensors, that are related to its performance. The mea_x0002_surement factors of the perception module are from the perception component, and include signal detection accuracy, obstacle detection accuracy, as well as the accuracy of distance between vehicle and obstacles. With the perceived information, the autonomous driving sys_x0002_tem needs to predict the behaviors of obstacles and traffic participants. The prediction module from the decision making component can be measured with the accuracy of the predicted trajectory of obstacles. For the planning module with the understanding of the environment and obstacles, which is also from the decision making component, au_x0002_tonomous driving model developers are interested in such measures as the differences between predicted vehicle speed and the actual speed at a given time, the differences between predicted position and actual po_x0002_sition, etc. The control module from the control component includes those factors related to the control of throttle, brake, steering wheel, etc. The comfort module, which concerns the feelings of the driver, uses measures like the acceleration rate of the vehicle, turning angle, etc. Table 1 summarizes these modules, involved factors in each module, their descriptions, and their evaluation criteria.",
      "data_code": {
        "temporal": 1,
        "tables": 1,
        "quantitative": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "In our case, we divided the autonomous driving process into five modules and selected relevant evaluation factors in each module. We combine AHP and TOPSIS in calculating the overall evaluation score for autonomous driving, because there are no officially defined eval_x0002_uation criteria for autonomous driving. Therefore, with the help of TOPSIS, for a driving record over a period of time and a given eval_x0002_uation factor, a time series of evaluation results for the factor can be obtained. In this way, we can obtain the time series of evaluation re_x0002_sults for all factors involved in the evaluation of autonomous driving over a period of time. Because there are no standard rules for ranking the importance of the factors of autonomous driving, AHP is used to determine the weight of each evaluation factor. Initially the weights are determined by module developers to equalize every evaluation module weight. With our visual evaluation system, they can determine the importance of each factor for different needs by themselves and the system can calculate the parameters that meet their needs with the help of AHP; With the input of the autonomous driving data record, we first calculatethe scores for each factor based on our model described in Sect. 4.1.The total score, module score and factor score range from 0 to 1. Thescore of each module is averaged by the scores of all the factors fromthis module at each time period. The score of each time period isaveraged by the scores of all the modules at this time period. Thetotal score is calculated by the average of all the scores in every timestep.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "From the radar view, the score distributions of five modules, including perception, planning, prediction, control, and comfort are visualized. We can compare and analyze each module to observe the performance of different autonomous driving evaluation modules. The radar view shows the overall performance of the five modules at the initial moment and the module scores at specific moments.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "radar",
        "axial_code": ["Co-axis"],
        "componenet_code": ["radar"]
      },
      {
        "solution_text": "It can be dynamically updated along with the timeline. We can find the time period with poor or unbalanced evaluation scores of each module.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 597,
    "paper_title": "Visual Evaluation for Autonomous Driving",
    "pub_year": 2022,
    "domain": "Autonomous Driving",
    "requirement": {
      "requirement_text": "R4: The system should allow module developers to observe the performances of individual module at any time period and to identify those factors that contribute to the observed performances.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Autonomous driving data can be obtained from the real car tests or simulation tests. Due to the limitations of the real testing environment, we use the autonomous driving data records obtained from the LGSVL simulator in this work. Based on the official classification [3] and the inputs from our collaborators, we divide our data into five mod_x0002_ules: perception, planning, prediction, control, and comfort. In each module, there are several measurement factors, i.e., the attributes captured by the sensors, that are related to its performance. The mea_x0002_surement factors of the perception module are from the perception component, and include signal detection accuracy, obstacle detection accuracy, as well as the accuracy of distance between vehicle and obstacles. With the perceived information, the autonomous driving sys_x0002_tem needs to predict the behaviors of obstacles and traffic participants. The prediction module from the decision making component can be measured with the accuracy of the predicted trajectory of obstacles. For the planning module with the understanding of the environment and obstacles, which is also from the decision making component, au_x0002_tonomous driving model developers are interested in such measures as the differences between predicted vehicle speed and the actual speed at a given time, the differences between predicted position and actual po_x0002_sition, etc. The control module from the control component includes those factors related to the control of throttle, brake, steering wheel, etc. The comfort module, which concerns the feelings of the driver, uses measures like the acceleration rate of the vehicle, turning angle, etc. Table 1 summarizes these modules, involved factors in each module, their descriptions, and their evaluation criteria.",
      "data_code": { "tables": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "In our case, we divided the autonomous driving process into five modules and selected relevant evaluation factors in each module. We combine AHP and TOPSIS in calculating the overall evaluation score for autonomous driving, because there are no officially defined eval_x0002_uation criteria for autonomous driving. Therefore, with the help of TOPSIS, for a driving record over a period of time and a given eval_x0002_uation factor, a time series of evaluation results for the factor can be obtained. In this way, we can obtain the time series of evaluation re_x0002_sults for all factors involved in the evaluation of autonomous driving over a period of time. Because there are no standard rules for ranking the importance of the factors of autonomous driving, AHP is used to determine the weight of each evaluation factor. Initially the weights are determined by module developers to equalize every evaluation module weight. With our visual evaluation system, they can determine the importance of each factor for different needs by themselves and the system can calculate the parameters that meet their needs with the help of AHP; With the input of the autonomous driving data record, we first calculatethe scores for each factor based on our model described in Sect. 4.1.The total score, module score and factor score range from 0 to 1. Thescore of each module is averaged by the scores of all the factors fromthis module at each time period. The score of each time period isaveraged by the scores of all the modules at this time period. Thetotal score is calculated by the average of all the scores in every timestep.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Therefore, a view of parallel coordinates is used to show the situation of each factor involved in evaluation. As shown in the figure, the top axis of parallel coordinates is the score axis, which shows the evaluation result of each time point. The polylines in parallel coordinates are color encoded by the total score at a specific moment. Under the score axis are factor axes, which indicate the evaluation result of each factor at each time point. With the help of parallel coordinates, users can easily see the connection of between overall scores and the values of individual factor, and observe the influence of the evaluation factors on the modules.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "parallelcoordinates",
        "axial_code": ["Repetition"],
        "componenet_code": ["parallelcoordinates"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 598,
    "paper_title": "Visual Evaluation for Autonomous Driving",
    "pub_year": 2022,
    "domain": "Autonomous Driving",
    "requirement": {
      "requirement_text": "R5: The system should allow module developers to customize the ranking of importance of each module based on their different goals and contexts in analysis.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Autonomous driving data can be obtained from the real car tests or simulation tests. Due to the limitations of the real testing environment, we use the autonomous driving data records obtained from the LGSVL simulator in this work. Based on the official classification [3] and the inputs from our collaborators, we divide our data into five mod_x0002_ules: perception, planning, prediction, control, and comfort. In each module, there are several measurement factors, i.e., the attributes captured by the sensors, that are related to its performance. The mea_x0002_surement factors of the perception module are from the perception component, and include signal detection accuracy, obstacle detection accuracy, as well as the accuracy of distance between vehicle and obstacles. With the perceived information, the autonomous driving sys_x0002_tem needs to predict the behaviors of obstacles and traffic participants. The prediction module from the decision making component can be measured with the accuracy of the predicted trajectory of obstacles. For the planning module with the understanding of the environment and obstacles, which is also from the decision making component, au_x0002_tonomous driving model developers are interested in such measures as the differences between predicted vehicle speed and the actual speed at a given time, the differences between predicted position and actual po_x0002_sition, etc. The control module from the control component includes those factors related to the control of throttle, brake, steering wheel, etc. The comfort module, which concerns the feelings of the driver, uses measures like the acceleration rate of the vehicle, turning angle, etc. Table 1 summarizes these modules, involved factors in each module, their descriptions, and their evaluation criteria.",
      "data_code": { "tables": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "In our case, we divided the autonomous driving process into five modules and selected relevant evaluation factors in each module. We combine AHP and TOPSIS in calculating the overall evaluation score for autonomous driving, because there are no officially defined eval_x0002_uation criteria for autonomous driving. Therefore, with the help of TOPSIS, for a driving record over a period of time and a given eval_x0002_uation factor, a time series of evaluation results for the factor can be obtained. In this way, we can obtain the time series of evaluation re_x0002_sults for all factors involved in the evaluation of autonomous driving over a period of time. Because there are no standard rules for ranking the importance of the factors of autonomous driving, AHP is used to determine the weight of each evaluation factor. Initially the weights are determined by module developers to equalize every evaluation module weight. With our visual evaluation system, they can determine the importance of each factor for different needs by themselves and the system can calculate the parameters that meet their needs with the help of AHP; With the input of the autonomous driving data record, we first calculatethe scores for each factor based on our model described in Sect. 4.1.The total score, module score and factor score range from 0 to 1. Thescore of each module is averaged by the scores of all the factors fromthis module at each time period. The score of each time period isaveraged by the scores of all the modules at this time period. Thetotal score is calculated by the average of all the scores in every timestep.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "The weights of all evaluation factors are equally distributed in the initial state. Because module developers with different background and interest may view different factors with different priorities. Users can drag the bars on the factor ranking visualization to reorder their own priority. Dragging the factors that users consider important and moving the unimportant factors to the bottom can achieve the priority customization and update the evaluation modelling. In this way, we can get the new relative importance r between factors according to the relative positions of those bars, construct a new judgment matrix R, and calculate the new evaluation results according to AHP.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "text",
        "axial_code": ["Repetition"],
        "componenet_code": ["text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 599,
    "paper_title": "Scalable Comparative Visualization of Ensembles of Call Graphs",
    "pub_year": 2023,
    "domain": "Optimizing the performance of large-scale parallel codes",
    "requirement": {
      "requirement_text": "A1. Compare Calling Contexts (T1) Across Runs (R1, R3).Due to potential differences in calling contexts of differentexecutions, it is essential to highlight any structural differ-ences, i.e., missing nodes/edges in the graph.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "Here, we are given an ensemble of multiprocess perfor_x0002_mance profiles to study weak scaling of an application across eight execution parameters: 1, 8, 27, 64, 125, 216, 343, and 512 processes. In particular, the application is LULESH [64], a hydrodynamics mini-application that uses both MPI and OpenMP to achieve parallelism. Recently, a similar case study was conducted by Bhatele et al. [52] to study run-to-run performance differences for identifying the most time-consuming regions of the code. We use a similar collection of profiles and showcase the advantages of visual analytics for such exploration using EnsembleCallFlow.",
      "data_code": { "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "In CallFlow, the performance profiles are converted into GraphFrames using Hatchet [52], an open-source profile analysis tool. A GraphFrame (G) is the key data source for CallFlow and consists of two data structures: a directed acyclic graph (G) that represents the CCT or call graph, and a Pandas [57] DataFrame (D) that stores the associated perfor_x0002_mance metrics. ",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Icicle plot places the call sites of the selected library based on their depth inside the supernode hierarchy from top to bottom. Each call site in the supernode hierarchy is visual- ized as horizontal rectangular bars. As with the ensemble- Sankey, ensemble gradients and runtime borders are used to encode the ensemble distribution and the runtime distri- bution for the call site, respectively.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "icicle",
        "axial_code": ["Repetition"],
        "componenet_code": ["icicle"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 600,
    "paper_title": "Scalable Comparative Visualization of Ensembles of Call Graphs",
    "pub_year": 2023,
    "domain": "Optimizing the performance of large-scale parallel codes",
    "requirement": {
      "requirement_text": "A1. Compare Calling Contexts (T1) Across Runs (R1, R3).Due to potential differences in calling contexts of differentexecutions, it is essential to highlight any structural differ-ences, i.e., missing nodes/edges in the graph.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "Here, we are given an ensemble of multiprocess perfor_x0002_mance profiles to study weak scaling of an application across eight execution parameters: 1, 8, 27, 64, 125, 216, 343, and 512 processes. In particular, the application is LULESH [64], a hydrodynamics mini-application that uses both MPI and OpenMP to achieve parallelism. Recently, a similar case study was conducted by Bhatele et al. [52] to study run-to-run performance differences for identifying the most time-consuming regions of the code. We use a similar collection of profiles and showcase the advantages of visual analytics for such exploration using EnsembleCallFlow.",
      "data_code": { "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "In CallFlow, the performance profiles are converted into GraphFrames using Hatchet [52], an open-source profile analysis tool. A GraphFrame (G) is the key data source for CallFlow and consists of two data structures: a directed acyclic graph (G) that represents the CCT or call graph, and a Pandas [57] DataFrame (D) that stores the associated perfor_x0002_mance metrics. ",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Metric correlation view. In order to compare inclusive and exclusive metrics for a given call site, we produce a scatterplot that captures the correlation between the two. For ensembles, each \u201cdot\u201d in the scatter plot represents a callsite for a single run, making it possible to study such correlations across ensemble members, e.g., by comparing a target run to the ensemble, as shown in the figure. The scatter plot itself is also interactive, as hovering over a dot highlights call site by their names for the user to compare the runtime metrics.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 601,
    "paper_title": "Scalable Comparative Visualization of Ensembles of Call Graphs",
    "pub_year": 2023,
    "domain": "Optimizing the performance of large-scale parallel codes",
    "requirement": {
      "requirement_text": "A2. Analyze Performance Variability (T2) Across Runs(R2.3). Identifying the overall trend as well as outlier runsrequires analyzing the performance metrics across the ensem-ble. Furthermore, to provide a full context and details neces-sary for performance improvements, such analyses must bemade available for each module, library, function, etc.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Here, we are given an ensemble of multiprocess perfor_x0002_mance profiles to study weak scaling of an application across eight execution parameters: 1, 8, 27, 64, 125, 216, 343, and 512 processes. In particular, the application is LULESH [64], a hydrodynamics mini-application that uses both MPI and OpenMP to achieve parallelism. Recently, a similar case study was conducted by Bhatele et al. [52] to study run-to-run performance differences for identifying the most time-consuming regions of the code. We use a similar collection of profiles and showcase the advantages of visual analytics for such exploration using EnsembleCallFlow.",
      "data_code": { "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "In CallFlow, the performance profiles are converted into GraphFrames using Hatchet [52], an open-source profile analysis tool. A GraphFrame (G) is the key data source for CallFlow and consists of two data structures: a directed acyclic graph (G) that represents the CCT or call graph, and a Pandas [57] DataFrame (D) that stores the associated perfor_x0002_mance metrics. ",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Although we could directly compute a density estimate (e.g., using KDE) instead of a histogram to generate a smoother distribution, such techniques introduce additional parameters (i.e., kernel width), which are typically harder to interpret and may miss features regardless due to an unsuit- able choice. Instead, we use histograms (with a customizable bin count) and use linear gradients to smooth the distribution. In this way, the resulting ensemble gradient shows the full distri- bution of the selected metric across runs. The default choice of color for the distribution is a single-hue white\u2014red colormap. The colors are mapped consistently across all nodes to allow evaluation of distributions not just within a single supernode, but also across supernodes.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 602,
    "paper_title": "Scalable Comparative Visualization of Ensembles of Call Graphs",
    "pub_year": 2023,
    "domain": "Optimizing the performance of large-scale parallel codes",
    "requirement": {
      "requirement_text": "A3. Analyze Performance Variability (T2) Across MPIRanks (R2.4). Summary statistics are not sufficient to ana-lyze performance distribution across resources because thedistribution is expected to be nontrivial. To aid the explora-tion, complete distributions are essential to visualize.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Here, we are given an ensemble of multiprocess perfor_x0002_mance profiles to study weak scaling of an application across eight execution parameters: 1, 8, 27, 64, 125, 216, 343, and 512 processes. In particular, the application is LULESH [64], a hydrodynamics mini-application that uses both MPI and OpenMP to achieve parallelism. Recently, a similar case study was conducted by Bhatele et al. [52] to study run-to-run performance differences for identifying the most time-consuming regions of the code. We use a similar collection of profiles and showcase the advantages of visual analytics for such exploration using EnsembleCallFlow.",
      "data_code": { "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "In CallFlow, the performance profiles are converted into GraphFrames using Hatchet [52], an open-source profile analysis tool. A GraphFrame (G) is the key data source for CallFlow and consists of two data structures: a directed acyclic graph (G) that represents the CCT or call graph, and a Pandas [57] DataFrame (D) that stores the associated perfor_x0002_mance metrics. ",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Call site correspondence view (see Fig. 3c) lets us focus on process-level distribution, and also scan call site informa- tion based on data and graph properties. To explore varia- tions across multiple target metrics, we utilize a boxplot to study the different runtime ranges occupied by the observed runtime distribution for each call site (A3). Box- plots use quartiles of the distribution to indicate the spread of data, and can also reveal outliers. In this view, we enu- merate the call sites that are not present in the ensemble view, and highlight the median, the interquartile range (IQR = Q3Q1), as well as outliers (above and below 1.5 the IQR). Additional information is also provided as text labels (e.g., minimum, median, and maximum runtime).",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "box",
        "axial_code": ["Repetition"],
        "componenet_code": ["box"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 603,
    "paper_title": "Scalable Comparative Visualization of Ensembles of Call Graphs",
    "pub_year": 2023,
    "domain": "Optimizing the performance of large-scale parallel codes",
    "requirement": {
      "requirement_text": "A4. Compare a Selected Run With an Ensemble (R1, R3).For both our targets, it is important to compare a selected runwith the ensemble behavior. An important constraint to con-sider is that the constructed baseline must match the expert\u2019sunderstanding of the overall calling context, despite minor dif-ferences in the individual ensemble members.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "Here, we are given an ensemble of multiprocess perfor_x0002_mance profiles to study weak scaling of an application across eight execution parameters: 1, 8, 27, 64, 125, 216, 343, and 512 processes. In particular, the application is LULESH [64], a hydrodynamics mini-application that uses both MPI and OpenMP to achieve parallelism. Recently, a similar case study was conducted by Bhatele et al. [52] to study run-to-run performance differences for identifying the most time-consuming regions of the code. We use a similar collection of profiles and showcase the advantages of visual analytics for such exploration using EnsembleCallFlow.",
      "data_code": { "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "Target-ensemble comparison mode is triggered when the user selects a particular execution from the ensemble to study in detail from \u201cSelect Target run\u201d, which lists all ensemble members. This operation allows the user to compare a selected run\u2019s performance with the ensemble across all 6 views.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 604,
    "paper_title": "Scalable Comparative Visualization of Ensembles of Call Graphs",
    "pub_year": 2023,
    "domain": "Optimizing the performance of large-scale parallel codes",
    "requirement": {
      "requirement_text": "A5. Compare Call Graphs Across Levels of Detail (R2.2).Although comparisons across super graphs (semanticallyaggregated call graphs) is useful, when experts identifyproblem areas, they are then interested in looking atselected regions in more detail (T1). Therefore, it is impor-tant to manage \ufb01ne-level details and visualize them uponrequest.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "Here, we are given an ensemble of multiprocess perfor_x0002_mance profiles to study weak scaling of an application across eight execution parameters: 1, 8, 27, 64, 125, 216, 343, and 512 processes. In particular, the application is LULESH [64], a hydrodynamics mini-application that uses both MPI and OpenMP to achieve parallelism. Recently, a similar case study was conducted by Bhatele et al. [52] to study run-to-run performance differences for identifying the most time-consuming regions of the code. We use a similar collection of profiles and showcase the advantages of visual analytics for such exploration using EnsembleCallFlow.",
      "data_code": { "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "In CallFlow, the performance profiles are converted into GraphFrames using Hatchet [52], an open-source profile analysis tool. A GraphFrame (G) is the key data source for CallFlow and consists of two data structures: a directed acyclic graph (G) that represents the CCT or call graph, and a Pandas [57] DataFrame (D) that stores the associated perfor_x0002_mance metrics. ",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "the diff view provided by EnsembleCallFlow canreproduce such analysis through a more-effective visualmedium (see Figs. 6b and 6c). The result highlights not onlythe modules that are slower (with respect to the diff order)but also communicates the relative degree of performancedegradation easily (A5).",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "sankey+bar",
        "axial_code": ["Stack"],
        "componenet_code": ["sankey", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 605,
    "paper_title": "SimuExplorer: Visual Exploration of Game Simulation in Table Tennis",
    "pub_year": 2023,
    "domain": "Sports",
    "requirement": {
      "requirement_text": "What are the impacts brought by return adjustments of different players? Players have individual styles of playing, and the impacts of returns differ by player. The experts hope to browse different players\u2019 impact distributions over return adjustments.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We collected data from 306 table tennis matches among 21 top players (10 males and 11 females) from 2005 to 2012. The data was collected by manually coding videos. We have tried using machine learning models to automatically extract structured data from video. However, low video quality means that the accuracy of automatically extracted stroke features, such as stroke technique, is relatively low (around 75%). Each match was collected as a CSV file, which contained hundreds of rows representing the hun_x0002_dreds of strokes in the match. Each row records different features of a stroke as follows. _x0001_Rally ID denotes the rally the stroke belongs to. Stroke ID is the sequence number of the stroke in the rally. Stroke player denotes the player who gave the stroke. Stroke technique denotes the technique the player uses to give the stroke. There are 14 techniques. Ball position denotes the drop point of the stroke. There are ten drop points.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Lames [6] first proposed simulating a table tennis rally through a Markov chain model. The model views a rally as a sequence of possible stroke states, and simulates the impacts of strokes on players\u2019 scoring rates. Here we go through how Lames\u2019 model [6] has been applied in previous studies and explain how the model simulates a rally. Then we propose a modified model and explain how intermediate results of the model measure returns and impacts of return adjustments.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "Click a player in the player view, and examine correspond- ing impacts in the adjustment view.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      },
      {
        "solution_text": "Bar charts in the player view represent the impact distribution over different returns for a chosen player.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "text+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar", "text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 606,
    "paper_title": "SimuExplorer: Visual Exploration of Game Simulation in Table Tennis",
    "pub_year": 2023,
    "domain": "Sports",
    "requirement": {
      "requirement_text": "What are the impacts brought by return adjustments of different players? Players have individual styles of playing, and the impacts of returns differ by player. The experts hope to browse different players\u2019 impact distributions over return adjustments.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We collected data from 306 table tennis matches among 21 top players (10 males and 11 females) from 2005 to 2012. The data was collected by manually coding videos. We have tried using machine learning models to automatically extract structured data from video. However, low video quality means that the accuracy of automatically extracted stroke features, such as stroke technique, is relatively low (around 75%). Each match was collected as a CSV file, which contained hundreds of rows representing the hun_x0002_dreds of strokes in the match. Each row records different features of a stroke as follows. _x0001_Rally ID denotes the rally the stroke belongs to. Stroke ID is the sequence number of the stroke in the rally. Stroke player denotes the player who gave the stroke. Stroke technique denotes the technique the player uses to give the stroke. There are 14 techniques. Ball position denotes the drop point of the stroke. There are ten drop points.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Lames [6] first proposed simulating a table tennis rally through a Markov chain model. The model views a rally as a sequence of possible stroke states, and simulates the impacts of strokes on players\u2019 scoring rates. Here we go through how Lames\u2019 model [6] has been applied in previous studies and explain how the model simulates a rally. Then we propose a modified model and explain how intermediate results of the model measure returns and impacts of return adjustments.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "The player view (Fig. 4A) provides a browsable overview ofthe impacts of return adjustments for different players (T1).This view contains a table that lists all the top table tennisplayers. A bar chart (Fig. 4A-4) in each row in the tablepresents the impact distribution of return adjustments for aplayer. Because stroke technique is the most important attri-bute, in the left column, we group typical returns accordingto six stroke technique values based on the first stroke in thereturn, obtaining six groups. In the right column, we grouptypical returns according to five stroke technique valuesbased on the second stroke in the return, obtaining fivegroups. We encode the average impact of adjusting returnsin each group as a bar in the bar chart. The orange barsdenote return adjustments with positive impacts, while theblue bars denote return adjustments with negative impacts.The axes are scaled according to the largest impact.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "text+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar", "text"]
      },
      {
        "solution_text": "Withthis view, users can quickly detect whether a player canchange their scoring rate by adjusting a group of returnsinvolving specific stroke technique values. Two switch but-tons (Figs. 4A-1 and 4A-2) allow switching between serveand other rallies and between male and female players.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 607,
    "paper_title": "SimuExplorer: Visual Exploration of Game Simulation in Table Tennis",
    "pub_year": 2023,
    "domain": "Sports",
    "requirement": {
      "requirement_text": "What are the impacts brought by different return adjust- ments of a player? Adjustments to different returns exert varying impacts for a player. The experts need to detect the impact patterns of different return adjustments.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We collected data from 306 table tennis matches among 21 top players (10 males and 11 females) from 2005 to 2012. The data was collected by manually coding videos. We have tried using machine learning models to automatically extract structured data from video. However, low video quality means that the accuracy of automatically extracted stroke features, such as stroke technique, is relatively low (around 75%). Each match was collected as a CSV file, which contained hundreds of rows representing the hun_x0002_dreds of strokes in the match. Each row records different features of a stroke as follows. _x0001_Rally ID denotes the rally the stroke belongs to. Stroke ID is the sequence number of the stroke in the rally. Stroke player denotes the player who gave the stroke. Stroke technique denotes the technique the player uses to give the stroke. There are 14 techniques. Ball position denotes the drop point of the stroke. There are ten drop points.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Lames [6] first proposed simulating a table tennis rally through a Markov chain model. The model views a rally as a sequence of possible stroke states, and simulates the impacts of strokes on players\u2019 scoring rates. Here we go through how Lames\u2019 model [6] has been applied in previous studies and explain how the model simulates a rally. Then we propose a modified model and explain how intermediate results of the model measure returns and impacts of return adjustments.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "The player view (Fig. 4A) provides a browsable overview ofthe impacts of return adjustments for different players (T1).This view contains a table that lists all the top table tennisplayers. A bar chart (Fig. 4A-4) in each row in the tablepresents the impact distribution of return adjustments for aplayer. Because stroke technique is the most important attri-bute, in the left column, we group typical returns accordingto six stroke technique values based on the first stroke in thereturn, obtaining six groups. In the right column, we grouptypical returns according to five stroke technique valuesbased on the second stroke in the return, obtaining fivegroups. We encode the average impact of adjusting returnsin each group as a bar in the bar chart. The orange barsdenote return adjustments with positive impacts, while theblue bars denote return adjustments with negative impacts.The axes are scaled according to the largest impact.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "text+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar", "text"]
      },
      {
        "solution_text": "When a user is interested in the impact distribution of a particular player, she/he can click and select the player in the player view.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      },
      {
        "solution_text": "The adjustment view will then display the impacts of adjusting different returns for the player.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "matrix+text",
        "axial_code": ["Stack"],
        "componenet_code": ["text", "matrix"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 608,
    "paper_title": "SimuExplorer: Visual Exploration of Game Simulation in Table Tennis",
    "pub_year": 2023,
    "domain": "Sports",
    "requirement": {
      "requirement_text": "What are the impacts brought by different return adjust- ments of a player? Adjustments to different returns exert varying impacts for a player. The experts need to detect the impact patterns of different return adjustments.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We collected data from 306 table tennis matches among 21 top players (10 males and 11 females) from 2005 to 2012. The data was collected by manually coding videos. We have tried using machine learning models to automatically extract structured data from video. However, low video quality means that the accuracy of automatically extracted stroke features, such as stroke technique, is relatively low (around 75%). Each match was collected as a CSV file, which contained hundreds of rows representing the hun_x0002_dreds of strokes in the match. Each row records different features of a stroke as follows. _x0001_Rally ID denotes the rally the stroke belongs to. Stroke ID is the sequence number of the stroke in the rally. Stroke player denotes the player who gave the stroke. Stroke technique denotes the technique the player uses to give the stroke. There are 14 techniques. Ball position denotes the drop point of the stroke. There are ten drop points.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Lames [6] first proposed simulating a table tennis rally through a Markov chain model. The model views a rally as a sequence of possible stroke states, and simulates the impacts of strokes on players\u2019 scoring rates. Here we go through how Lames\u2019 model [6] has been applied in previous studies and explain how the model simulates a rally. Then we propose a modified model and explain how intermediate results of the model measure returns and impacts of return adjustments.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "The player view (Fig. 4A) provides a browsable overview ofthe impacts of return adjustments for different players (T1).This view contains a table that lists all the top table tennisplayers. A bar chart (Fig. 4A-4) in each row in the tablepresents the impact distribution of return adjustments for aplayer. Because stroke technique is the most important attri-bute, in the left column, we group typical returns accordingto six stroke technique values based on the first stroke in thereturn, obtaining six groups. In the right column, we grouptypical returns according to five stroke technique valuesbased on the second stroke in the return, obtaining fivegroups. We encode the average impact of adjusting returnsin each group as a bar in the bar chart. The orange barsdenote return adjustments with positive impacts, while theblue bars denote return adjustments with negative impacts.The axes are scaled according to the largest impact.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "text+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar", "text"]
      },
      {
        "solution_text": "After a user selects a player for analysis, the adjustmentview (Fig. 4B) provides an overview of the impacts ofadjusting the player\u2019s different returns (T2).",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      },
      {
        "solution_text": "This view contains a matrix similar to that in Fig. 2A.Each entry in the matrix (Fig. 4B-1) represents one return ofPlayer A. The column headers denote the combinations ofstroke technique and ball position values at the \ufb01rst strokeof the return (given by Player A), and row headers denotethose at the second stroke (given by opponents of Player A).The area of the rectangle in an entry encodes the impact ofadjusting the corresponding return. (To reiterate, adjustinga return means successfully in\ufb02uencing the opponents ofPlayer A to use a speci\ufb01c type of second stroke more fre-quently after Player A uses another speci\ufb01c type of stroke.)The matrix hides any rows and columns with small valuesin order to emphasize key return adjustments. To ease navi-gation of this large amount of returns, we group columnsand rows according to the stroke technique of the \ufb01rst andsecond stroke in the return, respectively. Initially, detailedinformation for each entry is not visible. Instead, groupblocks are displayed, where the lightness of a block encodesthe average impacts of returns within that block.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "matrix+text",
        "axial_code": ["Stack"],
        "componenet_code": ["text", "matrix"]
      },
      {
        "solution_text": "If a user isinterested in a block of returns, she/he can click the blockand examine the detailed impacts of each return within theblock (Fig. 4B-2). When a user hovers over an entry, the sizeof the impact will be displayed (Fig. 4B-3). Users can alsoclick an entry to select particular a return adjustment ofPlayer A, to explore how the impacts of this adjustment aregenerated and accumulated.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 609,
    "paper_title": "SimuExplorer: Visual Exploration of Game Simulation in Table Tennis",
    "pub_year": 2023,
    "domain": "Sports",
    "requirement": {
      "requirement_text": "How do the impacts brought by a return adjustment accu- mulate over tactics in a rally? A rally in table tennis contains a sequence of tactics. The experts hope to explore how the impacts of a return adjustment in sequential tactics accumulate in a rally.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "We collected data from 306 table tennis matches among 21 top players (10 males and 11 females) from 2005 to 2012. The data was collected by manually coding videos. We have tried using machine learning models to automatically extract structured data from video. However, low video quality means that the accuracy of automatically extracted stroke features, such as stroke technique, is relatively low (around 75%). Each match was collected as a CSV file, which contained hundreds of rows representing the hun_x0002_dreds of strokes in the match. Each row records different features of a stroke as follows. _x0001_Rally ID denotes the rally the stroke belongs to. Stroke ID is the sequence number of the stroke in the rally. Stroke player denotes the player who gave the stroke. Stroke technique denotes the technique the player uses to give the stroke. There are 14 techniques. Ball position denotes the drop point of the stroke. There are ten drop points.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Lames [6] first proposed simulating a table tennis rally through a Markov chain model. The model views a rally as a sequence of possible stroke states, and simulates the impacts of strokes on players\u2019 scoring rates. Here we go through how Lames\u2019 model [6] has been applied in previous studies and explain how the model simulates a rally. Then we propose a modified model and explain how intermediate results of the model measure returns and impacts of return adjustments.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "This view contains a matrix similar to that in Fig. 2A.Each entry in the matrix (Fig. 4B-1) represents one return ofPlayer A. The column headers denote the combinations ofstroke technique and ball position values at the \ufb01rst strokeof the return (given by Player A), and row headers denotethose at the second stroke (given by opponents of Player A).The area of the rectangle in an entry encodes the impact ofadjusting the corresponding return. (To reiterate, adjustinga return means successfully in\ufb02uencing the opponents ofPlayer A to use a speci\ufb01c type of second stroke more fre-quently after Player A uses another speci\ufb01c type of stroke.)The matrix hides any rows and columns with small valuesin order to emphasize key return adjustments. To ease navi-gation of this large amount of returns, we group columnsand rows according to the stroke technique of the \ufb01rst andsecond stroke in the return, respectively. Initially, detailedinformation for each entry is not visible. Instead, groupblocks are displayed, where the lightness of a block encodesthe average impacts of returns within that block.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "matrix+text",
        "axial_code": ["Stack"],
        "componenet_code": ["text", "matrix"]
      },
      {
        "solution_text": "Click an impact in the adjustment view and examine its accumulation in the accumulation view. The user can further select the impact of an interesting return adjustment by clicking it in the adjustment view.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      },
      {
        "solution_text": "Click an impact in the adjustment view and examine its accumulation in the accumulation view. The user can further select the impact of an interesting return adjustment by clicking it in the adjustment view. The accumulation view provides visual tracking of the accumulation of impacts over tactics.",
        "solution_category": "visualization",
        "solution_axial": "Annotation",
        "solution_compoent": "flow+bar+pie",
        "axial_code": ["Annotation"],
        "componenet_code": ["flow", "pie", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 610,
    "paper_title": "SimuExplorer: Visual Exploration of Game Simulation in Table Tennis",
    "pub_year": 2023,
    "domain": "Sports",
    "requirement": {
      "requirement_text": "How do the impacts brought by a return adjustment accu- mulate over tactics in a rally? A rally in table tennis contains a sequence of tactics. The experts hope to explore how the impacts of a return adjustment in sequential tactics accumulate in a rally.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "We collected data from 306 table tennis matches among 21 top players (10 males and 11 females) from 2005 to 2012. The data was collected by manually coding videos. We have tried using machine learning models to automatically extract structured data from video. However, low video quality means that the accuracy of automatically extracted stroke features, such as stroke technique, is relatively low (around 75%). Each match was collected as a CSV file, which contained hundreds of rows representing the hun_x0002_dreds of strokes in the match. Each row records different features of a stroke as follows. _x0001_Rally ID denotes the rally the stroke belongs to. Stroke ID is the sequence number of the stroke in the rally. Stroke player denotes the player who gave the stroke. Stroke technique denotes the technique the player uses to give the stroke. There are 14 techniques. Ball position denotes the drop point of the stroke. There are ten drop points.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Lames [6] first proposed simulating a table tennis rally through a Markov chain model. The model views a rally as a sequence of possible stroke states, and simulates the impacts of strokes on players\u2019 scoring rates. Here we go through how Lames\u2019 model [6] has been applied in previous studies and explain how the model simulates a rally. Then we propose a modified model and explain how intermediate results of the model measure returns and impacts of return adjustments.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "This view contains a matrix similar to that in Fig. 2A.Each entry in the matrix (Fig. 4B-1) represents one return ofPlayer A. The column headers denote the combinations ofstroke technique and ball position values at the \ufb01rst strokeof the return (given by Player A), and row headers denotethose at the second stroke (given by opponents of Player A).The area of the rectangle in an entry encodes the impact ofadjusting the corresponding return. (To reiterate, adjustinga return means successfully in\ufb02uencing the opponents ofPlayer A to use a speci\ufb01c type of second stroke more fre-quently after Player A uses another speci\ufb01c type of stroke.)The matrix hides any rows and columns with small valuesin order to emphasize key return adjustments. To ease navi-gation of this large amount of returns, we group columnsand rows according to the stroke technique of the \ufb01rst andsecond stroke in the return, respectively. Initially, detailedinformation for each entry is not visible. Instead, groupblocks are displayed, where the lightness of a block encodesthe average impacts of returns within that block.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "matrix+text",
        "axial_code": ["Stack"],
        "componenet_code": ["text", "matrix"]
      },
      {
        "solution_text": "After a user selects a return adjustment of Player A, theaccumulation view (Fig. 4C) enables visual tracking of howthe impacts of this return adjustment accumulate in eachtactic (T3).",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      },
      {
        "solution_text": "This view contains a \ufb02ow chart that represents the varia-tion in Player A\u2019s scoring rate after he or she adjusts a returnduring his or her tactics (Fig. 3C). Flow charts are widelyused to visualize temporal variations in variables [50], [51],[52]. In the accumulation view, the \ufb02ow chart intuitivelyillustrates the process through which impacts accumulategradually over the course of different tactics and \ufb01nallyaggregate into the total impact. In the accumulation view,the width of the main \ufb02ow (Fig. 4C-4) encodes the accumu-lated changed scoring rate, and the width of branches(Fig. 4C-1) encodes the variation of the scoring rate at eachtactic. When users hover on each branch, the original scor-ing rate (above) and changed scoring rate (below) are dis-played in a detail view (Fig. 4C-3). For the changed scoringrate, a pie chart displays how much of this rate change wascaused by the adjustment at the current tactic versus howmuch was caused by adjustments at previous tactics.The bar chart (Fig. 4C-7) of each tactic shows six strokestates, arranged based on their impact on scoring probabili-ties when they are at that tactic\u2019s last stroke. The \ufb01rst threestates displayed are those that do the most to increase scor-ing probabilities. The \ufb01nal three displayed are those that dothe most to decrease those probabilities. The height of theorange bar encodes the increase in scoring probability,while the height of the blue bar encodes the decrease. Thishelps users to see how adjustments impact a tactic\u2019s laststroke and the corresponding scoring probabilities.",
        "solution_category": "visualization",
        "solution_axial": "Annotation",
        "solution_compoent": "flow+bar+pie",
        "axial_code": ["Annotation"],
        "componenet_code": ["flow", "pie", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 611,
    "paper_title": "SimuExplorer: Visual Exploration of Game Simulation in Table Tennis",
    "pub_year": 2023,
    "domain": "Sports",
    "requirement": {
      "requirement_text": "How does a return adjustment influence the scoring chan- ces at a tactic? The experts hope to examine how the impacts are generated in a tactic and how many impacts in a tactic are due to previous tactics.",
      "requirement_code": { "identify_main_cause_aggregate": 1 }
    },
    "data": {
      "data_text": "We collected data from 306 table tennis matches among 21 top players (10 males and 11 females) from 2005 to 2012. The data was collected by manually coding videos. We have tried using machine learning models to automatically extract structured data from video. However, low video quality means that the accuracy of automatically extracted stroke features, such as stroke technique, is relatively low (around 75%). Each match was collected as a CSV file, which contained hundreds of rows representing the hun_x0002_dreds of strokes in the match. Each row records different features of a stroke as follows. _x0001_Rally ID denotes the rally the stroke belongs to. Stroke ID is the sequence number of the stroke in the rally. Stroke player denotes the player who gave the stroke. Stroke technique denotes the technique the player uses to give the stroke. There are 14 techniques. Ball position denotes the drop point of the stroke. There are ten drop points.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Lames [6] first proposed simulating a table tennis rally through a Markov chain model. The model views a rally as a sequence of possible stroke states, and simulates the impacts of strokes on players\u2019 scoring rates. Here we go through how Lames\u2019 model [6] has been applied in previous studies and explain how the model simulates a rally. Then we propose a modified model and explain how intermediate results of the model measure returns and impacts of return adjustments.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "This view contains a \ufb02ow chart that represents the varia-tion in Player A\u2019s scoring rate after he or she adjusts a returnduring his or her tactics (Fig. 3C). Flow charts are widelyused to visualize temporal variations in variables [50], [51],[52]. In the accumulation view, the \ufb02ow chart intuitivelyillustrates the process through which impacts accumulategradually over the course of different tactics and \ufb01nallyaggregate into the total impact. In the accumulation view,the width of the main \ufb02ow (Fig. 4C-4) encodes the accumu-lated changed scoring rate, and the width of branches(Fig. 4C-1) encodes the variation of the scoring rate at eachtactic. When users hover on each branch, the original scor-ing rate (above) and changed scoring rate (below) are dis-played in a detail view (Fig. 4C-3). For the changed scoringrate, a pie chart displays how much of this rate change wascaused by the adjustment at the current tactic versus howmuch was caused by adjustments at previous tactics.The bar chart (Fig. 4C-7) of each tactic shows six strokestates, arranged based on their impact on scoring probabili-ties when they are at that tactic\u2019s last stroke. The \ufb01rst threestates displayed are those that do the most to increase scor-ing probabilities. The \ufb01nal three displayed are those that dothe most to decrease those probabilities. The height of theorange bar encodes the increase in scoring probability,while the height of the blue bar encodes the decrease. Thishelps users to see how adjustments impact a tactic\u2019s laststroke and the corresponding scoring probabilities.",
        "solution_category": "visualization",
        "solution_axial": "Annotation",
        "solution_compoent": "flow+bar+pie",
        "axial_code": ["Annotation"],
        "componenet_code": ["flow", "pie", "bar"]
      },
      {
        "solution_text": "Click a tactic in the accumulation view and examine how the impact is generated at this tactic in the impact view.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      },
      {
        "solution_text": "Click a tactic in the accumulation view and examine how the impact is generated at this tactic in the impact view. When the user clicks a tactic in the accumulation view, the impact view presents the generation process of the impact at that tactic.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "bar+matrix",
        "axial_code": ["Nesting"],
        "componenet_code": ["matrix", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 612,
    "paper_title": "SimuExplorer: Visual Exploration of Game Simulation in Table Tennis",
    "pub_year": 2023,
    "domain": "Sports",
    "requirement": {
      "requirement_text": "How does a return adjustment influence the scoring chan- ces at a tactic? The experts hope to examine how the impacts are generated in a tactic and how many impacts in a tactic are due to previous tactics.",
      "requirement_code": { "identify_main_cause_aggregate": 1 }
    },
    "data": {
      "data_text": "We collected data from 306 table tennis matches among 21 top players (10 males and 11 females) from 2005 to 2012. The data was collected by manually coding videos. We have tried using machine learning models to automatically extract structured data from video. However, low video quality means that the accuracy of automatically extracted stroke features, such as stroke technique, is relatively low (around 75%). Each match was collected as a CSV file, which contained hundreds of rows representing the hun_x0002_dreds of strokes in the match. Each row records different features of a stroke as follows. _x0001_Rally ID denotes the rally the stroke belongs to. Stroke ID is the sequence number of the stroke in the rally. Stroke player denotes the player who gave the stroke. Stroke technique denotes the technique the player uses to give the stroke. There are 14 techniques. Ball position denotes the drop point of the stroke. There are ten drop points.",
      "data_code": { "tables": 1, "categorical": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Lames [6] first proposed simulating a table tennis rally through a Markov chain model. The model views a rally as a sequence of possible stroke states, and simulates the impacts of strokes on players\u2019 scoring rates. Here we go through how Lames\u2019 model [6] has been applied in previous studies and explain how the model simulates a rally. Then we propose a modified model and explain how intermediate results of the model measure returns and impacts of return adjustments.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "This view contains a \ufb02ow chart that represents the varia-tion in Player A\u2019s scoring rate after he or she adjusts a returnduring his or her tactics (Fig. 3C). Flow charts are widelyused to visualize temporal variations in variables [50], [51],[52]. In the accumulation view, the \ufb02ow chart intuitivelyillustrates the process through which impacts accumulategradually over the course of different tactics and \ufb01nallyaggregate into the total impact. In the accumulation view,the width of the main \ufb02ow (Fig. 4C-4) encodes the accumu-lated changed scoring rate, and the width of branches(Fig. 4C-1) encodes the variation of the scoring rate at eachtactic. When users hover on each branch, the original scor-ing rate (above) and changed scoring rate (below) are dis-played in a detail view (Fig. 4C-3). For the changed scoringrate, a pie chart displays how much of this rate change wascaused by the adjustment at the current tactic versus howmuch was caused by adjustments at previous tactics.The bar chart (Fig. 4C-7) of each tactic shows six strokestates, arranged based on their impact on scoring probabili-ties when they are at that tactic\u2019s last stroke. The \ufb01rst threestates displayed are those that do the most to increase scor-ing probabilities. The \ufb01nal three displayed are those that dothe most to decrease those probabilities. The height of theorange bar encodes the increase in scoring probability,while the height of the blue bar encodes the decrease. Thishelps users to see how adjustments impact a tactic\u2019s laststroke and the corresponding scoring probabilities.",
        "solution_category": "visualization",
        "solution_axial": "Annotation",
        "solution_compoent": "flow+bar+pie",
        "axial_code": ["Annotation"],
        "componenet_code": ["flow", "pie", "bar"]
      },
      {
        "solution_text": "After a user selects a tactic, the impact view (Fig. 4D)presents how the impact is generated through the tworeturns within the tactic.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      },
      {
        "solution_text": "After a user selects a tactic, the impact view (Fig. 4D)presents how the impact is generated through the tworeturns within the tactic. The impact view must display theadjusted many-to-many probabilities of states in the \ufb01rstreturn, the changed probabilities of states in the secondreturn, and the scoring rate after the third stroke (T4).Justi\ufb01cation: There exist more than two hundred returnstates, with each state involving a speci\ufb01c \ufb01rst stroke andsecond stroke (Fig. 2). Displaying probabilities of returnstates is a many-to-many relation visualization problem.Flow or Matrix. As discussed in Section 3.3, existing visu-alization studies employ node-link diagrams, \ufb02ow charts,and matrix views [53] to display many-to-many relations.We initially employed \ufb02ow charts to present the changedprobabilities, thinking they would be intuitive to under-stand. However, these ended up visually cluttered due tothe many probabilities involved. We thus switched to a clut-ter-free matrix view to help users browse how probabilitieswere changed by the adjustments.Layout. Two matrices display changing probabilities overtwo returns. As discussed in Section 3.3, the layouts used byMatrixWave [49] and iTTVis [22] were not suitable for thisproject. Instead, we propose our current design, whichincludes clutter-free matrix views and straight leader lineswith interactions that help users understand and interprethow impacts compile within a tactic. The details of thedesign are introduced as follows.Description. The impact view uses a pair of matrices to rep-resent the changed probabilities of the states at the tworeturns of a tactic, thus explaining the varying scoring rateafter the third stroke of this tactic. The \ufb01rst matrix (Fig. 4D-2)presents how adjustments are made at the \ufb01rst return of thetactic. The second matrix (Fig. 4D-4) displays how the adjust-ment changes the probabilities of states at the second returnand further changes the scoring rate after the third stroke.The three columns of stroke bars (Figs. 4D-1, 4D-3, and 4D-5)from left to right represent the changed probabilities ofstroke states (different stroke technique and ball position val-ues) at the three strokes in a tactic. These probabilities in eachbar column are derived by aggregating the probabilities incorresponding columns in the prior matrix. The detailedencodings are as follows.Bars for Strokes. Three columns of bars (Figs. 4D-1, 4D-3,and 4 D-5) present the changed probabilities of stroke statesat the \ufb01rst, second, and third strokes. The height of each barencodes the changed probability of each stroke state. The barcolor indicates whether the probability increases (orange) ordecreases (blue). The probability of each stroke state isdivided into two parts. The \ufb01rst part is the changed scoringprobability after the stroke, i.e., the change in the probabilitythat the player gives a stroke with this state and scoresdirectly. This is encoded by bars on the left. (This part of the\ufb01rst stroke is not shown, because it is not considered in thecurrent tactic). The second part is the change in the probabil-ity that the player gives a stroke with this state and does notscore. This part is encoded by bars on the right. When usershover on each part of a bar, the corresponding rows and col-umns in the two matrices are highlighted (Fig. 4D). The origi-nal and changed probabilities are also displayed in the detailview (Fig. 5C). A pie chart displays how much of this proba-bility change was caused by the adjustment at the currenttactic versus how much was caused by adjustments at previ-ous tactics.Matrices for Returns. A pair of matrices presents thechanged probabilities of all states that make up the tworeturns within a tactic. The \ufb01rst matrix (Fig. 4D-2) presentsthe adjustments (Fig. 3B) to the \ufb01rst return in the tactic. Thesecond matrix (Fig. 4D-4) presents the impacts on the sec-ond return of the tactic (Fig. 3A). Similar to the layout ofreturn states in Fig. 2A, the changed probabilities of returnstates are arranged as a matrix whose column and rowheaders represent the stroke state at the \ufb01rst and second ofthe two strokes, respectively. The headers are linked bybars representing aggregate changed probabilities. As theimpacts pass through the matrix, the changed probabilitiesof stroke states branch into the changed probabilities ofreturn states, and then merge into the changed probabilitiesof the next stroke states. With this detailed display ofchanged probabilities of return states, experts can betterunderstand how state probabilities of a return are adjusted,how adjustments to previous returns change the state prob-abilities of following returns, and how these changed proba-bilities aggregate into changed scoring rates. When theserve tactic (i.e., the tactic comprising Strokes 1\u20133) is pre-sented in the impact view, the columns of the \ufb01rst matrixare altered because the \ufb01rst stroke in the serve tactic mustbe a serve stroke.In each matrix entry, the area of the circle encodes thechanged probability as it transforms from the columnheader to the row header. The circle color indicates whetherthe probability increases (orange) or decreases (blue). Aswitch button is placed at the bottom of the bars for thethird stroke. The right part of the button (corresponding tothe right part of the bars) is enabled by default. Circles inentries of the second matrix represent the changed probabil-ities from the column headers to the row headers. Whenusers switch to the left part of the button (which corre-sponds to the left part of the bars), the circles in the entriesof the second matrix are transformed into rectangles. Theareas of the rectangles represent the changed scoring proba-bilities at the third stroke due to the changed probabilitiesof the \ufb01rst and second. When users hover on an entry, thecorresponding row and column are highlighted, and adetail view (Fig. 5B) is displayed.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "bar+matrix",
        "axial_code": ["Nesting"],
        "componenet_code": ["matrix", "bar"]
      },
      {
        "solution_text": "When users switch to the left part of the button (which corre-sponds to the left part of the bars), the circles in the entriesof the second matrix are transformed into rectangles. Theareas of the rectangles represent the changed scoring proba-bilities at the third stroke due to the changed probabilitiesof the \ufb01rst and second. When users hover on an entry, thecorresponding row and column are highlighted, and adetail view (Fig. 5B) is displayed.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 613,
    "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling",
    "pub_year": 2023,
    "domain": "urban planning",
    "requirement": {
      "requirement_text": "E1: Summarize Trip Set. Users analyze data with spe-cific conditions, such as trips that took place duringweekend or peak time, rather than the entire data.Thus, they apply filters to summarize the trip set[Summarize ! Trip Set].",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.",
      "data_code": { "tables": 1, "ordinal": 1, "temporal": 1, "sequential": 1 }
    },
    "solution": [
      {
        "solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["Excluding", "AlgorithmicCalculation"],
        "componenet_code": ["excluding", "algorithmic_calculation"]
      },
      {
        "solution_text": "The OD-Trip view (Fig. 2B) supports the interactive modifica_x0002_tion of filtering conditions applied to the active trip set (Task E1). The filtering conditions are represented as badges in the view header (Fig. 2 (1)). The conditions can be divided into two types: by departure time and by attributes. The two time bar charts (i.e., two bar charts on the left of the OD-Trip view) summarize the num_x0002_ber of trips aggregated by departure time, such as time of day (AM peak (from 07:00 to 10:00), Mid-day (between AM and PM peak), PM peak (from 17:00 to 20:00), and Overnight (between PM and AM peak)), and day of the week. All these time spans were determined, reflecting domain experts\u2019 exploration practice identified during the domain situation analysis. The attributes panel on the right visualizes the active trip set\u2019s OD pairs and associated trips with their attributes. In this panel, a column represents either an OD or route attri_x0002_bute of OD pairs. A column header shows the distribution of the corresponding attribute as a matrix or a histogram. Below the column headers, each row (Fig. 4 (1)) represents an OD pair and its attribute values.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "bar+matrix",
        "axial_code": ["Stack"],
        "componenet_code": ["matrix", "bar"]
      },
      {
        "solution_text": "The OD-Trip view (Fig. 2B) supports the interactive modifica_x0002_tion of filtering conditions applied to the active trip set (Task E1).",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 614,
    "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling",
    "pub_year": 2023,
    "domain": "urban planning",
    "requirement": {
      "requirement_text": "E2: Explore Geographical Distribution of Trips. Usersexplore how riders\u2019 trips are geographically distrib-uted, especially areas, flows, or roads with heavy traf-fic [Explore ! Feature].",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.",
      "data_code": { "geometry": 1, "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["Excluding", "AlgorithmicCalculation"],
        "componenet_code": ["excluding", "algorithmic_calculation"]
      },
      {
        "solution_text": "To represent the other two targets (i.e., OD pair and road segment), we overlay two vis- ualizations on the map view: a flow map and a road heatmap. These allow users to explore the geo- graphical distribution of different targets; in the flow map, trips are aggregated and shown as flows between OD pairs, while in the road heatmap, the traffic on individ- ual roads is color-encoded.",
        "solution_category": "visualization",
        "solution_axial": "coordinate",
        "solution_compoent": "map+flow+heatmap+glyph",
        "axial_code": ["coordinate"],
        "componenet_code": ["flow", "heatmap", "glyph", "map"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 615,
    "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling",
    "pub_year": 2023,
    "domain": "urban planning",
    "requirement": {
      "requirement_text": "E2: Explore Geographical Distribution of Trips. Usersexplore how riders\u2019 trips are geographically distrib-uted, especially areas, flows, or roads with heavy traf-fic [Explore ! Feature].",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.",
      "data_code": { "geometry": 1, "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["Excluding", "AlgorithmicCalculation"],
        "componenet_code": ["excluding", "algorithmic_calculation"]
      },
      {
        "solution_text": "In designing the station view, we mainly considered theconsistency and interactivity of the map. The reason fordoing so is to allow users to identify the geographical distri-bution of data represented in the station view to performTask E2. For example, we make the color of the total trafficbars the same as that of the station symbol in the map view.The shape on the left of the total traffic bar represents a sta-tion type and is also the same as the map view. The OD barsof in-flow and out-flow share the same color and thicknessas the map view\u2019s edge.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "bar+matrix",
        "axial_code": ["Stack"],
        "componenet_code": ["matrix", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 616,
    "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling",
    "pub_year": 2023,
    "domain": "urban planning",
    "requirement": {
      "requirement_text": "E2: Explore Geographical Distribution of Trips. Usersexplore how riders\u2019 trips are geographically distrib-uted, especially areas, flows, or roads with heavy traf-fic [Explore ! Feature].",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.",
      "data_code": { "geometry": 1, "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["Excluding", "AlgorithmicCalculation"],
        "componenet_code": ["excluding", "algorithmic_calculation"]
      },
      {
        "solution_text": "The route view shows the details of all routes taken between a particular OD pair, such as matched paths and route attributes. Unlike the aforementioned views, the route view allows users to take a detailed look at geographical dis- tribution or route attribute distribution within a single OD pair.",
        "solution_category": "visualization",
        "solution_axial": "coordinate",
        "solution_compoent": "map+path",
        "axial_code": ["coordinate"],
        "componenet_code": ["path", "map"]
      },
      {
        "solution_text": "Unlike the aforementioned views, the route view allows users to take a detailed look at geographical dis- tribution or route attribute distribution within a single OD pair.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 617,
    "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling",
    "pub_year": 2023,
    "domain": "urban planning",
    "requirement": {
      "requirement_text": "E3: Identify Attribute Distribution of Chosen Routes.Users inspect distributions of chosen routes\u2019 attri-bute values in each OD pair [Identify ! Distribution].Thus, they can obtain an overview of riders\u2019 percep-tions of the route attributes and how biased the cho-sen attribute values are before modeling.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.",
      "data_code": { "geometry": 1, "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["Excluding", "AlgorithmicCalculation"],
        "componenet_code": ["excluding", "algorithmic_calculation"]
      },
      {
        "solution_text": "The route view shows the details of all routes taken between a particular OD pair, such as matched paths and route attributes. Unlike the aforementioned views, the route view allows users to take a detailed look at geographical dis- tribution or route attribute distribution within a single OD pair.",
        "solution_category": "visualization",
        "solution_axial": "coordinate",
        "solution_compoent": "map+path",
        "axial_code": ["coordinate"],
        "componenet_code": ["path", "map"]
      },
      {
        "solution_text": "Unlike the aforementioned views, the route view allows users to take a detailed look at geographical dis- tribution or route attribute distribution within a single OD pair.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 618,
    "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling",
    "pub_year": 2023,
    "domain": "urban planning",
    "requirement": {
      "requirement_text": "E3: Identify Attribute Distribution of Chosen Routes.Users inspect distributions of chosen routes\u2019 attri-bute values in each OD pair [Identify ! Distribution].Thus, they can obtain an overview of riders\u2019 percep-tions of the route attributes and how biased the cho-sen attribute values are before modeling.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.",
      "data_code": { "geometry": 1, "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["Excluding", "AlgorithmicCalculation"],
        "componenet_code": ["excluding", "algorithmic_calculation"]
      },
      {
        "solution_text": "Route Choice Behaviors. Before modeling, the experts attempted to hypothesize about route choice behavior by checking the distortion of the distribution of route attributes. In the OD bubble plot, the total mean nonpara- metric skew for Route Distance was about 0.23, indicating that route choices were biased toward routes with relatively short distances.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "bubble",
        "axial_code": ["Co-axis"],
        "componenet_code": ["bubble"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 619,
    "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling",
    "pub_year": 2023,
    "domain": "urban planning",
    "requirement": {
      "requirement_text": "M1: Perform Modeling with Different Sets of Hyperpara-meters. Users perform choice set generation andmodel estimation with various sets of hyperpara-meters to \ufb01nd a meaningful model instance with ahigh goodness of \ufb01t [Derive ! Model Instance].",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes; The general process of route choice modeling is twofold: choice set generation and model estimation. In this section, we will briefly describe the concept of each step and intro_x0002_duce the methods used in our study.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Excluding", "AlgorithmicCalculation", "Modeling"],
        "componenet_code": ["excluding", "algorithmic_calculation", "modeling"]
      },
      {
        "solution_text": "The configuration view allows users to specify hyperparameter configurations for the two procedures. The configuration view (Fig. 6B) allows users to produce hyperparameter configurations for choice set generation and model estimation. We chose a data-driven approach to gener_x0002_ate choice sets, where we cluster the observed routes (routes that are actually taken). Once the choice sets for all trips are generated, we fit a model that predicts the probability of routes being chosen from their characteristics (i.e., route attributes).",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "table+matrix+bar",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "bar", "matrix"]
      },
      {
        "solution_text": "The configuration view allows users to specify hyperparameter configurations for the two procedures.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 620,
    "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling",
    "pub_year": 2023,
    "domain": "urban planning",
    "requirement": {
      "requirement_text": "M2: Obtain an Overview of Model Instances. Usersobtain an overview of many different model instancesto identify their common or different patterns [Sum-marize ! Model Instance].",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes; The general process of route choice modeling is twofold: choice set generation and model estimation. In this section, we will briefly describe the concept of each step and intro_x0002_duce the methods used in our study.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Excluding", "AlgorithmicCalculation", "Modeling"],
        "componenet_code": ["excluding", "algorithmic_calculation", "modeling"]
      },
      {
        "solution_text": "The model view (Fig. 6C) supports an Overview+Detail approach for exploring the model instances. From the overview (Fig. 6C.1), users can grasp overall pat_x0002_terns of the model instances (Task M2).",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "scatter",
        "axial_code": ["Co-axis"],
        "componenet_code": ["scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 621,
    "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling",
    "pub_year": 2023,
    "domain": "urban planning",
    "requirement": {
      "requirement_text": "M3: Compare Model Instances. Users compare statis-tics and estimates between model instances to choosea model instance for explaining route choice behav-iors [Compare ! Model Instance].",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes; The general process of route choice modeling is twofold: choice set generation and model estimation. In this section, we will briefly describe the concept of each step and intro_x0002_duce the methods used in our study.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Excluding", "AlgorithmicCalculation", "Modeling"],
        "componenet_code": ["excluding", "algorithmic_calculation", "modeling"]
      },
      {
        "solution_text": "The model view (Fig. 6C) supports an Overview+Detail approach for exploring the model instances. From the detail (Fig. 6C.2), users can compare the instances with the help of the interactions, such as sorting, grouping, and hiding unnecessary results (Task M3). If there is an interesting model instance during the analysis, further investigation of the instance can be done in the reasoning interface.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "table+bar",
        "axial_code": ["Nesting"],
        "componenet_code": ["table", "bar"]
      },
      {
        "solution_text": "The model view (Fig. 6C) supports an Overview+Detail approach for exploring the model instances. From the detail (Fig. 6C.2), users can compare the instances with the help of the interactions, such as sorting, grouping, and hiding unnecessary results (Task M3). If there is an interesting model instance during the analysis, further investigation of the instance can be done in the reasoning interface.",
        "solution_category": "interaction",
        "solution_axial": "Reconfigure,Participation/Collaboration,Filtering",
        "solution_compoent": "",
        "axial_code": [
          "Reconfigure",
          "Participation/Collaboration",
          "Filtering"
        ],
        "componenet_code": [
          "reconfigure",
          "participation/collaboration",
          "filtering"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 622,
    "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling",
    "pub_year": 2023,
    "domain": "urban planning",
    "requirement": {
      "requirement_text": "M3: Compare Model Instances. Users compare statis-tics and estimates between model instances to choosea model instance for explaining route choice behav-iors [Compare ! Model Instance].",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.",
      "data_code": { "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "We found that the raw data had erroneous records, such as trips with missing fields. To clean the data, we referred to Wang et al. [18] and modified their cleaning criteria. We fil_x0002_tered out the trips that met one of the following conditions; After cleaning the trip dataset, we matched the path records with the street network of Seoul to reduce possible noise in GPS records. We used a well-known map matching algo_x0002_rithm, ST-Matching [34], to convert the raw path records to road network-bounded routes; The general process of route choice modeling is twofold: choice set generation and model estimation. In this section, we will briefly describe the concept of each step and intro_x0002_duce the methods used in our study.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,AlgorithmicCalculation,Modeling",
        "solution_compoent": "",
        "axial_code": ["Excluding", "AlgorithmicCalculation", "Modeling"],
        "componenet_code": ["excluding", "algorithmic_calculation", "modeling"]
      },
      {
        "solution_text": "For an effective comparison between the model instances(Task M3), the model instance table supports sorting orgrouping rows by each column or hiding rows that do notseem important. The columns representing numerical val-ues, such as the _x0016_r2 (rhoSB in the interface), can be used tosort rows. Other columns related to the set of hyperpara-meters, such as the k (LL), the set of distances (GG), or the setof model attributes (SS), can be used to group rows. Thecommon analysis scenario using grouping is to investigatethe effect of the set composition of model attributes (SS) onmodel instances; users can group by SS, as in Fig. 6C.2.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "table+bar",
        "axial_code": ["Nesting"],
        "componenet_code": ["table", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 623,
    "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling",
    "pub_year": 2023,
    "domain": "urban planning",
    "requirement": {
      "requirement_text": "R1: Discover Route Choices Contributing to an Estima-tion Result. Users discover trips and OD pairs thatfollow the model\u2019s estimated coef\ufb01cients well. Forexample, when a model instance has a negative coef-\ufb01cient for Route Distance, trips with a relatively shorttravel distance within their OD pair are deemed tocontribute to the estimation result. As users investi-gate such route choices, they aim to gain a deeperunderstanding of the model and better explain theroute choice behaviors [Summarize ! Trip Set].",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.",
      "data_code": { "geometry": 1, "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "To this end, users can brush OD pairs having high ECS in the OD bubble plot and closely inspect their characteristics in the map view or the station view. By doing so, users can determine which trips or OD pairs mainly contributed to estimating the coefficients.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 624,
    "paper_title": "RCMVis: A Visual Analytics System for Route Choice Modeling",
    "pub_year": 2023,
    "domain": "urban planning",
    "requirement": {
      "requirement_text": "R2: Re-estimate to Obtain Better Fitting Model Instances.An essential premise of RCM is that all individualroute choices have rationality. Therefore, if usersencounter route choices that seem irrational in rea-soning, they remove these trips or OD pairs and re-estimate the model. Their goal is to get a re\ufb01nedmodel instance that is well \ufb01tted to the data and betterre\ufb02ects riders\u2019 perceptions [Derive ! Model Instance].",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "We used a real-world bicycle trip dataset from the Seoul bike-sharing system [33]. The dataset included information on 210 K trips that took place in March 2018. Each trip con_x0002_sisted of GPS-tracked path records (recorded every minute), origin and destination stations, rental and return times, travel distance, and duration.",
      "data_code": { "geometry": 1, "tables": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "The reasoning interface facilitates re-estimation of the active model instance by applying more filtering conditions. The general workflow of the re-estimation pro- cess is shown in figure.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 0,
    "paper_title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts",
    "pub_year": 2023,
    "domain": "Disease",
    "requirement": {
      "requirement_text": "DG 1: Guided Analysis based on Three Modalities. To facilitate an effective workflow, with computational analysis support (specifically ML), our system should help the user prioritize more salient (1) features, (2) regions, and (3) subjects to choose data subsets for detailed analysis. The regions and features should be standard and interpretable so that experts can easily grasp the physiological basis, assimilate existing literature, and make hypotheses. Due to a large feature space relative to the number of scans, we must strive to avoid overfitting, reduce ranking instability, and highlight the uncertainties. ",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "MRI parameters, such as gradient direction, b-value, and voxel resolution, have a crucial impact on the scalar meas_x0002_urements used for a clinical study. To prevent errors, the MRI images provided by PPMI are collected based on stan_x0002_dardized and strict acquisition protocols developed by the steering committee on 3T Siemens scanners. Each visit includes DTI and T1-weighted images. For each DTI image, a 2D echo-planar DTI sequence is acquired with the following parameters: TR \u00bc 900 ms, TE \u00bc 88 ms, image matrix \u00bc 116 _x0003_ 116 _x0003_ 72 and voxel resolution \u00bc 1:98 _x0003_ 1:98_x0003_ 2 _x0003_ mm3, 64 gradient volumes (b \u00bc 1; 000s=mm2), and one non-gradient volume (b \u00bc 0s=mm2). The acquisition parame_x0002_ters for T1-weighted images are as follows: TR \u00bc 2; 300 ms, TE \u00bc 2:98 ms, image matrix\u00bc 160 _x0003_ 240 _x0003_ 256, and voxel res_x0002_olution \u00bc 1 _x0003_ 1 _x0003_ 1 _x0003_ mm3.",
      "data_code": { "media": 1 }
    },
    "solution": [
      {
        "solution_text": "Here describe the details of data processing, which con_x0002_sists of three steps: fiber tracking, feature extraction, and cohort formulation. These are performed outside of the VA system. 1) Fiber Tracking. This step generates white matter fiber tracts from the RAW images (from DTI to white matter fiber tracts in Fig. 1). We first convert the RAW images from the Digital Image and Communications in Medicine (DICOM) format to the Neuroimaging Informatics Technology Initia_x0002_tive (NIFTI) format. Then, we perform MRI data denoising and preprocessing, including eddy-current induced distor_x0002_tion correction, motion correction, and susceptibility induced distortion correction, using \u201cdwidenoise\u201d and \u201cdwipreproc\u201d scripts in MRtrix3, which is a recommended data cleaning process that uses FSL\u2019s \u201ceddy\u201d [69],\u201ctoppup\u201d [70], and \u201capplytopup\u201d [71] tools. This can reduce artifacts in MRI images and address many additional effects of noise during brain fiber reconstruction, such as the bias of fiber orientation estimation and error tracking of bifurcated fibers. Then, we fix magnet inhomogeneity (e.g., intensity loss and blurring) and perform image correction (e.g., eddy current correction and head motion correction) using the standard \u201crecon-all\u201d script in FreeSurfer [74]. Afterward, we align the T1-weighted images to the DTI images (intra_x0002_subject registration) using FSL [72]. Intra-subject registra_x0002_tion reduces the distortion in the anatomical structure of fibers extracted from the region of interest (ROI) in a subject. We also perform inter-subject registration using FSL, which applies the standard template (MNI152) to each of the sub_x0002_ject\u2019s MRI images. After intra-registration and inter-registra_x0002_tion, we perform brain parcellation using FSL, which splits the brain into regions. The parcellation based on Free_x0002_Surfer\u2019s default atlas (the Desikan/Killiany cortical atlas), which consists of 42 cortical regions [73]. We then perform brain fiber tractography using a state-of-the-art framework [7], which can facilitate biologically plausible fiber recon_x0002_struction and provide anatomically reliable brain fiber tracts. Afterward, by referring to the brain parcellation information, we can categorize fiber tracts and obtain the corresponding features at the whole-brain level and brain region level.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "The diffusion measures we compute with FreeSurfer include: the raw T2 signal (S0), the eigen values (1;2, and3) representing diffusion in the directions of each of the three eigen vectors of the diffusion tensor, fractional anisotropy (FA), mode of anisotropy (MO), and mean diffu-sivity (MD). With MRtrix3, we also compute the other met-rics, including radial diffusivity (RD), relative anisotropy(RA), axial diffusivity (AD), and the Westin metrics (linear-ity (CL), planarity (CP), and sphericity (CS)). O\u2019Donnellet al. provide the definitions of these measures [77].We use MRtrix3 and FreeSurfer to bundle fibers based on which cortical regions are passed by each fiber. Here, we use FreeSurfer to apply the cortical structure parcellation,which assigns a neuro anatomical label to each corticalregion.Also, since PD has been reported to start from one region and then spread to others, we further divide the bundles into two categories: intra- and inter-connects by referring to the information of passed cortical regions. Intra-connects (or intra-parcel connections) represent connections that both start and end within the same cortical region while the inter-connects (or inter-parcel connections) represent the connec-tions that start and end different cortical regions. Each corti-cal region has both intra- and inter-connects. Note that this definition of inter-connects does not refer to fibers that con-nect the two hemispheres (e.g., commissural fibers).Also, since cortical asymmetry and hemispheric predom-inance have been discovered in neuro degenerative disease[78], the features \u2018Delta-LR\u2019 are extracted to represent asym-metry between the left and right hemispheres. The tract-based features include two categories: cortical region meas-ures and whole brain measures. The former includes the number of fibers, average fiber length, intra- and inter-fiber numbers, intra- and inter-fiber lengths, and \u2018Delta-LR\u2019 aver-age fiber length. The latter includes the number of associa-tion fibers, projection fibers, commissural fibers, and the fibers in each brain lobe. Tensor-based features are averagedover the different bundles of fibers.  Our choice of features is motivated by literature review and to favor interpretability, follow standard conventions,and support fiber-tract-based analysis.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 1,
    "paper_title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts",
    "pub_year": 2023,
    "domain": "Disease",
    "requirement": {
      "requirement_text": "DG 1: Guided Analysis based on Three Modalities. To facilitate an effective workflow, with computational analysis support (specifically ML), our system should help the user prioritize more salient (1) features, (2) regions, and (3) subjects to choose data subsets for detailed analysis. The regions and features should be standard and interpretable so that experts can easily grasp the physiological basis, assimilate existing literature, and make hypotheses. Due to a large feature space relative to the number of scans, we must strive to avoid overfitting, reduce ranking instability, and highlight the uncertainties. ",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "MRI parameters, such as gradient direction, b-value, and voxel resolution, have a crucial impact on the scalar meas_x0002_urements used for a clinical study. To prevent errors, the MRI images provided by PPMI are collected based on stan_x0002_dardized and strict acquisition protocols developed by the steering committee on 3T Siemens scanners. Each visit includes DTI and T1-weighted images. For each DTI image, a 2D echo-planar DTI sequence is acquired with the following parameters: TR \u00bc 900 ms, TE \u00bc 88 ms, image matrix \u00bc 116 _x0003_ 116 _x0003_ 72 and voxel resolution \u00bc 1:98 _x0003_ 1:98_x0003_ 2 _x0003_ mm3, 64 gradient volumes (b \u00bc 1; 000s=mm2), and one non-gradient volume (b \u00bc 0s=mm2). The acquisition parame_x0002_ters for T1-weighted images are as follows: TR \u00bc 2; 300 ms, TE \u00bc 2:98 ms, image matrix\u00bc 160 _x0003_ 240 _x0003_ 256, and voxel res_x0002_olution \u00bc 1 _x0003_ 1 _x0003_ 1 _x0003_ mm3.",
      "data_code": { "media": 1 }
    },
    "solution": [
      {
        "solution_text": "Here describe the details of data processing, which con_x0002_sists of three steps: fiber tracking, feature extraction, and cohort formulation. These are performed outside of the VA system. 1) Fiber Tracking. This step generates white matter fiber tracts from the RAW images (from DTI to white matter fiber tracts in Fig. 1). We first convert the RAW images from the Digital Image and Communications in Medicine (DICOM) format to the Neuroimaging Informatics Technology Initia_x0002_tive (NIFTI) format. Then, we perform MRI data denoising and preprocessing, including eddy-current induced distor_x0002_tion correction, motion correction, and susceptibility induced distortion correction, using \u201cdwidenoise\u201d and \u201cdwipreproc\u201d scripts in MRtrix3, which is a recommended data cleaning process that uses FSL\u2019s \u201ceddy\u201d [69],\u201ctoppup\u201d [70], and \u201capplytopup\u201d [71] tools. This can reduce artifacts in MRI images and address many additional effects of noise during brain fiber reconstruction, such as the bias of fiber orientation estimation and error tracking of bifurcated fibers. Then, we fix magnet inhomogeneity (e.g., intensity loss and blurring) and perform image correction (e.g., eddy current correction and head motion correction) using the standard \u201crecon-all\u201d script in FreeSurfer [74]. Afterward, we align the T1-weighted images to the DTI images (intra_x0002_subject registration) using FSL [72]. Intra-subject registra_x0002_tion reduces the distortion in the anatomical structure of fibers extracted from the region of interest (ROI) in a subject. We also perform inter-subject registration using FSL, which applies the standard template (MNI152) to each of the sub_x0002_ject\u2019s MRI images. After intra-registration and inter-registra_x0002_tion, we perform brain parcellation using FSL, which splits the brain into regions. The parcellation based on Free_x0002_Surfer\u2019s default atlas (the Desikan/Killiany cortical atlas), which consists of 42 cortical regions [73]. We then perform brain fiber tractography using a state-of-the-art framework [7], which can facilitate biologically plausible fiber recon_x0002_struction and provide anatomically reliable brain fiber tracts. Afterward, by referring to the brain parcellation information, we can categorize fiber tracts and obtain the corresponding features at the whole-brain level and brain region level.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Feature Extraction. We extract fiber features from the constructed brain fiber tracts (Fig. 1) and diffusion tensors, using MRtrix3 [28] and FreeSurfer [74]. The features fall into two categories: tract-based and tensor-based. The for_x0002_mer measures regional fiber structures (e.g., density and length) while the latter measures water diffusivity patterns based on a tensor model. Evidence suggests that both cate_x0002_gories are affected by neurodegenerative disease [75], [76]. The diffusion measures we compute with FreeSurfer include: the raw T2 signal (S0), the eigenvalues (_x0002_ 1; _x0002_2, and 3) representing diffusion in the directions of each of the three eigenvectors of the diffusion tensor, fractional anisotropy (FA), mode of anisotropy (MO), and mean diffu_x0002_sivity (MD). With MRtrix3, we also compute the other met_x0002_rics, including radial diffusivity (RD), relative anisotropy (RA), axial diffusivity (AD), and the Westin metrics (linear_x0002_ity (CL), planarity (CP), and sphericity (CS)). O\u2019Donnell et al. provide the definitions of these measures [77]. We use MRtrix3 and FreeSurfer to bundle fibers based on which cortical regions are passed by each fiber. Here, we use FreeSurfer to apply the cortical structure parcellation, which assigns a neuroanatomical label to each cortical region. Also, since PD has been reported to start from one region and then spread to others, we further divide the bundles into two categories: intra- and inter-connects by referring to the information of passed cortical regions. Intra-connects (or intra-parcel connections) represent connections that both start and end within the same cortical region while the inter_x0002_connects (or inter-parcel connections) represent the connec_x0002_tions that start and end different cortical regions. Each corti_x0002_cal region has both intra- and inter-connects. Note that this definition of inter-connects does not refer to fibers that con_x0002_nect the two hemispheres (e.g., commissural fibers). Also, since cortical asymmetry and hemispheric predom_x0002_inance have been discovered in neurodegenerative disease [78], the features \u2018Delta-LR\u2019 are extracted to represent asym_x0002_metry between the left and right hemispheres. The tract_x0002_based features include two categories: cortical region meas_x0002_ures and whole brain measures. The former includes the number of fibers, average fiber length, intra- and inter-fiber numbers, intra- and inter-fiber lengths, and \u2018Delta-LR\u2019 aver_x0002_age fiber length. The latter includes the number of associa_x0002_tion fibers, projection fibers, commissural fibers, and the fibers in each brain lobe. Tensor-based features are averaged over the different bundles of fibers. Our choice of features is motivated by literature review and to favor interpretability, follow standard conventions, and support fiber-tract-based analysis (DG1).",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection"],
        "componenet_code": ["feature_selection"]
      },
      {
        "solution_text": "Cohort Formation. This step formulates cohort data (the table in Fig. 1) from all scans and their attributes, including the extracted features and the corresponding demographics(age and gender) as well as their annotations, including a label of their brain status (e.g., PD or HC) and visits of scan_x0002_ning MRI. Each subject has multiple scans if they have mul_x0002_tiple visits. The formulated cohort data is used in the ML learning pipeline described below. Note that we only use the label and extracted features to train the ML models, while the demographics and visit dates provide context when displaying the ML results.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "The goal of our ML pipeline is to guide the user to effec_x0002_tively explore the data by providing measures of saliency for each feature, region, and subject (DG1). The feature saliency indicates how strongly the corresponding aspect (e.g., fiber length) relates to, for example, the differences of scans with different labels (i.e., PD or HC). The ML pipeline, corresponding to \u201cSalience Guided Exploration Interface\u201d in Fig. 2, is described in detail in Fig. 3. The pipeline\u2019s input is the cohort data generated in Section 4.2 and the outputs are feature scores, region scores, and subject/scan class probabilities. The whole pipeline is executed inside CV iterations. In each CV iteration, we exe_x0002_cute feature ranking and binary classification to obtain the saliency measures. Then, we produce the averages and stan_x0002_dard deviations of the scores over all iterations as the final outputs. In the following, we describe the details. One of our objectives is subject-level exploration (DG1) using probabilistic predictions. However, both bootstrap_x0002_ping and repeated randomized CV cannot guarantee that each scan appears in a test set an equal number of times. Standard k-fold CV guarantees this but may suffer from sensitivity to variance (which is a particular problem in our domain). For these reasons, we use an extension of k-fold CV, which is performed t times with randomization, result_x0002_ing t _x0003_ k iterations in total. This allows equal testing of scans (t _x0003_ \u00f0k _x0004_ 1\u00de each)\u3002 This stage relies on a binary classification model that learns a function f\u00f0X\u00de \u00bc ^y, where X is a matrix of the cohort data (Fig. 1), with which rows and columns represent scans and attributes respectively, and ^y is a prediction as to the true class labels, y, that the scans belong to. In our case, we obtain probabilistic predictions as to whether the scan belongs to the disease (PD) or healthy (HC) group, which are used as saliency measures representing an estimation of how closely the scan exhibits patterns that are associated with the disease in the given features. The probabilities are then thresholded at 0.5 to obtain the binary prediction.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "This stage relies on a binary classification model that learns a function f\u00f0X\u00de \u00bc ^y, where X is a matrix of the cohort data(Fig. 1), with which rows and columns represent scans and attributes respectively, and ^y is a prediction as to the true class labels, y, that the scans belong to. In our case, we obtain probabilistic predictions as to whether the scan belongs to the disease (PD) or healthy (HC) group, which are used as saliency measures representing an estimation of how closely the scan exhibits patterns that are associated with the disease in the given features. The probabilities are then thresholded at 0.5 to obtain the binary prediction. Since the input X is a standard form that is compatible with many classification models, we can use many different models (e.g., SVM, decision trees, and neural networks). As a default model and the one used through this paper, we use a linear SVM. This model is popular due to its high performance with various data (including neurological data [64], [81]), robustness against overfitting, and ability to return class probabilities (rather than just predictions). With all of these qualities, a linear SVM is a good model for our domain and the objectives stated in DG1.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 2,
    "paper_title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts",
    "pub_year": 2023,
    "domain": "Disease",
    "requirement": {
      "requirement_text": "DG 2: Quality Visualization For anatomical understanding, VA of the fiber tracts and salient variables of Brain Fibers. For anatomical understanding, VA of the fiber tracts and salient variables in the physical space is required. High-quality graphics rendering can help understand the spatial relations of brain fibers. While the rendering should be effective at showing the structure, it should be also efficient enough to interactively render multiple large fiber sets.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "MRI parameters, such as gradient direction, b-value, and voxel resolution, have a crucial impact on the scalar meas_x0002_urements used for a clinical study. To prevent errors, the MRI images provided by PPMI are collected based on stan_x0002_dardized and strict acquisition protocols developed by the steering committee on 3T Siemens scanners. Each visit includes DTI and T1-weighted images. For each DTI image, a 2D echo-planar DTI sequence is acquired with the following parameters: TR \u00bc 900 ms, TE \u00bc 88 ms, image matrix \u00bc 116 _x0003_ 116 _x0003_ 72 and voxel resolution \u00bc 1:98 _x0003_ 1:98_x0003_ 2 _x0003_ mm3, 64 gradient volumes (b \u00bc 1; 000s=mm2), and one non-gradient volume (b \u00bc 0s=mm2). The acquisition parame_x0002_ters for T1-weighted images are as follows: TR \u00bc 2; 300 ms, TE \u00bc 2:98 ms, image matrix\u00bc 160 _x0003_ 240 _x0003_ 256, and voxel res_x0002_olution \u00bc 1 _x0003_ 1 _x0003_ 1 _x0003_ mm3.",
      "data_code": { "media": 1 }
    },
    "solution": [
      {
        "solution_text": "The brain fibers are rendered (Fig. 5) as path tubes with SSAO, which produces a high-quality visualization with an enhanced spatial perception [35], [36]. The path tubes are constructed on the fly through the GPU rendering pipeline in the geometry shader. This allows the path tubes to be constructed and rendered quickly with an interactively adjustable radius without additional memory overhead.  ",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "The brain fibers are rendered (Fig. 5) as path tubes with SSAO, which produces a high-quality visualization with an enhanced spatial perception [35], [36]. The path tubes are constructed on the fly through the GPU rendering pipeline in the geometry shader. This allows the path tubes to be constructed and rendered quickly with an interactively adjustable radius without additional memory overhead. The scans/subjects, regions, and features selected from their respective exploration modules, automatically determine which fibers are rendered and which features are used for color mapping. For example, when one brain region is selected, this view only renders the fibers related to the selected region. In addition to direct color mapping without value scaling, we provide two scaling options: contrastive color mapping\u2014using the difference from the mean value of all the given measures in a specific brain region over the entire HC group\u2014to emphasize anomaly and logarithmic scaling to better reflect subtle value differences. However, it is important to understand that one cannot find a direct fiber-to-fiber correspondence between subjects. These design decisions reflect DG2 by providing a high-quality visualization of fiber-microstructure with interactive framerates for large sets of brain fibers.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Symmetrical",
        "solution_compoent": "Others",
        "axial_code": ["Juxtaposition-Similar-Symmetrical"],
        "componenet_code": ["Others"]
      },
      {
        "solution_text": "In addition to direct color mapping without value scaling, we provide two scaling options: contrastive color mapping\u2014using the difference from the mean value of all the given measures in a specific brain region over the entire HC group\u2014to emphasize anomaly and logarithmic scaling to better reflect subtle value differences. However, it is important to understand that one cannot find a direct fiber-to-fiber correspondence between subjects. These design decisions reflect DG2 by providing a high-quality visualization of fiber-microstructure with interactive framerates for large sets of brain fibers.",
        "solution_category": "interaction",
        "solution_axial": "Filtering+Extractionoffeatures",
        "solution_compoent": "",
        "axial_code": ["Extractionoffeatures", "Filtering"],
        "componenet_code": ["extraction_of_features", "filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 3,
    "paper_title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts",
    "pub_year": 2023,
    "domain": "Disease",
    "requirement": {
      "requirement_text": "DG 3: Modalities for Comparison.To address the hypothesis generation, it is fundamental to perform the comparison of different subjects and/or groups with different modalities and time steps. Through the comparison, experts can relate diverse aspects to clinical outcomes (e.g., healthy versus PD). As we utilize ML for the comparison, the system should also depict the relationships between data and the prediction.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "MRI parameters, such as gradient direction, b-value, and voxel resolution, have a crucial impact on the scalar meas_x0002_urements used for a clinical study. To prevent errors, the MRI images provided by PPMI are collected based on stan_x0002_dardized and strict acquisition protocols developed by the steering committee on 3T Siemens scanners. Each visit includes DTI and T1-weighted images. For each DTI image, a 2D echo-planar DTI sequence is acquired with the following parameters: TR \u00bc 900 ms, TE \u00bc 88 ms, image matrix \u00bc 116 _x0003_ 116 _x0003_ 72 and voxel resolution \u00bc 1:98 _x0003_ 1:98_x0003_ 2 _x0003_ mm3, 64 gradient volumes (b \u00bc 1; 000s=mm2), and one non-gradient volume (b \u00bc 0s=mm2). The acquisition parame_x0002_ters for T1-weighted images are as follows: TR \u00bc 2; 300 ms, TE \u00bc 2:98 ms, image matrix\u00bc 160 _x0003_ 240 _x0003_ 256, and voxel res_x0002_olution \u00bc 1 _x0003_ 1 _x0003_ 1 _x0003_ mm3.",
      "data_code": { "media": 1 }
    },
    "solution": [
      {
        "solution_text": "Here describe the details of data processing, which con_x0002_sists of three steps: fiber tracking, feature extraction, and cohort formulation. These are performed outside of the VA system. 1) Fiber Tracking. This step generates white matter fiber tracts from the RAW images (from DTI to white matter fiber tracts in Fig. 1). We first convert the RAW images from the Digital Image and Communications in Medicine (DICOM) format to the Neuroimaging Informatics Technology Initia_x0002_tive (NIFTI) format. Then, we perform MRI data denoising and preprocessing, including eddy-current induced distor_x0002_tion correction, motion correction, and susceptibility induced distortion correction, using \u201cdwidenoise\u201d and \u201cdwipreproc\u201d scripts in MRtrix3, which is a recommended data cleaning process that uses FSL\u2019s \u201ceddy\u201d [69],\u201ctoppup\u201d [70], and \u201capplytopup\u201d [71] tools. This can reduce artifacts in MRI images and address many additional effects of noise during brain fiber reconstruction, such as the bias of fiber orientation estimation and error tracking of bifurcated fibers. Then, we fix magnet inhomogeneity (e.g., intensity loss and blurring) and perform image correction (e.g., eddy current correction and head motion correction) using the standard \u201crecon-all\u201d script in FreeSurfer [74]. Afterward, we align the T1-weighted images to the DTI images (intra_x0002_subject registration) using FSL [72]. Intra-subject registra_x0002_tion reduces the distortion in the anatomical structure of fibers extracted from the region of interest (ROI) in a subject. We also perform inter-subject registration using FSL, which applies the standard template (MNI152) to each of the sub_x0002_ject\u2019s MRI images. After intra-registration and inter-registra_x0002_tion, we perform brain parcellation using FSL, which splits the brain into regions. The parcellation based on Free_x0002_Surfer\u2019s default atlas (the Desikan/Killiany cortical atlas), which consists of 42 cortical regions [73]. We then perform brain fiber tractography using a state-of-the-art framework [7], which can facilitate biologically plausible fiber recon_x0002_struction and provide anatomically reliable brain fiber tracts. Afterward, by referring to the brain parcellation information, we can categorize fiber tracts and obtain the corresponding features at the whole-brain level and brain region level.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Feature Extraction. We extract fiber features from the constructed brain fiber tracts (Fig. 1) and diffusion tensors, using MRtrix3 [28] and FreeSurfer [74]. The features fall into two categories: tract-based and tensor-based. The for_x0002_mer measures regional fiber structures (e.g., density and length) while the latter measures water diffusivity patterns based on a tensor model. Evidence suggests that both cate_x0002_gories are affected by neurodegenerative disease [75], [76]. The diffusion measures we compute with FreeSurfer include: the raw T2 signal (S0), the eigenvalues (_x0002_ 1; _x0002_2, and 3) representing diffusion in the directions of each of the three eigenvectors of the diffusion tensor, fractional anisotropy (FA), mode of anisotropy (MO), and mean diffu_x0002_sivity (MD). With MRtrix3, we also compute the other met_x0002_rics, including radial diffusivity (RD), relative anisotropy (RA), axial diffusivity (AD), and the Westin metrics (linear_x0002_ity (CL), planarity (CP), and sphericity (CS)). O\u2019Donnell et al. provide the definitions of these measures [77]. We use MRtrix3 and FreeSurfer to bundle fibers based on which cortical regions are passed by each fiber. Here, we use FreeSurfer to apply the cortical structure parcellation, which assigns a neuroanatomical label to each cortical region. Also, since PD has been reported to start from one region and then spread to others, we further divide the bundles into two categories: intra- and inter-connects by referring to the information of passed cortical regions. Intra-connects (or intra-parcel connections) represent connections that both start and end within the same cortical region while the inter_x0002_connects (or inter-parcel connections) represent the connec_x0002_tions that start and end different cortical regions. Each corti_x0002_cal region has both intra- and inter-connects. Note that this definition of inter-connects does not refer to fibers that con_x0002_nect the two hemispheres (e.g., commissural fibers). Also, since cortical asymmetry and hemispheric predom_x0002_inance have been discovered in neurodegenerative disease [78], the features \u2018Delta-LR\u2019 are extracted to represent asym_x0002_metry between the left and right hemispheres. The tract_x0002_based features include two categories: cortical region meas_x0002_ures and whole brain measures. The former includes the number of fibers, average fiber length, intra- and inter-fiber numbers, intra- and inter-fiber lengths, and \u2018Delta-LR\u2019 aver_x0002_age fiber length. The latter includes the number of associa_x0002_tion fibers, projection fibers, commissural fibers, and the fibers in each brain lobe. Tensor-based features are averaged over the different bundles of fibers. Our choice of features is motivated by literature review and to favor interpretability, follow standard conventions, and support fiber-tract-based analysis (DG1).",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection"],
        "componenet_code": ["feature_selection"]
      },
      {
        "solution_text": "Cohort Formation. This step formulates cohort data (the table in Fig. 1) from all scans and their attributes, including the extracted features and the corresponding demographics(age and gender) as well as their annotations, including a label of their brain status (e.g., PD or HC) and visits of scan_x0002_ning MRI. Each subject has multiple scans if they have mul_x0002_tiple visits. The formulated cohort data is used in the ML learning pipeline described below. Note that we only use the label and extracted features to train the ML models, while the demographics and visit dates provide context when displaying the ML results.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "The goal of our ML pipeline is to guide the user to effec_x0002_tively explore the data by providing measures of saliency for each feature, region, and subject (DG1). The feature saliency indicates how strongly the corresponding aspect (e.g., fiber length) relates to, for example, the differences of scans with different labels (i.e., PD or HC). The ML pipeline, corresponding to \u201cSalience Guided Exploration Interface\u201d in Fig. 2, is described in detail in Fig. 3. The pipeline\u2019s input is the cohort data generated in Section 4.2 and the outputs are feature scores, region scores, and subject/scan class probabilities. The whole pipeline is executed inside CV iterations. In each CV iteration, we exe_x0002_cute feature ranking and binary classification to obtain the saliency measures. Then, we produce the averages and stan_x0002_dard deviations of the scores over all iterations as the final outputs. In the following, we describe the details. One of our objectives is subject-level exploration (DG1) using probabilistic predictions. However, both bootstrap_x0002_ping and repeated randomized CV cannot guarantee that each scan appears in a test set an equal number of times. Standard k-fold CV guarantees this but may suffer from sensitivity to variance (which is a particular problem in our domain). For these reasons, we use an extension of k-fold CV, which is performed t times with randomization, result_x0002_ing t _x0003_ k iterations in total. This allows equal testing of scans (t _x0003_ \u00f0k _x0004_ 1\u00de each)\u3002 This stage relies on a binary classification model that learns a function f\u00f0X\u00de \u00bc ^y, where X is a matrix of the cohort data (Fig. 1), with which rows and columns represent scans and attributes respectively, and ^y is a prediction as to the true class labels, y, that the scans belong to. In our case, we obtain probabilistic predictions as to whether the scan belongs to the disease (PD) or healthy (HC) group, which are used as saliency measures representing an estimation of how closely the scan exhibits patterns that are associated with the disease in the given features. The probabilities are then thresholded at 0.5 to obtain the binary prediction.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "The average predicted class probabilities and their standard deviations over all CV iterations are also shown in the table view and guide across-subject exploration. This can aid the comparison and hypothesis generation (DG3) in a number of ways, including: model failure (why some scans do not fit the model, possible confounding conditions), model success (an obvious case of neurodegenerative expression might be found), model ambiguity (subtle expression might be found through VA or different features might be needed to disambiguate these subjects). In addition, we want to explore the same subject\u2019s expression across time. Changes in prediction across time can guide clinical analysis and disease progression.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Symmetrical",
        "solution_compoent": "Table",
        "axial_code": ["Juxtaposition-Similar-Symmetrical"],
        "componenet_code": ["Table"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 4,
    "paper_title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts",
    "pub_year": 2023,
    "domain": "Disease",
    "requirement": {
      "requirement_text": "DG 3: Modalities for Comparison.To address the hypothesis generation, it is fundamental to perform the comparison of different subjects and/or groups with different modalities and time steps. Through the comparison, experts can relate diverse aspects to clinical outcomes (e.g., healthy versus PD). As we utilize ML for the comparison, the system should also depict the relationships between data and the prediction.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "MRI parameters, such as gradient direction, b-value, and voxel resolution, have a crucial impact on the scalar meas_x0002_urements used for a clinical study. To prevent errors, the MRI images provided by PPMI are collected based on stan_x0002_dardized and strict acquisition protocols developed by the steering committee on 3T Siemens scanners. Each visit includes DTI and T1-weighted images. For each DTI image, a 2D echo-planar DTI sequence is acquired with the following parameters: TR \u00bc 900 ms, TE \u00bc 88 ms, image matrix \u00bc 116 _x0003_ 116 _x0003_ 72 and voxel resolution \u00bc 1:98 _x0003_ 1:98_x0003_ 2 _x0003_ mm3, 64 gradient volumes (b \u00bc 1; 000s=mm2), and one non-gradient volume (b \u00bc 0s=mm2). The acquisition parame_x0002_ters for T1-weighted images are as follows: TR \u00bc 2; 300 ms, TE \u00bc 2:98 ms, image matrix\u00bc 160 _x0003_ 240 _x0003_ 256, and voxel res_x0002_olution \u00bc 1 _x0003_ 1 _x0003_ 1 _x0003_ mm3.",
      "data_code": { "media": 1 }
    },
    "solution": [
      {
        "solution_text": "The brain fibers are rendered (Fig. 5) as path tubes with SSAO, which produces a high-quality visualization with an enhanced spatial perception [35], [36]. The path tubes are constructed on the fly through the GPU rendering pipeline in the geometry shader. This allows the path tubes to be constructed and rendered quickly with an interactively adjustable radius without additional memory overhead.  ",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "The comparative visual analysis (DG3, Fig. 2 D and 2 E) is performed with fiber rendering and linked information visualization modules. These views show the information related to features, regions, and scans/subjects of interest that are selected from the exploration modules.The 3D fiber rendering module facilitates comparative analysis together with the linked information visualization module. To aid comparison, we provide two views (as shown in Fig. 2 E) and use the same colormap across all the selected scans from the other modules by referring to a global value range across the scans. Also, the contrastive color mapping helps supports the comparison of the individuals against the HC group, which may better emphasize important differences.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Symmetrical",
        "solution_compoent": "Others",
        "axial_code": ["Juxtaposition-Similar-Symmetrical"],
        "componenet_code": ["Others"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 5,
    "paper_title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts",
    "pub_year": 2023,
    "domain": "Disease",
    "requirement": {
      "requirement_text": "DG 3: Modalities for Comparison.To address the hypothesis generation, it is fundamental to perform the comparison of different subjects and/or groups with different modalities and time steps. Through the comparison, experts can relate diverse aspects to clinical outcomes (e.g., healthy versus PD). As we utilize ML for the comparison, the system should also depict the relationships between data and the prediction.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "MRI parameters, such as gradient direction, b-value, and voxel resolution, have a crucial impact on the scalar meas_x0002_urements used for a clinical study. To prevent errors, the MRI images provided by PPMI are collected based on stan_x0002_dardized and strict acquisition protocols developed by the steering committee on 3T Siemens scanners. Each visit includes DTI and T1-weighted images. For each DTI image, a 2D echo-planar DTI sequence is acquired with the following parameters: TR \u00bc 900 ms, TE \u00bc 88 ms, image matrix \u00bc 116 _x0003_ 116 _x0003_ 72 and voxel resolution \u00bc 1:98 _x0003_ 1:98_x0003_ 2 _x0003_ mm3, 64 gradient volumes (b \u00bc 1; 000s=mm2), and one non-gradient volume (b \u00bc 0s=mm2). The acquisition parame_x0002_ters for T1-weighted images are as follows: TR \u00bc 2; 300 ms, TE \u00bc 2:98 ms, image matrix\u00bc 160 _x0003_ 240 _x0003_ 256, and voxel res_x0002_olution \u00bc 1 _x0003_ 1 _x0003_ 1 _x0003_ mm3.",
      "data_code": { "media": 1 }
    },
    "solution": [
      {
        "solution_text": "The brain fibers are rendered (Fig. 5) as path tubes with SSAO, which produces a high-quality visualization with an enhanced spatial perception [35], [36]. The path tubes are constructed on the fly through the GPU rendering pipeline in the geometry shader. This allows the path tubes to be constructed and rendered quickly with an interactively adjustable radius without additional memory overhead.  ",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "The comparative visual analysis (DG3, Fig. 2 D and 2 E) is performed with fiber rendering and linked information visualization modules. These views show the information related to features, regions, and scans/subjects of interest that are selected from the exploration modules.The 3D fiber rendering module facilitates comparative analysis together with the linked information visualization module. To aid comparison, we provide two views (as shown in Fig. 2 E) and use the same colormap across all the selected scans from the other modules by referring to a global value range across the scans. Also, the contrastive color mapping helps supports the comparison of the individuals against the HC group, which may better emphasize important differences.",
        "solution_category": "visualization",
        "solution_axial": "Non-composite",
        "solution_compoent": "Line+Scatter+Bar",
        "axial_code": ["Non-composite"],
        "componenet_code": ["Scatter", "Bar", "Line"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 6,
    "paper_title": "A Predictive Visual Analytics System for Studying Neurodegenerative Disease Based on DTI Fiber Tracts",
    "pub_year": 2023,
    "domain": "Disease",
    "requirement": {
      "requirement_text": "DG 4: Easy Non-Linear Exploration. Due to a large number of features, fiber tracts, and subjects, diverse aspects could be explored. The analytical process may proceed and change according to emerged patterns or discovered knowledge during the analysis. Therefore, our system should provide an intuitive and fully interactive UI to serve the neuroscientists\u2019 changing analysis needs.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "MRI parameters, such as gradient direction, b-value, and voxel resolution, have a crucial impact on the scalar meas_x0002_urements used for a clinical study. To prevent errors, the MRI images provided by PPMI are collected based on stan_x0002_dardized and strict acquisition protocols developed by the steering committee on 3T Siemens scanners. Each visit includes DTI and T1-weighted images. For each DTI image, a 2D echo-planar DTI sequence is acquired with the following parameters: TR \u00bc 900 ms, TE \u00bc 88 ms, image matrix \u00bc 116 _x0003_ 116 _x0003_ 72 and voxel resolution \u00bc 1:98 _x0003_ 1:98_x0003_ 2 _x0003_ mm3, 64 gradient volumes (b \u00bc 1; 000s=mm2), and one non-gradient volume (b \u00bc 0s=mm2). The acquisition parame_x0002_ters for T1-weighted images are as follows: TR \u00bc 2; 300 ms, TE \u00bc 2:98 ms, image matrix\u00bc 160 _x0003_ 240 _x0003_ 256, and voxel res_x0002_olution \u00bc 1 _x0003_ 1 _x0003_ 1 _x0003_ mm3.",
      "data_code": { "media": 1 }
    },
    "solution": [
      {
        "solution_text": "Since each region has its own set of features and feature saliency measures, when a region is selected, all other related views are updated (e.g., the feature exploration module). One design consideration is whether to compute the saliencies for all regions at once or on demand. To make the VA process interactive and support easy non-linear exploration (DG4), we choose to do it all at once as this way can avoid waiting time when the user interactively explores different brain regions.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 7,
    "paper_title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches",
    "pub_year": 2022,
    "domain": "Feature selection",
    "requirement": {
      "requirement_text": "G1: Division of Data Space Into Slices Based on Predicted Probabilities, for Transparent Local Feature Contribution.Our goal is to assist in the search for distinctive features that might contribute more to instances that are harder or easier to classify. By splitting the data space into quadrants, we aim to assure that users\u2019 interference with the engineering of specific features does not cause problems in key parts of the data space. The tool should begin by training an initial model with the original features of each data set, which will be a starting point (i.e., \u201cstate zero\u201d) for users to compare future interactions and take further decisions.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "Features",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "quantitative": 1,
        "textual": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "In FeatureEnVi, data instances are sorted according to the predicted probability of belonging to the ground truth class, as shown in Fig. 1a. The initial step before the exploration of features is to pre-train the XGBoost [29] on the original pool of features, and then divide the data space into four groups automatically (i.e., Worst, Bad, Good, and Best). The vertical black line is the stable threshold anchored precisely at 50% predictive probability, which separates the correctly from the wrongly classified instances. The other two thresholds partition the prior subspace into their half areas for the default option. However, the user can alter the vertical gray lines as indicated in Fig. 5a.1\u2013a.4, with a degree of freedom set to =t20% from the defaults. The vertical positioning of the instances is purely used to avoid\u2014as much as possible\u2014overlapping/cluttering issues via jittering. The data space will always be divided into four parts conveying extra information to the user. If no instances belong to a slice of the data space, the system works normally, but there will be no values for the statistical measures (see Section 4.3). Overall, the user\u2019s goal is to move as many instances as possible from the left side (Worst and Bad subspaces) to the right side (Good and Best subspaces) while avoiding the opposite. Nevertheless, the primary purpose of this view is to provide better local and global explainabilities of the impact of features according to the user-defined slices.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 8,
    "paper_title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches",
    "pub_year": 2022,
    "domain": "Feature selection",
    "requirement": {
      "requirement_text": "G2: Deployment of Different Types of Feature Selection Techniques to Support Stepwise Selection. There are several different techniques for computing feature importance that produce diverse outcomes per feature. The tool should facilitate the visual comparison of alternative feature selection techniques for each feature (T2). Another key point is that users should have the ability to include and exclude features during the entire exploration phase.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "Features",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "quantitative": 1,
        "textual": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "In FeatureEnVi, data instances are sorted according to the predicted probability of belonging to the ground truth class, as shown in Fig. 1a. The initial step before the exploration of features is to pre-train the XGBoost [29] on the original pool of features, and then divide the data space into four groups automatically (i.e., Worst, Bad, Good, and Best). The vertical black line is the stable threshold anchored precisely at 50% predictive probability, which separates the correctly from the wrongly classified instances. The other two thresholds partition the prior subspace into their half areas for the default option. However, the user can alter the vertical gray lines as indicated in Fig. 5a.1\u2013a.4, with a degree of freedom set to =t20% from the defaults. The vertical positioning of the instances is purely used to avoid\u2014as much as possible\u2014overlapping/cluttering issues via jittering. The data space will always be divided into four parts conveying extra information to the user. If no instances belong to a slice of the data space, the system works normally, but there will be no values for the statistical measures (see Section 4.3). Overall, the user\u2019s goal is to move as many instances as possible from the left side (Worst and Bad subspaces) to the right side (Good and Best subspaces) while avoiding the opposite. Nevertheless, the primary purpose of this view is to provide better local and global explainabilities of the impact of features according to the user-defined slices.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "It is a table heatmap view with five automatic feature selection techniques, their Average contribution, and an # Action # button to exclude any number of features. As we originally train our ML algorithm with all features, the yellow color (one of the standard colors used for highlighting [77]) in the last column symbolizes that all features are included in the current phase (if excluded, then B/W stripe patterns appear).",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Matrix",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Matrix"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 9,
    "paper_title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches",
    "pub_year": 2022,
    "domain": "Feature selection",
    "requirement": {
      "requirement_text": "G3: Application of Alternative Feature Transformations According to Feedback Received From Statistical Measures. In continuation of the preceding goal, the tool should provide sufficient visual guidance to users to choose between diverse feature transformations (T4). Statistical measures such as target correlation and mutual information shared between features, along with per class correlation, are necessary to evaluate the features\u2019 influences in the result. Also, the tool should use variance influence factor and in-between features\u2019 correlation for identifying colinearity issues. When checking how to modify features, users should be able to estimate the impact of such transformations.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "feature space detail",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "quantitative": 1,
        "textual": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "In FeatureEnVi, data instances are sorted according to the predicted probability of belonging to the ground truth class, as shown in Fig. 1a. The initial step before the exploration of features is to pre-train the XGBoost [29] on the original pool of features, and then divide the data space into four groups automatically (i.e., Worst, Bad, Good, and Best). The vertical black line is the stable threshold anchored precisely at 50% predictive probability, which separates the correctly from the wrongly classified instances. The other two thresholds partition the prior subspace into their half areas for the default option. However, the user can alter the vertical gray lines as indicated in Fig. 5a.1\u2013a.4, with a degree of freedom set to =t20% from the defaults. The vertical positioning of the instances is purely used to avoid\u2014as much as possible\u2014overlapping/cluttering issues via jittering. The data space will always be divided into four parts conveying extra information to the user. If no instances belong to a slice of the data space, the system works normally, but there will be no values for the statistical measures (see Section 4.3). Overall, the user\u2019s goal is to move as many instances as possible from the left side (Worst and Bad subspaces) to the right side (Good and Best subspaces) while avoiding the opposite. Nevertheless, the primary purpose of this view is to provide better local and global explainabilities of the impact of features according to the user-defined slices.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "the radial tree providing an overview of the features with statistical measures for the different groups of instances, as set by the user-defined data slices. The graph visualization in Fig. 3e uses glyphs to encode further details for every feature, i.e., node. The per target correlation and MI are the same as in the radial tree, but an additional horizontal bar chart encodes the per class correlation of each feature with three colors (olive for fine class, purple for superior class, and turquoise for inferior class). Moreover, there are four states that indicate if the VIF per feature was greater than 10, between 10 and 5, between 5 and 2.5, and finally, less than 2.5. They are represented by up to four circular segments in gray (see Fig. 3e) laid out clockwise within the node. We decided for these states as the prior research suggests they reflect concerns or problem_x0002_atic cases of colinearity [89], [90], [91]. Edges are displayed between feature nodes with the correlation above the cur_x0002_rent threshold (0.6 in our example), with edge widths encoding correlation values.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "Circle+Bar",
        "axial_code": ["Nesting"],
        "componenet_code": ["Bar", "Circle"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 10,
    "paper_title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches",
    "pub_year": 2022,
    "domain": "Feature selection",
    "requirement": {
      "requirement_text": "G4: Generation of new Features and Comparison With the Original Features. With the same statistical evidence as defined in G3, users should get visually informed about Strongly correlated features that perform the same for each class. Next, the tool can use automatic feature selection techniques to compare the new features with the original ones, using the same methods as in G2. Finally, the tool should let users select the proper mathematical operation according to their prior experience and the visual feedback.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "feature space detail",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "quantitative": 1,
        "textual": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "In FeatureEnVi, data instances are sorted according to the predicted probability of belonging to the ground truth class, as shown in Fig. 1a. The initial step before the exploration of features is to pre-train the XGBoost [29] on the original pool of features, and then divide the data space into four groups automatically (i.e., Worst, Bad, Good, and Best). The vertical black line is the stable threshold anchored precisely at 50% predictive probability, which separates the correctly from the wrongly classified instances. The other two thresholds partition the prior subspace into their half areas for the default option. However, the user can alter the vertical gray lines as indicated in Fig. 5a.1\u2013a.4, with a degree of freedom set to =t20% from the defaults. The vertical positioning of the instances is purely used to avoid\u2014as much as possible\u2014overlapping/cluttering issues via jittering. The data space will always be divided into four parts conveying extra information to the user. If no instances belong to a slice of the data space, the system works normally, but there will be no values for the statistical measures (see Section 4.3). Overall, the user\u2019s goal is to move as many instances as possible from the left side (Worst and Bad subspaces) to the right side (Good and Best subspaces) while avoiding the opposite. Nevertheless, the primary purpose of this view is to provide better local and global explainabilities of the impact of features according to the user-defined slices.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "during the detailed examination phase, check the different transformations of the features with statistical measures and compare the combinations of two or three features that result in newlygenerated features. The graph visualization in Fig. 3e uses glyphs to encode further details for every feature, i.e., node. The per target correlation and MI are the same as in the radial tree, but an additional horizontal bar chart encodes the per class correlation of each feature with three colors (olive for fine class, purple for superior class, and turquoise for inferior class). Moreover, there are four states that indicate if the VIF per feature was greater than 10, between 10 and 5, between 5 and 2.5, and finally, less than 2.5. They are represented by up to four circular segments in gray (see Fig. 3e) laid out clockwise within the node. We decided for these states as the prior research suggests they reflect concerns or problem_x0002_atic cases of colinearity [89], [90], [91]. Edges are displayed between feature nodes with the correlation above the cur_x0002_rent threshold (0.6 in our example), with edge widths encoding correlation values.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "Circle+Bar",
        "axial_code": ["Nesting"],
        "componenet_code": ["Bar", "Circle"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 11,
    "paper_title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise Selection and Semi-Automatic Extraction Approaches",
    "pub_year": 2022,
    "domain": "Feature selection",
    "requirement": {
      "requirement_text": "G5: Reassessment of the Instances\u2019 Predicted Probabilities and Performance, Computed With Appropriate Validation Metrics. In the end, users\u2019 interactions should be tracked in order to preserve a history of modifications in the features, and the performance should be monitored with validation metrics. At all stages, the tool should highlight the movement of the instances from one data slice to the other due to the new prediction probability values of every instance. The best case is for all instances to relocate from the left to the right-most side.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "feature space detail",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "quantitative": 1,
        "textual": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "In FeatureEnVi, data instances are sorted according to the predicted probability of belonging to the ground truth class, as shown in Fig. 1a. The initial step before the exploration of features is to pre-train the XGBoost [29] on the original pool of features, and then divide the data space into four groups automatically (i.e., Worst, Bad, Good, and Best). The vertical black line is the stable threshold anchored precisely at 50% predictive probability, which separates the correctly from the wrongly classified instances. The other two thresholds partition the prior subspace into their half areas for the default option. However, the user can alter the vertical gray lines as indicated in Fig. 5a.1\u2013a.4, with a degree of freedom set to =t20% from the defaults. The vertical positioning of the instances is purely used to avoid\u2014as much as possible\u2014overlapping/cluttering issues via jittering. The data space will always be divided into four parts conveying extra information to the user. If no instances belong to a slice of the data space, the system works normally, but there will be no values for the statistical measures (see Section 4.3). Overall, the user\u2019s goal is to move as many instances as possible from the left side (Worst and Bad subspaces) to the right side (Good and Best subspaces) while avoiding the opposite. Nevertheless, the primary purpose of this view is to provide better local and global explainabilities of the impact of features according to the user-defined slices.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "Each action of the user is registered in the punchcard visualization (see Fig. 1e). The basic recorded steps are: (1) Include, (2) Exclude, (3) Transform, and (4) Generate for each feature. The size of the circle encodes the order of the main actions, with larger radii for recent steps. The brown color is used only if the overall performance increases. The calculation is according to three validation metrics after we subtract their standard deviations. The grouped bar chart presents the performance based on accuracy, weighted precision, and weighted recall and their standard deviations due to crossvalidation (error margins in black). Teal color encodes the current action\u2019s score, and brown the best result reached so far. The choice of colors was made deliberately because they complement each other, and the former denotes the current action since it is brighter than the latter. If the list of features is long, the user can scroll this view. Finally, there is a button to extract the best combination of features (i.e., the \u201cnew\u201d data set). The brown circles in the punchcard in Fig. 1e enable us to acknowledge that the feature generation boosted the overall performance of the classifier. High scores were reached in terms of accuracy, precision, and recall. All in all with FeatureEnVi, we improve the total combined score by using 6 well-engineered features instead of the original 11. On the contrary, Rojo et al. [33] reported a slight decrease in performance when selecting 6 features for this task as a regression problem.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Providing",
        "solution_compoent": "Matrix+Circle",
        "axial_code": ["Overlay-Coordinatesystemrelated-Providing"],
        "componenet_code": ["Matrix", "Circle"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 12,
    "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures",
    "pub_year": 2023,
    "domain": "History",
    "requirement": {
      "requirement_text": "T1: Exploring a chronology from multiple aspects. Historians are interested in the ups and downs in the lifetime of each person. The life story of a historical figure generally comprises multiple aspects, such as political events and social activities.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "historical figures and events",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "sequential": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "Evidence from interviews of historians suggest that a sim_x0002_plified overview is needed to help them understand the var_x0002_iation of a historical figure\u2019s life (T1). Thus, we set a scoring rule to evaluate the situation of a historical figure during a specific period. A higher score indicates that the corre_x0002_sponding historical figures are more likely to have a better reputation. To evaluate the score of an event e, five key factors are considered: K1. Importance of the involved figures. For example, criticisms from the emperor have more impacts than that from anybody else. K2. Figure\u2019s role in the event. For example, if the per_x0002_son received an award or was promoted, the effect on the historical figure is positive. In contrast, critics have a negative effect. K3. Event occurrence time. The influence of an event diminishes over time. K4. Frequency of the events\u2019 types. Sparse events such as major successes and weddings should be highlighted. K5. Interests of historians. Event types that historians are interested in are more important. Lexicon-based sentiment analysis [48] takes the sum of emotional scores of each word in the text to measure the  text\u2019s positive or negative sentiment. We refine this approach by computing the scores over time for each historical figure p in year t. The definition of a score can be expressed as follows: Grade\u00f0e; re\u00de is a user-labeled value that considers the tar_x0002_get figures\u2019 role re (e.g., victim or inflicter) in the event (K2). According to the needs of historians, a range of discrete val_x0002_ues of about [-10,10] provides sufficient precision for histori_x0002_ans to express their negative or positive cognition toward the event type (K5). Five historians are invited to grade 836 event types. We gather their grades and average them. In addition, I\u00f0e\u00de measures the importance of event e: I\u00f0e\u00de \u00bc e_x0004_ tf\u00f0e\u00de _x0003_ idf\u00f0e\u00de _x0003_ w\u00f0typee\u00de (2) To reduce the bias induced by repeating with a high fre_x0002_quency normal events (K4), the component e_x0004_ tf\u00f0e\u00de _x0003_ idf\u00f0e\u00de is used from the TF-IDF method in text mining [50]. For example, records of promotion can be easily gathered through historical official documents, which may be over_x0002_sampled compared to other event types. Last, w\u00f0typee\u00de is the weight of a specific event type, which can be controlled by historians (K5). For each year t, we apply the weighted-accumulated scores of events before year t within N years. Because his_x0002_torical figures have a short lifespan on average, their situa_x0002_tions may change relatively quickly. Historians typically consider the influence of events within five years, and ear_x0002_lier events may be ignored (K3). So we set N as 5, and use the exponential decay function to represent the attenuation value of influence before year t: f\u00f0Dt\u00de \u00bc e_x0004_ Dt ; Dt 2 f0; 1; ... ; N _x0004_ 1g (3) Thus, the averaged score in year t should be given as: score\u00f0t\u00de \u00bc X N_x0004_ 1 Dt\u00bc0 f\u00f0Dt\u00de _x0003_ score\u00f0t _x0004_ Dt\u00de (4) Based on the proposed model, we calculate the score for each historical figure and for each year, and visualize the variance of their life time with line charts, as shown in Fig. 3b. It provides an an overview of the chronology of his_x0002_torical figures.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 13,
    "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures",
    "pub_year": 2023,
    "domain": "History",
    "requirement": {
      "requirement_text": "T1: Exploring a chronology from multiple aspects. Historians are interested in the ups and downs in the lifetime of each person. The life story of a historical figure generally comprises multiple aspects, such as political events and social activities.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "historical figures and events",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "sequential": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "In the general pipeline of uncertainty reasoning in the system, analysts need to select a historical figure and identify uncertain events from the corresponding chronology timeline to trigger the visualization (T1).The control panel (Fig. 3a) shows the selected figure\u2019s brief biography and provides a set of adjustment operations. The weight of each event type, wetypeeT (Section 4.2), can also be interactively defined in the control panel. The Life Mountain view (Fig. 3b) is the entrance for historical exploration and uncertainty reasoning, which provides an overview of Wang\u2019s life experience. Inspired by the met_x0002_aphor of typical Chinese landscape painting (Fig. 4a), a streamgraph-based visual design is proposed to intuitively depict the ups and downs in Wang\u2019s lifetime. A horizontal timeline (Fig. 4b) is adopted, where the important events in history are marked at their time of occurrence and visualized as short vertical lines. Hence, historians can associate Wang\u2019s life experience with the historic envi_x0002_ronment. Wang\u2019s life duration is also highlighted to help his_x0002_torians identify events that occurred before his birth or after his death. The height of the stream graph at time t encodes scoreetT (Section 4.2), indicating the evolution of Wang\u2019s life_x0002_time. Different streams show Wang\u2019s evolution from differ_x0002_ent perspectives according to the event categories (e.g., political, academic, military, etc.). These streams are stacked together to depict Wang\u2019s overall rise and fall. The events in which Wang is involved are encoded in dots and superim_x0002_posed on the stream graph as an analogy of the brushwork in a Chinese landscape painting. Along the timeline, an event is positive if the dot is above the center of the mountain, and is otherwise negative. The closer the dot is near the top or the bottom of the mountain, the more positive or negative the event is, respectively. The types of all presented events are vertically arranged in the top-right corner according to when they first appear, like an analogy of the inscription in a Chinese painting snapshot. The grayscale of each topic indicates its number of occurrences. A darker color indicates a larger number of events and vice versa. Events with uncertain occurrence times are also encoded as dots below the timeline. Each uncertain event is located at the recommended time extracted from the most similar event (Section 4.3). The size of the dot indicates the event\u2019s importance, while the transparency represents the probabil_x0002_ity of the recommended occurrence time. Historians can therefore select uncertain events with relatively higher cred_x0002_ibility to support further reasoning. This view also supports the comparison of multiple historical figures. When histori_x0002_ans choose two figures, for example, in the control panel, their lifelines are simultaneously shown in the Mountain view.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Sharing",
        "solution_compoent": "Line+Area",
        "axial_code": ["Overlay-Coordinatesystemrelated-Sharing"],
        "componenet_code": ["Line", "Area"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 14,
    "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures",
    "pub_year": 2023,
    "domain": "History",
    "requirement": {
      "requirement_text": "T1: Exploring a chronology from multiple aspects. Historians are interested in the ups and downs in the lifetime of each person. The life story of a historical figure generally comprises multiple aspects, such as political events and social activities.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "historical figures and events",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "sequential": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "We design the map view to visualize uncertain spatio-temporal information (Fig. 3d). The example shows Wang\u2019s moving pattern by displaying the event locations on the map. For each location, we obtain a set of events that involved Wang and occurred at this location. Events with missing location information are recommended to occur at the location extracted from the most similar event (Section 4.3). A set of pie charts shown in Fig. 3d represent the propor_x0002_tion of certain and uncertain events in event sets at different locations. The size of the pie chart encodes the number of all events in this set. Events with certain time and location information are connected in chronological order to show Wang\u2019s moving pattern. Zooming and panning tools are also provided. Once historians select a pie chart, the detailed information on the events in this set will be listed in a tooltip. The listed events are sorted by probability, with certain events on the top followed by uncertain events.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemreleted-Providing",
        "solution_compoent": "Map+Circle",
        "axial_code": ["Overlay-Coordinatesystemreleted-Providing"],
        "componenet_code": ["Circle", "Map"]
      },
      {
        "solution_text": "Zooming and panning tools are also provided. Once historians select a pie chart, the detailed information on the events in this set will be listed in a tooltip. The listed events are sorted by probability, with certain events on the top followed by uncertain events.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore;Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering", "OverviewandExplore"],
        "componenet_code": ["filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 15,
    "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures",
    "pub_year": 2023,
    "domain": "History",
    "requirement": {
      "requirement_text": "T1: Exploring a chronology from multiple aspects. Historians are interested in the ups and downs in the lifetime of each person. The life story of a historical figure generally comprises multiple aspects, such as political events and social activities.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "historical figures and events",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "sequential": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "We design the map view to visualize uncertain spatio-temporal information (Fig. 3d). The example shows Wang\u2019s moving pattern by displaying the event locations on the map. For each location, we obtain a set of events that involved Wang and occurred at this location. Events with missing location information are recommended to occur at the location extracted from the most similar event (Section 4.3). A set of pie charts shown in Fig. 3d represent the propor_x0002_tion of certain and uncertain events in event sets at different locations. The size of the pie chart encodes the number of all events in this set. Events with certain time and location information are connected in chronological order to show Wang\u2019s moving pattern. Zooming and panning tools are also provided. Once historians select a pie chart, the detailed information on the events in this set will be listed in a tooltip. The listed events are sorted by probability, with certain events on the top followed by uncertain events.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Matrix",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Matrix"]
      },
      {
        "solution_text": "Zooming and panning tools are also provided. Once historians select a pie chart, the detailed information on the events in this set will be listed in a tooltip. The listed events are sorted by probability, with certain events on the top followed by uncertain events.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 17,
    "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures",
    "pub_year": 2023,
    "domain": "History",
    "requirement": {
      "requirement_text": "T2: Identifying and reducing the spatio-temporal uncertainty of events. The system should provide historians with visual analysis of spatio-temporal uncertainty, and enables them to perform cross-validations across multiple views using domain knowledge. Reducing uncertainty allows historians to obtain accurate information, so as to improve the analysis efficiency.",
      "requirement_code": { "discover_observation": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "historical figures and events",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "sequential": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 18,
    "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures",
    "pub_year": 2023,
    "domain": "History",
    "requirement": {
      "requirement_text": "T2: Identifying and reducing the spatio-temporal uncertainty of events. The system should provide historians with visual analysis of spatio-temporal uncertainty, and enables them to perform cross-validations across multiple views using domain knowledge. Reducing uncertainty allows historians to obtain accurate information, so as to improve the analysis efficiency.",
      "requirement_code": { "discover_observation": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "historical figures and events",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "sequential": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "We construct a heterogeneous network to store the data from CBDB and BSADB.For the sake of simplicity, we only store historical events and related records that support spatio-temporal information reasoning. Two types of records are treated as events, including status (e.g., friendship, adversary relationship, etc.) and actions (e.g., criticizing, writing articles, becoming friends,etc.). Two types of records are treated as events, including status (e.g., friendship, adversary relationship, etc.) and actions (e.g., criticizing, writing articles, becoming friends,etc.).",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling;Excluding",
        "solution_compoent": "",
        "axial_code": ["Excluding", "Modeling"],
        "componenet_code": ["excluding", "modeling"]
      },
      {
        "solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "Inference of the selected event is then enabled in the uncertainty reasoning view based on the entity recommendation. We thus design the proposed reasoning view (Fig. 3e). Once an uncertain event is selected in the chronology/map view, it will appear at the center of the reasoning view, with other similar events surrounded for uncertainty inference and cross-verification. As shown in Fig. 6, the reasoning view consists of two major components: reasoning content and reasoning rules. Reasoning content includes a central event (the selected uncertain event to be reasoned, CE) and other supplementary events (SE) that provide important reasoning cues. We use the top-50 recommended SEs that are most similar to the CE (Section 4.3) as default. This threshold can be adjusted in the control panel. The selection of SEs is based on the aforementioned hypothesis that events sharing the same entities might be related. Therefore, we further extract the entities ([description, time, location, person]) from SEs. Reasoning rules refer to the user-defined rules for filtering the displayed SEs and entities in the reasoning content(Fig. 6), in order to obtain more supportive evidence. Two rules are supported: 1) intersection that preserves SEs containing all selected entities and 2) union that preserves SEs containing at least one of the entities. Historians can interactively drag one or a set of entities from the reasoning content and add rules. Fig. 6 shows the visual design of the reasoning rules. Entities are organized according to the drag operation, and rules are represented by circular nodes. Pink nodes indi_x0002_cate the intersection operations, and green nodes indicate the union. In particular, historians can add new rules to previous rules, ultimately formulating a rule tree.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Word",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Word"]
      },
      {
        "solution_text": "Historians can interactively drag one or a set of entities from the reasoning content and add rules. Fig. 6 shows the visual design of the reasoning rules. Entities are organized according to the drag operation, and rules are represented by circular nodes. Pink nodes indi_x0002_cate the intersection operations, and green nodes indicate the union. In particular, historians can add new rules to previous rules, ultimately formulating a rule tree.",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Filtering",
        "solution_compoent": "",
        "axial_code": ["Selecting", "Filtering"],
        "componenet_code": ["selecting", "filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 19,
    "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures",
    "pub_year": 2023,
    "domain": "History",
    "requirement": {
      "requirement_text": "T2: Identifying and reducing the spatio-temporal uncertainty of events. The system should provide historians with visual analysis of spatio-temporal uncertainty, and enables them to perform cross-validations across multiple views using domain knowledge. Reducing uncertainty allows historians to obtain accurate information, so as to improve the analysis efficiency.",
      "requirement_code": { "discover_observation": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "historical figures and events",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "sequential": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "The Life Mountain view (Fig. 3b) is the entrance for historical exploration and uncertainty reasoning, which provides an overview of Wang\u2019s life experience. Inspired by the met_x0002_aphor of typical Chinese landscape painting (Fig. 4a), a streamgraph-based visual design is proposed to intuitively depict the ups and downs in Wang\u2019s lifetime. A horizontal timeline (Fig. 4b) is adopted, where the important events in history are marked at their time of occurrence and visualized as short vertical lines. Hence, historians can associate Wang\u2019s life experience with the historic environment. Wang\u2019s life duration is also highlighted to help historians identify events that occurred before his birth or after his death. The height of the stream graph at time t encodes scoreetT (Section 4.2), indicating the evolution of Wang\u2019s life_x0002_time. Different streams show Wang\u2019s evolution from differ_x0002_ent perspectives according to the event categories (e.g., political, academic, military, etc.). These streams are stacked together to depict Wang\u2019s overall rise and fall. The events in which Wang is involved are encoded in dots and superim_x0002_posed on the stream graph as an analogy of the brushwork in a Chinese landscape painting. Along the timeline, an event is positive if the dot is above the center of the mountain, and is otherwise negative. The closer the dot is near the top or the bottom of the mountain, the more positive or negative the event is, respectively. The types of all presented events are vertically arranged in the top-right corner according to when they first appear, like an analogy of the inscription in a Chinese painting snapshot. The grayscale of each topic indicates its number of occurrences. A darker color indicates a larger number of events and vice versa. Events with uncertain occurrence times are also encoded as dots below the timeline. Each uncertain event is located at the recommended time extracted from the most similar event (Section 4.3). The size of the dot indicates the event\u2019s importance, while the transparency represents the probabil_x0002_ity of the recommended occurrence time. Historians can therefore select uncertain events with relatively higher cred_x0002_ibility to support further reasoning. This view also supports the comparison of multiple historical figures. When histori_x0002_ans choose two figures, for example, in the control panel, their lifelines are simultaneously shown in the Mountain view.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Sharing",
        "solution_compoent": "Line+Area",
        "axial_code": ["Overlay-Coordinatesystemrelated-Sharing"],
        "componenet_code": ["Line", "Area"]
      },
      {
        "solution_text": "Historians can therefore select uncertain events with relatively higher cred_x0002_ibility to support further reasoning. This view also supports the comparison of multiple historical figures. When histori_x0002_ans choose two figures, for example, in the control panel, their lifelines are simultaneously shown in the Mountain view.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 20,
    "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures",
    "pub_year": 2023,
    "domain": "History",
    "requirement": {
      "requirement_text": "T2: Identifying and reducing the spatio-temporal uncertainty of events. The system should provide historians with visual analysis of spatio-temporal uncertainty, and enables them to perform cross-validations across multiple views using domain knowledge. Reducing uncertainty allows historians to obtain accurate information, so as to improve the analysis efficiency.",
      "requirement_code": { "discover_observation": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "historical figures and events",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "sequential": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "Based on this result, we can identify the nearest k similar events of the event. Those events are more likely to have cor_x0002_relations, causal relationships, and sequential relationships with the target event. We further evaluate the hit ratio (the probability that the nearest k similar events contain the correct answer) of the proposed algorithm with the test data. We collect 1,000 completed events from CBDB, where some entities (e.g., location ortime of the events) are manually hidden to sim_x0002_ulate the uncertainty. The hidden parts are viewed as labels. We divide the data into training and testing sets and apply a\n10-fold cross-validation method on the testing sets. We run the algorithm to identify the k most similar events for each tar_x0002_getevent. In the k similarevents, if any similarevent has the hid_x0002_den entity of the target event (e.g., a given location), the target event is considered to be hit and a hit ratio is computed. The top-5 hit ratio of the 10-fold cross-validation is 48.1%, and the top-15 is 70.7%. Meanwhile, this process markedly reduces the search space for finding similar entities. With the visuali_x0002_zation of suggested candidates, historians can better investi_x0002_gate uncertainty with domain knowledge.",
        "solution_category": "data_manipulation",
        "solution_axial": "SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["SimilarityCalculation"],
        "componenet_code": ["similarity_calculation"]
      },
      {
        "solution_text": "We design the relation view to enable historians to study historical figures\u2019 group behaviors in more details (Fig. 3f). With the assistance of PageRank (Section 4.2), we extract the top-20 historical figures most related to Wang by default. The threshold is adjustable. The grid ei; jT in the relation view represents a set of events that involve person i and person j. Historians can switch among three modes to display different types of information of these grids. The first mode displays the size of the event set in grayscale, where the darker color indicates more events. The second mode shows the positiveness and negativeness of the event set. A yellow color indicates that the average grade of events is positive, and blue indicates negative. These grades are man_x0002_ually labeled by historians, as described in Section 4.2. The third mode uses a consistent color mapping, as in the Life Mountain view, to depict the leading category (e.g., politics or literature) of the event set. The leading category represents the one shared by the events that occupy the largest proportion in the set. In the relation view, the grids are sorted by the Girvan Newman algorithm [54] so that histori_x0002_cal figures with closer relations are gathered.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Unsimilar-Continuous",
        "solution_compoent": "Matrix",
        "axial_code": ["Juxtaposition-Unsimilar-Continuous"],
        "componenet_code": ["Matrix"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 21,
    "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures",
    "pub_year": 2023,
    "domain": "History",
    "requirement": {
      "requirement_text": "T2: Identifying and reducing the spatio-temporal uncertainty of events. The system should provide historians with visual analysis of spatio-temporal uncertainty, and enables them to perform cross-validations across multiple views using domain knowledge. Reducing uncertainty allows historians to obtain accurate information, so as to improve the analysis efficiency.",
      "requirement_code": { "discover_observation": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "historical figures and events",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "sequential": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "Based on this result, we can identify the nearest k similar events of the event. Those events are more likely to have cor_x0002_relations, causal relationships, and sequential relationships with the target event. We further evaluate the hit ratio (the probability that the nearest k similar events contain the correct answer) of the proposed algorithm with the test data. We collect 1,000 completed events from CBDB, where some entities (e.g., location ortime of the events) are manually hidden to sim_x0002_ulate the uncertainty. The hidden parts are viewed as labels. We divide the data into training and testing sets and apply a\n10-fold cross-validation method on the testing sets. We run the algorithm to identify the k most similar events for each tar_x0002_getevent. In the k similarevents, if any similarevent has the hid_x0002_den entity of the target event (e.g., a given location), the target event is considered to be hit and a hit ratio is computed. The top-5 hit ratio of the 10-fold cross-validation is 48.1%, and the top-15 is 70.7%. Meanwhile, this process markedly reduces the search space for finding similar entities. With the visuali_x0002_zation of suggested candidates, historians can better investi_x0002_gate uncertainty with domain knowledge.",
        "solution_category": "data_manipulation",
        "solution_axial": "SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["SimilarityCalculation"],
        "componenet_code": ["similarity_calculation"]
      },
      {
        "solution_text": "We design the relation view to enable historians to study historical figures\u2019 group behaviors in more details (Fig. 3f). With the assistance of PageRank (Section 4.2), we extract the top-20 historical figures most related to Wang by default. The threshold is adjustable. The grid ei; jT in the relation view represents a set of events that involve person i and person j. Historians can switch among three modes to display different types of information of these grids. The first mode displays the size of the event set in grayscale, where the darker color indicates more events. The second mode shows the positiveness and negativeness of the event set. A yellow color indicates that the average grade of events is positive, and blue indicates negative. These grades are man_x0002_ually labeled by historians, as described in Section 4.2. The third mode uses a consistent color mapping, as in the Life Mountain view, to depict the leading category (e.g., politics or literature) of the event set. The leading category represents the one shared by the events that occupy the largest proportion in the set. In the relation view, the grids are sorted by the Girvan Newman algorithm [54] so that histori_x0002_cal figures with closer relations are gathered.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Unsimilar-Continuous",
        "solution_compoent": "Matrix",
        "axial_code": ["Juxtaposition-Unsimilar-Continuous"],
        "componenet_code": ["Matrix"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 22,
    "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures",
    "pub_year": 2023,
    "domain": "History",
    "requirement": {
      "requirement_text": "T3: Reasoning the collective behavior of historical figures based on refined results. The final goal of information refinement is to discover group behaviors, such as inter- and intraparty relationships.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "historical figures and events",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "sequential": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "Inference of the selected event is then enabled in the uncertainty reasoning view based on the entity recommendation. We thus design the proposed reasoning view (Fig. 3e). Once an uncertain event is selected in the chronology/map view, it will appear at the center of the reasoning view, with other similar events surrounded for uncertainty inference and cross-verification. As shown in Fig. 6, the reasoning view consists of two major components: reasoning content and reasoning rules. Reasoning content includes a central event (the selected uncertain event to be reasoned, CE) and other supplementary events (SE) that provide important reasoning cues. We use the top-50 recommended SEs that are most similar to the CE (Section 4.3) as default. This threshold can be adjusted in the control panel. The selection of SEs is based on the aforementioned hypothesis that events sharing the same entities might be related. Therefore, we further extract the entities ([description, time, location, person]) from SEs. Reasoning rules refer to the user-defined rules for filtering the displayed SEs and entities in the reasoning content(Fig. 6), in order to obtain more supportive evidence. Two rules are supported: 1) intersection that preserves SEs containing all selected entities and 2) union that preserves SEs containing at least one of the entities. Historians can interactively drag one or a set of entities from the reasoning content and add rules. Fig. 6 shows the visual design of the reasoning rules. Entities are organized according to the drag operation, and rules are represented by circular nodes. Pink nodes indi_x0002_cate the intersection operations, and green nodes indicate the union. In particular, historians can add new rules to previous rules, ultimately formulating a rule tree.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Word",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Word"]
      },
      {
        "solution_text": "In particular, historians can add new rules to previous rules, ultimately formulating a rule tree.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 23,
    "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures",
    "pub_year": 2023,
    "domain": "History",
    "requirement": {
      "requirement_text": "T3: Reasoning the collective behavior of historical figures based on refined results. The final goal of information refinement is to discover group behaviors, such as inter- and intraparty relationships.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "historical figures and events",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "sequential": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "The Life Mountain view (Fig. 3b) is the entrance for historical exploration and uncertainty reasoning, which provides an overview of Wang\u2019s life experience. Inspired by the met_x0002_aphor of typical Chinese landscape painting (Fig. 4a), a streamgraph-based visual design is proposed to intuitively depict the ups and downs in Wang\u2019s lifetime. A horizontal timeline (Fig. 4b) is adopted, where the important events in history are marked at their time of occurrence and visualized as short vertical lines. Hence, historians can associate Wang\u2019s life experience with the historic environment. Wang\u2019s life duration is also highlighted to help historians identify events that occurred before his birth or after his death. The height of the stream graph at time t encodes scoreetT (Section 4.2), indicating the evolution of Wang\u2019s life_x0002_time. Different streams show Wang\u2019s evolution from differ_x0002_ent perspectives according to the event categories (e.g., political, academic, military, etc.). These streams are stacked together to depict Wang\u2019s overall rise and fall. The events in which Wang is involved are encoded in dots and superim_x0002_posed on the stream graph as an analogy of the brushwork in a Chinese landscape painting. Along the timeline, an event is positive if the dot is above the center of the mountain, and is otherwise negative. The closer the dot is near the top or the bottom of the mountain, the more positive or negative the event is, respectively. The types of all presented events are vertically arranged in the top-right corner according to when they first appear, like an analogy of the inscription in a Chinese painting snapshot. The grayscale of each topic indicates its number of occurrences. A darker color indicates a larger number of events and vice versa. Events with uncertain occurrence times are also encoded as dots below the timeline. Each uncertain event is located at the recommended time extracted from the most similar event (Section 4.3). The size of the dot indicates the event\u2019s importance, while the transparency represents the probabil_x0002_ity of the recommended occurrence time. Historians can therefore select uncertain events with relatively higher cred_x0002_ibility to support further reasoning. This view also supports the comparison of multiple historical figures. When histori_x0002_ans choose two figures, for example, in the control panel, their lifelines are simultaneously shown in the Mountain view.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Sharing",
        "solution_compoent": "Line+Area",
        "axial_code": ["Overlay-Coordinatesystemrelated-Sharing"],
        "componenet_code": ["Line", "Area"]
      },
      {
        "solution_text": "Historians can therefore select uncertain events with relatively higher cred_x0002_ibility to support further reasoning. This view also supports the comparison of multiple historical figures. When histori_x0002_ans choose two figures, for example, in the control panel, their lifelines are simultaneously shown in the Mountain view.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 24,
    "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures",
    "pub_year": 2023,
    "domain": "History",
    "requirement": {
      "requirement_text": "T3: Reasoning the collective behavior of historical figures based on refined results. The final goal of information refinement is to discover group behaviors, such as inter- and intraparty relationships.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "historical figures and events",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "sequential": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "We design the map view to visualize uncertain spatio-temporal information (Fig. 3d). The example shows Wang\u2019s moving pattern by displaying the event locations on the map. For each location, we obtain a set of events that involved Wang and occurred at this location. Events with missing location information are recommended to occur at the location extracted from the most similar event (Section 4.3). A set of pie charts shown in Fig. 3d represent the propor_x0002_tion of certain and uncertain events in event sets at different locations. The size of the pie chart encodes the number of all events in this set. Events with certain time and location information are connected in chronological order to show Wang\u2019s moving pattern. Zooming and panning tools are also provided. Once historians select a pie chart, the detailed information on the events in this set will be listed in a tooltip. The listed events are sorted by probability, with certain events on the top followed by uncertain events.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemreleted-Providing",
        "solution_compoent": "Map+Circle",
        "axial_code": ["Overlay-Coordinatesystemreleted-Providing"],
        "componenet_code": ["Circle", "Map"]
      },
      {
        "solution_text": "Zooming and panning tools are also provided. Once historians select a pie chart, the detailed information on the events in this set will be listed in a tooltip. The listed events are sorted by probability, with certain events on the top followed by uncertain events.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore;Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering", "OverviewandExplore"],
        "componenet_code": ["filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 25,
    "paper_title": "Visual Reasoning for Uncertainty in Spatio-Temporal Events of Historical Figures",
    "pub_year": 2023,
    "domain": "History",
    "requirement": {
      "requirement_text": "T3: Reasoning the collective behavior of historical figures based on refined results. The final goal of information refinement is to discover group behaviors, such as inter- and intraparty relationships.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "historical figures and events",
      "data_code": {
        "geometry": 1,
        "textual": 1,
        "temporal": 1,
        "sequential": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "We provide the historians with an automatic method to identify possible locations or time of uncertain events, as well as the related events that can be used in reasoning (T2, T3). It is equivalent to finding the most reasonable time or location entities linked to the reasoned event with relations \u201ctime is\u201d or \u201clocation is\u201d in the heterogeneous network. The reasoning method of this study is based on the hypothesis that related historical events tend to share the same entities (e.g., figures, locations, and time), and vice versa. The plausibility of this hypothesis is confirmed by interviews with historians. The locations and time of related events are more likely to encounter missing or erroneous information. We apply a representation learning method to find related events and recommend possible reasons of the observed uncertainties. The representation learning method learns semantic information of the heterogeneous network, embeds entities into low-dimensional vectors, and measures the similarity between two events. In addition, redundant information is filtered by means of the dimension reduction. Thus the infer_x0002_ence process is faster than deductive reasoning [27], graph structure-based reasoning [28] and association rule mining [29], which consider redundant information in each predic_x0002_tion. However, since the representation is learned in a vector space, it is difficult to explain the reasons behind similarity observed between two entities. To address the issue, visua methods are used to help explore the potential causes of sim_x0002_ilarity. More details are provided in Section 5.4. The system aims at learning both the semantic informa_x0002_tion of events\u2019 properties and the background knowledge, such as the hierarchical relation of the locations. Hence, we apply the random walk technique [28] to generate sequen_x0002_ces of entities, which describe more detailed information about events. The sequences are taken as input of the bag_x0002_of-words model [51]. Its output is the vector representation of each entity. Each path of the random walk process denotes P = V1_x0004_!V2_x0004_! _x0005_ _x0005_ _x0005_ Vt _x0005_ _x0005_ _x0005_ _x0004_!Vl on all entities. Vt is the type of the entity vt chosen in tth step of the random walk.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "We design the map view to visualize uncertain spatio-temporal information (Fig. 3d). The example shows Wang\u2019s moving pattern by displaying the event locations on the map. For each location, we obtain a set of events that involved Wang and occurred at this location. Events with missing location information are recommended to occur at the location extracted from the most similar event (Section 4.3). A set of pie charts shown in Fig. 3d represent the propor_x0002_tion of certain and uncertain events in event sets at different locations. The size of the pie chart encodes the number of all events in this set. Events with certain time and location information are connected in chronological order to show Wang\u2019s moving pattern. Zooming and panning tools are also provided. Once historians select a pie chart, the detailed information on the events in this set will be listed in a tooltip. The listed events are sorted by probability, with certain events on the top followed by uncertain events.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Matrix",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Matrix"]
      },
      {
        "solution_text": "Zooming and panning tools are also provided. Once historians select a pie chart, the detailed information on the events in this set will be listed in a tooltip. The listed events are sorted by probability, with certain events on the top followed by uncertain events.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 26,
    "paper_title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks",
    "pub_year": 2023,
    "domain": "Graph neural networks",
    "requirement": {
      "requirement_text": "R1: Provide an Overview of GNN Results.To gain an overview of the dataset and classification results, the system needs to summarize various types of information, such as degree distribution and ground truth label distribution. This information, covering various aspects of a GNN model, needs to be organized and presented in a clear manner. Meanwhile, the correlation among this information should be presented to help users develop initial hypotheses about any possible error patterns in GNN results, i.e., a set of wrong predictions that share similar characteristics.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "GNN prediction results",
      "data_code": { "network_and_trees": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Inspired by the fact that GNN prediction results are influ_x0002_enced by both graph structure and node features [14], we define two proxy models to analyze the influence of the graph structure and node features on GNN prediction results. Through expert interviews, experts are concerned about whether the graph structure or node features have a greater impact on GNN prediction, and then determine which components will have more impact. Hence, similar to the ablation study when evaluating GNN models [29], we define two proxy models such as GNN Without Using Fea_x0002_tures (GNNWUF) and Multi-Layer Perceptron (MLP). The two models are chosen, since the two proxy models have the same model architectures as the GNN but are trained using different input data. GNNWUF is trained only using the graph structure while MLP is trained only using the node features. When training GNNWUF, we use one hot encoding as the node feature for each node, meaning GNNWUF considers only the graph structures. When GNN considers only the features of the node itself, then it can degenerate into an MLP model. Hence, MLP is chosen as the other GNN proxy model that only considers the node features and is used to evaluate the influence of node struc_x0002_tures. We train both proxy models with the same settings as the training of GNN. To further help users understand the impact of the graph structure and node features, we also provide a number of metrics, including graph structure based metrics that take into account the graph structure but ignore the node fea_x0002_tures, and node feature based metrics that take the node fea_x0002_tures into account but ignore the graph structure. Those metrics are derived from expert interviews. Details are pre_x0002_sented in the following paragraphs.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "visualize node-level metrics using Parallel Sets. We propose to use Parallel Sets to investigate error patterns in GNN prediction results. Users can select what metrics are to be displayed in the Parallel Sets through Parallel Sets Settings Modal. In general, displaying fewer than five axes in Parallel Sets is a good practice to reduce visual clutter and make efficient use of functions in Parallel Sets. Due to the constraint that the Parallel Sets are used to display the categorical variables, we need to convert the continuous metrics to categorical variables by grouping a range of values into one category. Then we can also show them in the Parallel Sets View.The axis is partitioned into multiple segments representing different categories of the variable. The width of each segment represents the number of nodes falling into that category. We can directly see the distribution of the categories on the axis. Between two consecutive axes, multiple ribbons are shown to connect the two axes, each simultaneously representing the nodes that satisfy the conditions specified by the two axes.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Unsimilar-Continuous",
        "solution_compoent": "Stripe",
        "axial_code": ["Juxtaposition-Unsimilar-Continuous"],
        "componenet_code": ["Stripe"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 27,
    "paper_title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks",
    "pub_year": 2023,
    "domain": "Graph neural networks",
    "requirement": {
      "requirement_text": "R2: Identify Error Patterns. After developing initial hypotheses about the error patterns, users need more detailed information to verify them. Specifically, user need to examine the characteristics shared by a set of wrong predictions and verify whether error patterns formed by these characteristics make sense in analyzing GNNs based on their domain knowledge. During the interview, experts agreed that they usually use several characteristics to group the wrong predictions and identify error patterns.The system should support users in examining these characteristics and identifying error patterns.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "GNN prediction results",
      "data_code": { "network_and_trees": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Inspired by the fact that GNN prediction results are influ_x0002_enced by both graph structure and node features [14], we define two proxy models to analyze the influence of the graph structure and node features on GNN prediction results. Through expert interviews, experts are concerned about whether the graph structure or node features have a greater impact on GNN prediction, and then determine which components will have more impact. Hence, similar to the ablation study when evaluating GNN models [29], we define two proxy models such as GNN Without Using Fea_x0002_tures (GNNWUF) and Multi-Layer Perceptron (MLP). The two models are chosen, since the two proxy models have the same model architectures as the GNN but are trained using different input data. GNNWUF is trained only using the graph structure while MLP is trained only using the node features. When training GNNWUF, we use one hot encoding as the node feature for each node, meaning GNNWUF considers only the graph structures. When GNN considers only the features of the node itself, then it can degenerate into an MLP model. Hence, MLP is chosen as the other GNN proxy model that only considers the node features and is used to evaluate the influence of node struc_x0002_tures. We train both proxy models with the same settings as the training of GNN. To further help users understand the impact of the graph structure and node features, we also provide a number of metrics, including graph structure based metrics that take into account the graph structure but ignore the node fea_x0002_tures, and node feature based metrics that take the node fea_x0002_tures into account but ignore the graph structure. Those metrics are derived from expert interviews. Details are pre_x0002_sented in the following paragraphs.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "visualize node-level metrics using Parallel Sets. We propose to use Parallel Sets to investigate error patterns in GNN prediction results. Users can select what metrics are to be displayed in the Parallel Sets through Parallel Sets Settings Modal. In general, displaying fewer than five axes in Parallel Sets is a good practice to reduce visual clutter and make efficient use of functions in Parallel Sets. Due to the constraint that the Parallel Sets are used to display the categorical variables, we need to convert the continuous metrics to categorical variables by grouping a range of values into one category. Then we can also show them in the Parallel Sets View.The axis is partitioned into multiple segments representing different categories of the variable. The width of each segment represents the number of nodes falling into that category. We can directly see the distribution of the categories on the axis. Between two consecutive axes, multiple ribbons are shown to connect the two axes, each simultaneously representing the nodes that satisfy the conditions specified by the two axes.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Unsimilar-Continuous",
        "solution_compoent": "Stripe",
        "axial_code": ["Juxtaposition-Unsimilar-Continuous"],
        "componenet_code": ["Stripe"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 28,
    "paper_title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks",
    "pub_year": 2023,
    "domain": "Graph neural networks",
    "requirement": {
      "requirement_text": "R2: Identify Error Patterns. After developing initial hypotheses about the error patterns, users need more detailed information to verify them. Specifically, user need to examine the characteristics shared by a set of wrong predictions and verify whether error patterns formed by these characteristics make sense in analyzing GNNs based on their domain knowledge. During the interview, experts agreed that they usually use several characteristics to group the wrong predictions and identify error patterns.The system should support users in examining these characteristics\nand identifying error patterns.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "GNN prediction results",
      "data_code": { "network_and_trees": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Inspired by the fact that GNN prediction results are influ_x0002_enced by both graph structure and node features [14], we define two proxy models to analyze the influence of the graph structure and node features on GNN prediction results. Through expert interviews, experts are concerned about whether the graph structure or node features have a greater impact on GNN prediction, and then determine which components will have more impact. Hence, similar to the ablation study when evaluating GNN models [29], we define two proxy models such as GNN Without Using Fea_x0002_tures (GNNWUF) and Multi-Layer Perceptron (MLP). The two models are chosen, since the two proxy models have the same model architectures as the GNN but are trained using different input data. GNNWUF is trained only using the graph structure while MLP is trained only using the node features. When training GNNWUF, we use one hot encoding as the node feature for each node, meaning GNNWUF considers only the graph structures. When GNN considers only the features of the node itself, then it can degenerate into an MLP model. Hence, MLP is chosen as the other GNN proxy model that only considers the node features and is used to evaluate the influence of node struc_x0002_tures. We train both proxy models with the same settings as the training of GNN. To further help users understand the impact of the graph structure and node features, we also provide a number of metrics, including graph structure based metrics that take into account the graph structure but ignore the node fea_x0002_tures, and node feature based metrics that take the node fea_x0002_tures into account but ignore the graph structure. Those metrics are derived from expert interviews. Details are pre_x0002_sented in the following paragraphs.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "To project those node glyphs in 2D plane, users can choose to use the t-SNE [58] or UMAP [59] projection as the basic layout algorithm. Moreover, the force-directed collision-avoidance method is integrated into the basic layout algorithm to prevent the overlapping of node glyphs.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "We group a subset of node-level metrics, display them in glyphs, and further project them to the 2D plane. The Projection View allows users to investigate the similarity of nodes regarding different perspectives. It can be helpful for investigating whether the nodes with similar node metrics share similar error patterns. In the Projection View, we provide a set of linked projection planes of the nodes that use different features. Different from similar designs in EmbeddingVis [57], we design different node glyphs to display different combinations of node-level metrics. To project those node glyphs in 2D plane, users can choose to use the t-SNE [58] or UMAP [59] projection as the basic layout algorithm. Moreover, the force-directed collision-avoidance method is integrated into the basic layout algorithm to prevent the overlapping of node glyphs. When users lasso-select a set of nodes in a projection plane, the links between the same nodes in different planes will be shown to help users identify the nodes and other aspects of those nodes\u2019 properties, as shown in Fig. 4. After users hover on the node glyphs, the legend and detailed information of those node glyphs will be displayed. However, due to the limited screen space, it cannot display hundreds let alone thousands of node glyphs. Therefore, we apply a hierarchical clustering algorithm with complete linkage to cluster these nodes based on the corresponding distance function [60]. Two clusters will be merged into one cluster when the distance of two clusters is less than or equal to a threshold, which is empirically set to 0.5.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Providing",
        "solution_compoent": "Circle+Bar",
        "axial_code": ["Overlay-Coordinatesystemrelated-Providing"],
        "componenet_code": ["Bar", "Circle"]
      },
      {
        "solution_text": "We group a subset of node-level metrics, display them in glyphs, and further project them to the 2D plane. The Projection View allows users to investigate the similarity of nodes regarding different perspectives. It can be helpful for investigating whether the nodes with similar node metrics share similar error patterns. In the Projection View, we provide a set of linked projection planes of the nodes that use different features. Different from similar designs in EmbeddingVis [57], we design different node glyphs to display different combinations of node-level metrics. To project those node glyphs in 2D plane, users can choose to use the t-SNE [58] or UMAP [59] projection as the basic layout algorithm. Moreover, the force-directed collision-avoidance method is integrated into the basic layout algorithm to prevent the overlapping of node glyphs. When users lasso-select a set of nodes in a projection plane, the links between the same nodes in different planes will be shown to help users identify the nodes and other aspects of those nodes\u2019 properties, as shown in Fig. 4. After users hover on the node glyphs, the legend and detailed information of those node glyphs will be displayed. However, due to the limited screen space, it cannot display hundreds let alone thousands of node glyphs. Therefore, we apply a hierarchical clustering algorithm with complete linkage to cluster these nodes based on the corresponding distance function [60]. Two clusters will be merged into one cluster when the distance of two clusters is less than or equal to a threshold, which is empirically set to 0.5.",
        "solution_category": "interaction",
        "solution_axial": "History",
        "solution_compoent": "",
        "axial_code": ["History"],
        "componenet_code": ["history"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 29,
    "paper_title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks",
    "pub_year": 2023,
    "domain": "Graph neural networks",
    "requirement": {
      "requirement_text": "R2: Identify Error Patterns. After developing initial hypotheses about the error patterns, users need more detailed information to verify them. Specifically, user need to examine the characteristics shared by a set of wrong predictions and verify whether error patterns formed by these characteristics make sense in analyzing GNNs based on their domain knowledge. During the interview, experts agreed that they usually use several characteristics to group the wrong predictions and identify error patterns.The system should support users in examining these characteristics\nand identifying error patterns.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "GNN prediction results",
      "data_code": { "network_and_trees": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Inspired by the fact that GNN prediction results are influ_x0002_enced by both graph structure and node features [14], we define two proxy models to analyze the influence of the graph structure and node features on GNN prediction results. Through expert interviews, experts are concerned about whether the graph structure or node features have a greater impact on GNN prediction, and then determine which components will have more impact. Hence, similar to the ablation study when evaluating GNN models [29], we define two proxy models such as GNN Without Using Fea_x0002_tures (GNNWUF) and Multi-Layer Perceptron (MLP). The two models are chosen, since the two proxy models have the same model architectures as the GNN but are trained using different input data. GNNWUF is trained only using the graph structure while MLP is trained only using the node features. When training GNNWUF, we use one hot encoding as the node feature for each node, meaning GNNWUF considers only the graph structures. When GNN considers only the features of the node itself, then it can degenerate into an MLP model. Hence, MLP is chosen as the other GNN proxy model that only considers the node features and is used to evaluate the influence of node struc_x0002_tures. We train both proxy models with the same settings as the training of GNN. To further help users understand the impact of the graph structure and node features, we also provide a number of metrics, including graph structure based metrics that take into account the graph structure but ignore the node fea_x0002_tures, and node feature based metrics that take the node fea_x0002_tures into account but ignore the graph structure. Those metrics are derived from expert interviews. Details are pre_x0002_sented in the following paragraphs.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "We use the classic node-link diagram with the force directed collision-avoidance layout to visualize the graph dataset. Users can get a sense of the distribution of the selected nodes in the graph, and inspect the neighborhood of the nodes. To further facilitate the convenient exploration of the reasons for errors, we also design a node glyph to encode a group of node-level metrics. Users can hover a node in the Graph View, which will be further highlighted with the radius doubled. The Graph View allows users to quickly check any interesting neighboring nodes. Users can also switch to the \u201cSubgraph\u201d mode. It will display the one-hop and two-hop neighbors of selected nodes, enabling users to explore different hops of neighborhood nodes. Users can visualize the specific subgraphs on their own and can explore them by changing the \u201cSubgraph\u201d options. An overview of the graph is displayed in the bottom right-hand corner to support users navigating the graph. Users can click the specific position in the overview to navigate the displayed area of the graph. Users can choose to filter out the unfocused nodes to accelerate the rendering and reduce the visual clutter in the graph. To investigate the node features and most similar features of training nodes, users can click on the nodes of interest in the Graph View and further explore the node-level features in the Feature Matrix View.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "Line+Circle+Pie",
        "axial_code": ["Nesting"],
        "componenet_code": ["Line", "Circle", "Pie"]
      },
      {
        "solution_text": "The Graph View allows users to quickly check any interesting neighboring nodes. Users can also switch to the \u201cSubgraph\u201d mode. It will display the one-hop and two-hop neighbors of selected nodes, enabling users to explore different hops of neighborhood nodes. Users can visualize the specific subgraphs on their own and can explore them by changing the \u201cSubgraph\u201d options. An overview of the graph is displayed in the bottom right-hand corner to support users navigating the graph. Users can click the specific position in the overview to navigate the displayed area of the graph. Users can choose to filter out the unfocused nodes to accelerate the rendering and reduce the visual clutter in the graph. To investigate the node features and most similar features of training nodes, users can click on the nodes of interest in the Graph View and further explore the node-level features in the Feature Matrix View.",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Filtering+OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore", "Selecting", "Filtering"],
        "componenet_code": ["overview_and_explore", "selecting", "filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 30,
    "paper_title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks",
    "pub_year": 2023,
    "domain": "Graph neural networks",
    "requirement": {
      "requirement_text": "R3: Analyze the Cause of Error Patterns. After identifying error patterns, finding the causes of these errors is important for users to understand, diagnose, and improve the GNNs. More detailed information is needed to understand the possible causes of error patterns. Specifically, users need to inspect the graph structures and node features to determine the causes of error patterns. According to the feedback from expert interviews, there are two main sources of wrong GNN predictions: noise in the training data and inaccurate feature aggregation in GNNs. To predict the label of a node, GNN aggregates the node\u2019s own feature with the features of the neighboring nodes at each layer. Noise in the training data, e.g., the same nodes but different labels, can confuse the GNN and lead to wrong predictions. Inaccurate feature aggregation at any layer will also influence the GNN prediction of the node.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "GNN prediction results",
      "data_code": { "network_and_trees": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Inspired by the fact that GNN prediction results are influ_x0002_enced by both graph structure and node features [14], we define two proxy models to analyze the influence of the graph structure and node features on GNN prediction results. Through expert interviews, experts are concerned about whether the graph structure or node features have a greater impact on GNN prediction, and then determine which components will have more impact. Hence, similar to the ablation study when evaluating GNN models [29], we define two proxy models such as GNN Without Using Fea_x0002_tures (GNNWUF) and Multi-Layer Perceptron (MLP). The two models are chosen, since the two proxy models have the same model architectures as the GNN but are trained using different input data. GNNWUF is trained only using the graph structure while MLP is trained only using the node features. When training GNNWUF, we use one hot encoding as the node feature for each node, meaning GNNWUF considers only the graph structures. When GNN considers only the features of the node itself, then it can degenerate into an MLP model. Hence, MLP is chosen as the other GNN proxy model that only considers the node features and is used to evaluate the influence of node struc_x0002_tures. We train both proxy models with the same settings as the training of GNN. To further help users understand the impact of the graph structure and node features, we also provide a number of metrics, including graph structure based metrics that take into account the graph structure but ignore the node fea_x0002_tures, and node feature based metrics that take the node fea_x0002_tures into account but ignore the graph structure. Those metrics are derived from expert interviews. Details are pre_x0002_sented in the following paragraphs.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "To project those node glyphs in 2D plane, users can choose to use the t-SNE [58] or UMAP [59] projection as the basic layout algorithm. Moreover, the force-directed collision-avoidance method is integrated into the basic layout algorithm to prevent the overlapping of node glyphs.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "We group a subset of node-level metrics, display them in glyphs, and further project them to the 2D plane. The Projection View allows users to investigate the similarity of nodes regarding different perspectives. It can be helpful for investigating whether the nodes with similar node metrics share similar error patterns. In the Projection View, we provide a set of linked projection planes of the nodes that use different features. Different from similar designs in EmbeddingVis [57], we design different node glyphs to display different combinations of node-level metrics. To project those node glyphs in 2D plane, users can choose to use the t-SNE [58] or UMAP [59] projection as the basic layout algorithm. Moreover, the force-directed collision-avoidance method is integrated into the basic layout algorithm to prevent the overlapping of node glyphs. When users lasso-select a set of nodes in a projection plane, the links between the same nodes in different planes will be shown to help users identify the nodes and other aspects of those nodes\u2019 properties, as shown in Fig. 4. After users hover on the node glyphs, the legend and detailed information of those node glyphs will be displayed. However, due to the limited screen space, it cannot display hundreds let alone thousands of node glyphs. Therefore, we apply a hierarchical clustering algorithm with complete linkage to cluster these nodes based on the corresponding distance function [60]. Two clusters will be merged into one cluster when the distance of two clusters is less than or equal to a threshold, which is empirically set to 0.5.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Providing",
        "solution_compoent": "Circle+Bar",
        "axial_code": ["Overlay-Coordinatesystemrelated-Providing"],
        "componenet_code": ["Bar", "Circle"]
      },
      {
        "solution_text": "We group a subset of node-level metrics, display them in glyphs, and further project them to the 2D plane. The Projection View allows users to investigate the similarity of nodes regarding different perspectives. It can be helpful for investigating whether the nodes with similar node metrics share similar error patterns. In the Projection View, we provide a set of linked projection planes of the nodes that use different features. Different from similar designs in EmbeddingVis [57], we design different node glyphs to display different combinations of node-level metrics. To project those node glyphs in 2D plane, users can choose to use the t-SNE [58] or UMAP [59] projection as the basic layout algorithm. Moreover, the force-directed collision-avoidance method is integrated into the basic layout algorithm to prevent the overlapping of node glyphs. When users lasso-select a set of nodes in a projection plane, the links between the same nodes in different planes will be shown to help users identify the nodes and other aspects of those nodes\u2019 properties, as shown in Fig. 4. After users hover on the node glyphs, the legend and detailed information of those node glyphs will be displayed. However, due to the limited screen space, it cannot display hundreds let alone thousands of node glyphs. Therefore, we apply a hierarchical clustering algorithm with complete linkage to cluster these nodes based on the corresponding distance function [60]. Two clusters will be merged into one cluster when the distance of two clusters is less than or equal to a threshold, which is empirically set to 0.5.",
        "solution_category": "interaction",
        "solution_axial": "History",
        "solution_compoent": "",
        "axial_code": ["History"],
        "componenet_code": ["history"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 31,
    "paper_title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks",
    "pub_year": 2023,
    "domain": "Graph neural networks",
    "requirement": {
      "requirement_text": "R3: Analyze the Cause of Error Patterns. After identifying error patterns, finding the causes of these errors is important for users to understand, diagnose, and improve the GNNs. More detailed information is needed to understand the possible causes of error patterns. Specifically, users need to inspect the graph structures and node features to determine the causes of error patterns. According to the feedback from expert interviews, there are two main sources of wrong GNN predictions: noise in the training data and inaccurate feature aggregation in GNNs. To predict the label of a node, GNN aggregates the node\u2019s own feature with the features of the neighboring nodes at each layer. Noise in the training data, e.g., the same nodes but different labels, can confuse the GNN and lead to wrong predictions. Inaccurate feature aggregation at any layer will also influence the GNN prediction of the node.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "GNN prediction results",
      "data_code": { "network_and_trees": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Inspired by the fact that GNN prediction results are influ_x0002_enced by both graph structure and node features [14], we define two proxy models to analyze the influence of the graph structure and node features on GNN prediction results. Through expert interviews, experts are concerned about whether the graph structure or node features have a greater impact on GNN prediction, and then determine which components will have more impact. Hence, similar to the ablation study when evaluating GNN models [29], we define two proxy models such as GNN Without Using Fea_x0002_tures (GNNWUF) and Multi-Layer Perceptron (MLP). The two models are chosen, since the two proxy models have the same model architectures as the GNN but are trained using different input data. GNNWUF is trained only using the graph structure while MLP is trained only using the node features. When training GNNWUF, we use one hot encoding as the node feature for each node, meaning GNNWUF considers only the graph structures. When GNN considers only the features of the node itself, then it can degenerate into an MLP model. Hence, MLP is chosen as the other GNN proxy model that only considers the node features and is used to evaluate the influence of node struc_x0002_tures. We train both proxy models with the same settings as the training of GNN. To further help users understand the impact of the graph structure and node features, we also provide a number of metrics, including graph structure based metrics that take into account the graph structure but ignore the node fea_x0002_tures, and node feature based metrics that take the node fea_x0002_tures into account but ignore the graph structure. Those metrics are derived from expert interviews. Details are pre_x0002_sented in the following paragraphs.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "We use the classic node-link diagram with the force directed collision-avoidance layout to visualize the graph dataset. Users can get a sense of the distribution of the selected nodes in the graph, and inspect the neighborhood of the nodes. To further facilitate the convenient exploration of the reasons for errors, we also design a node glyph to encode a group of node-level metrics. Users can hover a node in the Graph View, which will be further highlighted with the radius doubled. The Graph View allows users to quickly check any interesting neighboring nodes. Users can also switch to the \u201cSubgraph\u201d mode. It will display the one-hop and two-hop neighbors of selected nodes, enabling users to explore different hops of neighborhood nodes. Users can visualize the specific subgraphs on their own and can explore them by changing the \u201cSubgraph\u201d options. An overview of the graph is displayed in the bottom right-hand corner to support users navigating the graph. Users can click the specific position in the overview to navigate the displayed area of the graph. Users can choose to filter out the unfocused nodes to accelerate the rendering and reduce the visual clutter in the graph. To investigate the node features and most similar features of training nodes, users can click on the nodes of interest in the Graph View and further explore the node-level features in the Feature Matrix View.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "Line+Circle+Pie",
        "axial_code": ["Nesting"],
        "componenet_code": ["Line", "Circle", "Pie"]
      },
      {
        "solution_text": "The Graph View allows users to quickly check any interesting neighboring nodes. Users can also switch to the \u201cSubgraph\u201d mode. It will display the one-hop and two-hop neighbors of selected nodes, enabling users to explore different hops of neighborhood nodes. Users can visualize the specific subgraphs on their own and can explore them by changing the \u201cSubgraph\u201d options. An overview of the graph is displayed in the bottom right-hand corner to support users navigating the graph. Users can click the specific position in the overview to navigate the displayed area of the graph. Users can choose to filter out the unfocused nodes to accelerate the rendering and reduce the visual clutter in the graph. To investigate the node features and most similar features of training nodes, users can click on the nodes of interest in the Graph View and further explore the node-level features in the Feature Matrix View.",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Filtering+OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore", "Selecting", "Filtering"],
        "componenet_code": ["overview_and_explore", "selecting", "filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 32,
    "paper_title": "GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks",
    "pub_year": 2023,
    "domain": "Graph neural networks",
    "requirement": {
      "requirement_text": "R3: Analyze the Cause of Error Patterns. After identifying error patterns, finding the causes of these errors is important for users to understand, diagnose, and improve the GNNs. More detailed information is needed to understand the possible causes of error patterns. Specifically, users need to inspect the graph structures and node features to determine the causes of error patterns. According to the feedback from expert interviews, there are two main sources of wrong GNN predictions: noise in the training data and inaccurate feature aggregation in GNNs. To predict the label of a node, GNN aggregates the node\u2019s own feature with the features of the neighboring nodes at each layer. Noise in the training data, e.g., the same nodes but different labels, can confuse the GNN and lead to wrong predictions. Inaccurate feature aggregation at any layer will also influence the GNN prediction of the node.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "GNN prediction results",
      "data_code": { "network_and_trees": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Inspired by the fact that GNN prediction results are influ_x0002_enced by both graph structure and node features [14], we define two proxy models to analyze the influence of the graph structure and node features on GNN prediction results. Through expert interviews, experts are concerned about whether the graph structure or node features have a greater impact on GNN prediction, and then determine which components will have more impact. Hence, similar to the ablation study when evaluating GNN models [29], we define two proxy models such as GNN Without Using Fea_x0002_tures (GNNWUF) and Multi-Layer Perceptron (MLP). The two models are chosen, since the two proxy models have the same model architectures as the GNN but are trained using different input data. GNNWUF is trained only using the graph structure while MLP is trained only using the node features. When training GNNWUF, we use one hot encoding as the node feature for each node, meaning GNNWUF considers only the graph structures. When GNN considers only the features of the node itself, then it can degenerate into an MLP model. Hence, MLP is chosen as the other GNN proxy model that only considers the node features and is used to evaluate the influence of node struc_x0002_tures. We train both proxy models with the same settings as the training of GNN. To further help users understand the impact of the graph structure and node features, we also provide a number of metrics, including graph structure based metrics that take into account the graph structure but ignore the node fea_x0002_tures, and node feature based metrics that take the node fea_x0002_tures into account but ignore the graph structure. Those metrics are derived from expert interviews. Details are pre_x0002_sented in the following paragraphs.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "For categorical features, one-hot encoding can be used to convert them to one-hot vectors, where the value of each element of the vector will be either 0 and 1 [61",
        "solution_category": "data_manipulation",
        "solution_axial": "Reconfigure",
        "solution_compoent": "",
        "axial_code": ["Reconfigure"],
        "componenet_code": ["reconfigure"]
      },
      {
        "solution_text": "We design the Feature Matrix View to help users further explore the node features, as shown in Fig. 6. The Feature Matrix View consists of two components, i.e., a brushable bar chart and a feature matrix. We first assume that all the features used in our dataset range from zero to one. For categorical features, one-hot encoding can be used to convert them to one-hot vectors, where the value of each element of the vector will be either 0 and 1 [61]. Thus, it can satisfy our assumption as well. The feature matrix indicates all the node features. The color enc_x0002_odes the prediction label of that node and the opacity enco_x0002_des the specific feature value. In the brushable bar chart, the bar height encodes the count of any features with a value larger than 0 in the feature matrix. Users can brush a range of bars in the brushable bar chart and thus the feature matrix will display the specific range of feature dimensions. This makes it really convenient for users to inspect the fea_x0002_tures of nodes with a high dimensionality and without this design, the scalability of this view is not guaranteed. Users can change the sorting methods of feature dimensions. It can be sorted based on node ordering or frequency of features. When users select a subset of nodes in Parallel Sets View and Projection View, it will display the features of selected nodes. The hierarchical clustering algorithm and optimal leaf ordering [62] will be employed to generate the node ordering. After sorting the nodes, the similarity will be cal_x0002_culated between two consecutive nodes. If they are very similar, we highlight them by adding a border in the rectan_x0002_gle in the rows of corresponding nodes. When a node is selected in the Graph View, it will display the features of that node and the top-k most similar feature training nodes. The training nodes will be sorted based on feature similarity with that node. When sorting feature dimensions based on the frequency of features, the following strategy is used. For each dimension of the features, we first count the frequency jNj and then calculate the frequency of support jSUPPj, i.e., the number of nodes with the same features and model prediction label as the first node. We then calculate the sup_x0002_port rate of the features SUPPRATE by using formula: SUPPRATE \u00bc jSUPPj=jNj. Therefore, when the support rate is high, it will have a higher ranking. When the support\nrate between two dimensions of a feature is the same, it is sorted based on the frequency of features. Then we can figure out what features can be supportive of the predic_x0002_tions for the GNN model.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Unsimilar-Continuous",
        "solution_compoent": "Bar+Matrix",
        "axial_code": ["Juxtaposition-Unsimilar-Continuous"],
        "componenet_code": ["Bar", "Matrix"]
      },
      {
        "solution_text": "We design the Feature Matrix View to help users further explore the node features, as shown in Fig. 6. The Feature Matrix View consists of two components, i.e., a brushable bar chart and a feature matrix. We first assume that all the features used in our dataset range from zero to one. For categorical features, one-hot encoding can be used to convert them to one-hot vectors, where the value of each element of the vector will be either 0 and 1 [61]. Thus, it can satisfy our assumption as well. The feature matrix indicates all the node features. The color enc_x0002_odes the prediction label of that node and the opacity enco_x0002_des the specific feature value. In the brushable bar chart, the bar height encodes the count of any features with a value larger than 0 in the feature matrix. Users can brush a range of bars in the brushable bar chart and thus the feature matrix will display the specific range of feature dimensions. This makes it really convenient for users to inspect the fea_x0002_tures of nodes with a high dimensionality and without this design, the scalability of this view is not guaranteed. Users can change the sorting methods of feature dimensions. It can be sorted based on node ordering or frequency of features. When users select a subset of nodes in Parallel Sets View and Projection View, it will display the features of selected nodes. The hierarchical clustering algorithm and optimal leaf ordering [62] will be employed to generate the node ordering. After sorting the nodes, the similarity will be cal_x0002_culated between two consecutive nodes. If they are very similar, we highlight them by adding a border in the rectan_x0002_gle in the rows of corresponding nodes. When a node is selected in the Graph View, it will display the features of that node and the top-k most similar feature training nodes. The training nodes will be sorted based on feature similarity with that node. When sorting feature dimensions based on the frequency of features, the following strategy is used. For each dimension of the features, we first count the frequency jNj and then calculate the frequency of support jSUPPj, i.e., the number of nodes with the same features and model prediction label as the first node. We then calculate the sup_x0002_port rate of the features SUPPRATE by using formula: SUPPRATE \u00bc jSUPPj=jNj. Therefore, when the support rate is high, it will have a higher ranking. When the support\nrate between two dimensions of a feature is the same, it is sorted based on the frequency of features. Then we can figure out what features can be supportive of the predic_x0002_tions for the GNN model.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 33,
    "paper_title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents",
    "pub_year": 2023,
    "domain": "Legal",
    "requirement": {
      "requirement_text": "T1: Identification of explicit and potential citations: The system should exhibit decisions that explicitly cite a specific binding precedent and identify/exhibit those that could potentially mention it (non-explicit citation).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In total, we have 58 binding precedents, idealized as a mechanism to create a consolidated understanding among the STF\u2019s justices. We collected our dataset from a partner_x0002_ship with the Supremo em N_x0001_umeros (\u201cSTF in Numbers\u201d, in free translation project [43] \u2014 a project that seeks to assess legal and computer knowledge to produce unprecedented data on the Supreme Court. This dataset contains more than 2,500,000 documents since 1988 and metadata such as the number of the BP being cited (if that is the case) and docu_x0002_ment type. Considering only decisions that explicitly cite at least one of the 58 BPs, there are 38,364 documents, totaling 41,031 citations (some documents may mention more than one BP). The number of citations per BP does not follow a uniform distribution, as we can see in Fig. 2.",
      "data_code": { "tables": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Text Embedding. As mentioned above, we use a classifica_x0002_tion model to assess the probability of a document citing a BP. To assess different classifiers\u2019 performance, we rely on text embedding methods to generate a vector representation of the documents. We test different embeddings to evaluate their performance: TF-IDF [47], Doc2vec [44], Universal Sen_x0002_tence Encoder (USE) [10], and Longformer [48]. In TF-IDF\u2019s particular case, we have also applied a dimensionality reduction procedure to map the documents to a 50-dimen_x0002_sional space using Truncated Singular Value Decomposition (SVD) [49]. We call the final representation Truncated TF_x0002_IDF. The reason to consider Longformer, instead of a more popular language model, e.g., BERT [50], is that Longformer overcomes BERT\u2019s limitation of 512 tokens. Note that USE and Longformer do not need a pre-processed text (as described in the previous paragraph) because both have their tokenization mechanism.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Classification. Support Vector Machines (SVM) are com_x0002_plex enough for real-world classification problems and sim_x0002_ple enough to be analyzed mathematically [51]. Our study considers SVM with linear and Radial Basis Function (RBF) kernels. Moreover, given that Longformer [48] has a linear classification neural network layer plugged into it, we also fine-tuned it to assess its classification performance in our context. The described classifiers are used to search for potential citations in unlabeled data, assigning probabilities to each document to belong to each class. To get probabilities from an SVM, we need to make calibration: using labeled data, create a map from the classifier\u2019s output (SVM scores) to a probability estimate for each class, which sum up to 1. To create this map, we use the calibration method from Platt et al. [52] for SVM with linear kernel and the Wu et al.\u2019s [53] method for RBF kernel. Platt et al.\u2019s method, precisely, does not support the multiclass case, so we calibrate each class in a \u201cone-versus-rest\u201d approach and normalize the results in the end to sum up to 1. This map will receive the SVM scores from a data instance and output a probability esti_x0002_mate for each class.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "This section describes how we use the classifiers to identify potential citations and understand the models\u2019 decisions (see Fig. 3b). Citation Inference. The association between a text embed_x0002_ding technique and a classifier gives us what we call a model. A model receives the raw text and returns the classifier\u2019s probabilities, which we interpret as the document\u2019s proba_x0002_bility to cite each precedent. We consider a potential citation when the citation probability is greater or equal to a thresh_x0002_old tc 2 \u00bd0; 1_x0003_ (chosen by the user). Interpretability. A concern when dealing with machine learning models is their interpretability. For instance, if Truncated TF-IDF combined with SVM points out that a document potentially cites BP 10 because of high returned probability, how can we understand the reasons behind this decision? Generally speaking, how can we trust this model? Understanding why a model is taking a particular decision is of paramount importance [45], especially in sensitive sce_x0002_narios like a legal document analysis. Consider document X and its embedding vector x 2 Rd (e.g., TF-IDF vector). This document\u2019s probability of belong_x0002_ing to class C is given by fC\u00f0x\u00de, with fC : Rd ! \u00bd0; 1_x0003_ a mod_x0002_el\u2019s returned probability for class C. Our particular interest is to know the importance of each sentence from document X to the given probability, i.e., if each sentence has positive, negative, or neutral importance, and the magnitude of this importance. After preliminary tests have discarded the employment of the leave-one-out feature importance (LOO) [54] for interpretability due to its high sensitivity, we chose the Local Interpretable Model-agnostic Explanations (Lime) [45] method to tackle this issue. The intuition is that, by removing a spe_x0002_cific sentence and obtaining the variation of probability DfC, we can measure the importance of this sentence for the prediction fC\u00f0x\u00de. More formally, we randomly remove some sentences from document X, vectorize this new docu_x0002_ment to z, and add it to set Z. Doing this many times, we have a collection Z of vectors around x in high dimensional space. We approximate fC\u00f0z\u00de using a linear model g\u00f0z\u00de, minimizing the approximation error L\u00f0fC; g; z\u00de in z 2 Z, but also constraining the complexity This task can be interpreted as a weighted linear regres_x0002_V\u00f0g\u00de of g. sion with regularization. We end with a model g\u00f0z\u00de that is linear over the sentences (i.e., the presence or absence of a sentence), where the linear model\u2019s coefficients can be inter_x0002_preted as the importance score of each sentence to the final prediction By default, Lime works with words, and there is also the fC\u00f0x\u00de. possibility of interpreting entire paragraphs. A word divi_x0002_sion brings some advantages, such as detailing. Still, it does not immediately assess each sentence\u2019s importance to the decision (e.g., sentences similar to the BP text). A paragraph division is also interesting for visualization purposes (see, for example, Section 6.3), but it merges various sentences into one, hampering the precise identification of relevant parts. Therefore, our choice of working with sentences com_x0002_prises the \u201cbest of both worlds\u201d. In our experiments, Lime proved to be very robust and reliable when dealing with sentences.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "In this view, the x-axis represents the time a document was published (monthly resolution), and the y-axis represents the BP. The red pins ( ) refer to the publication date of the corresponding BPs. For each one, a vertical bar on a particular date indicates the existence of documents published on that date that cite such BP.The height of each bar reflects the number of documents published on that date, and its color is defined such that (i) blue bars (without borders) indicate that every document in that month cites the BP explicitly, (ii) orange bars (without borders) suggest that every document potentially (rather than explicitly) cites the BP, and (iii) blue bars with orange edges indicate the presence of both explicit and potential citations.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Sharing",
        "solution_compoent": "Bar",
        "axial_code": ["Overlay-Coordinatesystemrelated-Sharing"],
        "componenet_code": ["Bar"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 34,
    "paper_title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents",
    "pub_year": 2023,
    "domain": "Legal",
    "requirement": {
      "requirement_text": "T1: Identification of explicit and potential citations: The system should exhibit decisions that explicitly cite a specific binding precedent and identify/exhibit those that could potentially mention it (non-explicit citation).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In total, we have 58 binding precedents, idealized as a mechanism to create a consolidated understanding among the STF\u2019s justices. We collected our dataset from a partner_x0002_ship with the Supremo em N_x0001_umeros (\u201cSTF in Numbers\u201d, in free translation project [43] \u2014 a project that seeks to assess legal and computer knowledge to produce unprecedented data on the Supreme Court. This dataset contains more than 2,500,000 documents since 1988 and metadata such as the number of the BP being cited (if that is the case) and docu_x0002_ment type. Considering only decisions that explicitly cite at least one of the 58 BPs, there are 38,364 documents, totaling 41,031 citations (some documents may mention more than one BP). The number of citations per BP does not follow a uniform distribution, as we can see in Fig. 2.",
      "data_code": { "geometry": 1, "tables": 1, "fields": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Text Embedding. As mentioned above, we use a classifica_x0002_tion model to assess the probability of a document citing a BP. To assess different classifiers\u2019 performance, we rely on text embedding methods to generate a vector representation of the documents. We test different embeddings to evaluate their performance: TF-IDF [47], Doc2vec [44], Universal Sen_x0002_tence Encoder (USE) [10], and Longformer [48]. In TF-IDF\u2019s particular case, we have also applied a dimensionality reduction procedure to map the documents to a 50-dimen_x0002_sional space using Truncated Singular Value Decomposition (SVD) [49]. We call the final representation Truncated TF_x0002_IDF. The reason to consider Longformer, instead of a more popular language model, e.g., BERT [50], is that Longformer overcomes BERT\u2019s limitation of 512 tokens. Note that USE and Longformer do not need a pre-processed text (as described in the previous paragraph) because both have their tokenization mechanism.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Classification. Support Vector Machines (SVM) are com_x0002_plex enough for real-world classification problems and sim_x0002_ple enough to be analyzed mathematically [51]. Our study considers SVM with linear and Radial Basis Function (RBF) kernels. Moreover, given that Longformer [48] has a linear classification neural network layer plugged into it, we also fine-tuned it to assess its classification performance in our context. The described classifiers are used to search for potential citations in unlabeled data, assigning probabilities to each document to belong to each class. To get probabilities from an SVM, we need to make calibration: using labeled data, create a map from the classifier\u2019s output (SVM scores) to a probability estimate for each class, which sum up to 1. To create this map, we use the calibration method from Platt et al. [52] for SVM with linear kernel and the Wu et al.\u2019s [53] method for RBF kernel. Platt et al.\u2019s method, precisely, does not support the multiclass case, so we calibrate each class in a \u201cone-versus-rest\u201d approach and normalize the results in the end to sum up to 1. This map will receive the SVM scores from a data instance and output a probability esti_x0002_mate for each class.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "This section describes how we use the classifiers to identify potential citations and understand the models\u2019 decisions (see Fig. 3b). Citation Inference. The association between a text embed_x0002_ding technique and a classifier gives us what we call a model. A model receives the raw text and returns the classifier\u2019s probabilities, which we interpret as the document\u2019s proba_x0002_bility to cite each precedent. We consider a potential citation when the citation probability is greater or equal to a thresh_x0002_old tc 2 \u00bd0; 1_x0003_ (chosen by the user). Interpretability. A concern when dealing with machine learning models is their interpretability. For instance, if Truncated TF-IDF combined with SVM points out that a document potentially cites BP 10 because of high returned probability, how can we understand the reasons behind this decision? Generally speaking, how can we trust this model? Understanding why a model is taking a particular decision is of paramount importance [45], especially in sensitive sce_x0002_narios like a legal document analysis. Consider document X and its embedding vector x 2 Rd (e.g., TF-IDF vector). This document\u2019s probability of belong_x0002_ing to class C is given by fC\u00f0x\u00de, with fC : Rd ! \u00bd0; 1_x0003_ a mod_x0002_el\u2019s returned probability for class C. Our particular interest is to know the importance of each sentence from document X to the given probability, i.e., if each sentence has positive, negative, or neutral importance, and the magnitude of this importance. After preliminary tests have discarded the employment of the leave-one-out feature importance (LOO) [54] for interpretability due to its high sensitivity, we chose the Local Interpretable Model-agnostic Explanations (Lime) [45] method to tackle this issue. The intuition is that, by removing a spe_x0002_cific sentence and obtaining the variation of probability DfC, we can measure the importance of this sentence for the prediction fC\u00f0x\u00de. More formally, we randomly remove some sentences from document X, vectorize this new docu_x0002_ment to z, and add it to set Z. Doing this many times, we have a collection Z of vectors around x in high dimensional space. We approximate fC\u00f0z\u00de using a linear model g\u00f0z\u00de, minimizing the approximation error L\u00f0fC; g; z\u00de in z 2 Z, but also constraining the complexity This task can be interpreted as a weighted linear regres_x0002_V\u00f0g\u00de of g. sion with regularization. We end with a model g\u00f0z\u00de that is linear over the sentences (i.e., the presence or absence of a sentence), where the linear model\u2019s coefficients can be inter_x0002_preted as the importance score of each sentence to the final prediction By default, Lime works with words, and there is also the fC\u00f0x\u00de. possibility of interpreting entire paragraphs. A word divi_x0002_sion brings some advantages, such as detailing. Still, it does not immediately assess each sentence\u2019s importance to the decision (e.g., sentences similar to the BP text). A paragraph division is also interesting for visualization purposes (see, for example, Section 6.3), but it merges various sentences into one, hampering the precise identification of relevant parts. Therefore, our choice of working with sentences com_x0002_prises the \u201cbest of both worlds\u201d. In our experiments, Lime proved to be very robust and reliable when dealing with sentences.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "the color of each stacked bar stands for explicit (blue) or potential (orange) citation. Once the user has found a set of documents (i.e., a bar) of interest in Global View, he/she can select such a set by clicking on the bar. The user is then redirected to the Paragraph Similarities View (Fig. 5b), which presents in the y-axis all documents from the selected bar, that is, documents that cite a particular BP on a given date. Each document is divided into paragraphs, indicated by the horizontal stack of bars. The bar\u2019s size means the size of the paragraph, and the color intensity reflects the similarity between the corresponding paragraph and the BP text; the darker the color, the greater the similarity, and the more common parts exist between the BP and the paragraph (T5). Similarly to Global View, the color of each stacked bar stands for explicit (blue) or potential (orange) citation (T1). The similarity is given by the angular dis_x0002_tance [10] between two Truncated-TF-IDF vectors, but other embeddings and similarities (e.g., the raw cosine similarity) could be adopted. To guide users further exploring the set of documents, we group them into clusters (T4 \u2013 details below). Inside each cluster, the documents are positioned in descending order of similarity between the document and the BP, which is defined as the maximum similarity between its paragraphs and the BP (T4). Showing documents in descending order of similarity is helpful because the user can promptly identify the top-k most similar, and therefore most interesting, documents. Further_x0002_more, to quickly assess the document distribution w.r.t. the similarity values, we also show an interactive bar chart above each color bar. Document Clustering. Since a binding precedent may cover decisions related to different subjects, the Brazilian Supreme Court website provides, for each BP, some clusters of decisions created according to their subjects. There are only a few clusters for each BP, each containing a few labeled documents (for instance, there are 8 clusters for BP 4, each containing two documents on average). We employed NLP text pre-processing and applied topic modeling strategies to cluster documents according to their relevant words to take advantage of this limited but useful ground truth. We have tested different and well-established topic extraction methods, including LDA [57], NMF [58], SNMF [59], and PSMF [60]. The NMF (Frobenius norm) method presented the best results, so we adopted it as the default method.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Unsimilar-Continuous",
        "solution_compoent": "Bar",
        "axial_code": ["Juxtaposition-Unsimilar-Continuous"],
        "componenet_code": ["Bar"]
      },
      {
        "solution_text": "Once the user has found a set of documents (i.e., a bar) of interest in Global View, he/she can select such a set by clicking on the bar. The user is then redirected to the Paragraph Similarities View (Fig. 5b), which presents in the y-axis all documents from the selected bar, that is, documents that cite a particular BP on a given date. Each document is divided into paragraphs, indicated by the horizontal stack of bars. ",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Connect/Relate+Encode",
        "solution_compoent": "",
        "axial_code": ["Connect/Relate", "Selecting", "Encode"],
        "componenet_code": ["connect/relate", "selecting", "encode"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 35,
    "paper_title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents",
    "pub_year": 2023,
    "domain": "Legal",
    "requirement": {
      "requirement_text": "T1: Identification of explicit and potential citations: The system should exhibit decisions that explicitly cite a specific binding precedent and identify/exhibit those that could potentially mention it (non-explicit citation).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In total, we have 58 binding precedents, idealized as a mechanism to create a consolidated understanding among the STF\u2019s justices. We collected our dataset from a partner_x0002_ship with the Supremo em N_x0001_umeros (\u201cSTF in Numbers\u201d, in free translation project [43] \u2014 a project that seeks to assess legal and computer knowledge to produce unprecedented data on the Supreme Court. This dataset contains more than 2,500,000 documents since 1988 and metadata such as the number of the BP being cited (if that is the case) and docu_x0002_ment type. Considering only decisions that explicitly cite at least one of the 58 BPs, there are 38,364 documents, totaling 41,031 citations (some documents may mention more than one BP). The number of citations per BP does not follow a uniform distribution, as we can see in Fig. 2.",
      "data_code": { "clusters_and_sets_and_lists": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Text Embedding. As mentioned above, we use a classifica_x0002_tion model to assess the probability of a document citing a BP. To assess different classifiers\u2019 performance, we rely on text embedding methods to generate a vector representation of the documents. We test different embeddings to evaluate their performance: TF-IDF [47], Doc2vec [44], Universal Sen_x0002_tence Encoder (USE) [10], and Longformer [48]. In TF-IDF\u2019s particular case, we have also applied a dimensionality reduction procedure to map the documents to a 50-dimen_x0002_sional space using Truncated Singular Value Decomposition (SVD) [49]. We call the final representation Truncated TF_x0002_IDF. The reason to consider Longformer, instead of a more popular language model, e.g., BERT [50], is that Longformer overcomes BERT\u2019s limitation of 512 tokens. Note that USE and Longformer do not need a pre-processed text (as described in the previous paragraph) because both have their tokenization mechanism.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Classification. Support Vector Machines (SVM) are com_x0002_plex enough for real-world classification problems and sim_x0002_ple enough to be analyzed mathematically [51]. Our study considers SVM with linear and Radial Basis Function (RBF) kernels. Moreover, given that Longformer [48] has a linear classification neural network layer plugged into it, we also fine-tuned it to assess its classification performance in our context. The described classifiers are used to search for potential citations in unlabeled data, assigning probabilities to each document to belong to each class. To get probabilities from an SVM, we need to make calibration: using labeled data, create a map from the classifier\u2019s output (SVM scores) to a probability estimate for each class, which sum up to 1. To create this map, we use the calibration method from Platt et al. [52] for SVM with linear kernel and the Wu et al.\u2019s [53] method for RBF kernel. Platt et al.\u2019s method, precisely, does not support the multiclass case, so we calibrate each class in a \u201cone-versus-rest\u201d approach and normalize the results in the end to sum up to 1. This map will receive the SVM scores from a data instance and output a probability esti_x0002_mate for each class.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "This section describes how we use the classifiers to identify potential citations and understand the models\u2019 decisions (see Fig. 3b). Citation Inference. The association between a text embed_x0002_ding technique and a classifier gives us what we call a model. A model receives the raw text and returns the classifier\u2019s probabilities, which we interpret as the document\u2019s proba_x0002_bility to cite each precedent. We consider a potential citation when the citation probability is greater or equal to a thresh_x0002_old tc 2 \u00bd0; 1_x0003_ (chosen by the user). Interpretability. A concern when dealing with machine learning models is their interpretability. For instance, if Truncated TF-IDF combined with SVM points out that a document potentially cites BP 10 because of high returned probability, how can we understand the reasons behind this decision? Generally speaking, how can we trust this model? Understanding why a model is taking a particular decision is of paramount importance [45], especially in sensitive sce_x0002_narios like a legal document analysis. Consider document X and its embedding vector x 2 Rd (e.g., TF-IDF vector). This document\u2019s probability of belong_x0002_ing to class C is given by fC\u00f0x\u00de, with fC : Rd ! \u00bd0; 1_x0003_ a mod_x0002_el\u2019s returned probability for class C. Our particular interest is to know the importance of each sentence from document X to the given probability, i.e., if each sentence has positive, negative, or neutral importance, and the magnitude of this importance. After preliminary tests have discarded the employment of the leave-one-out feature importance (LOO) [54] for interpretability due to its high sensitivity, we chose the Local Interpretable Model-agnostic Explanations (Lime) [45] method to tackle this issue. The intuition is that, by removing a spe_x0002_cific sentence and obtaining the variation of probability DfC, we can measure the importance of this sentence for the prediction fC\u00f0x\u00de. More formally, we randomly remove some sentences from document X, vectorize this new docu_x0002_ment to z, and add it to set Z. Doing this many times, we have a collection Z of vectors around x in high dimensional space. We approximate fC\u00f0z\u00de using a linear model g\u00f0z\u00de, minimizing the approximation error L\u00f0fC; g; z\u00de in z 2 Z, but also constraining the complexity This task can be interpreted as a weighted linear regres_x0002_V\u00f0g\u00de of g. sion with regularization. We end with a model g\u00f0z\u00de that is linear over the sentences (i.e., the presence or absence of a sentence), where the linear model\u2019s coefficients can be inter_x0002_preted as the importance score of each sentence to the final prediction By default, Lime works with words, and there is also the fC\u00f0x\u00de. possibility of interpreting entire paragraphs. A word divi_x0002_sion brings some advantages, such as detailing. Still, it does not immediately assess each sentence\u2019s importance to the decision (e.g., sentences similar to the BP text). A paragraph division is also interesting for visualization purposes (see, for example, Section 6.3), but it merges various sentences into one, hampering the precise identification of relevant parts. Therefore, our choice of working with sentences com_x0002_prises the \u201cbest of both worlds\u201d. In our experiments, Lime proved to be very robust and reliable when dealing with sentences.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "After finding a decision that seems interesting, the user selects one of its paragraphs by clicking on it. At this moment, he/she is redirected to Document Reader (Fig. 5c), a view that enables the analysis of both the document\u2019s content and the BP text. In this view, the colored bars from the Paragraph Similarities View are also visible alongside the paragraphs (Figs. 5c and 6). When a paragraph is hovered over, its content and the BP text are shown as illustrated in Fig. 6, with the common parts highlighted or not depending on the user\u2019s needs (T5\u2014 flag \u201cShow text similarity\u201d). In the example shown in the figure, one may notice that the text of the paragraph associ_x0002_ated with the darkest blue bar is contained in the BP text (see yellow highlights). To analyze any other paragraph of the current document, one may hover over its text without revisiting Paragraph Similarities View. Document Reader also incorporates Lime when the opened document refers to a potential citation (T1, T5). In this case, instead of just showing the document\u2019s raw text (as in Fig. 6), the parts of the document\u2019s content are highlighted according to their influence (negative, neutral, or positive) in the potential citation identification process (see Fig. 5c). Since one may analyze many documents published on different dates and citing different BPs, Document Reader also keeps track of recently opened documents (T6), allow_x0002_ing the user to quickly revisit them through the document browsing history (Fig. 6). In practice, it is not uncommon to find complex cases in which a single justice writes his or her decision in a document with more than 100 pages. Document Reader minimizes experts\u2019 reading time by tracking recently opened documents of interest and pointing out exactly which para_x0002_graphs of the document are likely to mention the binding precedent, even if not explicitly. These characteristics increase the time efficiency of those who search for specific citations in long documents.",
        "solution_category": "visualization",
        "solution_axial": "Non-composite",
        "solution_compoent": "Text",
        "axial_code": ["Non-composite"],
        "componenet_code": ["Text"]
      },
      {
        "solution_text": "After finding a decision that seems interesting, the user selects one of its paragraphs by clicking on it. At this moment, he/she is redirected to Document Reader (Fig. 5c), a view that enables the analysis of both the document\u2019s content and the BP text. In this view, the colored bars from the Paragraph Similarities View are also visible alongside the paragraphs (Figs. 5c and 6). When a paragraph is hovered over, its content and the BP text are shown as illustrated in Fig. 6, with the common parts highlighted or not depending on the user\u2019s needs (T5\u2014 flag \u201cShow text similarity\u201d). In the example shown in the figure, one may notice that the text of the paragraph associ_x0002_ated with the darkest blue bar is contained in the BP text (see yellow highlights). To analyze any other paragraph of the current document, one may hover over its text without revisiting Paragraph Similarities View. Document Reader also incorporates Lime when the opened document refers to a potential citation (T1, T5). In this case, instead of just showing the document\u2019s raw text (as in Fig. 6), the parts of the document\u2019s content are highlighted according to their influence (negative, neutral, or positive) in the potential citation identification process (see Fig. 5c). Since one may analyze many documents published on different dates and citing different BPs, Document Reader also keeps track of recently opened documents (T6), allow_x0002_ing the user to quickly revisit them through the document browsing history (Fig. 6). In practice, it is not uncommon to find complex cases in which a single justice writes his or her decision in a document with more than 100 pages. Document Reader minimizes experts\u2019 reading time by tracking recently opened documents of interest and pointing out exactly which para_x0002_graphs of the document are likely to mention the binding precedent, even if not explicitly. These characteristics increase the time efficiency of those who search for specific citations in long documents.",
        "solution_category": "interaction",
        "solution_axial": "Selecting;Connect/Relate;Encode",
        "solution_compoent": "",
        "axial_code": ["Connect/Relate", "Selecting", "Encode"],
        "componenet_code": ["connect/relate", "selecting", "encode"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 36,
    "paper_title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents",
    "pub_year": 2023,
    "domain": "Legal",
    "requirement": {
      "requirement_text": "T2: Overview of decisions and binding precedents: The system should provide an overview of all the decisions and binding precedents, highlighting the chronological order they appear.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "In total, we have 58 binding precedents, idealized as a mechanism to create a consolidated understanding among the STF\u2019s justices. We collected our dataset from a partner_x0002_ship with the Supremo em N_x0001_umeros (\u201cSTF in Numbers\u201d, in free translation project [43] \u2014 a project that seeks to assess legal and computer knowledge to produce unprecedented data on the Supreme Court. This dataset contains more than 2,500,000 documents since 1988 and metadata such as the number of the BP being cited (if that is the case) and docu_x0002_ment type. Considering only decisions that explicitly cite at least one of the 58 BPs, there are 38,364 documents, totaling 41,031 citations (some documents may mention more than one BP). The number of citations per BP does not follow a uniform distribution, as we can see in Fig. 2.",
      "data_code": { "textual": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Text Embedding. As mentioned above, we use a classifica_x0002_tion model to assess the probability of a document citing a BP. To assess different classifiers\u2019 performance, we rely on text embedding methods to generate a vector representation of the documents. We test different embeddings to evaluate their performance: TF-IDF [47], Doc2vec [44], Universal Sen_x0002_tence Encoder (USE) [10], and Longformer [48]. In TF-IDF\u2019s particular case, we have also applied a dimensionality reduction procedure to map the documents to a 50-dimen_x0002_sional space using Truncated Singular Value Decomposition (SVD) [49]. We call the final representation Truncated TF_x0002_IDF. The reason to consider Longformer, instead of a more popular language model, e.g., BERT [50], is that Longformer overcomes BERT\u2019s limitation of 512 tokens. Note that USE and Longformer do not need a pre-processed text (as described in the previous paragraph) because both have their tokenization mechanism.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Classification. Support Vector Machines (SVM) are com_x0002_plex enough for real-world classification problems and sim_x0002_ple enough to be analyzed mathematically [51]. Our study considers SVM with linear and Radial Basis Function (RBF) kernels. Moreover, given that Longformer [48] has a linear classification neural network layer plugged into it, we also fine-tuned it to assess its classification performance in our context. The described classifiers are used to search for potential citations in unlabeled data, assigning probabilities to each document to belong to each class. To get probabilities from an SVM, we need to make calibration: using labeled data, create a map from the classifier\u2019s output (SVM scores) to a probability estimate for each class, which sum up to 1. To create this map, we use the calibration method from Platt et al. [52] for SVM with linear kernel and the Wu et al.\u2019s [53] method for RBF kernel. Platt et al.\u2019s method, precisely, does not support the multiclass case, so we calibrate each class in a \u201cone-versus-rest\u201d approach and normalize the results in the end to sum up to 1. This map will receive the SVM scores from a data instance and output a probability esti_x0002_mate for each class.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "This section describes how we use the classifiers to identify potential citations and understand the models\u2019 decisions (see Fig. 3b). Citation Inference. The association between a text embed_x0002_ding technique and a classifier gives us what we call a model. A model receives the raw text and returns the classifier\u2019s probabilities, which we interpret as the document\u2019s proba_x0002_bility to cite each precedent. We consider a potential citation when the citation probability is greater or equal to a thresh_x0002_old tc 2 \u00bd0; 1_x0003_ (chosen by the user). Interpretability. A concern when dealing with machine learning models is their interpretability. For instance, if Truncated TF-IDF combined with SVM points out that a document potentially cites BP 10 because of high returned probability, how can we understand the reasons behind this decision? Generally speaking, how can we trust this model? Understanding why a model is taking a particular decision is of paramount importance [45], especially in sensitive sce_x0002_narios like a legal document analysis. Consider document X and its embedding vector x 2 Rd (e.g., TF-IDF vector). This document\u2019s probability of belong_x0002_ing to class C is given by fC\u00f0x\u00de, with fC : Rd ! \u00bd0; 1_x0003_ a mod_x0002_el\u2019s returned probability for class C. Our particular interest is to know the importance of each sentence from document X to the given probability, i.e., if each sentence has positive, negative, or neutral importance, and the magnitude of this importance. After preliminary tests have discarded the employment of the leave-one-out feature importance (LOO) [54] for interpretability due to its high sensitivity, we chose the Local Interpretable Model-agnostic Explanations (Lime) [45] method to tackle this issue. The intuition is that, by removing a spe_x0002_cific sentence and obtaining the variation of probability DfC, we can measure the importance of this sentence for the prediction fC\u00f0x\u00de. More formally, we randomly remove some sentences from document X, vectorize this new docu_x0002_ment to z, and add it to set Z. Doing this many times, we have a collection Z of vectors around x in high dimensional space. We approximate fC\u00f0z\u00de using a linear model g\u00f0z\u00de, minimizing the approximation error L\u00f0fC; g; z\u00de in z 2 Z, but also constraining the complexity This task can be interpreted as a weighted linear regres_x0002_V\u00f0g\u00de of g. sion with regularization. We end with a model g\u00f0z\u00de that is linear over the sentences (i.e., the presence or absence of a sentence), where the linear model\u2019s coefficients can be inter_x0002_preted as the importance score of each sentence to the final prediction By default, Lime works with words, and there is also the fC\u00f0x\u00de. possibility of interpreting entire paragraphs. A word divi_x0002_sion brings some advantages, such as detailing. Still, it does not immediately assess each sentence\u2019s importance to the decision (e.g., sentences similar to the BP text). A paragraph division is also interesting for visualization purposes (see, for example, Section 6.3), but it merges various sentences into one, hampering the precise identification of relevant parts. Therefore, our choice of working with sentences com_x0002_prises the \u201cbest of both worlds\u201d. In our experiments, Lime proved to be very robust and reliable when dealing with sentences.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "This view is responsible for showing an overview of the dataset under a temporal perspective. In this view, the x-axis represents the time a document was published (monthly resolution), and the y-axis represents the BP. The red pins ( ) refer to the publication date of the corresponding BPs. For each one, a vertical bar on a particular date indicates the existence of documents published on that date that cite such BP.The height of each bar reflects the number of documents published on that date, and its color is defined such that (i) blue bars (without borders) indicate that every document in that month cites the BP explicitly, (ii) orange bars (without borders) suggest that every document potentially (rather than explicitly) cites the BP, and (iii) blue bars with orange edges indicate the presence of both explicit and potential citations.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Sharing",
        "solution_compoent": "Bar",
        "axial_code": ["Overlay-Coordinatesystemrelated-Sharing"],
        "componenet_code": ["Bar"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 38,
    "paper_title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents",
    "pub_year": 2023,
    "domain": "Legal",
    "requirement": {
      "requirement_text": "T4: Grouping and ordering decisions: The system should identify and group similar decisions that cite the same binding precedent on a given date (Q2.2) and order them in the layout according to the similarity between binding precedent and parts of a decision.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "In total, we have 58 binding precedents, idealized as a mechanism to create a consolidated understanding among the STF\u2019s justices. We collected our dataset from a partner_x0002_ship with the Supremo em N_x0001_umeros (\u201cSTF in Numbers\u201d, in free translation project [43] \u2014 a project that seeks to assess legal and computer knowledge to produce unprecedented data on the Supreme Court. This dataset contains more than 2,500,000 documents since 1988 and metadata such as the number of the BP being cited (if that is the case) and docu_x0002_ment type. Considering only decisions that explicitly cite at least one of the 58 BPs, there are 38,364 documents, totaling 41,031 citations (some documents may mention more than one BP). The number of citations per BP does not follow a uniform distribution, as we can see in Fig. 2.",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Text Embedding. As mentioned above, we use a classifica_x0002_tion model to assess the probability of a document citing a BP. To assess different classifiers\u2019 performance, we rely on text embedding methods to generate a vector representation of the documents. We test different embeddings to evaluate their performance: TF-IDF [47], Doc2vec [44], Universal Sen_x0002_tence Encoder (USE) [10], and Longformer [48]. In TF-IDF\u2019s particular case, we have also applied a dimensionality reduction procedure to map the documents to a 50-dimen_x0002_sional space using Truncated Singular Value Decomposition (SVD) [49]. We call the final representation Truncated TF_x0002_IDF. The reason to consider Longformer, instead of a more popular language model, e.g., BERT [50], is that Longformer overcomes BERT\u2019s limitation of 512 tokens. Note that USE and Longformer do not need a pre-processed text (as described in the previous paragraph) because both have their tokenization mechanism.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Classification. Support Vector Machines (SVM) are com_x0002_plex enough for real-world classification problems and sim_x0002_ple enough to be analyzed mathematically [51]. Our study considers SVM with linear and Radial Basis Function (RBF) kernels. Moreover, given that Longformer [48] has a linear classification neural network layer plugged into it, we also fine-tuned it to assess its classification performance in our context. The described classifiers are used to search for potential citations in unlabeled data, assigning probabilities to each document to belong to each class. To get probabilities from an SVM, we need to make calibration: using labeled data, create a map from the classifier\u2019s output (SVM scores) to a probability estimate for each class, which sum up to 1. To create this map, we use the calibration method from Platt et al. [52] for SVM with linear kernel and the Wu et al.\u2019s [53] method for RBF kernel. Platt et al.\u2019s method, precisely, does not support the multiclass case, so we calibrate each class in a \u201cone-versus-rest\u201d approach and normalize the results in the end to sum up to 1. This map will receive the SVM scores from a data instance and output a probability esti_x0002_mate for each class.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "This section describes how we use the classifiers to identify potential citations and understand the models\u2019 decisions (see Fig. 3b). Citation Inference. The association between a text embed_x0002_ding technique and a classifier gives us what we call a model. A model receives the raw text and returns the classifier\u2019s probabilities, which we interpret as the document\u2019s proba_x0002_bility to cite each precedent. We consider a potential citation when the citation probability is greater or equal to a thresh_x0002_old tc 2 \u00bd0; 1_x0003_ (chosen by the user). Interpretability. A concern when dealing with machine learning models is their interpretability. For instance, if Truncated TF-IDF combined with SVM points out that a document potentially cites BP 10 because of high returned probability, how can we understand the reasons behind this decision? Generally speaking, how can we trust this model? Understanding why a model is taking a particular decision is of paramount importance [45], especially in sensitive sce_x0002_narios like a legal document analysis. Consider document X and its embedding vector x 2 Rd (e.g., TF-IDF vector). This document\u2019s probability of belong_x0002_ing to class C is given by fC\u00f0x\u00de, with fC : Rd ! \u00bd0; 1_x0003_ a mod_x0002_el\u2019s returned probability for class C. Our particular interest is to know the importance of each sentence from document X to the given probability, i.e., if each sentence has positive, negative, or neutral importance, and the magnitude of this importance. After preliminary tests have discarded the employment of the leave-one-out feature importance (LOO) [54] for interpretability due to its high sensitivity, we chose the Local Interpretable Model-agnostic Explanations (Lime) [45] method to tackle this issue. The intuition is that, by removing a spe_x0002_cific sentence and obtaining the variation of probability DfC, we can measure the importance of this sentence for the prediction fC\u00f0x\u00de. More formally, we randomly remove some sentences from document X, vectorize this new docu_x0002_ment to z, and add it to set Z. Doing this many times, we have a collection Z of vectors around x in high dimensional space. We approximate fC\u00f0z\u00de using a linear model g\u00f0z\u00de, minimizing the approximation error L\u00f0fC; g; z\u00de in z 2 Z, but also constraining the complexity This task can be interpreted as a weighted linear regres_x0002_V\u00f0g\u00de of g. sion with regularization. We end with a model g\u00f0z\u00de that is linear over the sentences (i.e., the presence or absence of a sentence), where the linear model\u2019s coefficients can be inter_x0002_preted as the importance score of each sentence to the final prediction By default, Lime works with words, and there is also the fC\u00f0x\u00de. possibility of interpreting entire paragraphs. A word divi_x0002_sion brings some advantages, such as detailing. Still, it does not immediately assess each sentence\u2019s importance to the decision (e.g., sentences similar to the BP text). A paragraph division is also interesting for visualization purposes (see, for example, Section 6.3), but it merges various sentences into one, hampering the precise identification of relevant parts. Therefore, our choice of working with sentences com_x0002_prises the \u201cbest of both worlds\u201d. In our experiments, Lime proved to be very robust and reliable when dealing with sentences.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "we group them into clusters (T4 \u2013 details below). Inside each cluster, the documents are positioned in descending order of similarity between the document and the BP, which is defined as the maximum similarity between its paragraphs and the BP. the color of each stacked bar stands for explicit (blue) or potential (orange) citation. Once the user has found a set of documents (i.e., a bar) of interest in Global View, he/she can select such a set by clicking on the bar. The user is then redirected to the Paragraph Similarities View (Fig. 5b), which presents in the y-axis all documents from the selected bar, that is, documents that cite a particular BP on a given date. Each document is divided into paragraphs, indicated by the horizontal stack of bars. The bar\u2019s size means the size of the paragraph, and the color intensity reflects the similarity between the corresponding paragraph and the BP text; the darker the color, the greater the similarity, and the more common parts exist between the BP and the paragraph (T5). Similarly to Global View, the color of each stacked bar stands for explicit (blue) or potential (orange) citation (T1). The similarity is given by the angular dis_x0002_tance [10] between two Truncated-TF-IDF vectors, but other embeddings and similarities (e.g., the raw cosine similarity) could be adopted. To guide users further exploring the set of documents, we group them into clusters (T4 \u2013 details below). Inside each cluster, the documents are positioned in descending order of similarity between the document and the BP, which is defined as the maximum similarity between its paragraphs and the BP (T4). Showing documents in descending order of similarity is helpful because the user can promptly identify the top-k most similar, and therefore most interesting, documents. Further_x0002_more, to quickly assess the document distribution w.r.t. the similarity values, we also show an interactive bar chart above each color bar. Document Clustering. Since a binding precedent may cover decisions related to different subjects, the Brazilian Supreme Court website provides, for each BP, some clusters of decisions created according to their subjects. There are only a few clusters for each BP, each containing a few labeled documents (for instance, there are 8 clusters for BP 4, each containing two documents on average). We employed NLP text pre-processing and applied topic modeling strategies to cluster documents according to their relevant words to take advantage of this limited but useful ground truth. We have tested different and well-established topic extraction methods, including LDA [57], NMF [58], SNMF [59], and PSMF [60]. The NMF (Frobenius norm) method presented the best results, so we adopted it as the default method.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Unsimilar-Continuous",
        "solution_compoent": "Bar",
        "axial_code": ["Juxtaposition-Unsimilar-Continuous"],
        "componenet_code": ["Bar"]
      },
      {
        "solution_text": "Once the user has found a set of documents (i.e., a bar) of interest in Global View, he/she can select such a set by clicking on the bar. The user is then redirected to the Paragraph Similarities View (Fig. 5b), which presents in the y-axis all documents from the selected bar, that is, documents that cite a particular BP on a given date. Each document is divided into paragraphs, indicated by the horizontal stack of bars. ",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Connect/Relate+Encode",
        "solution_compoent": "",
        "axial_code": ["Connect/Relate", "Selecting", "Encode"],
        "componenet_code": ["connect/relate", "selecting", "encode"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 39,
    "paper_title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents",
    "pub_year": 2023,
    "domain": "Legal",
    "requirement": {
      "requirement_text": "T5: Highlight in decisions\u2019 relevant parts: The system should quickly identify the most similar paragraphs/sentences to the binding precedent. It should also highlight those parts that (either explicitly or potentially) cite that precedent.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "In total, we have 58 binding precedents, idealized as a mechanism to create a consolidated understanding among the STF\u2019s justices. We collected our dataset from a partner_x0002_ship with the Supremo em N_x0001_umeros (\u201cSTF in Numbers\u201d, in free translation project [43] \u2014 a project that seeks to assess legal and computer knowledge to produce unprecedented data on the Supreme Court. This dataset contains more than 2,500,000 documents since 1988 and metadata such as the number of the BP being cited (if that is the case) and docu_x0002_ment type. Considering only decisions that explicitly cite at least one of the 58 BPs, there are 38,364 documents, totaling 41,031 citations (some documents may mention more than one BP). The number of citations per BP does not follow a uniform distribution, as we can see in Fig. 2.",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Text Embedding. As mentioned above, we use a classifica_x0002_tion model to assess the probability of a document citing a BP. To assess different classifiers\u2019 performance, we rely on text embedding methods to generate a vector representation of the documents. We test different embeddings to evaluate their performance: TF-IDF [47], Doc2vec [44], Universal Sen_x0002_tence Encoder (USE) [10], and Longformer [48]. In TF-IDF\u2019s particular case, we have also applied a dimensionality reduction procedure to map the documents to a 50-dimen_x0002_sional space using Truncated Singular Value Decomposition (SVD) [49]. We call the final representation Truncated TF_x0002_IDF. The reason to consider Longformer, instead of a more popular language model, e.g., BERT [50], is that Longformer overcomes BERT\u2019s limitation of 512 tokens. Note that USE and Longformer do not need a pre-processed text (as described in the previous paragraph) because both have their tokenization mechanism.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Classification. Support Vector Machines (SVM) are com_x0002_plex enough for real-world classification problems and sim_x0002_ple enough to be analyzed mathematically [51]. Our study considers SVM with linear and Radial Basis Function (RBF) kernels. Moreover, given that Longformer [48] has a linear classification neural network layer plugged into it, we also fine-tuned it to assess its classification performance in our context. The described classifiers are used to search for potential citations in unlabeled data, assigning probabilities to each document to belong to each class. To get probabilities from an SVM, we need to make calibration: using labeled data, create a map from the classifier\u2019s output (SVM scores) to a probability estimate for each class, which sum up to 1. To create this map, we use the calibration method from Platt et al. [52] for SVM with linear kernel and the Wu et al.\u2019s [53] method for RBF kernel. Platt et al.\u2019s method, precisely, does not support the multiclass case, so we calibrate each class in a \u201cone-versus-rest\u201d approach and normalize the results in the end to sum up to 1. This map will receive the SVM scores from a data instance and output a probability esti_x0002_mate for each class.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "This section describes how we use the classifiers to identify potential citations and understand the models\u2019 decisions (see Fig. 3b). Citation Inference. The association between a text embed_x0002_ding technique and a classifier gives us what we call a model. A model receives the raw text and returns the classifier\u2019s probabilities, which we interpret as the document\u2019s proba_x0002_bility to cite each precedent. We consider a potential citation when the citation probability is greater or equal to a thresh_x0002_old tc 2 \u00bd0; 1_x0003_ (chosen by the user). Interpretability. A concern when dealing with machine learning models is their interpretability. For instance, if Truncated TF-IDF combined with SVM points out that a document potentially cites BP 10 because of high returned probability, how can we understand the reasons behind this decision? Generally speaking, how can we trust this model? Understanding why a model is taking a particular decision is of paramount importance [45], especially in sensitive sce_x0002_narios like a legal document analysis. Consider document X and its embedding vector x 2 Rd (e.g., TF-IDF vector). This document\u2019s probability of belong_x0002_ing to class C is given by fC\u00f0x\u00de, with fC : Rd ! \u00bd0; 1_x0003_ a mod_x0002_el\u2019s returned probability for class C. Our particular interest is to know the importance of each sentence from document X to the given probability, i.e., if each sentence has positive, negative, or neutral importance, and the magnitude of this importance. After preliminary tests have discarded the employment of the leave-one-out feature importance (LOO) [54] for interpretability due to its high sensitivity, we chose the Local Interpretable Model-agnostic Explanations (Lime) [45] method to tackle this issue. The intuition is that, by removing a spe_x0002_cific sentence and obtaining the variation of probability DfC, we can measure the importance of this sentence for the prediction fC\u00f0x\u00de. More formally, we randomly remove some sentences from document X, vectorize this new docu_x0002_ment to z, and add it to set Z. Doing this many times, we have a collection Z of vectors around x in high dimensional space. We approximate fC\u00f0z\u00de using a linear model g\u00f0z\u00de, minimizing the approximation error L\u00f0fC; g; z\u00de in z 2 Z, but also constraining the complexity This task can be interpreted as a weighted linear regres_x0002_V\u00f0g\u00de of g. sion with regularization. We end with a model g\u00f0z\u00de that is linear over the sentences (i.e., the presence or absence of a sentence), where the linear model\u2019s coefficients can be inter_x0002_preted as the importance score of each sentence to the final prediction By default, Lime works with words, and there is also the fC\u00f0x\u00de. possibility of interpreting entire paragraphs. A word divi_x0002_sion brings some advantages, such as detailing. Still, it does not immediately assess each sentence\u2019s importance to the decision (e.g., sentences similar to the BP text). A paragraph division is also interesting for visualization purposes (see, for example, Section 6.3), but it merges various sentences into one, hampering the precise identification of relevant parts. Therefore, our choice of working with sentences com_x0002_prises the \u201cbest of both worlds\u201d. In our experiments, Lime proved to be very robust and reliable when dealing with sentences.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Each document is divided into paragraphs, indicated by the horizontal stack of bars. The bar\u2019s size means the size of the paragraph, and the color intensity reflects the similarity between the corresponding paragraph and the BP text; the darker the color, the greater the similarity, and the more common parts exist between the BP and the paragraph. the color of each stacked bar stands for explicit (blue) or potential (orange) citation. Once the user has found a set of documents (i.e., a bar) of interest in Global View, he/she can select such a set by clicking on the bar. The user is then redirected to the Paragraph Similarities View (Fig. 5b), which presents in the y-axis all documents from the selected bar, that is, documents that cite a particular BP on a given date. Each document is divided into paragraphs, indicated by the horizontal stack of bars. The bar\u2019s size means the size of the paragraph, and the color intensity reflects the similarity between the corresponding paragraph and the BP text; the darker the color, the greater the similarity, and the more common parts exist between the BP and the paragraph (T5). Similarly to Global View, the color of each stacked bar stands for explicit (blue) or potential (orange) citation (T1). The similarity is given by the angular dis_x0002_tance [10] between two Truncated-TF-IDF vectors, but other embeddings and similarities (e.g., the raw cosine similarity) could be adopted. To guide users further exploring the set of documents, we group them into clusters (T4 \u2013 details below). Inside each clus_ter, the documents are positioned in descending order of similarity between the document and the BP, which is defined as the maximum similarity between its paragraphs and the BP (T4). Showing documents in descending order of similarity is helpful because the user can promptly identify the top-k most similar, and therefore most interesting, documents. Further_x0002_more, to quickly assess the document distribution w.r.t. the similarity values, we also show an interactive bar chart above each color bar. Document Clustering. Since a binding precedent may cover decisions related to different subjects, the Brazilian Supreme Court website provides, for each BP, some clusters of decisions created according to their subjects. There are only a few clusters for each BP, each containing a few labeled documents (for instance, there are 8 clusters for BP 4, each containing two documents on average). We employed NLP text pre-processing and applied topic modeling strategies to cluster documents according to their relevant words to take advantage of this limited but useful ground truth. We have tested different and well-established topic extraction methods, including LDA [57], NMF [58], SNMF [59], and PSMF [60]. The NMF (Frobenius norm) method presented the best results, so we adopted it as the default method.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Unsimilar-Continuous",
        "solution_compoent": "Bar",
        "axial_code": ["Juxtaposition-Unsimilar-Continuous"],
        "componenet_code": ["Bar"]
      },
      {
        "solution_text": "Once the user has found a set of documents (i.e., a bar) of interest in Global View, he/she can select such a set by clicking on the bar. The user is then redirected to the Paragraph Similarities View (Fig. 5b), which presents in the y-axis all documents from the selected bar, that is, documents that cite a particular BP on a given date. Each document is divided into paragraphs, indicated by the horizontal stack of bars. ",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Connect/Relate+Encode",
        "solution_compoent": "",
        "axial_code": ["Connect/Relate", "Selecting", "Encode"],
        "componenet_code": ["connect/relate", "selecting", "encode"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 40,
    "paper_title": "LegalVis: Exploring and Inferring Precedent Citations in Legal Documents",
    "pub_year": 2023,
    "domain": "Legal",
    "requirement": {
      "requirement_text": "T5: Highlight in decisions\u2019 relevant parts: The system should quickly identify the most similar paragraphs/sentences to the binding precedent. It should also highlight those parts that (either explicitly or potentially) cite that precedent.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "In total, we have 58 binding precedents, idealized as a mechanism to create a consolidated understanding among the STF\u2019s justices. We collected our dataset from a partner_x0002_ship with the Supremo em N_x0001_umeros (\u201cSTF in Numbers\u201d, in free translation project [43] \u2014 a project that seeks to assess legal and computer knowledge to produce unprecedented data on the Supreme Court. This dataset contains more than 2,500,000 documents since 1988 and metadata such as the number of the BP being cited (if that is the case) and docu_x0002_ment type. Considering only decisions that explicitly cite at least one of the 58 BPs, there are 38,364 documents, totaling 41,031 citations (some documents may mention more than one BP). The number of citations per BP does not follow a uniform distribution, as we can see in Fig. 2.",
      "data_code": { "textual": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "When a paragraph is hovered over, its content and the BP text are shown as illustrated in Fig. 6, with the common parts highlighted or not depending on the user\u2019s needs.Document Reader also incorporates Lime when the opened document refers to a potential citation. Comparison between a selected paragraph and the binding precedent. By hovering over the paragraphs of a selected document in Doc_x0002_ument Reader, users can compare their contents with the binding precedent being cited. Optionally, common parts are highlighted to help in the analysis (yellow).",
        "solution_category": "interaction",
        "solution_axial": "Selecting;Abstract/Elaborate;Connect/Relate",
        "solution_compoent": "",
        "axial_code": ["Abstract/Elaborate", "Connect/Relate", "Selecting"],
        "componenet_code": ["abstract/elaborate", "connect/relate", "selecting"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 42,
    "paper_title": "SD2: Slicing and Dicing Scholarly Data for Interactive Evaluation of Academic Performance",
    "pub_year": 2023,
    "domain": "Scholarly",
    "requirement": {
      "requirement_text": "R0: Hierarchical Exploration With Visual Scent Cues. The tool should provide information on multiple levels of detail, allowing users to drill down to the desired publication and citation information gradually. At each level, additional information should be provided as visual cues to guide further navigation, as inspired by scented widgets. The specific tasks to be performed at each level (coarsest, intermediate and finest) and the visual scents to be presented will be explained in further detail in the later requirements.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Our data come from Microsoft Academic Graph (MAG) and Google Scholar (GS). MAG is a knowledge graph updated weekly. The version we use contains more than 164 million papers and one billion citation relationships. Each paper has its ID, title, keywords, abstract, authors and their institutions, venue name, topics, and fields of study. Each citation relationship is a pair of the reference paper ID and the citing paper ID. However, although MAG contains rich information, it provides limited power to distinguish researchers of the same name. In contrast, GS also maintains comprehensive citation records but suffers less from author name ambiguity as many authors retain their own paper lists. However, GS does not make its data open, which makes it impractical for large-scale analysis. In our experi_x0002_ment, we mostly rely on MAG for the publication and cita_x0002_tion information. We convert all data in MAG into a SQL database for efficient retrieval. We only use GS to mitigate name ambiguity. Author Name Disambiguation. We rely on GS to identify the list of papers authored by a specific researcher. Although GS is not entirely error-free, it is by far one of the most accurate data sources for this purpose. For each researcher to be examined, we crawl his/her paper list from GS. Then, we query the SQL database to obtain the information of each paper (including the papers citing this work and their infor_x0002_mation) from MAG. This avoids frequent queries to GS, which is prohibited by Google, but still connects the authors to their papers with reasonable accuracy.",
      "data_code": { "temporal": 1, "textual": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "the interface of SD2 consists of three tightly-coupled views, each of which corresponds to one step in the interaction model (R0). Namely, the three views are: (a) the scholar view (\u201csearch\u201d), (b) the publication view (\u201cshow context\u201d), and (c) the hierarchical histogram view (\u201cexpand on demand\u201d). Typically, users will start by adding scholars of interest in the scholar view (R1), as shown in Fig. 1a. They may further specify set operations to combine the papers of the existing scholars (R2) so that the desired relationships can be studied. Then, in the publication view, users can see how the number of papers changes over time for each paper set, as shown in Fig. 1b. Finally, users can add the paper sets of interest to the hierarchical histogram view for detail exploration, as shown in Fig. 1c. They can customize the hierarchical histogram by specifying attributes at each level to reveal the information of their interest (R3). Users can add two paper sets for detailed comparison as well (R4).",
        "solution_category": "interaction",
        "solution_axial": "Selecting;Reconfigure;Filtering;OverviewandExplore;Extractionoffeatures",
        "solution_compoent": "",
        "axial_code": [
          "Selecting",
          "OverviewandExplore",
          "Filtering",
          "Reconfigure",
          "Extractionoffeatures"
        ],
        "componenet_code": [
          "selecting",
          "overview_and_explore",
          "filtering",
          "reconfigure",
          "extraction_of_features"
        ]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 43,
    "paper_title": "SD2: Slicing and Dicing Scholarly Data for Interactive Evaluation of Academic Performance",
    "pub_year": 2023,
    "domain": "Scholarly",
    "requirement": {
      "requirement_text": "R1: Specifying Researchers of Interest and Generating Overview. At the coarsest level, users should be able to specify researchers for investigation, and the tool should generate an overview of their papers and collaboration record. The overview should provide the high-level information of a researcher, e.g., who are the most frequent collaborators of the researcher, and how many papers does the researcher publish with each collaborator, respectively? The overview will assist users in selecting additional researchers or specify how the information of existing researchers should be combined for further investigation.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "Our data come from Microsoft Academic Graph (MAG) and Google Scholar (GS). MAG is a knowledge graph updated weekly. The version we use contains more than 164 million papers and one billion citation relationships. Each paper has its ID, title, keywords, abstract, authors and their institutions, venue name, topics, and fields of study. Each citation relationship is a pair of the reference paper ID and the citing paper ID. However, although MAG contains rich information, it provides limited power to distinguish researchers of the same name. In contrast, GS also maintains comprehensive citation records but suffers less from author name ambiguity as many authors retain their own paper lists. However, GS does not make its data open, which makes it impractical for large-scale analysis. In our experi_x0002_ment, we mostly rely on MAG for the publication and cita_x0002_tion information. We convert all data in MAG into a SQL database for efficient retrieval. We only use GS to mitigate name ambiguity. Author Name Disambiguation. We rely on GS to identify the list of papers authored by a specific researcher. Although GS is not entirely error-free, it is by far one of the most accurate data sources for this purpose. For each researcher to be examined, we crawl his/her paper list from GS. Then, we query the SQL database to obtain the information of each paper (including the papers citing this work and their infor_x0002_mation) from MAG. This avoids frequent queries to GS, which is prohibited by Google, but still connects the authors to their papers with reasonable accuracy.",
      "data_code": { "temporal": 1, "textual": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "The scholar view is used to select scholars as paper sets and specify the set operations to \u201ccombine\u201d these paper sets. The design of this view is inspired by UpSet [17]. The upper panel of this view shows the scholars under exploration (left) and the co-authors of the scholar of focus (right), as shown in Fig. 1a. The ordered list of co-authors is obtained from the author\u2019s Google Scholar profile. The bar charts show the number of papers authored by each scholar. For each of the co-authors, we further display an orange bar to indicate the number of co-authored papers with the scholar of focus. For the example given in Fig. 1a, the orange bars indicate that more than half of Pascal Vincent\u2019s papers are co-authored with Yoshua Bengio, and Yoshua Bengio also collaborates with Aaron Courville frequently. Users can click on any scholar under exploration to switch the focus, so that his/her co-authors will be displayed on the right. Users can click on any co-author to add them to the selected list as well. To add a paper set for further examination, users can com_x0002_bine the scholars using set operations provided in the lower panel, as shown in Fig. 1a. This panel supports four opera_x0002_tors for each scholar: namely, \u201cnot\u201d, \u201cignore\u201d \u201cand\u201d, and \u201cor\u201d. The design is similar to UpSet but with an additional \u201cor\u201d operator to allow the union of scholars, which is essential for studying the combined research output. The default operator for a scholar is \u201cignore\u201d, meaning that the scholar is irrelevant to the paper set being generated. Users can specify any other operator to get a scholar involved. A textual description of the resulting paper set will be updated whenever an operator is changed. ",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Unsimilar-Continuous",
        "solution_compoent": "Bar",
        "axial_code": ["Juxtaposition-Unsimilar-Continuous"],
        "componenet_code": ["Bar"]
      },
      {
        "solution_text": "Users can click on any scholar under exploration to switch the focus, so that his/her co-authors will be displayed on the right. Users can click on any co-author to add them to the selected list as well. To add a paper set for further examination, users can com_x0002_bine the scholars using set operations provided in the lower panel, as shown in Fig. 1a. This panel supports four opera_x0002_tors for each scholar: namely, \u201cnot\u201d, \u201cignore\u201d \u201cand\u201d, and \u201cor\u201d. The design is similar to UpSet but with an additional \u201cor\u201d operator to allow the union of scholars, which is essential for studying the combined research output. The default operator for a scholar is \u201cignore\u201d, meaning that the scholar is irrelevant to the paper set being generated. Users can specify any other operator to get a scholar involved. A textual description of the resulting paper set will be updated whenever an operator is changed. ",
        "solution_category": "interaction",
        "solution_axial": "Selecting;Filtering",
        "solution_compoent": "",
        "axial_code": ["Selecting", "Filtering"],
        "componenet_code": ["selecting", "filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 44,
    "paper_title": "SD2: Slicing and Dicing Scholarly Data for Interactive Evaluation of Academic Performance",
    "pub_year": 2023,
    "domain": "Scholarly",
    "requirement": {
      "requirement_text": "R2: Combining of Record From Multiple Researchers and Revealing Relationship. At the intermediate level, users should be able to combine the record of existing researchers to reveal the desired relationships. For example, consider two researchers, A and B. Users should be able to generate the record with both A and B to examine their collaboration, the record with A but without B to evaluate the independence of A, and the record with either A or B to investigate their combined impact. Similar rules should be allowed to combine more researchers. At this level, the tool should provide temporal patterns of the combined record (e.g., the paper numbers over the years) for users to verify that the combination is indeed meaningful.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "Our data come from Microsoft Academic Graph (MAG) and Google Scholar (GS). MAG is a knowledge graph updated weekly. The version we use contains more than 164 million papers and one billion citation relationships. Each paper has its ID, title, keywords, abstract, authors and their institutions, venue name, topics, and fields of study. Each citation relationship is a pair of the reference paper ID and the citing paper ID. However, although MAG contains rich information, it provides limited power to distinguish researchers of the same name. In contrast, GS also maintains comprehensive citation records but suffers less from author name ambiguity as many authors retain their own paper lists. However, GS does not make its data open, which makes it impractical for large-scale analysis. In our experi_x0002_ment, we mostly rely on MAG for the publication and cita_x0002_tion information. We convert all data in MAG into a SQL database for efficient retrieval. We only use GS to mitigate name ambiguity. Author Name Disambiguation. We rely on GS to identify the list of papers authored by a specific researcher. Although GS is not entirely error-free, it is by far one of the most accurate data sources for this purpose. For each researcher to be examined, we crawl his/her paper list from GS. Then, we query the SQL database to obtain the information of each paper (including the papers citing this work and their infor_x0002_mation) from MAG. This avoids frequent queries to GS, which is prohibited by Google, but still connects the authors to their papers with reasonable accuracy.",
      "data_code": { "temporal": 1, "tables": 1, "sequential": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "The scholar view is used to select scholars as paper sets and specify the set operations to \u201ccombine\u201d these paper sets. The design of this view is inspired by UpSet [17]. The upper panel of this view shows the scholars under exploration (left) and the co-authors of the scholar of focus (right), as shown in Fig. 1a. The ordered list of co-authors is obtained from the author\u2019s Google Scholar profile. The bar charts show the number of papers authored by each scholar. For each of the co-authors, we further display an orange bar to indicate the number of co-authored papers with the scholar of focus. For the example given in Fig. 1a, the orange bars indicate that more than half of Pascal Vincent\u2019s papers are co-authored with Yoshua Bengio, and Yoshua Bengio also collaborates with Aaron Courville frequently. Users can click on any scholar under exploration to switch the focus, so that his/her co-authors will be displayed on the right. Users can click on any co-author to add them to the selected list as well. To add a paper set for further examination, users can com_x0002_bine the scholars using set operations provided in the lower panel, as shown in Fig. 1a. This panel supports four opera_x0002_tors for each scholar: namely, \u201cnot\u201d, \u201cignore\u201d \u201cand\u201d, and \u201cor\u201d. The design is similar to UpSet but with an additional \u201cor\u201d operator to allow the union of scholars, which is essential for studying the combined research output. The default operator for a scholar is \u201cignore\u201d, meaning that the scholar is irrelevant to the paper set being generated. Users can specify any other operator to get a scholar involved. A textual description of the resulting paper set will be updated whenever an operator is changed. ",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Unsimilar-Continuous",
        "solution_compoent": "Bar",
        "axial_code": ["Juxtaposition-Unsimilar-Continuous"],
        "componenet_code": ["Bar"]
      },
      {
        "solution_text": "Users can click on any scholar under exploration to switch the focus, so that his/her co-authors will be displayed on the right. Users can click on any co-author to add them to the selected list as well. To add a paper set for further examination, users can com_x0002_bine the scholars using set operations provided in the lower panel, as shown in Fig. 1a. This panel supports four opera_x0002_tors for each scholar: namely, \u201cnot\u201d, \u201cignore\u201d \u201cand\u201d, and \u201cor\u201d. The design is similar to UpSet but with an additional \u201cor\u201d operator to allow the union of scholars, which is essential for studying the combined research output. The default operator for a scholar is \u201cignore\u201d, meaning that the scholar is irrelevant to the paper set being generated. Users can specify any other operator to get a scholar involved. A textual description of the resulting paper set will be updated whenever an operator is changed. ",
        "solution_category": "interaction",
        "solution_axial": "Selecting;Filtering",
        "solution_compoent": "",
        "axial_code": ["Selecting", "Filtering"],
        "componenet_code": ["selecting", "filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 45,
    "paper_title": "SD2: Slicing and Dicing Scholarly Data for Interactive Evaluation of Academic Performance",
    "pub_year": 2023,
    "domain": "Scholarly",
    "requirement": {
      "requirement_text": "R3: Partitioning the Information in Multiple Ways and Showing the Details. At the finest level, users should be able to slice and dice the information so that different aspects of the publication and citation record can be revealed and studied. For example, when a collection of papers is first partitioned by the topics and then by the publication years, we may find out how the research interest of the corresponding researchers changes over time. When a collection of citations is first partitioned by the topics of citing papers and then by their citation years, we may find out how the corresponding researchers\u2019influence on different topics evolves. When a collection of citations is first partitioned by the venue ranks of reference and then by the ranks of citing papers, we may verify whether the papers published at better venues receive more citations from top venues, etc. Leveraging the rich information associated with the papers, we should be able to answer questions from diverse aspects.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Our data come from Microsoft Academic Graph (MAG) and Google Scholar (GS). MAG is a knowledge graph updated weekly. The version we use contains more than 164 million papers and one billion citation relationships. Each paper has its ID, title, keywords, abstract, authors and their institutions, venue name, topics, and fields of study. Each citation relationship is a pair of the reference paper ID and the citing paper ID. However, although MAG contains rich information, it provides limited power to distinguish researchers of the same name. In contrast, GS also maintains comprehensive citation records but suffers less from author name ambiguity as many authors retain their own paper lists. However, GS does not make its data open, which makes it impractical for large-scale analysis. In our experi_x0002_ment, we mostly rely on MAG for the publication and cita_x0002_tion information. We convert all data in MAG into a SQL database for efficient retrieval. We only use GS to mitigate name ambiguity. Author Name Disambiguation. We rely on GS to identify the list of papers authored by a specific researcher. Although GS is not entirely error-free, it is by far one of the most accurate data sources for this purpose. For each researcher to be examined, we crawl his/her paper list from GS. Then, we query the SQL database to obtain the information of each paper (including the papers citing this work and their infor_x0002_mation) from MAG. This avoids frequent queries to GS, which is prohibited by Google, but still connects the authors to their papers with reasonable accuracy.",
      "data_code": { "temporal": 1, "textual": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Venue Classification. For publication/citation venues, we follow the list of computer science conferences and journals recommended by China Computer Federation (CCF) [4]. The CCF recommendation list classifies 571 major venues into ten categories: system and architecture, networks, secu_x0002_rity, database and mining, software engineering, theory, computer graphics, artificial intelligence, human-computer interaction, and interdisciplinary. For each category, the venues are further divided into three ranks from A to C with rank A venues being the most prestigious ones. During exploration, we use the categories and ranks as additional attributes to group the venues.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "The hierarchical histogram view allows users to break down a set of papers or citations into a hierarchy of bars to answer specific questions. Without loss of generality, we describe our approach using paper sets. The interface of the hierarchical histogram view is shown in Fig. 1c. Up to two paper sets can be added in this view as diverging bar charts for efficient comparison: one will be mapped to the upper histogram and the other to the lower histogram. Unlike the tradi_x0002_tional bar charts, which list all bars of a histogram at a single level, the hierarchical histogram features a multilevel design: several levels of horizontal bars at the intermediate lev_x0002_els of the hierarchy where the width of each bar indicates the corresponding number of leaf nodes, followed by one level of vertical bars at the finest (i.e., leaf) level of the hierarchy where the height of each bar can indicate the number of papers, the total number of citations received by the papers, or the h-index of the papers.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Symmetrical",
        "solution_compoent": "Bar",
        "axial_code": ["Juxtaposition-Similar-Symmetrical"],
        "componenet_code": ["Bar"]
      },
      {
        "solution_text": "Bar Grouping. We provide the bar grouping feature, which allows users to manually group the bars to further reduce visual complexity. Fig. 1c illustrates such an example. Users can brush the histogram of the \u201cP. Year\u201d attribute to create periods of years. In the resulting hierarchical histogram, each period, instead of each year, forms a bar. Once a bar group is formed, users can also remove it (acting as a filter). For example, users can update the \u201cP. CCF Rank\u201d attribute to form bar groups and remove the one containing papers that are not in CCF rank A, leaving only CCF rank A papers to be shown and explored in the hierarchical histogram. To remove a group, users can either rename the group as \u201cignore\u201d or simply click the button with a minus sign at the upper left corner of the group. When both the attribute lock and alignment are disabled, the two mini-maps can be used independently to scale and scroll the corresponding histograms. When the attribute lock is enabled, the bars in both histograms carry similar mean_x0002_ing. Therefore, we link the scaling operation of both histo_x0002_grams to enforce the same width for all the bars at the top level. When the alignment is enabled, we further link the scrolling of the two histograms, meaning that scrolling one histogram will move the other histogram simultaneously",
        "solution_category": "interaction",
        "solution_axial": "Filtering+Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration", "Filtering"],
        "componenet_code": ["participation/collaboration", "filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 46,
    "paper_title": "SD2: Slicing and Dicing Scholarly Data for Interactive Evaluation of Academic Performance",
    "pub_year": 2023,
    "domain": "Scholarly",
    "requirement": {
      "requirement_text": "R4: Aligning Partitioned Data for Comparison. The comparison between two collections of papers should be performed at the finest level using the partitioned information. Individual comparison is supported when each collection is produced from a single researcher, and group comparison is enabled when each collection corresponds to a group of researchers. The visual representation should allow the information to be aligned so that the corresponding parts can be easily compared. Both automatic and manual alignment should be provided. Automatic alignment should coordinate the same items in the two collections, while manual alignment should allow users to customize the alignment to match items carrying similar meanings. For example, to address the difference between different career stages, users should be able to dynamically align the time axes of two researchers.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "Our data come from Microsoft Academic Graph (MAG) and Google Scholar (GS). MAG is a knowledge graph updated weekly. The version we use contains more than 164 million papers and one billion citation relationships. Each paper has its ID, title, keywords, abstract, authors and their institutions, venue name, topics, and fields of study. Each citation relationship is a pair of the reference paper ID and the citing paper ID. However, although MAG contains rich information, it provides limited power to distinguish researchers of the same name. In contrast, GS also maintains comprehensive citation records but suffers less from author name ambiguity as many authors retain their own paper lists. However, GS does not make its data open, which makes it impractical for large-scale analysis. In our experi_x0002_ment, we mostly rely on MAG for the publication and cita_x0002_tion information. We convert all data in MAG into a SQL database for efficient retrieval. We only use GS to mitigate name ambiguity. Author Name Disambiguation. We rely on GS to identify the list of papers authored by a specific researcher. Although GS is not entirely error-free, it is by far one of the most accurate data sources for this purpose. For each researcher to be examined, we crawl his/her paper list from GS. Then, we query the SQL database to obtain the information of each paper (including the papers citing this work and their infor_x0002_mation) from MAG. This avoids frequent queries to GS, which is prohibited by Google, but still connects the authors to their papers with reasonable accuracy.",
      "data_code": { "temporal": 1, "textual": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Venue Classification. For publication/citation venues, we follow the list of computer science conferences and journals recommended by China Computer Federation (CCF) [4]. The CCF recommendation list classifies 571 major venues into ten categories: system and architecture, networks, secu_x0002_rity, database and mining, software engineering, theory, computer graphics, artificial intelligence, human-computer interaction, and interdisciplinary. For each category, the venues are further divided into three ranks from A to C with rank A venues being the most prestigious ones. During exploration, we use the categories and ranks as additional attributes to group the venues.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "The hierarchical histogram view allows users to break down a set of papers or citations into a hierarchy of bars to answer specific questions. Without loss of generality, we describe our approach using paper sets. The interface of the hierarchical histogram view is shown in Fig. 1c. Up to two paper sets can be added in this view as diverging bar charts for efficient comparison: one will be mapped to the upper histogram and the other to the lower histogram. Unlike the tradi_x0002_tional bar charts, which list all bars of a histogram at a single level, the hierarchical histogram features a multilevel design: several levels of horizontal bars at the intermediate lev_x0002_els of the hierarchy where the width of each bar indicates the corresponding number of leaf nodes, followed by one level of vertical bars at the finest (i.e., leaf) level of the hierarchy where the height of each bar can indicate the number of papers, the total number of citations received by the papers, or the h-index of the papers.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Symmetrical",
        "solution_compoent": "Bar",
        "axial_code": ["Juxtaposition-Similar-Symmetrical"],
        "componenet_code": ["Bar"]
      },
      {
        "solution_text": "Bar Grouping. We provide the bar grouping feature, which allows users to manually group the bars to further reduce visual complexity. Fig. 1c illustrates such an example. Users can brush the histogram of the \u201cP. Year\u201d attribute to create periods of years. In the resulting hierarchical histogram, each period, instead of each year, forms a bar. Once a bar group is formed, users can also remove it (acting as a filter). For example, users can update the \u201cP. CCF Rank\u201d attribute to form bar groups and remove the one containing papers that are not in CCF rank A, leaving only CCF rank A papers to be shown and explored in the hierarchical histogram. To remove a group, users can either rename the group as \u201cignore\u201d or simply click the button with a minus sign at the upper left corner of the group. When both the attribute lock and alignment are disabled, the two mini-maps can be used independently to scale and scroll the corresponding histograms. When the attribute lock is enabled, the bars in both histograms carry similar mean_x0002_ing. Therefore, we link the scaling operation of both histo_x0002_grams to enforce the same width for all the bars at the top level. When the alignment is enabled, we further link the scrolling of the two histograms, meaning that scrolling one histogram will move the other histogram simultaneously",
        "solution_category": "interaction",
        "solution_axial": "Filtering+Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration", "Filtering"],
        "componenet_code": ["participation/collaboration", "filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 49,
    "paper_title": "EpiMob: Interactive Visual Analytics of Citywide Human Mobility Restrictions for Epidemic Control",
    "pub_year": 2023,
    "domain": "Human mobility",
    "requirement": {
      "requirement_text": "E1: A Set of Spatial Views Assists Policy Exploration. To solve P2, a special brainstorming session was held to collect reference information on developing regional mobility con_x0002_trol policies, which are summarized as follows: (i) infection hotspots. An infection hotspot means that many people are infected in the area relative to other areas. The ability to efficiently locate these areas is beneficial for epidemic control. To map the citywide infection hotspots, the user must first acquire the infection locations. However, determining the accurate locations of real infected individuals during an outbreak is difficult. Thus, running the simulation to trace and collect infection locations becomes a common strategy. The experts\u2019 current solution for infection hotspot visualiza_x0002_tion included the heatmap and scatter map, which are sim_x0002_ple plots without informative interactive design and lacking in-depth exploration. The experts requested us to design a hotspot view that integrates the interaction function and allows for in-depth analysis. (ii) workplaces visited with high_x0002_frequency. Many workplaces have emerged as clusters of infection during COVID-19. Identifying workplaces with high-frequency visitation is important in deciding where to enforce remote working. The approach of the experts in rep_x0002_resenting workplace distribution was limited to the choro_x0002_pleth map at the municipality level. For finer granularity, data-driven workplace estimation is necessary while they do not have related background. Therefore, they wanted us to provide a view displaying the spatial density distribution of people\u2019s workplaces. (iii) screening point exploration. The government often set up screening points in certain areas during COVID-19. In practice, the experts identified these areas by plotting a scatter plot of the POI distributions. Then the locations where certain types of POIs are denser were selected. Specifically, these POIs are either at high risk of exposure, such as entertainment places (e.g., bars, kar_x0002_aokes) and restaurants, or have high human traffic such as stations, shopping malls, and public spaces (e.g., parks, zoos, and attractions). The experts suggested integrating this reference information into the system.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In this work, the data in Japan over a three-year period (from August 1, 2010, to July 31, 2013) was employed. It con_x0002_tains approximately 30 billion GPS records from about 1.6 million mobile phone users, covering around 1% of the real_x0002_world population. Furthermore, the Greater Tokyo Area (including Tokyo Metropolis and the prefectures of Kana_x0002_gawa, Chiba, and Saitama) was selected as the target area for research. The user is picked to the experimental data if 80% of its trajectory points are located in the target area. As a result, 145,507 user trajectories were obtained.",
      "data_code": { "textual": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "To satisfy requirement S2, a two-stage simulation mecha_x0002_nism was devised: Stage 1-Restricted Mobility Generation. Given a mobility restriction policy or a combination of several policies F, citywide human mobility forcibly changes owing to the given F. In this study, a mobility replacement model denoted as F MOB was utilized to generate the restricted human mobility G0 w.r.t F as follows: G0 \u00bc F MOB\u00f0G ; F\u00de (6) Stage 2-Epidemic Simulation with Restricted Mobility. Given the restricted citywide human mobility G0 w.r.t F and dis_x0002_ease transmission parameters Q0 , simulation of the epidemic for the restriction policy settings E0 sim can be implemented as follows: E0 sim \u00bc F EPI \u00f0G0 ; Q0 \u00de (7) This reflects the extensibility of the simulation mechanism. Developers can design new restriction strategies at Stage 1 and use other epidemic models at Stage 2.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "An iterative simulation process called \u201ccontagion + move_x0002_ment\u201d is proposed (Fig. 2). In the contagion stage, it is assumed that each person has a certain probability of being infected by others inside the same grid at the same time slot. In the movement stage, the user movements update the user lists of grids and introduce the disease to other grids. By iterating the process over the target time range, each user\u2019s state change is traced and the infection events are col_x0002_lected as simulation results. During the simulation, once a user\u2019s health state changes from susceptible to exposed, the corresponding infection time and location are recorded as an infection event. To start the iterative simulation process, I0 initially infected individuals was selected by random sampling, and hence, their user state was updated to \u201cinfected.\u201d By default, I0 was set to 10 in this study. Unlike the classical SEIR model, the proposed model exhib_x0002_its randomness. Given \u00f0G; Q\u00de, the simulation result Esim is unstable with respect to the total number of infection events and their occurrence times and locations. Therefore, multiple simulations are necessary. The number of repetitions m was set to 100 in this study. In addition, parallel computing was applied for the repetition simulations to ensure computational efficiency.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "When a user is affected by a mobility restriction policy, the mobility behavior is influenced accordingly. For example, a user frequents a shopping mall every weekend. The mall is locked on a particular day, and the user\u2019s future trajectory will not cover it. The affected trajectory is replaced by an unaffected trajectory, which is referred to as the trajectory replacement strategy. Here, the replacement methodology is elaborated for each restriction policy. Telecommuting. To implement telecommuting, first mobile phone users\u2019 homes and workplaces are extracted from the raw trajectory. After that, the administrative districts of the workplaces are identified to support the policy at the admin_x0002_istrative district level. Given a group of telecommuting dis_x0002_tricts and a corresponding time range, users whose workplaces are in the target districts are initially determined. Then, each affected user\u2019s status in the time range is deter_x0002_mined on a day-by-day basis, that is, whether they go to work. For the days on which a user goes to work, the trajec_x0002_tory of that day is replaced with the home address (i.e., make a stay at home and work remotely). For the days on which the user does not go to work, no changes are made. Regional Lockdown. Given a group of lockdown regions and time ranges, the regions are first mapped to a set of mesh grids covering them. Then, the affected users are divided into two categories. For users who stayed in the restricted area at the beginning of the lockdown period, their location remains unchanged throughout this period. For users who visited the restricted area during the lock_x0002_down, the affected trajectories are replaced with the unaf_x0002_fected trajectories. Specifically, a historical trajectory database is built for the users. For the affected day, the user\u2019s one-day trajectory that did not cross the restricted area is randomly selected from the historical database as the replacement. Screening. In our system, users are allowed to set temper_x0002_ature screening points at the grid level. Contrary to the other two mobility restriction policies, the selected grids are\nscreened during the epidemic simulation stage. Specifically, given a set of screening points and time ranges, all people in the grid are detected at each timestamp. If a person is healthy or in an incubation/latent period, it is assumed that the probability of an abnormal temperature is 0. If a person is infected, the probability of abnormal temperature is 87.9%, which is set according to the latest research on COVID-19[44]. Once an infected person is detected, he/she is quarantined (i.e., subsequent trajectories are discontin_x0002_ued) to prevent infecting others. Without loss of generality, EpiMob can also support other screening types (e.g., the nucleic acid test to detect latent patients).",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "In this section, three interactive views of the restriction settings are introduced for E1 and E2. The view of each setting includes a spatial view displayed on a map (E1) and a set_x0002_ting panel (E2). The switches located in the upper-right corner of each panel (Figs. 1 A1, A2, and A3) set the visibility of corresponding spatial views, thus users can obtain sufficient and effective prior knowledge. Regional Lockdown View: To help users formulate appropriate lockdown policies, we devised the regional lockdown view, which consists of a map view showing the regional infection severity (Fig. 4) and a setting panel (Fig. 1 A1) allowing the corresponding parameters to be specified. Here, a definition of regional nfection severity is provided\u2014Given a region R and a simulation result Esim, the infection severity of R indicates the accumulated number of infection events that occurred. The regional infection severity was defined as a reference indicator to determine the hotspots. To visualize the regional infection severity, specifying the simulation result Esim is required. A select box is placed\u2014\u201cBind with\u201d in the setting panel (Fig. 1 A1), where all existing simulation results are listed to allow users to choose. When using the regional lockdown view for the first time, it is recommended to perform a no policy simulation and then bind it to explore the infection hotspots under no intervention. Telecommuting View: To help determine remote work policies, a telecommuting view was designed, including a spatial view to identify workplaces that are frequented more regularly, along with a setting panel to finish the configuration of the concrete policy. Fig. 5 C shows the spatial view that displays all users\u2019workplaces with a heatmap. The darker the color (i.e., red in this case), the more people work there. Because workplaces tend to have a high degree of overlap, for example, many people working in a single building, a heatmap was selected to depict the distribution of workplaces. Compared to scatter plots (Fig. 5 A&B), the heatmap depicts the spatial accumulation of points more effectively. Moreover, when the mouse hovers over the heatmap, the name of the administrative dis_x0002_trict of the area is displayed. With this information, users can obtain the names of the regions to implement telecommuting. Fig. 1 A3 shows the panel for setting the telecommuting view. Users can set a series of regions to execute telecommuting here. This panel enables the \u201creduction rate\u201d of regions to be specified, i.e., the percentage of people working from home in the target region, to control the intensity of execution. For example, \u201cRegion Toshima Reduction 70%\u201d means that 70% of people working in Toshima are working from home. Moreover, according to regional conditions, the strength of the policy execution can also be quantified by set_x0002_ting the start date and duration.  Screening View: A superimposable scatter plot is integrated to display POI information (Fig. 6). Users can select one or several types of POIs simultaneously to explore potential screening points. To achieve E2-iii, the user is allowed to set up while explor_x0002_ing. When the user finds a target region, they can draw selection areas directly on the map or drag markers to the locations of interest, as shown in the screening control panel(Fig. 1 A2). After successfully adding a screening point, a mark is generated on the selected grid, indicating that screening will be performed. The time range for policy implementation can also be set. In the subsequent simula_x0002_tion, all people passing the screening points during the implementation of the policy are screened.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Providing",
        "solution_compoent": "Map+Circle",
        "axial_code": ["Overlay-Coordinatesystemrelated-Providing"],
        "componenet_code": ["Circle", "Map"]
      },
      {
        "solution_text": "A select box is placed\u2014\u201cBind with\u201d in the setting panel (Fig. 1 A1), where all existing simulation results are listed to allow users to choose. Moreover, when the mouse hovers over the heatmap, the name of the administrative dis_x0002_trict of the area is displayed. Users can select one or several types of POIs simultaneously to explore potential screening points. To achieve E2-iii, the user is allowed to set up while explor_x0002_ing. When the user finds a target region, they can draw selection areas directly on the map or drag markers to the locations of interest, as shown in the screening control panel(Fig. 1 A2). After successfully adding a screening point, a mark is generated on the selected grid, indicating that screening will be performed. The time range for policy implementation can also be set. In the subsequent simula_x0002_tion, all people passing the screening points during the implementation of the policy are screened.",
        "solution_category": "interaction",
        "solution_axial": "Selecting;Filtering;Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration", "Selecting", "Filtering"],
        "componenet_code": [
          "participation/collaboration",
          "selecting",
          "filtering"
        ]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 53,
    "paper_title": "EpiMob: Interactive Visual Analytics of Citywide Human Mobility Restrictions for Epidemic Control",
    "pub_year": 2023,
    "domain": "Human mobility",
    "requirement": {
      "requirement_text": "L1: Display of Single Control Policy Result. Basic requirements: plotting the infection curve is a basic operation to present the results. Meanwhile, because contagion simulations generally have randomness, confidence intervals should be considered. (ii) Distinguishability: There may be sets of simulations with only slight setting differences (e.g., two lockdown policies intersect to a large extent at the target regions). (iii) In-depth analysis of results: \u201cWhat are the secondary effects of implementing the policy? e.g., the new infection hotspots.\u201d The infection hotspot view can be equipped to directly perceive the transition of hotspots. \u201cWhat roles do the regions perform in the spread? Are there any striking patterns of transmission?\u201c, a network view is required to observe and explore the spatial propagation feature.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "In this work, the data in Japan over a three-year period (from August 1, 2010, to July 31, 2013) was employed. It con_x0002_tains approximately 30 billion GPS records from about 1.6 million mobile phone users, covering around 1% of the real_x0002_world population. Furthermore, the Greater Tokyo Area (including Tokyo Metropolis and the prefectures of Kana_x0002_gawa, Chiba, and Saitama) was selected as the target area for research. The user is picked to the experimental data if 80% of its trajectory points are located in the target area. As a result, 145,507 user trajectories were obtained.",
      "data_code": { "textual": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "To satisfy requirement S2, a two-stage simulation mecha_x0002_nism was devised: Stage 1-Restricted Mobility Generation. Given a mobility restriction policy or a combination of several policies F, citywide human mobility forcibly changes owing to the given F. In this study, a mobility replacement model denoted as F MOB was utilized to generate the restricted human mobility G0 w.r.t F as follows: G0 \u00bc F MOB\u00f0G ; F\u00de (6) Stage 2-Epidemic Simulation with Restricted Mobility. Given the restricted citywide human mobility G0 w.r.t F and dis_x0002_ease transmission parameters Q0 , simulation of the epidemic for the restriction policy settings E0 sim can be implemented as follows: E0 sim \u00bc F EPI \u00f0G0 ; Q0 \u00de (7) This reflects the extensibility of the simulation mechanism. Developers can design new restriction strategies at Stage 1 and use other epidemic models at Stage 2.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "An iterative simulation process called \u201ccontagion + move_x0002_ment\u201d is proposed (Fig. 2). In the contagion stage, it is assumed that each person has a certain probability of being infected by others inside the same grid at the same time slot. In the movement stage, the user movements update the user lists of grids and introduce the disease to other grids. By iterating the process over the target time range, each user\u2019s state change is traced and the infection events are col_x0002_lected as simulation results. During the simulation, once a user\u2019s health state changes from susceptible to exposed, the corresponding infection time and location are recorded as an infection event. To start the iterative simulation process, I0 initially infected individuals was selected by random sampling, and hence, their user state was updated to \u201cinfected.\u201d By default, I0 was set to 10 in this study. Unlike the classical SEIR model, the proposed model exhib_x0002_its randomness. Given \u00f0G; Q\u00de, the simulation result Esim is unstable with respect to the total number of infection events and their occurrence times and locations. Therefore, multiple simulations are necessary. The number of repetitions m was set to 100 in this study. In addition, parallel computing was applied for the repetition simulations to ensure computational efficiency.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "This view was designed to assist users to intuitively observe the simulation results and conduct a comparative analysis(L1, L2), including two subviews: a single policy result view(L1) and a comparative analysis view (L2). Furthermore, a single policy result view consists of an Infection Curve (L1-i) and a Spatial Propagation Feature View (L1-iii). Spatial Propagation Feature View. This view (Fig. 7 B) was designed to analyze the policies\u2019 secondary effects from the perspective of spatial propagation, and to understand the roles and patterns of different regions, including a map component (B1) to display the spatial transmission network, a filter component (B2) to explore the significant cross-region propagation patterns, and a tracing panel (B3) to localize the local infection patterns of interest. To this end, the \u201cinitial infection location, secondary propagation location\u201c pairs of infected individuals were captured and treated as the subject of analysis. It is easy to find that the data conforms to origin and destination (OD) data characteristics. A spatial line chart (Fig. 7 C1) was first employed to visualize all OD pairs to display the spatial transmission network, but the clutter problem is severe. The transparency of the line (C2) was further reduced, but the problem remained. One possible solution is OD aggregation visualization, but the clutter still exists according to [46]. The final solution is presented in Fig. 7 B, which inherits from a third party library[47] designed for flow visualization. The bidirection of the edges encodes the direction of spatial diffusion. Both the width and color of the edges map the intensity of the transition. This library solves the clutter by stacking and highlighting the edges according to their transition intensity. It also supports scaling and clustering when the zoom level changes (B4). To trace the local infection patterns of interest, all infections at the nodes were counted and categorized by infection type: input propagation\u2013the infection source comes from outside the area, and internal/local propagation\u2013the infection source comes from within the area. The local infection pattern is encoded as the respec_x0002_tive proportion of the two infection types in the total infec_x0002_tion (blue and red bar) to facilitate exploration and comparison. Finally, the infection distribution bars are listed in the order of node infection count (B3). When the mouse hovers on the bar of interest, its in-out flow, and spatial coverage are highlighted on the map. In this case, the area with the highest input propagation ratio in the top 50 infection count nodes was highlighted and it was determined that the infection mainly originated in Higashi-Kanagawa (B5). To explore the significant cross-region propagation patterns, a filter panel is provided to identify the primary risk sources and output destinations of nodes (B2). The user is allowed to fil_x0002_ter based on the proportion of flow to the total input in the target node and on the proportion of flow to the total output in the source node. The flows of nodes that meet the criteria are highlighted (in yellow in this study).",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Sharing",
        "solution_compoent": "Line+Map",
        "axial_code": ["Overlay-Coordinatesystemrelated-Sharing"],
        "componenet_code": ["Line", "Map"]
      },
      {
        "solution_text": "It also supports scaling and clustering when the zoom level changes (B4). To trace the local infection patterns of interest, all infections at the nodes were counted and categorized by infection type: input propagation\u2013the infection source comes from outside the area, and internal/local propagation\u2013the infection source comes from within the area. The local infection pattern is encoded as the respec_x0002_tive proportion of the two infection types in the total infec_x0002_tion (blue and red bar) to facilitate exploration and comparison. Finally, the infection distribution bars are listed in the order of node infection count (B3). When the mouse hovers on the bar of interest, its in-out flow, and spatial coverage are highlighted on the map. In this case, the area with the highest input propagation ratio in the top 50 infection count nodes was highlighted and it was determined that the infection mainly originated in Higashi-Kanagawa (B5). To explore the significant cross-region propagation patterns, a filter panel is provided to identify the primary risk sources and output destinations of nodes (B2). The user is allowed to fil_x0002_ter based on the proportion of flow to the total input in the target node and on the proportion of flow to the total output in the source node. The flows of nodes that meet the criteria are highlighted (in yellow in this study).",
        "solution_category": "interaction",
        "solution_axial": "Selecting;Encode;Filtering;OverviewandExplore",
        "solution_compoent": "",
        "axial_code": [
          "OverviewandExplore",
          "Selecting",
          "Filtering",
          "Encode"
        ],
        "componenet_code": [
          "overview_and_explore",
          "selecting",
          "filtering",
          "encode"
        ]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 54,
    "paper_title": "EpiMob: Interactive Visual Analytics of Citywide Human Mobility Restrictions for Epidemic Control",
    "pub_year": 2023,
    "domain": "Human mobility",
    "requirement": {
      "requirement_text": "L2: Comparative Analysis of Different Control Policies.To find the best policy, it is necessary to compare the effects of different control policies based on performance criteria. An interactive logic as convenient as \u201cselect, compare, and display\u201dis required.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "In this work, the data in Japan over a three-year period (from August 1, 2010, to July 31, 2013) was employed. It con_x0002_tains approximately 30 billion GPS records from about 1.6 million mobile phone users, covering around 1% of the real_x0002_world population. Furthermore, the Greater Tokyo Area (including Tokyo Metropolis and the prefectures of Kana_x0002_gawa, Chiba, and Saitama) was selected as the target area for research. The user is picked to the experimental data if 80% of its trajectory points are located in the target area. As a result, 145,507 user trajectories were obtained.",
      "data_code": { "textual": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "To satisfy requirement S2, a two-stage simulation mecha_x0002_nism was devised: Stage 1-Restricted Mobility Generation. Given a mobility restriction policy or a combination of several policies F, citywide human mobility forcibly changes owing to the given F. In this study, a mobility replacement model denoted as F MOB was utilized to generate the restricted human mobility G0 w.r.t F as follows: G0 \u00bc F MOB\u00f0G ; F\u00de (6) Stage 2-Epidemic Simulation with Restricted Mobility. Given the restricted citywide human mobility G0 w.r.t F and dis_x0002_ease transmission parameters Q0 , simulation of the epidemic for the restriction policy settings E0 sim can be implemented as follows: E0 sim \u00bc F EPI \u00f0G0 ; Q0 \u00de (7) This reflects the extensibility of the simulation mechanism. Developers can design new restriction strategies at Stage 1 and use other epidemic models at Stage 2.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "An iterative simulation process called \u201ccontagion + move_x0002_ment\u201d is proposed (Fig. 2). In the contagion stage, it is assumed that each person has a certain probability of being infected by others inside the same grid at the same time slot. In the movement stage, the user movements update the user lists of grids and introduce the disease to other grids. By iterating the process over the target time range, each user\u2019s state change is traced and the infection events are col_x0002_lected as simulation results. During the simulation, once a user\u2019s health state changes from susceptible to exposed, the corresponding infection time and location are recorded as an infection event. To start the iterative simulation process, I0 initially infected individuals was selected by random sampling, and hence, their user state was updated to \u201cinfected.\u201d By default, I0 was set to 10 in this study. Unlike the classical SEIR model, the proposed model exhib_x0002_its randomness. Given \u00f0G; Q\u00de, the simulation result Esim is unstable with respect to the total number of infection events and their occurrence times and locations. Therefore, multiple simulations are necessary. The number of repetitions m was set to 100 in this study. In addition, parallel computing was applied for the repetition simulations to ensure computational efficiency.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "This view was designed to assist users to intuitively observe the simulation results and conduct a comparative analysis(L1, L2), including two subviews: a single policy result view(L1) and a comparative analysis view (L2). Furthermore, a single policy result view consists of an Infection Curve (L1-i) and a Spatial Propagation Feature View (L1-iii). Spatial Propagation Feature View. This view (Fig. 7 B) was designed to analyze the policies\u2019 secondary effects from the perspective of spatial propagation, and to understand the roles and patterns of different regions, including a map component (B1) to display the spatial transmission network, a filter component (B2) to explore the significant cross-region propagation patterns, and a tracing panel (B3) to localize the local infection patterns of interest. To this end, the \u201cinitial infection location, secondary propagation location\u201c pairs of infected individuals were captured and treated as the subject of analysis. It is easy to find that the data conforms to origin and destination (OD) data characteristics. A spatial line chart (Fig. 7 C1) was first employed to visualize all OD pairs to display the spatial transmission network, but the clutter problem is severe. The transparency of the line (C2) was further reduced, but the problem remained. One possible solution is OD aggregation visualization, but the clutter still exists according to [46]. The final solution is presented in Fig. 7 B, which inherits from a third party library[47] designed for flow visualization. The bidirection of the edges encodes the direction of spatial diffusion. Both the width and color of the edges map the intensity of the transition. This library solves the clutter by stacking and highlighting the edges according to their transition intensity. It also supports scaling and clustering when the zoom level changes (B4). To trace the local infection patterns of interest, all infections at the nodes were counted and categorized by infection type: input propagation\u2013the infection source comes from outside the area, and internal/local propagation\u2013the infection source comes from within the area. The local infection pattern is encoded as the respec_x0002_tive proportion of the two infection types in the total infec_x0002_tion (blue and red bar) to facilitate exploration and comparison. Finally, the infection distribution bars are listed in the order of node infection count (B3). When the mouse hovers on the bar of interest, its in-out flow, and spatial coverage are highlighted on the map. In this case, the area with the highest input propagation ratio in the top 50 infection count nodes was highlighted and it was determined that the infection mainly originated in Higashi-Kanagawa (B5). To explore the significant cross-region propagation patterns, a filter panel is provided to identify the primary risk sources and output destinations of nodes (B2). The user is allowed to fil_x0002_ter based on the proportion of flow to the total input in the target node and on the proportion of flow to the total output in the source node. The flows of nodes that meet the criteria are highlighted (in yellow in this study).",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Sharing",
        "solution_compoent": "Line+Map",
        "axial_code": ["Overlay-Coordinatesystemrelated-Sharing"],
        "componenet_code": ["Line", "Map"]
      },
      {
        "solution_text": "It also supports scaling and clustering when the zoom level changes (B4). To trace the local infection patterns of interest, all infections at the nodes were counted and categorized by infection type: input propagation\u2013the infection source comes from outside the area, and internal/local propagation\u2013the infection source comes from within the area. The local infection pattern is encoded as the respec_x0002_tive proportion of the two infection types in the total infec_x0002_tion (blue and red bar) to facilitate exploration and comparison. Finally, the infection distribution bars are listed in the order of node infection count (B3). When the mouse hovers on the bar of interest, its in-out flow, and spatial coverage are highlighted on the map. In this case, the area with the highest input propagation ratio in the top 50 infection count nodes was highlighted and it was determined that the infection mainly originated in Higashi-Kanagawa (B5). To explore the significant cross-region propagation patterns, a filter panel is provided to identify the primary risk sources and output destinations of nodes (B2). The user is allowed to fil_x0002_ter based on the proportion of flow to the total input in the target node and on the proportion of flow to the total output in the source node. The flows of nodes that meet the criteria are highlighted (in yellow in this study).",
        "solution_category": "interaction",
        "solution_axial": "Selecting;Encode;Filtering;OverviewandExplore",
        "solution_compoent": "",
        "axial_code": [
          "OverviewandExplore",
          "Selecting",
          "Filtering",
          "Encode"
        ],
        "componenet_code": [
          "overview_and_explore",
          "selecting",
          "filtering",
          "encode"
        ]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 55,
    "paper_title": "GestureLens: Visual Analysis of Gestures in Presentation Videos",
    "pub_year": 2023,
    "domain": "presentation",
    "requirement": {
      "requirement_text": "T1: Obtain the spatial summary of gestures. Based on the interviews, a gesture summary can help coaches quickly explore gestures and identify examples. It is useful to obtain a summary of the spatial distribution of the hand movements, so that users can know where speakers tend to put their hands and what kind of gestures a speaker may employ. Such a spatial summary can also indicate speakers\u2019gesture styles.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In this paper, we focus on those videos recording a speak_x0002_er\u2019s presentation, where the speaker stands in front of the camera. Given a presentation video, the video can be mod_x0002_eled as a series of images: V \u00bc fI1; I2; ...; Ii; ...; IN g, where Ii indicates i-th frame and N indicates the frame number in\nthe video.",
      "data_code": { "geometry": 1, "clusters_and_sets_and_lists": 1 }
    },
    "solution": [
      {
        "solution_text": "Body Keypoint Detection. To obtain the detailed gestures of a speaker, we need to detect body keypoints first. Here we adopt OpenPose [48], a widely used real-time multi-person keypoint detection library for body and hand estimation. For each frame Ii, we can detect the corresponding body keypoints, Ji \u00bc \u00bdPi0; Pi 1; ...; Pi k; ...; Pi 24_x0003_ , where Pi k \u00bc \u00bdxk; yk; ck_x0003_ indicates the coordinates, xk is x-axis value, yk is y-axis value and ck is the confidence probability. Since we mainly focus on the upper body hand gestures, we extract 9 body key_x0002_points of the upper body, which are highlighted in red as shown in Fig. 2.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "Gesture Space Mapping. After detecting the body key_x0002_points, we can further obtain gesture coordinates. However, there are two major issues in analyzing gestures, i.e., coordi_x0002_nate normalization and spatial description of gestures. Coordi_x0002_nate normalization is important because it is hard to measure the gesture differences between different frames or presentation videos without a unified space. Thus, we normalize the gesture coordinates based on Keypoint 0 (the red circle annotated \u201c0\u201d in Fig. 2). To be specific, we regard Keypoint 0 as the coordinate origin. The coordi_x0002_nates of other keypoints are calculated based on Keypoint 0. Further, we normalize coordinates to \u00bd_x0004_1 _x0005_ 1_x0003_ range by using the height of the person. For the spatial description of gestures, we employ the gesture space defined in McNeill\u2019s gesture space theory [25], as shown in the blue dashed rectangles of Fig. 2. The three-level dashed rectan_x0002_gles represent the spatial regions of the center-center, center, and periphery from the inner to the outside. Specifically, the center-center region is the area directly in front of the chest; the center region is the area surrounding the center_x0002_center region, which stretches from the shoulders down to the waist and covers both sides of the body; the periphery region stretches from ears to the knees, surrounding the center-center region.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Gesture Segmentation. A speaker often uses various ges_x0002_tures for different speech content. We segment gestures according to word phrases, since we are interested in the relationships between gestures and speech content.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Gesture Distance Calculation. To find similar gestures, it is necessary to define gesture distance between different con_x0002_tinuous gestures. Since gestures may contain different frame lengths, we first define a distance function for measuring similarities between two static frames; then we utilize dynamic time warping (DTW) [49], a distance measure algorithm for time-series data with variable lengths. Given two keypoint coordinates in frames F and G, and each key_x0002_point vector is represented as Pi k \u00bc \u00bdxk; yk; ck_x0003_ , where k indi_x0002_cates k-th keypoint in F and G, xk is x-axis value, yk is y_x0002_axis value and ck is the confidence probability, the distance between gesture in two frames can be defined as [50]: D\u00f0F; G\u00de \u00bc 1 P8 k\u00bc0 Fck _x0006_ X 8 k\u00bc0 Fck _x0006_ kFxyk _x0004_ Gxyk k where Fck is the confidence probability for k-th keypoint, Fxyk and Gxyk is the coordinates of k-th keypoint of F and G, respectively.",
        "solution_category": "data_manipulation",
        "solution_axial": "SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["SimilarityCalculation"],
        "componenet_code": ["similarity_calculation"]
      },
      {
        "solution_text": "Gesture Type Calculation. After collecting different word phrase-based gestures, we classify gestures into three differ_x0002_ent categories, i.e., closed gestures, open gestures, and others[10]. Closed gestures refer to gestures where hands are put closely or overlapped with the torso; Open gestures refer to gesture where two hands are far away from each other and wrist points go outermost. For those gestures where hands are fall in the torso region, we named them others. More types can be calculated based on users\u2019 requirements",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Speech Content Processing. For a presentation video, the transcript can be obtained by adopting automatic transcrip_x0002_tion techniques.1 Further, to support word phrase analysis, we adopt an NLP library named textacy2 to extract semantic phrases, such as noun phrases (NP), verb phrases (VP), prep_x0002_ositional phrases (PP), and subject-verb object phrases (SVP).",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Data Alignment of Gesture Data and Transcripts. Following the aforementioned steps for body keypoints extraction, we extract keypoints frame by frame from a presentation video. Then, we can obtain the timestamp for keypoints of a frame. As for speech transcripts, the transcripts with timestamps can be obtained by adopting automatic transcription techniques (e.g., Amazon Transcribe). Therefore, the keypoints and speech transcripts are naturally aligned based on timestamps.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "To support intuitive and effective gesture-based and content-based explorations, we design an exploration view (Fig. 3B), which shows both spatial and temporal patterns of gestures, as well as corresponding content. Specifically, the exploration view contains a heatmap in the gesture space to reveal the spatial summary of gestures (T1). As shown in Fig. 3B1, to reveal spatial patterns of gestures used in presentation vid_x0002_eos, we layout a heatmap, a widely used technique for describing spatial patterns, over the gesture space. Three blue dashed rectangles are used to divide the gesture space into three areas, i.e., the center-center region, center region, and periphery region. A human stick-figure skeleton is also shown in the gesture space, which can provide some context for the gesture space and heatmap. The heatmap in the ges_x0002_ture space can clearly reveal where a speaker tends to put his/her hands during a presentation. Our coaches like this design and confirm that it is easy to understand. Further, they think the heatmap can reveal speakers\u2019 presentation styles in terms of spatial patterns.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Heatmap",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Heatmap"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 56,
    "paper_title": "GestureLens: Visual Analysis of Gestures in Presentation Videos",
    "pub_year": 2023,
    "domain": "presentation",
    "requirement": {
      "requirement_text": "T2: Show the temporal evolution of gestures. Since speakers can change gestures over time, it is necessary to explore the temporal evolution of gestures regarding\nthe presentation content. A temporal summary of gestures makes users aware of how speakers move hands along time and gain deep insights into the temporal patterns of gesture usage. Besides, users can know how often to use certain gestures.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In this paper, we focus on those videos recording a speak_x0002_er\u2019s presentation, where the speaker stands in front of the camera. Given a presentation video, the video can be mod_x0002_eled as a series of images: V \u00bc fI1; I2; ...; Ii; ...; IN g, where Ii indicates i-th frame and N indicates the frame number in\nthe video.",
      "data_code": { "geometry": 1, "clusters_and_sets_and_lists": 1 }
    },
    "solution": [
      {
        "solution_text": "Body Keypoint Detection. To obtain the detailed gestures of a speaker, we need to detect body keypoints first. Here we adopt OpenPose [48], a widely used real-time multi-person keypoint detection library for body and hand estimation. For each frame Ii, we can detect the corresponding body keypoints, Ji \u00bc \u00bdPi0; Pi 1; ...; Pi k; ...; Pi 24_x0003_ , where Pi k \u00bc \u00bdxk; yk; ck_x0003_ indicates the coordinates, xk is x-axis value, yk is y-axis value and ck is the confidence probability. Since we mainly focus on the upper body hand gestures, we extract 9 body key_x0002_points of the upper body, which are highlighted in red as shown in Fig. 2.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "Gesture Space Mapping. After detecting the body key_x0002_points, we can further obtain gesture coordinates. However, there are two major issues in analyzing gestures, i.e., coordi_x0002_nate normalization and spatial description of gestures. Coordi_x0002_nate normalization is important because it is hard to measure the gesture differences between different frames or presentation videos without a unified space. Thus, we normalize the gesture coordinates based on Keypoint 0 (the red circle annotated \u201c0\u201d in Fig. 2). To be specific, we regard Keypoint 0 as the coordinate origin. The coordi_x0002_nates of other keypoints are calculated based on Keypoint 0. Further, we normalize coordinates to \u00bd_x0004_1 _x0005_ 1_x0003_ range by using the height of the person. For the spatial description of gestures, we employ the gesture space defined in McNeill\u2019s gesture space theory [25], as shown in the blue dashed rectangles of Fig. 2. The three-level dashed rectan_x0002_gles represent the spatial regions of the center-center, center, and periphery from the inner to the outside. Specifically, the center-center region is the area directly in front of the chest; the center region is the area surrounding the center_x0002_center region, which stretches from the shoulders down to the waist and covers both sides of the body; the periphery region stretches from ears to the knees, surrounding the center-center region.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "As shown in Fig. 3B2 and B3, two mutually perpendicular timelines are used to describe hand movements. These two timelines are aligned with the gesture space in the heatmap part (Fig. 3B1). The horizontal timeline (Fig. 3B2) describes the vertical position of two hands. The purple line indicates the right hand, while the orange line indicates the left hand. The horizontal dashed line is aligned with the vertical cen_x0002_ter of the gesture space, so we can observe the vertical posi_x0002_tion of two hands. At the bottom of this timeline, there is a click area for users to seek to the corresponding parts of the presentation videos. A vertical black line indicates the cur_x0002_rent time frame. Similarly, the vertical timeline (Fig. 3B3) is used to describe the horizontal position of two hands. The same visual encoding is applied in this timeline. From the vertical timeline, users can observe how users move their hands horizontally. For example, whether users use open hand gestures or closed hand gestures can be observed from this timeline. Besides, two timelines are linked together, i.e., when brushing one timeline, the corresponding place in the other timeline will be highlighted.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Sharing",
        "solution_compoent": "Line",
        "axial_code": ["Overlay-Coordinatesystemrelated-Sharing"],
        "componenet_code": ["Line"]
      },
      {
        "solution_text": "Besides, two timelines are linked together, i.e., when brushing one timeline, the corresponding place in the other timeline will be highlighted.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 57,
    "paper_title": "GestureLens: Visual Analysis of Gestures in Presentation Videos",
    "pub_year": 2023,
    "domain": "presentation",
    "requirement": {
      "requirement_text": "T3: Explore the correlation between gestures and speech content. According to our interviews with the coaches, it is helpful for them to explore the correlation between\ngestures and speech content, e.g., whether the speakers use gestures that are harmonious with speech content. It is interesting to know what gestures are used to deliver certain content (content-based exploration). Furthermore, it is useful to know what content different gestures tend to convey (gesture-based exploration).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In this paper, we focus on those videos recording a speak_x0002_er\u2019s presentation, where the speaker stands in front of the camera. Given a presentation video, the video can be mod_x0002_eled as a series of images: V \u00bc fI1; I2; ...; Ii; ...; IN g, where Ii indicates i-th frame and N indicates the frame number in\nthe video.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "Body Keypoint Detection. To obtain the detailed gestures of a speaker, we need to detect body keypoints first. Here we adopt OpenPose [48], a widely used real-time multi-person keypoint detection library for body and hand estimation. For each frame Ii, we can detect the corresponding body keypoints, Ji \u00bc \u00bdPi0; Pi 1; ...; Pi k; ...; Pi 24_x0003_ , where Pi k \u00bc \u00bdxk; yk; ck_x0003_ indicates the coordinates, xk is x-axis value, yk is y-axis value and ck is the confidence probability. Since we mainly focus on the upper body hand gestures, we extract 9 body key_x0002_points of the upper body, which are highlighted in red as shown in Fig. 2.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "Gesture Space Mapping. After detecting the body key_x0002_points, we can further obtain gesture coordinates. However, there are two major issues in analyzing gestures, i.e., coordi_x0002_nate normalization and spatial description of gestures. Coordi_x0002_nate normalization is important because it is hard to measure the gesture differences between different frames or presentation videos without a unified space. Thus, we normalize the gesture coordinates based on Keypoint 0 (the red circle annotated \u201c0\u201d in Fig. 2). To be specific, we regard Keypoint 0 as the coordinate origin. The coordi_x0002_nates of other keypoints are calculated based on Keypoint 0. Further, we normalize coordinates to \u00bd_x0004_1 _x0005_ 1_x0003_ range by using the height of the person. For the spatial description of gestures, we employ the gesture space defined in McNeill\u2019s gesture space theory [25], as shown in the blue dashed rectangles of Fig. 2. The three-level dashed rectan_x0002_gles represent the spatial regions of the center-center, center, and periphery from the inner to the outside. Specifically, the center-center region is the area directly in front of the chest; the center region is the area surrounding the center_x0002_center region, which stretches from the shoulders down to the waist and covers both sides of the body; the periphery region stretches from ears to the knees, surrounding the center-center region.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Gesture Segmentation. A speaker often uses various ges_x0002_tures for different speech content. We segment gestures according to word phrases, since we are interested in the relationships between gestures and speech content.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Gesture Distance Calculation. To find similar gestures, it is necessary to define gesture distance between different con_x0002_tinuous gestures. Since gestures may contain different frame lengths, we first define a distance function for measuring similarities between two static frames; then we utilize dynamic time warping (DTW) [49], a distance measure algorithm for time-series data with variable lengths. Given two keypoint coordinates in frames F and G, and each key_x0002_point vector is represented as Pi k \u00bc \u00bdxk; yk; ck_x0003_ , where k indi_x0002_cates k-th keypoint in F and G, xk is x-axis value, yk is y_x0002_axis value and ck is the confidence probability, the distance between gesture in two frames can be defined as [50]: D\u00f0F; G\u00de \u00bc 1 P8 k\u00bc0 Fck _x0006_ X 8 k\u00bc0 Fck _x0006_ kFxyk _x0004_ Gxyk k where Fck is the confidence probability for k-th keypoint, Fxyk and Gxyk is the coordinates of k-th keypoint of F and G, respectively.",
        "solution_category": "data_manipulation",
        "solution_axial": "SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["SimilarityCalculation"],
        "componenet_code": ["similarity_calculation"]
      },
      {
        "solution_text": "Gesture Type Calculation. After collecting different word phrase-based gestures, we classify gestures into three differ_x0002_ent categories, i.e., closed gestures, open gestures, and others[10]. Closed gestures refer to gestures where hands are put closely or overlapped with the torso; Open gestures refer to gesture where two hands are far away from each other and wrist points go outermost. For those gestures where hands are fall in the torso region, we named them others. More types can be calculated based on users\u2019 requirements",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Speech Content Processing. For a presentation video, the transcript can be obtained by adopting automatic transcrip_x0002_tion techniques.1 Further, to support word phrase analysis, we adopt an NLP library named textacy2 to extract semantic phrases, such as noun phrases (NP), verb phrases (VP), prep_x0002_ositional phrases (PP), and subject-verb object phrases (SVP).",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Data Alignment of Gesture Data and Transcripts. Following the aforementioned steps for body keypoints extraction, we extract keypoints frame by frame from a presentation video. Then, we can obtain the timestamp for keypoints of a frame. As for speech transcripts, the transcripts with timestamps can be obtained by adopting automatic transcription techniques (e.g., Amazon Transcribe). Therefore, the keypoints and speech transcripts are naturally aligned based on timestamps.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Description: As shown in Fig. 3A, word phrases and gesture glyphs are projected into the middle 2D plane and the bottom 2D plane respectively by using the t-SNE algorithm. In this way, word phrases with similar meaning or similar gesture glyphs are close to each other in the corresponding plane. ",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "The relation view (Fig. 3A) shows the correlation between speech content and gestures (T3). To explore what gestures are used in what speech content(T3), we adopt a linked graph design to show the correlation between speech content and gestures. Description: As shown in Fig. 3A, word phrases and gesture glyphs are projected into the middle 2D plane and the bottom 2D plane respectively by using the t-SNE algorithm. In this way, word phrases with similar meaning or similar gesture glyphs are close to each other in the corresponding plane. To reveal the relationship between gesture and its context, word phrases in the middle part and gesture glyphs at the bottom part are linked with lines. When users select word phrases or gesture glyphs of their interests, the corre_x0002_sponding gesture glyphs or word phrases are highlighted and linked with lines. For word phrases, according to our discussion with coaches, we extract the common word phrases of their interests, including noun phrases (NP), verb phrases (VP), prepositional phrases (PP), and subject_x0002_verb-object phrases (SVP) from the video transcript. Then, we convert these phrases into pre-trained Glove embeddings [51] for projection. As for gesture glyphs, we design a glyph for each gesture phase to better reveal patterns. As shown in Fig. 3A3, human stick figures are integrated into the centers of circles. The background colors encode three_x0002_type gestures, i.e., open, closed, and others. The green radial area chart along the circle represents the variation of ges_x0002_tures. A large green area indicates Large variation. To better configure this view, filtering configuration and legend are shown at the top (Fig. 3A1). Users are allowed to filter text phrases with a certain time range or occurrence number. Fur_x0002_ther, by clicking a legend type, the corresponding part can be shown or hidden. For example, as shown in Fig. 3A1, legend \u201cSVO (subject-verb-object)\u201d and \u201cothers\u201d are not filled with colors, which indicates the corresponding types are filtered. Some other interactions are provided to facilitate explora_x0002_tion. For example, when users click a word phrase or gesture glyph of interest, users can refer to the context of the word in the video, as well as the transcript area of the exploration view (Fig. 3B4). Further, bookmark interactions are provided to allow users to save the gestures of their interest, enabling future explorations.",
        "solution_category": "visualization",
        "solution_axial": "Non-composite",
        "solution_compoent": "Scatter+Word+Circle",
        "axial_code": ["Non-composite"],
        "componenet_code": ["Scatter", "Circle", "Word"]
      },
      {
        "solution_text": "Users are allowed to filter text phrases with a certain time range or occurrence number. Fur_x0002_ther, by clicking a legend type, the corresponding part can be shown or hidden. For example, as shown in Fig. 3A1, legend \u201cSVO (subject-verb-object)\u201d and \u201cothers\u201d are not filled with colors, which indicates the corresponding types are filtered. Some other interactions are provided to facilitate explora_x0002_tion. For example, when users click a word phrase or gesture glyph of interest, users can refer to the context of the word in the video, as well as the transcript area of the exploration view (Fig. 3B4). Further, bookmark interactions are provided to allow users to save the gestures of their interest, enabling future explorations.",
        "solution_category": "interaction",
        "solution_axial": "Filtering+Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting", "Filtering"],
        "componenet_code": ["selecting", "filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 58,
    "paper_title": "GestureLens: Visual Analysis of Gestures in Presentation Videos",
    "pub_year": 2023,
    "domain": "presentation",
    "requirement": {
      "requirement_text": "T3: Explore the correlation between gestures and speech content. According to our interviews with the coaches, it is helpful for them to explore the correlation between\ngestures and speech content, e.g., whether the speakers use gestures that are harmonious with speech content. It is interesting to know what gestures are used to deliver certain content (content-based exploration). Furthermore, it is useful to know what content different gestures tend to convey (gesture-based exploration).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In this paper, we focus on those videos recording a speak_x0002_er\u2019s presentation, where the speaker stands in front of the camera. Given a presentation video, the video can be mod_x0002_eled as a series of images: V \u00bc fI1; I2; ...; Ii; ...; IN g, where Ii indicates i-th frame and N indicates the frame number in\nthe video.",
      "data_code": { "geometry": 1, "clusters_and_sets_and_lists": 1 }
    },
    "solution": [
      {
        "solution_text": "Body Keypoint Detection. To obtain the detailed gestures of a speaker, we need to detect body keypoints first. Here we adopt OpenPose [48], a widely used real-time multi-person keypoint detection library for body and hand estimation. For each frame Ii, we can detect the corresponding body keypoints, Ji \u00bc \u00bdPi0; Pi 1; ...; Pi k; ...; Pi 24_x0003_ , where Pi k \u00bc \u00bdxk; yk; ck_x0003_ indicates the coordinates, xk is x-axis value, yk is y-axis value and ck is the confidence probability. Since we mainly focus on the upper body hand gestures, we extract 9 body key_x0002_points of the upper body, which are highlighted in red as shown in Fig. 2.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "Gesture Space Mapping. After detecting the body key_x0002_points, we can further obtain gesture coordinates. However, there are two major issues in analyzing gestures, i.e., coordi_x0002_nate normalization and spatial description of gestures. Coordi_x0002_nate normalization is important because it is hard to measure the gesture differences between different frames or presentation videos without a unified space. Thus, we normalize the gesture coordinates based on Keypoint 0 (the red circle annotated \u201c0\u201d in Fig. 2). To be specific, we regard Keypoint 0 as the coordinate origin. The coordi_x0002_nates of other keypoints are calculated based on Keypoint 0. Further, we normalize coordinates to \u00bd_x0004_1 _x0005_ 1_x0003_ range by using the height of the person. For the spatial description of gestures, we employ the gesture space defined in McNeill\u2019s gesture space theory [25], as shown in the blue dashed rectangles of Fig. 2. The three-level dashed rectan_x0002_gles represent the spatial regions of the center-center, center, and periphery from the inner to the outside. Specifically, the center-center region is the area directly in front of the chest; the center region is the area surrounding the center_x0002_center region, which stretches from the shoulders down to the waist and covers both sides of the body; the periphery region stretches from ears to the knees, surrounding the center-center region.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Gesture Segmentation. A speaker often uses various ges_x0002_tures for different speech content. We segment gestures according to word phrases, since we are interested in the relationships between gestures and speech content.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Gesture Distance Calculation. To find similar gestures, it is necessary to define gesture distance between different con_x0002_tinuous gestures. Since gestures may contain different frame lengths, we first define a distance function for measuring similarities between two static frames; then we utilize dynamic time warping (DTW) [49], a distance measure algorithm for time-series data with variable lengths. Given two keypoint coordinates in frames F and G, and each key_x0002_point vector is represented as Pi k \u00bc \u00bdxk; yk; ck_x0003_ , where k indi_x0002_cates k-th keypoint in F and G, xk is x-axis value, yk is y_x0002_axis value and ck is the confidence probability, the distance between gesture in two frames can be defined as [50]: D\u00f0F; G\u00de \u00bc 1 P8 k\u00bc0 Fck _x0006_ X 8 k\u00bc0 Fck _x0006_ kFxyk _x0004_ Gxyk k where Fck is the confidence probability for k-th keypoint, Fxyk and Gxyk is the coordinates of k-th keypoint of F and G, respectively.",
        "solution_category": "data_manipulation",
        "solution_axial": "SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["SimilarityCalculation"],
        "componenet_code": ["similarity_calculation"]
      },
      {
        "solution_text": "Gesture Type Calculation. After collecting different word phrase-based gestures, we classify gestures into three differ_x0002_ent categories, i.e., closed gestures, open gestures, and others[10]. Closed gestures refer to gestures where hands are put closely or overlapped with the torso; Open gestures refer to gesture where two hands are far away from each other and wrist points go outermost. For those gestures where hands are fall in the torso region, we named them others. More types can be calculated based on users\u2019 requirements",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Speech Content Processing. For a presentation video, the transcript can be obtained by adopting automatic transcrip_x0002_tion techniques.1 Further, to support word phrase analysis, we adopt an NLP library named textacy2 to extract semantic phrases, such as noun phrases (NP), verb phrases (VP), prep_x0002_ositional phrases (PP), and subject-verb object phrases (SVP).",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Data Alignment of Gesture Data and Transcripts. Following the aforementioned steps for body keypoints extraction, we extract keypoints frame by frame from a presentation video. Then, we can obtain the timestamp for keypoints of a frame. As for speech transcripts, the transcripts with timestamps can be obtained by adopting automatic transcription techniques (e.g., Amazon Transcribe). Therefore, the keypoints and speech transcripts are naturally aligned based on timestamps.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "it annotates gesture glyphs on each word in the transcript, and highlights the dramatic changes in gestures (Fig. 3B4), which allows users to explore both gestures and speech content (T3). a transcript area to show the corresponding speech content and gestures (T3). As shown in Fig. 3B4, the speech content is explicitly shown here. To bet_x0002_ter reveal gestures used for speech content, we overlay human stick figures on top of the corresponding words. To be specific, gestures of the frames within a word are drawn on top of the word, which shows aggregate information of gestures for that word. We further define spatial variation to describe the variation of gestures within a word, and tempo_x0002_ral change to describe the change of gestures between two words. We first calculate the average gesture skeleton of each word, then we calculate the gesture skeleton variation within a word as spatial variation, while we calculate the gesture skeleton change between two words as temporal change. We normalize both spatial variation and temporal change value to \u00bd0 _x0005_ 1_x0003_ . End users can customize the thresh_x0002_olds. As shown in Fig. 3B4, the change threshold and varia_x0002_tion threshold are set to 0.5 and 0.4, respectively. As shown in the transcript area, high spatial variations are encoded with red strokes, and large temporal changes are encoded with green triangles.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Word+Others",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Others", "Word"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 59,
    "paper_title": "GestureLens: Visual Analysis of Gestures in Presentation Videos",
    "pub_year": 2023,
    "domain": "presentation",
    "requirement": {
      "requirement_text": "T4: Find similar gestures used in presentations. Research shows that speakers may unconsciously use similar or repetitive gestures of their own styles [46]. Our coaches expressed that it is helpful to explore similar or repetitive gestures, which can make speakers better aware of their own gestures. Further, coaches can compare different similar gestures with different speech content, whereby understanding why speakers use such gestures and providing suggestions for improvements. For example, coaches could detect repetitive non-meaningful gestures made by speakers.",
      "requirement_code": { "discover_observation": 1, "compare_entities": 1 }
    },
    "data": {
      "data_text": "In this paper, we focus on those videos recording a speak_x0002_er\u2019s presentation, where the speaker stands in front of the camera. Given a presentation video, the video can be mod_x0002_eled as a series of images: V \u00bc fI1; I2; ...; Ii; ...; IN g, where Ii indicates i-th frame and N indicates the frame number in\nthe video.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "Body Keypoint Detection. To obtain the detailed gestures of a speaker, we need to detect body keypoints first. Here we adopt OpenPose [48], a widely used real-time multi-person keypoint detection library for body and hand estimation. For each frame Ii, we can detect the corresponding body keypoints, Ji \u00bc \u00bdPi0; Pi 1; ...; Pi k; ...; Pi 24_x0003_ , where Pi k \u00bc \u00bdxk; yk; ck_x0003_ indicates the coordinates, xk is x-axis value, yk is y-axis value and ck is the confidence probability. Since we mainly focus on the upper body hand gestures, we extract 9 body key_x0002_points of the upper body, which are highlighted in red as shown in Fig. 2.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "Gesture Space Mapping. After detecting the body key_x0002_points, we can further obtain gesture coordinates. However, there are two major issues in analyzing gestures, i.e., coordi_x0002_nate normalization and spatial description of gestures. Coordi_x0002_nate normalization is important because it is hard to measure the gesture differences between different frames or presentation videos without a unified space. Thus, we normalize the gesture coordinates based on Keypoint 0 (the red circle annotated \u201c0\u201d in Fig. 2). To be specific, we regard Keypoint 0 as the coordinate origin. The coordi_x0002_nates of other keypoints are calculated based on Keypoint 0. Further, we normalize coordinates to \u00bd_x0004_1 _x0005_ 1_x0003_ range by using the height of the person. For the spatial description of gestures, we employ the gesture space defined in McNeill\u2019s gesture space theory [25], as shown in the blue dashed rectangles of Fig. 2. The three-level dashed rectan_x0002_gles represent the spatial regions of the center-center, center, and periphery from the inner to the outside. Specifically, the center-center region is the area directly in front of the chest; the center region is the area surrounding the center_x0002_center region, which stretches from the shoulders down to the waist and covers both sides of the body; the periphery region stretches from ears to the knees, surrounding the center-center region.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Gesture Segmentation. A speaker often uses various ges_x0002_tures for different speech content. We segment gestures according to word phrases, since we are interested in the relationships between gestures and speech content.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Gesture Distance Calculation. To find similar gestures, it is necessary to define gesture distance between different con_x0002_tinuous gestures. Since gestures may contain different frame lengths, we first define a distance function for measuring similarities between two static frames; then we utilize dynamic time warping (DTW) [49], a distance measure algorithm for time-series data with variable lengths. Given two keypoint coordinates in frames F and G, and each key_x0002_point vector is represented as Pi k \u00bc \u00bdxk; yk; ck_x0003_ , where k indi_x0002_cates k-th keypoint in F and G, xk is x-axis value, yk is y_x0002_axis value and ck is the confidence probability, the distance between gesture in two frames can be defined as [50]: D\u00f0F; G\u00de \u00bc 1 P8 k\u00bc0 Fck _x0006_ X 8 k\u00bc0 Fck _x0006_ kFxyk _x0004_ Gxyk k where Fck is the confidence probability for k-th keypoint, Fxyk and Gxyk is the coordinates of k-th keypoint of F and G, respectively.",
        "solution_category": "data_manipulation",
        "solution_axial": "SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["SimilarityCalculation"],
        "componenet_code": ["similarity_calculation"]
      },
      {
        "solution_text": "Gesture Type Calculation. After collecting different word phrase-based gestures, we classify gestures into three differ_x0002_ent categories, i.e., closed gestures, open gestures, and others[10]. Closed gestures refer to gestures where hands are put closely or overlapped with the torso; Open gestures refer to gesture where two hands are far away from each other and wrist points go outermost. For those gestures where hands are fall in the torso region, we named them others. More types can be calculated based on users\u2019 requirements",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Speech Content Processing. For a presentation video, the transcript can be obtained by adopting automatic transcrip_x0002_tion techniques.1 Further, to support word phrase analysis, we adopt an NLP library named textacy2 to extract semantic phrases, such as noun phrases (NP), verb phrases (VP), prep_x0002_ositional phrases (PP), and subject-verb object phrases (SVP).",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Data Alignment of Gesture Data and Transcripts. Following the aforementioned steps for body keypoints extraction, we extract keypoints frame by frame from a presentation video. Then, we can obtain the timestamp for keypoints of a frame. As for speech transcripts, the transcripts with timestamps can be obtained by adopting automatic transcription techniques (e.g., Amazon Transcribe). Therefore, the keypoints and speech transcripts are naturally aligned based on timestamps.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Description: As shown in Fig. 3A, word phrases and gesture glyphs are projected into the middle 2D plane and the bottom 2D plane respectively by using the t-SNE algorithm. In this way, word phrases with similar meaning or similar gesture glyphs are close to each other in the corresponding plane. ",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "The relation view (Fig. 3A) shows the correlation between speech content and gestures (T3). To explore what gestures are used in what speech content(T3), we adopt a linked graph design to show the correlation between speech content and gestures. Description: As shown in Fig. 3A, word phrases and gesture glyphs are projected into the middle 2D plane and the bottom 2D plane respectively by using the t-SNE algorithm. In this way, word phrases with similar meaning or similar gesture glyphs are close to each other in the corresponding plane. To reveal the relationship between gesture and its context, word phrases in the middle part and gesture glyphs at the bottom part are linked with lines. When users select word phrases or gesture glyphs of their interests, the corre_x0002_sponding gesture glyphs or word phrases are highlighted and linked with lines. For word phrases, according to our discussion with coaches, we extract the common word phrases of their interests, including noun phrases (NP), verb phrases (VP), prepositional phrases (PP), and subject_x0002_verb-object phrases (SVP) from the video transcript. Then, we convert these phrases into pre-trained Glove embeddings [51] for projection. As for gesture glyphs, we design a glyph for each gesture phase to better reveal patterns. As shown in Fig. 3A3, human stick figures are integrated into the centers of circles. The background colors encode three_x0002_type gestures, i.e., open, closed, and others. The green radial area chart along the circle represents the variation of ges_x0002_tures. A large green area indicates Large variation. To better configure this view, filtering configuration and legend are shown at the top (Fig. 3A1). Users are allowed to filter text phrases with a certain time range or occurrence number. Fur_x0002_ther, by clicking a legend type, the corresponding part can be shown or hidden. For example, as shown in Fig. 3A1, legend \u201cSVO (subject-verb-object)\u201d and \u201cothers\u201d are not filled with colors, which indicates the corresponding types are filtered. Some other interactions are provided to facilitate explora_x0002_tion. For example, when users click a word phrase or gesture glyph of interest, users can refer to the context of the word in the video, as well as the transcript area of the exploration view (Fig. 3B4). Further, bookmark interactions are provided to allow users to save the gestures of their interest, enabling future explorations.",
        "solution_category": "visualization",
        "solution_axial": "Non-composite",
        "solution_compoent": "Scatter+Word+Circle",
        "axial_code": ["Non-composite"],
        "componenet_code": ["Scatter", "Circle", "Word"]
      },
      {
        "solution_text": "Users are allowed to filter text phrases with a certain time range or occurrence number. Fur_x0002_ther, by clicking a legend type, the corresponding part can be shown or hidden. For example, as shown in Fig. 3A1, legend \u201cSVO (subject-verb-object)\u201d and \u201cothers\u201d are not filled with colors, which indicates the corresponding types are filtered. Some other interactions are provided to facilitate explora_x0002_tion. For example, when users click a word phrase or gesture glyph of interest, users can refer to the context of the word in the video, as well as the transcript area of the exploration view (Fig. 3B4). Further, bookmark interactions are provided to allow users to save the gestures of their interest, enabling future explorations.",
        "solution_category": "interaction",
        "solution_axial": "Filtering+Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting", "Filtering"],
        "componenet_code": ["selecting", "filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 60,
    "paper_title": "GestureLens: Visual Analysis of Gestures in Presentation Videos",
    "pub_year": 2023,
    "domain": "presentation",
    "requirement": {
      "requirement_text": "T5: Enable interactive exploration of the presentation videos. Coaches also need to check the original presentation video and the corresponding transcripts to confirm their findings of a speaker's gesture usage. Thus, it is necessary to provide coaches with interactive exploration of the original presentation videos.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "In this paper, we focus on those videos recording a speak_x0002_er\u2019s presentation, where the speaker stands in front of the camera. Given a presentation video, the video can be mod_x0002_eled as a series of images: V \u00bc fI1; I2; ...; Ii; ...; IN g, where Ii indicates i-th frame and N indicates the frame number in\nthe video.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "For video analysis, referring to the original video can sometimes provide a better explanation. Therefore, we embed the original video for exploration in the video view (Fig. 3C). After selecting a video of interest, the video is presented in this view. Based on the coaches\u2019 feedback, the screenshot interaction is added to record some interesting moments. The corresponding information (i.e., time and word) is placed under screenshots, which can facilitate gesture usage exploration. The video view is linked to other views. Users can refer to the video for detailed analysis.",
        "solution_category": "interaction",
        "solution_axial": "Selecting+History+Connect/Relate",
        "solution_compoent": "",
        "axial_code": ["Connect/Relate", "Selecting", "History"],
        "componenet_code": ["connect/relate", "selecting", "history"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 61,
    "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence",
    "pub_year": 2023,
    "domain": "semantic analysis",
    "requirement": {
      "requirement_text": "G1: Overview of the sequence distribution. The visualization should project music feature vectors into a 2D plane space as a distribution overview of music sequences. In this projection space, users can examine distribution patterns of music sequences and obtain guidance for further analysis.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Music",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection"],
        "componenet_code": ["feature_selection"]
      },
      {
        "solution_text": "Inspired by the idea proposed by Galex [26], which extracts article features based on the article summarization data by using Doc2Vec to generate the feature vectors for articles, we apply Doc2Vec to compute the music vector representations by using music text representation. At first, we use text representations of the music sequences to train the Doc2Vec model. Then, we compute the corresponding feature vector based on the text representation of each music sequence. As a result, each music MIDI file is extracted as a musical semantic sequence, and each musical semantic sequence equips a corresponding 128-dimension feature vector by constructing the text representation and employing the Doc2Vec model.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling;Rectification",
        "solution_compoent": "",
        "axial_code": ["Modeling", "Rectification"],
        "componenet_code": ["modeling", "rectification"]
      },
      {
        "solution_text": "To visualize the distribution pattern among music more intuitively, we project the result of Doc2Vec to a 2D plane. We have considered three alternative dimension reduction methods includes principal component analysis (PCA) [12], multidimensional scaling (MDS) [24], and and t-distributed\nstochastic neighbor embedding (t-SNE) [33]. Although these dimension reduction methods are based on different under_x0002_lying algorithms, we have examined multiple parameter groups to select a proper method to produce the distribu_x0002_tion results. The test data includes 6,632 musical sequence vectors, and the size of each vector is 1 _x0003_ 128. These vectors are equipped with 4 genre attributes which are represented by 4 color points including yellow (classical), green (folk), red (rock), and purple (jazz). As shown in Fig. 2b, the result of t-SNE has more intui_x0002_tive distribution patterns than the other two dimension reduction methods (MDS and PCA). Consequently, we select t-SNE to project musical sequences into a distribution view with one theoretical consideration. Compared with MDS and PCA, t-SNE maximally preserves local neighbor_x0002_hoods of each data point (high dimensional vector), which ensures each musical sequence to have its similar neighbors based on the feature vectors in the distribution view [49]. In the distribution view, we use points to represent the musical sequences. To assist users in discovering distribu_x0002_tion patterns intuitively, we map two music features (i.e., music genres and instruments) to the points. In addition, to help users recognize the distribution patterns in the distri_x0002_bution view, we equip the distribution view with two visual components: a density contour component and a pathfinding component. The density contour is helpful for users to reveal the overall density distribution of the numerous points. How_x0002_ever, the traditional density contour only considers the geo_x0002_metric position of points and ignores the similarity. Thus, inspired by the marching square algorithm [20] in the den_x0002_sity contour, we propose an extended algorithm to modify the generation of density contours based on the similarity ofmusical sequence vectors. We apply the similarity to modify the basic matrix in the original marching square algorithm. As shown in Fig. 2c, the left contour is the raw result. In the marching square algorithm matrix, each number repre_x0002_sents the number of musical sequences in a grid. Our algo_x0002_rithm calculates and integrates the musical sequence similarities to modify the density value of the original matrix. When the grid similarity is larger than the threshold, these grids tend to be similar so that the number in the two grids would be added with a virtual value to change the state of the matrix. After the steps mentioned above, the modified density contour is generated to ensure that similar sequences have the same contour line. From the density contour shown in the right part of Fig. 2c, the points indicat_x0002_ing the same musical genre (jazz) are circled by the con_x0002_tours, which may indicate the apparent similarity of thes musical sequences. Including the density contour, we integrate a semantic change path component into the distribution view based on our proposed pathfinding algorithm to help users analyze the semantic change from one pattern to another pattern. The pathfinding algorithm, whose perplexity is O(n2), inte_x0002_grates the distances of music sequence projection coordi_x0002_nates and feature vectors. This heuristic algorithm follows four steps (S1 to S4) to find a path from the source music sequence to the target one in the distribution view. Four steps are introduced as follows. S1 Divide music sequences into corresponding grids based on their projection coordinates. S2 Obtain music sequences in the neighbor search grids of the source music sequence. If the neighbor search grids have no available sequences, enlarge the scope of neigh_x0002_bor search grids to search. S3 Calculate and determine the smallest euclidean distance of feature vectors between the source music sequence and each music sequence obtained in step S2. S4 Put the music sequence (found in step S3) into the path and make it a new source music sequence. Repeat step S1 to step S4 until the target music sequence is in the neighbor search grids. To ensure each new source music sequence to be approaching the target music sequence, we set the princi_x0002_ples of neighbor search grids [44]. Comparing the coordi_x0002_nates of the source music sequence and the target one, as shown in Fig. 3, we summarize eight neighbor search grids schemes. The eight schemes are \u201cupper left, low right, upper right, low left, horizontally left, horizontally right, vertically upper, and vertically low\u201d based on position sta_x0002_tus from the source music sequence to the target one. Users can sample the path points (sequences) if the path has too many music sequences to analyze the semantic change. In addition, the scheme of grids can be customized in different scenes. In this work, we offer two grids schemes including 128 _x0003_ 128 (which is selected in case 2) and 256 _x0003_ 256.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "To visualize the distribution pattern among music more intuitively, we project the result of Doc2Vec to a 2D plane. We have considered three alternative dimension reduction methods includes principal component analysis (PCA) [12], multidimensional scaling (MDS) [24], and and t-distributed\nstochastic neighbor embedding (t-SNE) [33]. Although these dimension reduction methods are based on different under_x0002_lying algorithms, we have examined multiple parameter groups to select a proper method to produce the distribu_x0002_tion results. The test data includes 6,632 musical sequence vectors, and the size of each vector is 1 _x0003_ 128. These vectors are equipped with 4 genre attributes which are represented by 4 color points including yellow (classical), green (folk), red (rock), and purple (jazz). As shown in Fig. 2b, the result of t-SNE has more intui_x0002_tive distribution patterns than the other two dimension reduction methods (MDS and PCA). Consequently, we select t-SNE to project musical sequences into a distribution view with one theoretical consideration. Compared with MDS and PCA, t-SNE maximally preserves local neighbor_x0002_hoods of each data point (high dimensional vector), which ensures each musical sequence to have its similar neighbors based on the feature vectors in the distribution view [49]. In the distribution view, we use points to represent the musical sequences. To assist users in discovering distribu_x0002_tion patterns intuitively, we map two music features (i.e., music genres and instruments) to the points. In addition, to help users recognize the distribution patterns in the distri_x0002_bution view, we equip the distribution view with two visual components: a density contour component and a pathfinding component. The density contour is helpful for users to reveal the overall density distribution of the numerous points. How_x0002_ever, the traditional density contour only considers the geo_x0002_metric position of points and ignores the similarity. Thus, inspired by the marching square algorithm [20] in the den_x0002_sity contour, we propose an extended algorithm to modify the generation of density contours based on the similarity ofmusical sequence vectors. We apply the similarity to modify the basic matrix in the original marching square algorithm. As shown in Fig. 2c, the left contour is the raw result. In the marching square algorithm matrix, each number repre_x0002_sents the number of musical sequences in a grid. Our algo_x0002_rithm calculates and integrates the musical sequence similarities to modify the density value of the original matrix. When the grid similarity is larger than the threshold, these grids tend to be similar so that the number in the two grids would be added with a virtual value to change the state of the matrix. After the steps mentioned above, the modified density contour is generated to ensure that similar sequences have the same contour line. From the density contour shown in the right part of Fig. 2c, the points indicat_x0002_ing the same musical genre (jazz) are circled by the con_x0002_tours, which may indicate the apparent similarity of thes musical sequences. Including the density contour, we integrate a semantic change path component into the distribution view based on our proposed pathfinding algorithm to help users analyze the semantic change from one pattern to another pattern. The pathfinding algorithm, whose perplexity is O(n2), inte_x0002_grates the distances of music sequence projection coordi_x0002_nates and feature vectors. This heuristic algorithm follows four steps (S1 to S4) to find a path from the source music sequence to the target one in the distribution view. Four steps are introduced as follows. S1 Divide music sequences into corresponding grids based on their projection coordinates. S2 Obtain music sequences in the neighbor search grids of the source music sequence. If the neighbor search grids have no available sequences, enlarge the scope of neigh_x0002_bor search grids to search. S3 Calculate and determine the smallest euclidean distance of feature vectors between the source music sequence and each music sequence obtained in step S2. S4 Put the music sequence (found in step S3) into the path and make it a new source music sequence. Repeat step S1 to step S4 until the target music sequence is in the neighbor search grids. To ensure each new source music sequence to be approaching the target music sequence, we set the princi_x0002_ples of neighbor search grids [44]. Comparing the coordi_x0002_nates of the source music sequence and the target one, as shown in Fig. 3, we summarize eight neighbor search grids schemes. The eight schemes are \u201cupper left, low right, upper right, low left, horizontally left, horizontally right, vertically upper, and vertically low\u201d based on position sta_x0002_tus from the source music sequence to the target one. Users can sample the path points (sequences) if the path has too many music sequences to analyze the semantic change. In addition, the scheme of grids can be customized in different scenes. In this work, we offer two grids schemes including 128 _x0003_ 128 (which is selected in case 2) and 256 _x0003_ 256.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Providing",
        "solution_compoent": "Scatter+ContourLine",
        "axial_code": ["Overlay-Coordinatesystemrelated-Providing"],
        "componenet_code": ["Scatter", "ContourLine"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 62,
    "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence",
    "pub_year": 2023,
    "domain": "semantic analysis",
    "requirement": {
      "requirement_text": "G1: Overview of the sequence distribution. The visualization should project music feature vectors into a 2D plane space as a distribution overview of music sequences. In this projection space, users can examine distribution patterns of music sequences and obtain guidance for further analysis.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Music",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection"],
        "componenet_code": ["feature_selection"]
      },
      {
        "solution_text": "To present the connection between genres with instruments and assist users in accessing the distribution patterns in the distribution view, we design a genre instrument tree on the left of the system interface (G1 & G2). In the genre instrument tree, as shown in Fig. 6a, each genre is displayed as a circle in the first column of this tree plot. For example, the green circle represents the folk genre while the yellow circle represents the classical genre. Each circle in the second column indicates the instrument information. The fill color of the circles indicates the instrument name. All instruments are collected and divided into differ_x0002_ent categories of instruments (G2). Instruments in the same category are encoded as a series of same color class circles with different color saturation. Higher color saturation indicates that the proportion of the instrument is larger than other instruments in the same category. For example, in Fig. 6a, two circles filled by two kinds of red colors repre_x0002_sent two string instruments (i.e., violin and viola). The higher color saturation indicates that the violin has larger proportion than the viola in music sequences.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Tree",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Tree"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 63,
    "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence",
    "pub_year": 2023,
    "domain": "semantic analysis",
    "requirement": {
      "requirement_text": "G2: Representation of the semantic information. The visualization should present the semantic details and encode hierarchical information such as genre, instrument, and note to support effective exploration and comparison of semantic details.",
      "requirement_code": {
        "compare_entities": 1,
        "describe_observation_aggregate": 1
      }
    },
    "data": {
      "data_text": "Music",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection"],
        "componenet_code": ["feature_selection"]
      },
      {
        "solution_text": "Inspired by the idea proposed by Galex [26], which extracts article features based on the article summarization data by using Doc2Vec to generate the feature vectors for articles, we apply Doc2Vec to compute the music vector representations by using music text representation. At first, we use text representations of the music sequences to train the Doc2Vec model. Then, we compute the corresponding feature vector based on the text representation of each music sequence. As a result, each music MIDI file is extracted as a musical semantic sequence, and each musical semantic sequence equips a corresponding 128-dimension feature vector by constructing the text representation and employing the Doc2Vec model.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling;Rectification",
        "solution_compoent": "",
        "axial_code": ["Modeling", "Rectification"],
        "componenet_code": ["modeling", "rectification"]
      },
      {
        "solution_text": "To visualize the distribution pattern among music more intuitively, we project the result of Doc2Vec to a 2D plane. We have considered three alternative dimension reduction methods includes principal component analysis (PCA) [12], multidimensional scaling (MDS) [24], and and t-distributed\nstochastic neighbor embedding (t-SNE) [33]. Although these dimension reduction methods are based on different under_x0002_lying algorithms, we have examined multiple parameter groups to select a proper method to produce the distribu_x0002_tion results. The test data includes 6,632 musical sequence vectors, and the size of each vector is 1 _x0003_ 128. These vectors are equipped with 4 genre attributes which are represented by 4 color points including yellow (classical), green (folk), red (rock), and purple (jazz). As shown in Fig. 2b, the result of t-SNE has more intui_x0002_tive distribution patterns than the other two dimension reduction methods (MDS and PCA). Consequently, we select t-SNE to project musical sequences into a distribution view with one theoretical consideration. Compared with MDS and PCA, t-SNE maximally preserves local neighbor_x0002_hoods of each data point (high dimensional vector), which ensures each musical sequence to have its similar neighbors based on the feature vectors in the distribution view [49]. In the distribution view, we use points to represent the musical sequences. To assist users in discovering distribu_x0002_tion patterns intuitively, we map two music features (i.e., music genres and instruments) to the points. In addition, to help users recognize the distribution patterns in the distri_x0002_bution view, we equip the distribution view with two visual components: a density contour component and a pathfinding component. The density contour is helpful for users to reveal the overall density distribution of the numerous points. How_x0002_ever, the traditional density contour only considers the geo_x0002_metric position of points and ignores the similarity. Thus, inspired by the marching square algorithm [20] in the den_x0002_sity contour, we propose an extended algorithm to modify the generation of density contours based on the similarity ofmusical sequence vectors. We apply the similarity to modify the basic matrix in the original marching square algorithm. As shown in Fig. 2c, the left contour is the raw result. In the marching square algorithm matrix, each number repre_x0002_sents the number of musical sequences in a grid. Our algo_x0002_rithm calculates and integrates the musical sequence similarities to modify the density value of the original matrix. When the grid similarity is larger than the threshold, these grids tend to be similar so that the number in the two grids would be added with a virtual value to change the state of the matrix. After the steps mentioned above, the modified density contour is generated to ensure that similar sequences have the same contour line. From the density contour shown in the right part of Fig. 2c, the points indicat_x0002_ing the same musical genre (jazz) are circled by the con_x0002_tours, which may indicate the apparent similarity of thes musical sequences. Including the density contour, we integrate a semantic change path component into the distribution view based on our proposed pathfinding algorithm to help users analyze the semantic change from one pattern to another pattern. The pathfinding algorithm, whose perplexity is O(n2), inte_x0002_grates the distances of music sequence projection coordi_x0002_nates and feature vectors. This heuristic algorithm follows four steps (S1 to S4) to find a path from the source music sequence to the target one in the distribution view. Four steps are introduced as follows. S1 Divide music sequences into corresponding grids based on their projection coordinates. S2 Obtain music sequences in the neighbor search grids of the source music sequence. If the neighbor search grids have no available sequences, enlarge the scope of neigh_x0002_bor search grids to search. S3 Calculate and determine the smallest euclidean distance of feature vectors between the source music sequence and each music sequence obtained in step S2. S4 Put the music sequence (found in step S3) into the path and make it a new source music sequence. Repeat step S1 to step S4 until the target music sequence is in the neighbor search grids. To ensure each new source music sequence to be approaching the target music sequence, we set the princi_x0002_ples of neighbor search grids [44]. Comparing the coordi_x0002_nates of the source music sequence and the target one, as shown in Fig. 3, we summarize eight neighbor search grids schemes. The eight schemes are \u201cupper left, low right, upper right, low left, horizontally left, horizontally right, vertically upper, and vertically low\u201d based on position sta_x0002_tus from the source music sequence to the target one. Users can sample the path points (sequences) if the path has too many music sequences to analyze the semantic change. In addition, the scheme of grids can be customized in different scenes. In this work, we offer two grids schemes including 128 _x0003_ 128 (which is selected in case 2) and 256 _x0003_ 256.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "To visualize the distribution pattern among music more intuitively, we project the result of Doc2Vec to a 2D plane. We have considered three alternative dimension reduction methods includes principal component analysis (PCA) [12], multidimensional scaling (MDS) [24], and and t-distributed\nstochastic neighbor embedding (t-SNE) [33]. Although these dimension reduction methods are based on different under_x0002_lying algorithms, we have examined multiple parameter groups to select a proper method to produce the distribu_x0002_tion results. The test data includes 6,632 musical sequence vectors, and the size of each vector is 1 _x0003_ 128. These vectors are equipped with 4 genre attributes which are represented by 4 color points including yellow (classical), green (folk), red (rock), and purple (jazz). As shown in Fig. 2b, the result of t-SNE has more intui_x0002_tive distribution patterns than the other two dimension reduction methods (MDS and PCA). Consequently, we select t-SNE to project musical sequences into a distribution view with one theoretical consideration. Compared with MDS and PCA, t-SNE maximally preserves local neighbor_x0002_hoods of each data point (high dimensional vector), which ensures each musical sequence to have its similar neighbors based on the feature vectors in the distribution view [49]. In the distribution view, we use points to represent the musical sequences. To assist users in discovering distribu_x0002_tion patterns intuitively, we map two music features (i.e., music genres and instruments) to the points. In addition, to help users recognize the distribution patterns in the distri_x0002_bution view, we equip the distribution view with two visual components: a density contour component and a pathfinding component. The density contour is helpful for users to reveal the overall density distribution of the numerous points. How_x0002_ever, the traditional density contour only considers the geo_x0002_metric position of points and ignores the similarity. Thus, inspired by the marching square algorithm [20] in the den_x0002_sity contour, we propose an extended algorithm to modify the generation of density contours based on the similarity ofmusical sequence vectors. We apply the similarity to modify the basic matrix in the original marching square algorithm. As shown in Fig. 2c, the left contour is the raw result. In the marching square algorithm matrix, each number repre_x0002_sents the number of musical sequences in a grid. Our algo_x0002_rithm calculates and integrates the musical sequence similarities to modify the density value of the original matrix. When the grid similarity is larger than the threshold, these grids tend to be similar so that the number in the two grids would be added with a virtual value to change the state of the matrix. After the steps mentioned above, the modified density contour is generated to ensure that similar sequences have the same contour line. From the density contour shown in the right part of Fig. 2c, the points indicat_x0002_ing the same musical genre (jazz) are circled by the con_x0002_tours, which may indicate the apparent similarity of thes musical sequences. Including the density contour, we integrate a semantic change path component into the distribution view based on our proposed pathfinding algorithm to help users analyze the semantic change from one pattern to another pattern. The pathfinding algorithm, whose perplexity is O(n2), inte_x0002_grates the distances of music sequence projection coordi_x0002_nates and feature vectors. This heuristic algorithm follows four steps (S1 to S4) to find a path from the source music sequence to the target one in the distribution view. Four steps are introduced as follows. S1 Divide music sequences into corresponding grids based on their projection coordinates. S2 Obtain music sequences in the neighbor search grids of the source music sequence. If the neighbor search grids have no available sequences, enlarge the scope of neigh_x0002_bor search grids to search. S3 Calculate and determine the smallest euclidean distance of feature vectors between the source music sequence and each music sequence obtained in step S2. S4 Put the music sequence (found in step S3) into the path and make it a new source music sequence. Repeat step S1 to step S4 until the target music sequence is in the neighbor search grids. To ensure each new source music sequence to be approaching the target music sequence, we set the princi_x0002_ples of neighbor search grids [44]. Comparing the coordi_x0002_nates of the source music sequence and the target one, as shown in Fig. 3, we summarize eight neighbor search grids schemes. The eight schemes are \u201cupper left, low right, upper right, low left, horizontally left, horizontally right, vertically upper, and vertically low\u201d based on position sta_x0002_tus from the source music sequence to the target one. Users can sample the path points (sequences) if the path has too many music sequences to analyze the semantic change. In addition, the scheme of grids can be customized in different scenes. In this work, we offer two grids schemes including 128 _x0003_ 128 (which is selected in case 2) and 256 _x0003_ 256.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Providing",
        "solution_compoent": "Scatter+ContourLine",
        "axial_code": ["Overlay-Coordinatesystemrelated-Providing"],
        "componenet_code": ["Scatter", "ContourLine"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 64,
    "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence",
    "pub_year": 2023,
    "domain": "semantic analysis",
    "requirement": {
      "requirement_text": "G2: Representation of the semantic information. The visualization should present the semantic details and encode hierarchical information such as genre, instrument, and note to support effective exploration and comparison of semantic details.",
      "requirement_code": {
        "compare_entities": 1,
        "describe_observation_aggregate": 1
      }
    },
    "data": {
      "data_text": "Music",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection"],
        "componenet_code": ["feature_selection"]
      },
      {
        "solution_text": "After the visualization experiments of semantic details (Section 4.2), we utilize the Notes Aggregation scheme to present the semantic details of music sequences (G2 & G3). As shown in Fig. 5g, a row represents a music sequence respectively, which contains four parts to display the semantic information. The first part is the music sequence ID, which helps users explore the musical sequences in other visualization views. The second part is a genre\u2019s text description to provide users with the genre information. The third part is a pie chart component to display the proportion of instruments in the music sequence. The last part presents the semantic details of the music sequence. Each small rectangle represents the semantic information, which has three attributes including the width, height, and fill color. The width indicates the duration, the height represents the average pitch, and the color depicts the type of instruments. As shown in Fig. 5g, the music sequence could be aggregated based on the Notes Aggregation scheme (Sec_x0002_tion 4.2) with a collation width of 13, which is automatically calculated by the Formula (1), (2), and (3).",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Unsimilar-Continuous",
        "solution_compoent": "Bar+Table",
        "axial_code": ["Juxtaposition-Unsimilar-Continuous"],
        "componenet_code": ["Bar", "Table"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 65,
    "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence",
    "pub_year": 2023,
    "domain": "semantic analysis",
    "requirement": {
      "requirement_text": "G2: Representation of the semantic information. The visualization should present the semantic details and encode hierarchical information such as genre, instrument, and note to support effective exploration and comparison of semantic details.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Music",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection"],
        "componenet_code": ["feature_selection"]
      },
      {
        "solution_text": "To present the connection between genres with instruments and assist users in accessing the distribution patterns in the distribution view, we design a genre instrument tree on the left of the system interface (G1 & G2). In the genre instrument tree, as shown in Fig. 6a, each genre is displayed as a circle in the first column of this tree plot. For example, the green circle represents the folk genre while the yellow circle represents the classical genre. Each circle in the second column indicates the instrument information. The fill color of the circles indicates the instrument name. All instruments are collected and divided into differ_x0002_ent categories of instruments (G2). Instruments in the same category are encoded as a series of same color class circles with different color saturation. Higher color saturation indicates that the proportion of the instrument is larger than other instruments in the same category. For example, in Fig. 6a, two circles filled by two kinds of red colors repre_x0002_sent two string instruments (i.e., violin and viola). The higher color saturation indicates that the violin has larger proportion than the viola in music sequences.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Tree",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Tree"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 66,
    "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence",
    "pub_year": 2023,
    "domain": "semantic analysis",
    "requirement": {
      "requirement_text": "G2: Representation of the semantic information. The visualization should present the semantic details and encode hierarchical information such as genre, instrument, and note to support effective exploration and comparison of semantic details.",
      "requirement_code": {
        "compare_entities": 1,
        "describe_observation_aggregate": 1
      }
    },
    "data": {
      "data_text": "Music",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection"],
        "componenet_code": ["feature_selection"]
      },
      {
        "solution_text": "To help users identify and analyze the similarity to the music genre of each music sequence(G2), we design a radar plot (Fig. 6b). The radar plot has six axes/dimensions including musicID, genre, folk, classical, jazz, and rock. Each line in the radar plot represents a music sequence, and the color of the line is encoded as the genre information. The position in the musicID axis demonstrates the ID of the music sequence. The position in the genre axis indicates the genre information of the music sequence. The positions in the folk, classical, jazz, and rock axes present the similarity to music genres. The higher position depicts the larger similarities to the corresponding music genres. To access the genre similarity of each music sequence, we calculate music genre vectors. For an example of the folk genre vector, we collected all the feature vectors of folk music sequences. Then we average the collected feature vectors of folk music sequences as the folk genre vector. In the end, we calculate the euclidean distance from the music sequence vector to the folk genre vector as the similarity between the music sequence and the folk genre.",
        "solution_category": "data_manipulation",
        "solution_axial": "SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["SimilarityCalculation"],
        "componenet_code": ["similarity_calculation"]
      },
      {
        "solution_text": "To help users identify and analyze the similarity to the music genre of each music sequence(G2), we design a radar plot (Fig. 6b). The radar plot has six axes/dimensions including musicID, genre, folk, classical, jazz, and rock. Each line in the radar plot represents a music sequence, and the color of the line is encoded as the genre information. The position in the musicID axis demonstrates the ID of the music sequence. The position in the genre axis indicates the genre information of the music sequence. The positions in the folk, classical, jazz, and rock axes present the similarity to music genres. The higher position depicts the larger similarities to the corresponding music genres. To access the genre similarity of each music sequence, we calculate music genre vectors. For an example of the folk genre vector, we collected all the feature vectors of folk music sequences. Then we average the collected feature vectors of folk music sequences as the folk genre vector. In the end, we calculate the euclidean distance from the music sequence vector to the folk genre vector as the similarity between the music sequence and the folk genre.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Sharing",
        "solution_compoent": "Radar",
        "axial_code": ["Overlay-Coordinatesystemrelated-Sharing"],
        "componenet_code": ["Radar"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 67,
    "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence",
    "pub_year": 2023,
    "domain": "semantic analysis",
    "requirement": {
      "requirement_text": "G3: Comparison of multi-variate sequences. The visualization should support users in comparing the semantic information of multivariate sequences, which is helpful for\nthe exploration of hidden semantic change, genre, instruments patterns, etc. In patterns analysis, users should be allowed to explore hierarchical information efficiently and validate whether the potential uncertainty issue is avoided or not.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "Music",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection"],
        "componenet_code": ["feature_selection"]
      },
      {
        "solution_text": "After the visualization experiments of semantic details (Section 4.2), we utilize the Notes Aggregation scheme to present the semantic details of music sequences (G2 & G3). As shown in Fig. 5g, a row represents a music sequence respectively, which contains four parts to display the semantic information. The first part is the music sequence ID, which helps users explore the musical sequences in other visualization views. The second part is a genre\u2019s text description to provide users with the genre information. The third part is a pie chart component to display the proportion of instruments in the music sequence. The last part presents the semantic details of the music sequence. Each small rectangle represents the semantic information, which has three attributes including the width, height, and fill color. The width indicates the duration, the height represents the average pitch, and the color depicts the type of instruments. As shown in Fig. 5g, the music sequence could be aggregated based on the Notes Aggregation scheme (Sec_x0002_tion 4.2) with a collation width of 13, which is automatically calculated by the Formula (1), (2), and (3).",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Unsimilar-Continuous",
        "solution_compoent": "Bar+Table",
        "axial_code": ["Juxtaposition-Unsimilar-Continuous"],
        "componenet_code": ["Bar", "Table"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 68,
    "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence",
    "pub_year": 2023,
    "domain": "semantic analysis",
    "requirement": {
      "requirement_text": "G3: Comparison of multi-variate sequences. The visualization should support users in comparing the semantic information of multivariate sequences, which is helpful for\nthe exploration of hidden semantic change, genre, instruments patterns, etc. In patterns analysis, users should be allowed to explore hierarchical information efficiently and validate whether the potential uncertainty issue is avoided or not.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "Music",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection"],
        "componenet_code": ["feature_selection"]
      },
      {
        "solution_text": "To help users access the semantic distribution patterns of music sequences, we design a parallel coordinate plot to display the semantic details simultaneously in multiple dimensions(G2 & G3). As shown in Fig. 5f, each line is encoded as a note in the parallel coordinate plot. It has seven dimensions including musicID, genre, instrument, note class, pitch, start time, and type. The dimensions are divided into three levels the sequence level, instrument level, and note level. The sequence level contains musicID and genre dimensions. The MusicID indicates the music sequence ID of the note, while the genre indicates the music sequence genre. The Instrument level has one dimension named instrument, which presents the instrument name of the note. The color of the line reveals the instrument name. The last note level includes note class, pitch, start time, and type dimensions. The note class dimension indicates that the note is a note or a chord\u2019s note. The pitch dimension presents the pitch value of each note. The start time dimension displays the timestamp of the note. The last type dimension introduces note\u2019s type information such as half, quarter, eighth, 16th, and whole.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Sharing",
        "solution_compoent": "Line",
        "axial_code": ["Overlay-Coordinatesystemrelated-Sharing"],
        "componenet_code": ["Line"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 69,
    "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence",
    "pub_year": 2023,
    "domain": "semantic analysis",
    "requirement": {
      "requirement_text": "G3: Comparison of multi-variate sequences. The visualization should support users in comparing the semantic information of multivariate sequences, which is helpful for\nthe exploration of hidden semantic change, genre, instruments patterns, etc. In patterns analysis, users should be allowed to explore hierarchical information efficiently and validate whether the potential uncertainty issue is avoided or not.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "Music",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection"],
        "componenet_code": ["feature_selection"]
      },
      {
        "solution_text": "The line width of the \u201clink\u201d indicates the similar_x0002_ity value, which is the reciprocal of euclidean distance based on the feature vectors. Thicker line width indicates that the similarity between two music sequences is larger.",
        "solution_category": "data_manipulation",
        "solution_axial": "SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["SimilarityCalculation"],
        "componenet_code": ["similarity_calculation"]
      },
      {
        "solution_text": "To help users compare semantic features and explore the semantic change trends, we design a node-link graph to present statistics of music sequences (G3 & G4).This graph displays the basic information of music sequences and the similarity information between neighbor music sequences based on the feature vectors. In the sequence node-link graph, as shown in Fig. 6c, each music sequence is represented by a \u201cnode\u201d, which is a combination of one circle and three donuts. In the \u201cnode\u201d, the fill color of the circle represents the genre of the music sequence. As shown in Fig. 6c, the combined donuts are donut 1, donut 2, and donut 3 from inner to outer. The donut 1 displays the element number of the music sequence. The donut 2 reveals the proportions of notes and chords in the music sequence. In the donut 2, the light color part displays the ratio of notes in the music sequence, while another deep color part corresponds the ratio of chords. The donut 3 presents the ratios of instruments in the music sequence. In the donut 3, each part indicates statistic information of one instrument with four visual encodings including fill color, proportion, inner radius, and outer radius. The fill color is encoded as the instrument name. The proportion shows the ratio of the instrument in the music sequence. The inner radius displays the lowest pitch value performed by the instrument in the music sequence, while the outer radius displays the highest pitch value. As shown in Fig. 6c, the similarity between two music sequences is encoded as the \u201clink\u201d, which is a gray line in this graph. The line width of the \u201clink\u201d indicates the similar_x0002_ity value, which is the reciprocal of euclidean distance based on the feature vectors. Thicker line width indicates that the similarity between two music sequences is larger.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Pie",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Pie"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 71,
    "paper_title": "MUSE: Visual Analysis of Musical Semantic Sequence",
    "pub_year": 2023,
    "domain": "semantic analysis",
    "requirement": {
      "requirement_text": "G4: Exploration of semantic changes. The visualization should allow users to mine the semantic changes, which is helpful for the exploration and analysis of distribution patterns in the projection space. ",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Music",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "sequential": 1,
        "clusters_and_sets_and_lists": 1
      }
    },
    "solution": [
      {
        "solution_text": "Before extracting the music features, we constructed the semantic sequence and produced the text representation of each music. Music21 can extract performance elements from each music (MIDI files) to generate a music sequence with semantic information [32]. The elements (i.e., notes and chords) in a musical semantic sequence are sorted into a sequential data according to the order rule: (1) start time, (2) instrument type, and (3) pitch value.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "After extracting musical semantic sequence, we combine corresponding semantic information to form a word to rep_x0002_resent the sequence\u2019s element (note or chord). Next, we apply these words to produce the text representations of musical semantic sequences. Each note and chord have its\nsemantic information such as instrument name, pitch value, and type. Fig. 2a shows an example to generate words of a note and a chord. For each note, we construct three kinds of words including \u201cpitch-type, diff-type, instrument-pitch_x0002_type\u201d as its text representations. The \u201cdiff\u201d is the pitch difference value from the previous element to the current ele_x0002_ment, which represents the melodic features. For the chord, the \u201cpitch\u201d will be \u201cpitch-pitch-pitch\u201d to represent all notes\u2019 pitch in this chord, which is used in the \u201cdiff\u201d of the chord\u2019s word, too. During calculating the \u201cdiff\u201d, if the previous ele_x0002_ment is a chord, the pitch value of this \u201cprevious element\u201d is the pitch of the root note, which can be extracted by the \u201cchord.root()\u201d function in Music21 library. In the end, we apply these generated words to construct the text represen_x0002_tations of music sequences.",
        "solution_category": "data_manipulation",
        "solution_axial": "FeatureSelection",
        "solution_compoent": "",
        "axial_code": ["FeatureSelection"],
        "componenet_code": ["feature_selection"]
      },
      {
        "solution_text": "Inspired by the idea proposed by Galex [26], which extracts article features based on the article summarization data by using Doc2Vec to generate the feature vectors for articles, we apply Doc2Vec to compute the music vector representations by using music text representation. At first, we use text representations of the music sequences to train the Doc2Vec model. Then, we compute the corresponding feature vector based on the text representation of each music sequence. As a result, each music MIDI file is extracted as a musical semantic sequence, and each musical semantic sequence equips a corresponding 128-dimension feature vector by constructing the text representation and employing the Doc2Vec model.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling;Rectification",
        "solution_compoent": "",
        "axial_code": ["Modeling", "Rectification"],
        "componenet_code": ["modeling", "rectification"]
      },
      {
        "solution_text": "To visualize the distribution pattern among music more intuitively, we project the result of Doc2Vec to a 2D plane. We have considered three alternative dimension reduction methods includes principal component analysis (PCA) [12], multidimensional scaling (MDS) [24], and and t-distributed\nstochastic neighbor embedding (t-SNE) [33]. Although these dimension reduction methods are based on different under_x0002_lying algorithms, we have examined multiple parameter groups to select a proper method to produce the distribu_x0002_tion results. The test data includes 6,632 musical sequence vectors, and the size of each vector is 1 _x0003_ 128. These vectors are equipped with 4 genre attributes which are represented by 4 color points including yellow (classical), green (folk), red (rock), and purple (jazz). As shown in Fig. 2b, the result of t-SNE has more intui_x0002_tive distribution patterns than the other two dimension reduction methods (MDS and PCA). Consequently, we select t-SNE to project musical sequences into a distribution view with one theoretical consideration. Compared with MDS and PCA, t-SNE maximally preserves local neighbor_x0002_hoods of each data point (high dimensional vector), which ensures each musical sequence to have its similar neighbors based on the feature vectors in the distribution view [49]. In the distribution view, we use points to represent the musical sequences. To assist users in discovering distribu_x0002_tion patterns intuitively, we map two music features (i.e., music genres and instruments) to the points. In addition, to help users recognize the distribution patterns in the distri_x0002_bution view, we equip the distribution view with two visual components: a density contour component and a pathfinding component. The density contour is helpful for users to reveal the overall density distribution of the numerous points. How_x0002_ever, the traditional density contour only considers the geo_x0002_metric position of points and ignores the similarity. Thus, inspired by the marching square algorithm [20] in the den_x0002_sity contour, we propose an extended algorithm to modify the generation of density contours based on the similarity ofmusical sequence vectors. We apply the similarity to modify the basic matrix in the original marching square algorithm. As shown in Fig. 2c, the left contour is the raw result. In the marching square algorithm matrix, each number repre_x0002_sents the number of musical sequences in a grid. Our algo_x0002_rithm calculates and integrates the musical sequence similarities to modify the density value of the original matrix. When the grid similarity is larger than the threshold, these grids tend to be similar so that the number in the two grids would be added with a virtual value to change the state of the matrix. After the steps mentioned above, the modified density contour is generated to ensure that similar sequences have the same contour line. From the density contour shown in the right part of Fig. 2c, the points indicat_x0002_ing the same musical genre (jazz) are circled by the con_x0002_tours, which may indicate the apparent similarity of thes musical sequences. Including the density contour, we integrate a semantic change path component into the distribution view based on our proposed pathfinding algorithm to help users analyze the semantic change from one pattern to another pattern. The pathfinding algorithm, whose perplexity is O(n2), inte_x0002_grates the distances of music sequence projection coordi_x0002_nates and feature vectors. This heuristic algorithm follows four steps (S1 to S4) to find a path from the source music sequence to the target one in the distribution view. Four steps are introduced as follows. S1 Divide music sequences into corresponding grids based on their projection coordinates. S2 Obtain music sequences in the neighbor search grids of the source music sequence. If the neighbor search grids have no available sequences, enlarge the scope of neigh_x0002_bor search grids to search. S3 Calculate and determine the smallest euclidean distance of feature vectors between the source music sequence and each music sequence obtained in step S2. S4 Put the music sequence (found in step S3) into the path and make it a new source music sequence. Repeat step S1 to step S4 until the target music sequence is in the neighbor search grids. To ensure each new source music sequence to be approaching the target music sequence, we set the princi_x0002_ples of neighbor search grids [44]. Comparing the coordi_x0002_nates of the source music sequence and the target one, as shown in Fig. 3, we summarize eight neighbor search grids schemes. The eight schemes are \u201cupper left, low right, upper right, low left, horizontally left, horizontally right, vertically upper, and vertically low\u201d based on position sta_x0002_tus from the source music sequence to the target one. Users can sample the path points (sequences) if the path has too many music sequences to analyze the semantic change. In addition, the scheme of grids can be customized in different scenes. In this work, we offer two grids schemes including 128 _x0003_ 128 (which is selected in case 2) and 256 _x0003_ 256.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "To visualize the distribution pattern among music more intuitively, we project the result of Doc2Vec to a 2D plane. We have considered three alternative dimension reduction methods includes principal component analysis (PCA) [12], multidimensional scaling (MDS) [24], and and t-distributed\nstochastic neighbor embedding (t-SNE) [33]. Although these dimension reduction methods are based on different under_x0002_lying algorithms, we have examined multiple parameter groups to select a proper method to produce the distribu_x0002_tion results. The test data includes 6,632 musical sequence vectors, and the size of each vector is 1 _x0003_ 128. These vectors are equipped with 4 genre attributes which are represented by 4 color points including yellow (classical), green (folk), red (rock), and purple (jazz). As shown in Fig. 2b, the result of t-SNE has more intui_x0002_tive distribution patterns than the other two dimension reduction methods (MDS and PCA). Consequently, we select t-SNE to project musical sequences into a distribution view with one theoretical consideration. Compared with MDS and PCA, t-SNE maximally preserves local neighbor_x0002_hoods of each data point (high dimensional vector), which ensures each musical sequence to have its similar neighbors based on the feature vectors in the distribution view [49]. In the distribution view, we use points to represent the musical sequences. To assist users in discovering distribu_x0002_tion patterns intuitively, we map two music features (i.e., music genres and instruments) to the points. In addition, to help users recognize the distribution patterns in the distri_x0002_bution view, we equip the distribution view with two visual components: a density contour component and a pathfinding component. The density contour is helpful for users to reveal the overall density distribution of the numerous points. How_x0002_ever, the traditional density contour only considers the geo_x0002_metric position of points and ignores the similarity. Thus, inspired by the marching square algorithm [20] in the den_x0002_sity contour, we propose an extended algorithm to modify the generation of density contours based on the similarity ofmusical sequence vectors. We apply the similarity to modify the basic matrix in the original marching square algorithm. As shown in Fig. 2c, the left contour is the raw result. In the marching square algorithm matrix, each number repre_x0002_sents the number of musical sequences in a grid. Our algo_x0002_rithm calculates and integrates the musical sequence similarities to modify the density value of the original matrix. When the grid similarity is larger than the threshold, these grids tend to be similar so that the number in the two grids would be added with a virtual value to change the state of the matrix. After the steps mentioned above, the modified density contour is generated to ensure that similar sequences have the same contour line. From the density contour shown in the right part of Fig. 2c, the points indicat_x0002_ing the same musical genre (jazz) are circled by the con_x0002_tours, which may indicate the apparent similarity of thes musical sequences. Including the density contour, we integrate a semantic change path component into the distribution view based on our proposed pathfinding algorithm to help users analyze the semantic change from one pattern to another pattern. The pathfinding algorithm, whose perplexity is O(n2), inte_x0002_grates the distances of music sequence projection coordi_x0002_nates and feature vectors. This heuristic algorithm follows four steps (S1 to S4) to find a path from the source music sequence to the target one in the distribution view. Four steps are introduced as follows. S1 Divide music sequences into corresponding grids based on their projection coordinates. S2 Obtain music sequences in the neighbor search grids of the source music sequence. If the neighbor search grids have no available sequences, enlarge the scope of neigh_x0002_bor search grids to search. S3 Calculate and determine the smallest euclidean distance of feature vectors between the source music sequence and each music sequence obtained in step S2. S4 Put the music sequence (found in step S3) into the path and make it a new source music sequence. Repeat step S1 to step S4 until the target music sequence is in the neighbor search grids. To ensure each new source music sequence to be approaching the target music sequence, we set the princi_x0002_ples of neighbor search grids [44]. Comparing the coordi_x0002_nates of the source music sequence and the target one, as shown in Fig. 3, we summarize eight neighbor search grids schemes. The eight schemes are \u201cupper left, low right, upper right, low left, horizontally left, horizontally right, vertically upper, and vertically low\u201d based on position sta_x0002_tus from the source music sequence to the target one. Users can sample the path points (sequences) if the path has too many music sequences to analyze the semantic change. In addition, the scheme of grids can be customized in different scenes. In this work, we offer two grids schemes including 128 _x0003_ 128 (which is selected in case 2) and 256 _x0003_ 256.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Providing",
        "solution_compoent": "Scatter+ContourLine",
        "axial_code": ["Overlay-Coordinatesystemrelated-Providing"],
        "componenet_code": ["Scatter", "ContourLine"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 75,
    "paper_title": "Diagnosing Ensemble Few-Shot Classifiers",
    "pub_year": 2023,
    "domain": "Few-shot learning",
    "requirement": {
      "requirement_text": "R2: Improving the Quality of the Shots. The representativeness of the shots is essential for few-shot classification. As there are only a few labeled samples, mislabeled or confusing shots, such as the overlapped ones between two categories, decrease the model performance greatly. Removing such lowquality shots and adding necessary new ones improve the coverage of the shots and overall performance. When diagnosing an ensemble few-shot classifier, the experts need to understand the coverage of each shot and find the samples that are not well covered by the shots. In addition, the experts required a tool that can automatically recommend low-quality shots to be removed and candidate samples to be added to the shot set, so that they can only examine a small subset and then quickly decide which ones to remove/add.",
      "requirement_code": { "identify_main_cause_aggregate": 1 }
    },
    "data": {
      "data_text": "Images",
      "data_code": { "clusters_and_sets_and_lists": 1, "media": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Base learner selection aims to find a small subset of diverse and cooperative base learners to better predict the input samples (fitness). Here, U refers to the set of base learners fukgK k\u00bc1, and V is the set of samples fxigN i\u00bc1. As sparse subset selection encourages diversity among the selected learners, we then extend it by considering fitness and cooperation. Accordingly, Eq. (2) is rewritten as: where the first term is the representation cost, the second term is the sparsity term that prefers the learners with higher fitness, and the third term is the cooperation term. a1 and a2 control the trade-off among the three terms. Follow_x0002_ing Elhamifar et al. [37], a1 \u00bc a2 \u00bc 0:5amax, amax is the maximum distance between learners. In the first term, to calculate the representation cost, we need to define the distance between a base learner and a sample. A straightforward way is based on the prediction accuracy. However, we cannot evaluate the accuracy with_x0002_out ground-truth labels. Instead, we use the prediction con-fidence to measure the distance because samples with high prediction confidence tend to be classified correctly [39]. The prediction confidence of learner uk on xi is defined as the difference between the largest and the second-largest probabilities in the predicted label distribution yi, which is denoted as mki 2 \u00bd0; 1_x0007_ . The distance between the learner uk and the sample xi is then defined by dki \u00bc 1 _x0008_ mki because we prefer the base learners with larger confidence mki. In the second term, to encourage the selection of base learners with higher fitness, we emphasize the ones that bet_x0002_ter predict the given shots. A widely used measure, likeli_x0002_hood, is employed to estimate the fitness value. Accordingly, we add _x0002_ k for each learner uk, which is defined as its negative log-likelihood on the shots. In the third term, to encourage the cooperation between two learners, uk and ul, we penalize the difference between their predictions. Let yki and yli be the label distribution of sample xi predicted by uk and ul, respectively. Following the previous work of Dvornik et al. [1], the prediction difference is measured by the symmetric KL-divergence between their predictions: mkl \u00bc PN i\u00bc1\u00f0KL\u00f0ykijjyli\u00de \u00fe KL\u00f0ylijjyki\u00de\u00de=\u00f02N\u00de. mkl is 0 if the two learners make the same predictions.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Next, to achieve better class separation [44], we employ t-SNE to project the samples onto 2D space and utilize a scatterplot to visualize the projections.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "Users can also examine the coverage of the shots in the scatterplot and replace the low-quality shots with the high-quality ones (R2). sample view (Fig. 9b) to present the shots and unlabeled samples in context (R2). The sample view (Fig. 9b) enables users to examine the shots in the context of samples and tune their selection. For each sample, we first concatenate the features extracted by the base learners. Next, to achieve better class separation [44], we employ t-SNE to project the samples onto 2D space and utilize a scatterplot to visualize the projections. In the scatterplot, stars and circles are used to represent shots and unlabeled samples, respectively. Samples are colored according to their classes, and those with a confidence less than 0.2 are colored gray. For each shot, we utilize a clutter-aware label-layout algorithm [45] to place the image content close to the shot and reduce the overlap with other scatter points. When users select the samples of interest, the image content and label distributions are displayed at the bottom of the view (Fig. 9b). The label distributions are represented by colored bars, where the color encodes the class, and the length encodes the prediction probability. Users can click the checkbox on the right side to add it as a shot or remove it from the shot set.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Scatter",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Scatter"]
      },
      {
        "solution_text": "When users select the samples of interest, the image content and label distributions are displayed at the bottom of the view (Fig. 9b). The label distributions are represented by colored bars, where the color encodes the class, and the length encodes the prediction probability. Users can click the checkbox on the right side to add it as a shot or remove it from the shot set.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 77,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R1.1: Explore the timestamps of a time series the failure cases are mainly distributed at, and the states of the autonomous driving vehicle when failure cases happen (When).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Fig. 1-b2 shows the velocity, acceleration, and wheel steering data from the GPS monitoring of the vehicle, which provides a way to observe the states of the autonomous driving vehicle (R1.1). We integrated the line chart and two dashboards in AVS (the Autonomous Visualization System) [2] into Fig. 1-b2 to share the timeline in Temporal View and better align the analysis between the vehicle states and the model detection results in the following chart. The vertical line shows the values of line charts at the current time. The horizontal axis can be brushed. The corresponding data of the brushed time period will be highlighted in all views (R2.1).",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Sharing",
        "solution_compoent": "Line",
        "axial_code": ["Overlay-Coordinatesystemrelated-Sharing"],
        "componenet_code": ["Line"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 78,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R1.1: Explore the timestamps of a time series the failure cases are mainly distributed at, and the states of the autonomous driving vehicle when failure cases happen (When).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Fig. 1-b3 visualizes the detection sequences of objects from the tracklets, as well as the detection results from the model. This chart shows the correlation between object-level features and detection results in temporal distribution and further combines the analysis tasks of When and How (R1.1, R1.3).",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Unsimilar-Continuous",
        "solution_compoent": "Bar",
        "axial_code": ["Juxtaposition-Unsimilar-Continuous"],
        "componenet_code": ["Bar"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 79,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R1.1: Explore the timestamps of a time series the failure cases are mainly distributed at, and the states of the autonomous driving vehicle when failure cases happen (When).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Trajectory(Fig. 1-c2) combines the relative position in the bird\u2019s eye view from tracklets and detection results, and arranges them in the order of detection. Developers can combine such information in answering the questions of When and Where (R1.1, R1.2). The basic trajectory visualization method such as trajectory clustering by Lee et al. [31] usually visualizes the sequences that connect the absolute positions of the object. We combined the detection results and detection field with the object trajectory to have a global sense of the distribution of detection results in space. Considering that a global view in Scene is provided, the ego-centric view centered on the autonomous vehicle is also important, so we adopted such an ego-centric visualization form and relative positions. The rectangle at the bottom represents the camera of the autonomous driving vehicle, which is fixed as a reference. The arc in the view marks the position of 20 meters away from the camera, as a reference line for the relative distance perception. Each point indicates one object in one detection, and each line represents the trajectory of an object, and the direction of each arrow is the movement direction of the last two detections\u2019 positions of one object, which always combines trajectory lines to indicate the whole segment overall direction of relative movement. The dashed line indicates discon_x0002_tinuous detection time, i.e., the object has disappeared between the beginning and end detection of the dashed line. As shown in Fig. 7, three different trajectory patterns could be visualized. Developers can zoom in and brush later (R1.2). The brushed objects and corresponding paths will be reserved and redrawn. The update of other views will be triggered accordingly as well (R1.2, R2.1).",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Line",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Line"]
      },
      {
        "solution_text": "Developers can zoom in and brush later (R1.2). The brushed objects and corresponding paths will be reserved and redrawn. The update of other views will be triggered accordingly as well (R1.2, R2.1).",
        "solution_category": "interaction",
        "solution_axial": "Encode+Filtering+OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["Filtering", "OverviewandExplore", "Encode"],
        "componenet_code": ["filtering", "overview_and_explore", "encode"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 80,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R1.2: Explore the surroundings, positions of failure cases, especially the relative positions to the camera (Where).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Trajectory(Fig. 1-c2) combines the relative position in the bird\u2019s eye view from tracklets and detection results, and arranges them in the order of detection. Developers can combine such information in answering the questions of When and Where (R1.1, R1.2). The basic trajectory visualization method such as trajectory clustering by Lee et al. [31] usually visualizes the sequences that connect the absolute positions of the object. We combined the detection results and detection field with the object trajectory to have a global sense of the distribution of detection results in space. Considering that a global view in Scene is provided, the ego-centric view centered on the autonomous vehicle is also important, so we adopted such an ego-centric visualization form and relative positions. The rectangle at the bottom represents the camera of the autonomous driving vehicle, which is fixed as a reference. The arc in the view marks the position of 20 meters away from the camera, as a reference line for the relative distance perception. Each point indicates one object in one detection, and each line represents the trajectory of an object, and the direction of each arrow is the movement direction of the last two detections\u2019 positions of one object, which always combines trajectory lines to indicate the whole segment overall direc_x0002_tion of relative movement. The dashed line indicates discon_x0002_tinuous detection time, i.e., the object has disappeared between the beginning and end detection of the dashed line. As shown in Fig. 7, three different trajectory patterns could be visualized. Developers can zoom in and brush later (R1.2). The brushed objects and corresponding paths will be reserved and redrawn. The update of other views will be triggered accordingly as well (R1.2, R2.1).",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Line",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Line"]
      },
      {
        "solution_text": "Developers can zoom in and brush later (R1.2). The brushed objects and corresponding paths will be reserved and redrawn. The update of other views will be triggered accordingly as well (R1.2, R2.1).",
        "solution_category": "interaction",
        "solution_axial": "Encode+Filtering+OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["Filtering", "OverviewandExplore", "Encode"],
        "componenet_code": ["filtering", "overview_and_explore", "encode"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 81,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R1.2: Explore the surroundings, positions of failure cases, especially the relative positions to the camera (Where).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The Density Map in Fig. 1-a1 is obtained by the processing stages we discussed above. It provides an intuitive insight into what is captured by the model. The RGB image in Fig. 1-a1 tells us environmental information(R1.2).",
        "solution_category": "visualization",
        "solution_axial": "Non-composite",
        "solution_compoent": "Box+Road",
        "axial_code": ["Non-composite"],
        "componenet_code": ["Road", "Box"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 82,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R1.2: Explore the surroundings, positions of failure cases, especially the relative positions to the camera (Where).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Fig. 1-c1 is a 3D spatial stereogram drawn with raw data that contains the point cloud data from the vehicle-mounted LiDAR, latitude and longitude information from GPS, and tracklets of labeled objects (e.g., relative positions, dimensions, and classes). It is a key view for spatial temporal scene perception, which displays sufficient information of Where (R1.2).",
        "solution_category": "visualization",
        "solution_axial": "Non-composite",
        "solution_compoent": "Road+Car+Box",
        "axial_code": ["Non-composite"],
        "componenet_code": ["Box", "Road", "Car"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 83,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R1.3: Observe whether the feature maps of failure cases exhibit abnormalities compared to normals(How).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "This view can be replayed over time so that model devel_x0002_opers can review how the feature maps, detection results, and surroundings change over time (R2.1). By comparing the den_x0002_sity map with the temporal information and bounding boxes of detection results, developers can get an initial answer to the question of How the model detects objects (R1.3)",
        "solution_category": "visualization",
        "solution_axial": "Non-composite",
        "solution_compoent": "Road+Car+Box",
        "axial_code": ["Non-composite"],
        "componenet_code": ["Box", "Road", "Car"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 84,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R1.3: Observe whether the feature maps of failure cases exhibit abnormalities compared to normals(How).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The processed features and detection results are combined in Fig. 1-a2. It visualizes the processed features of each detection and corresponds to IoU on a twodimensional coordinate system, providing an overview to observe the relative distance and distribution of the objectlevel features, and further answers the question of How the model fails (R1.3).",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Scatter",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Scatter"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 85,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R1.3: Observe whether the feature maps of failure cases exhibit abnormalities compared to normals(How).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Fig. 1-b3 visualizes the detection sequences of objects from the tracklets, as well as the detection results from the model. This chart shows the correlation between object-level features and detection results in temporal distribution and further combines the analysis tasks of When and How (R1.1, R1.3).",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Unsimilar-Continuous",
        "solution_compoent": "Bar",
        "axial_code": ["Juxtaposition-Unsimilar-Continuous"],
        "componenet_code": ["Bar"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 86,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R2.1: In a whole sequence of images, for objects poorly detected over a period of interest, observe their surroundings and relative positions to the autonomous driving vehicle, their detection results, abnormalities of the feature maps, and the relationship between the detection results, the spatial location and the features (When ! Where and How).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "This view can be replayed over time so that model developers can review how the feature maps, detection results, and surroundings change over time (R2.1).",
        "solution_category": "interaction",
        "solution_axial": "History",
        "solution_compoent": "",
        "axial_code": ["History"],
        "componenet_code": ["history"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 87,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R2.1: In a whole sequence of images, for objects poorly detected over a period of interest, observe their surroundings and relative positions to the autonomous driving vehicle, their detection results, abnormalities of the feature maps, and the relationship between the detection results, the spatial location and the features (When ! Where and How).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Fig. 1-b1 is a timeline that provides the horizontal axis for the following charts. It can be selected, dragged and played, and the detected objects in current time will be visualized in blue in all the views (R2.1).",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Line",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Line"]
      },
      {
        "solution_text": "Fig. 1-b1 is a timeline that provides the horizontal axis for the following charts. It can be selected, dragged and played, and the detected objects in current time will be visualized in blue in all the views (R2.1).",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Filtering",
        "solution_compoent": "",
        "axial_code": ["Selecting", "Filtering"],
        "componenet_code": ["selecting", "filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 88,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R2.1: In a whole sequence of images, for objects poorly detected over a period of interest, observe their surroundings and relative positions to the autonomous driving vehicle, their detection results, abnormalities of the feature maps, and the relationship between the detection results, the spatial location and the features (When ! Where and How).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Fig. 1-b2 shows the velocity, acceleration, and wheel steering data from the GPS monitoring of the vehicle, which provides a way to observe the states of the autonomous driving vehicle (R1.1). We inte_x0002_grated the line chart and two dashboards in AVS (the Autonomous Visualization System) [2] into Fig. 1-b2 to share the timeline in Temporal View and better align the anal_x0002_ysis between the vehicle states and the model detection results in the following chart. The vertical line shows the values of line charts at the current time. The horizontal axis can be brushed. The corresponding data of the brushed time period will be highlighted in all views (R2.1).",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Sharing",
        "solution_compoent": "Line",
        "axial_code": ["Overlay-Coordinatesystemrelated-Sharing"],
        "componenet_code": ["Line"]
      },
      {
        "solution_text": "Fig. 1-b2 shows the velocity, acceleration, and wheel steering data from the GPS monitoring of the vehicle, which provides a way to observe the states of the autonomous driving vehicle (R1.1). We inte_x0002_grated the line chart and two dashboards in AVS (the Autonomous Visualization System) [2] into Fig. 1-b2 to share the timeline in Temporal View and better align the anal_x0002_ysis between the vehicle states and the model detection results in the following chart. The vertical line shows the values of line charts at the current time. The horizontal axis can be brushed. The corresponding data of the brushed time period will be highlighted in all views (R2.1).",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 89,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R2.2: Similarly, for objects detected in a distance from the autonomous driving vehicle, observe the timestamps they are distributed at, whether the feature maps of failure cases exhibit abnormalities compared to the normals, the detection results of the objects, and the relationship between the spatial distribution and the other three: temporal distribution, feature distribution and detection results (Where ! When and How).",
      "requirement_code": {
        "discover_observation": 1,
        "explain_differences": 1
      }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Brushing the objects closer to the camera in Fig. 8-a3, we observe that their distribution is more concentrated in Fig. 8-c3 and this is consistent with the previous analysis that cyclists and pedestrians mostly appear closer to the camera in Fig. 8-d3 (R2.2).",
        "solution_category": "interaction",
        "solution_axial": "History",
        "solution_compoent": "",
        "axial_code": ["History"],
        "componenet_code": ["history"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 90,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R2.3: For objects with abnormal feature maps, show their temporal information, their spatial information such as relative positions to the autonomous driving vehicle and surroundings, as well as their detection results. Moreover, the relationship between them (How ! When and Where).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "This chart supports zooming in or out and brushing a polygon area of projection points. The selected objects will be highlighted in this chart and the update of other views will be triggered accordingly (R2.3).",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Line",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Line"]
      },
      {
        "solution_text": "This chart supports zooming in or out and brushing a polygon area of projection points. The selected objects will be highlighted in this chart and the update of other views will be triggered accordingly (R2.3).",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Filtering",
        "solution_compoent": "",
        "axial_code": ["Selecting", "Filtering"],
        "componenet_code": ["selecting", "filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 91,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R2.3: For objects with abnormal feature maps, show their temporal information, their spatial information such as relative positions to the autonomous driving vehicle and surroundings, as well as their detection results. Moreover, the relationship between them (How ! When and Where).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The control in Fig. 1-d2 takes the form of parallel coordinates to show the high-dimensional labeled data, which is plotted with IoU and locations of labeled objects, including detailed relative orientation (alpha and rotation-y) and position (x, y, z in the camera coordinate system: rightward, upward, and forward distances). Ignored objects are not drawn on the first axis, i.e., IoU, since they are not included in the AP calculation. Each axis can be brushed, and the intersection will be taken to filter the data (R1, R2.3).",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Sharing",
        "solution_compoent": "Line",
        "axial_code": ["Overlay-Coordinatesystemrelated-Sharing"],
        "componenet_code": ["Line"]
      },
      {
        "solution_text": "The control in Fig. 1-d2 takes the form of parallel coordinates to show the high-dimensional labeled data, which is plotted with IoU and locations of labeled objects, including detailed relative orientation (alpha and rotation-y) and position (x, y, z in the camera coordinate system: rightward, upward, and forward distances). Ignored objects are not drawn on the first axis, i.e., IoU, since they are not included in the AP calculation. Each axis can be brushed, and the intersection will be taken to filter the data (R1, R2.3).",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 92,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R2.3: For objects with abnormal feature maps, show their temporal information, their spatial information such as relative positions to the autonomous driving vehicle and surroundings, as well as their detection results. Moreover, the relationship between them (How ! When and Where).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Each density map has a \u201cScale Up\u201d function for easy viewing when hovering as shown in Fig. 1-b3. Developers can click the detection sequence of one object or the final density map of interest to select an object (R2.3), which would also display the density maps of the whole sequence of this object at the bottom.",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Abstract/Elaborate",
        "solution_compoent": "",
        "axial_code": ["Abstract/Elaborate", "Selecting"],
        "componenet_code": ["abstract/elaborate", "selecting"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 93,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R3.1: In a sequence of images, for objects detected in a spatial-temporal range of interest, observe whether the feature maps of failure cases exhibit abnormalities compared to the normals (When and Where !How).",
      "requirement_code": {
        "discover_observation": 1,
        "explain_differences": 1
      }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "We chose other easily distinguishable colors referring to ColorBrewer [23] to encode interaction-related results. As shown in Fig. 1-e, the objects highlighted with orange, blue, and yellow strokes are linked to the selected object, current time and intersection of them in all other views (R3.1).",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Abstract/Elaborate+Connect/Relate",
        "solution_compoent": "",
        "axial_code": ["Abstract/Elaborate", "Connect/Relate", "Selecting"],
        "componenet_code": ["abstract/elaborate", "connect/relate", "selecting"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 94,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R3.1: In a sequence of images, for objects detected in a spatial-temporal range of interest, observe whether the feature maps of failure cases exhibit abnormalities compared to the normals (When and Where !How).",
      "requirement_code": {
        "discover_observation": 1,
        "explain_differences": 1
      }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Two of the three views are used as an overview together to explore the other one and the control panel. For example, combining the spatial-temporal information and detection results from the Temporal View, parallel coordinate diagram and Spatial View, developers can select the objects by the iterative brush of their interest and playing the timeline. Then, they are capable of observing the selected objects in other views. This interaction helps to explore how objects in some spatial-temporal regions are detected, their feature distribution, detection results, and the relationship between results and features (R3.1).",
        "solution_category": "interaction",
        "solution_axial": "Connect/Related+Selelcting",
        "solution_compoent": "",
        "axial_code": ["Selelcting", "Connect/Related"],
        "componenet_code": ["selecting", "connect/relate"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 95,
    "paper_title": "When, Where and How Does it Fail? A Spatial-Temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving",
    "pub_year": 2023,
    "domain": "spatial-temporal visual analytics",
    "requirement": {
      "requirement_text": "R3.3: For objects in a spatial range and feature distribution region or feature map characteristics of interest, observe the timestamps they are distributed at, the states of the autonomous driving vehicle, their detection results, and the relationship between the results and temporal information (Where and How ! When).",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The data related to object detection tasks in autonomous driving includes two main categories: raw data and model output. Raw data includes tracklets, GPS, point clouds and images. Model output includes features and detection results. More specifically, tracklets refer to classes, sizes (length, width and height) and locations of labeled objects which are ground truth in object detection. GPS data records the latitude, longitude, speed and other driving sta_x0002_tus data of the autonomous driving vehicle. Point clouds are detected by the vehicle\u2019s LiDAR. Images are taken by the vehicle\u2019s camera. The images are input to the model, and then the model outputs the detection results. Features inside the model can also be extracted. Based on our goals, we abstracted relevant data into three categories: model data, spatial data, and temporal data. Model data refers to images inputs, the features of a model, and detection results. Spatial data includes point clouds and tracklets. Temporal data is related to vehicle state data from GPS and object sequences from tracklets.",
      "data_code": {
        "geometry": 1,
        "media": 1,
        "temporal": 1,
        "tables": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "For the cluster and the object-level density maps we just mentioned, we continue to explore and find that this cluster represents only two objects, as shown in Fig. 8-d4, reflecting the fact that the feature distribution in Fig. 8-c3 knows not only the aggregation of the detection results (Fig. 8-c2), but also aggregations of the same object (R3.3).",
        "solution_category": "visualization",
        "solution_axial": "Non-composite",
        "solution_compoent": "Cluster",
        "axial_code": ["Non-composite"],
        "componenet_code": ["Cluster"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 96,
    "paper_title": "DOMINO: Visual Causal Reasoning With Time-Dependent Phenomena",
    "pub_year": 2023,
    "domain": "Causality analysis",
    "requirement": {
      "requirement_text": "T1: Generating causal propositions and hypotheses is often the first step in causality analysis. Most current works on temporal causality achieve this either by manually grouping relevant values and then assigning them semantic meanings, or by conducting an exhaustive search after evenly partitioning the time series data into a large amount of sections each considered an event. Both of these approaches are limited in efficiency and flexibility. Since in logic-based causality a causal relation is defined over a time lagged conditional distribution, analysts should be given direct access to such information by allowing them to generate causal propositions and hypotheses with visual support. Also, since an effect can have multiple causes, an overview of the values and boolean labels of each time series in a synchronized fashion will help the understanding of the compound relations.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "the Air Quality dataset",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "The testing of an event c being a cause of the effect e is based on the assumption that a true cause always increases the probability of the effect (we can view a preventative, which lowers the probability of e, as raising the probability of :e). Thus, we say c is a potential cause [25] of e if, taking into con_x0002_sideration the relative time delay window, it satisfies P\u00f0e\u00de < p and P\u00f0ejc\u00de _x0005_ p; (2) where P\u00f0ejc\u00de is calculated in the same fashion as Eq. (1). Additionally, if the effect e is defined on a continuous variable ve and we are looking for events that are potentially shifting the distribution of ve (as opposed to a value of ve falling into a specific range), the expected value of ve can be used instead for better sensitivity to change [23]. As such, c is considered a potential cause of e when E\u00bdve_x0004_ \u00bc E\u00bdvejc_x0004_ : (3) Here, the \u00bc sign can be replaced by either > or < to stipu_x0002_late only positive or negative causes. And the conditional expected value can be calculated as E\u00bdvejc_x0004_ \u00bc XyyQ\u00f0\u00bdve \u00bc y_x0004_ ^ c\u00deQ\u00f0c\u00de ; (4) where y are values in ve\u2019s domain and Q\u00f0x\u00de denotes the number of time points where x holds. To illustrate, Fig. 1 shows short sequences of a continu_x0002_ous variable ve and a causal event c. Averaging all values of ve, we have E\u00bdve_x0004_ =1.5. Then, when considering a time shift of exactly 1 unit, we have E\u00bdvejc_x0004_\u00bc\u00f00:9 \u00fe 3 \u00fe 2:3 \u00fe 1:3\u00de=4 \u00bc 1:875 (these are the values of ve exactly 1 unit after c is T). Since E\u00bdvejc_x0004_ > E\u00bde_x0004_ , according to Eq. (3), c increases the expected value of ve and thus is a potential cause of it. However, if we try to find the positive cause by instead bounding ve to a specific range, or to a specific value such as the mean of ve the event e would be defined as [ve > E\u00bdve_x0004_ ]. Then we would have P\u00f0e\u00de \u00bc 0:5 (e occurs 4 times out of 8 time points) and P\u00f0ejc\u00de \u00bc 0:5 (2 out of 4), where c would not be considered a potential cause because it is not raising the probability of e. This shows the reduced sensitivity to change that comes with trying to be more specific. We can generalize this framework to a set of causes X of an effect e. We measure the influence of X towards e by cal_x0002_culating the probability change of e as P\u00f0ejX\u00de _x0007_ P\u00f0e\u00de or the change of expected value of ve as E\u00bdvejX_x0004_ _x0007_ E\u00bdve_x0004_ , depend_x0002_ing on the definition of e. Note that while the conditional probability is bounded within \u00bd0; 1_x0004_ , the expected value could be any amount, and either positive or negative. As mentioned, a causal relation is only potential; it may not be direct or it may be spurious, even if Eqs. (2) or (3) holds. This can be due to two possible situations: (1) c and e are actually independent but are commonly caused by another event x (the confounder) with c being caused earlier than e (Fig. 2a), or (2) c causes e indirectly via x (mediation, Fig. 2b). In either condition, we may observe that Eqs. (2) or (3) holds and erroneously mark c as directly causing e. One way to eliminate such error is to compare the distribution of e when c and x both occur, i.e., P\u00f0ejc ^ x\u00de, to that when only x is present, i.e., P\u00f0ej:c ^ x\u00de. Then the two will be found equal (or almost equal) if c is a spurious cause of e. Note that this requires the time window \u00bdr; s_x0004_ to be sufficiently wide such that both x and c could have caused e [24]. This idea can be generalized to multivariate time series. When considering multiple time series in a dataset, for a given effect, we usually can recognize a number of potential causes. To identify the real causes that can better explain the effect, Eells [8] proposed the average significance of a potential cause c, among all potential causes X toward the effect e, calculated as \"avg\u00f0c; e\u00de \u00bc X x2X=c P\u00f0ejc ^ x\u00de _x0007_ P\u00f0ej:c ^ x\u00de jX=cj : (5) Here X=c is the set of potential causes excluding c and jX=cj is the number of events in it. We need at least two potential causes to make the computation meaningful and all calcula_x0002_tions are associated with a preset time window. Then, by setting a certain threshold \", c is called an \"-significant cause of e if j\"avg\u00f0c; e\u00dej _x0005_ \". Further, if e stands for the increase or decrease of a continuous variable ve over the time window, the conditional probability in Eq. (5) can be replaced by the conditional expected value such that \"avg\u00f0c; e\u00de \u00bc X x2X= E\u00bdvejc ^ x_x0004_ _x0007_ E\u00bdvej:c ^ x_x0004_ jX=cj : (6) Although the \" threshold is decisive in testing if a cause is significant, its value can be difficult to determine auto_x0002_matically in practice. In presence of a large number of (say, thousands of) potential causes where significant causes are rare, all such \"avg values usually follow a Gaussian distribu_x0002_tion [23]. As a result, the problem can be solved by testing the significance of a null hypothesis where p values rejecting the null hypothesis deviate from the mean [9]. However, we find that this theoretical method cannot really be applied in most of our applications since we rarely encounter such a large number of time-series and causal events, especially when we just wish to explore the impact of some specific causes on the target. In such cases, the \" threshold can only be assigned empirically and interactively by the analyst. This requirement for user assistance, together with other analytical tasks that will be discussed later, motivated the visual analytics system that is at the heart of our work. Since a potential cause elevates the probability (Eq. (2)) oralters the expected value (Eq. (3)) of the effect, the processof searching for a cause c is the same as deciding an appro\\x02priate numerical constraint on the cause variable vc, onwhich c is made, so that Eqs. (2) or (3) can be satisfied. Thisis relatively easy and straightforward when vc has discretevalues, where we can simply scan through vc\u2019s domain andmake c take all the values satisfying the condition.The search becomes more complex when vc\u2019s domain iscontinuous. One solution could be to discretize vc and thenapply the same scanning process, but determining a gooddiscretization strategy is difficult. Our approach is toinstead only look at vc at time points t, noted as vc\u00f0t\u00de, wheree holds after the specified time delay (vc\u00f0t\u00de\u02c6 \\x05 r;\\x03 se). Record\\x02ing all such vc\u00f0t\u00de as Tc, we then discretize vc adaptively byclustering values in Tc. The idea is to consider values that vcfrequently takes and lead to e as possibly causing e.The clustering process follows Algorithm 3.3, whichtakes the same approach as the incremental clustering forhigh-dimensional data [41] but is applied in 1-D. The original algorithm has shown great performance in learning 'informative value segments. The adapted version here iteratively scans values in Tc until all clusters converge or themaximum number of iterations is reached. In each iteration,a value is assigned to a cluster center if the distance betweenthem is smaller than some threshold u. A new cluster isadded when a point is too far away from all clusters. Thethreshold u controls the size of the clusters, which decideshow vc will be discretized later. At last, we transform vc byconsidering the value range each cluster covers as a level,and test if it fulfills Eqs. (2) or (3). If multiple levels arereturned, we try to merge them if they overlap and take theone that best elevates e as the most possible cause. 'The algorithm can be easily parallelized [41] if we modify the incremental process such that it searches clusters inbatches instead of inspecting them one by one, enablingscalability. Also, the trade-off of taking different u valuesis that a larger u tends to produce a looser constraint (alarger value range of vc) in c, often resulting in a smallerP\u00f0ejc\u00de or an E\u00bdvejccloser to E\u00bdve\u2013 a smaller u does theopposite. This is similar to the problem of under-/over-fitting. In our experiments, we found when u equals 0.15 ofvc\u2019s value range 5 iterations were usually sufficient toreach a plausible result \u2013 one that would be close to manual adjustment.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "The testing of an event c being a cause of the effect e is based on the assumption that a true cause always increases the probability of the effect (we can view a preventative, which lowers the probability of e, as raising the probability of :e). Thus, we say c is a potential cause [25] of e if, taking into con_x0002_sideration the relative time delay window, it satisfies P\u00f0e\u00de < p and P\u00f0ejc\u00de _x0005_ p; (2) where P\u00f0ejc\u00de is calculated in the same fashion as Eq. (1). Additionally, if the effect e is defined on a continuous variable ve and we are looking for events that are potentially shifting the distribution of ve (as opposed to a value of ve falling into a specific range), the expected value of ve can be used instead for better sensitivity to change [23]. As such, c is considered a potential cause of e when E\u00bdve_x0004_ \u00bc E\u00bdvejc_x0004_ : (3) Here, the \u00bc sign can be replaced by either > or < to stipu_x0002_late only positive or negative causes. And the conditional expected value can be calculated as E\u00bdvejc_x0004_ \u00bc XyyQ\u00f0\u00bdve \u00bc y_x0004_ ^ c\u00deQ\u00f0c\u00de ; (4) where y are values in ve\u2019s domain and Q\u00f0x\u00de denotes the number of time points where x holds. To illustrate, Fig. 1 shows short sequences of a continu_x0002_ous variable ve and a causal event c. Averaging all values of ve, we have E\u00bdve_x0004_ =1.5. Then, when considering a time shift of exactly 1 unit, we have E\u00bdvejc_x0004_\u00bc\u00f00:9 \u00fe 3 \u00fe 2:3 \u00fe 1:3\u00de=4 \u00bc 1:875 (these are the values of ve exactly 1 unit after c is T). Since E\u00bdvejc_x0004_ > E\u00bde_x0004_ , according to Eq. (3), c increases the expected value of ve and thus is a potential cause of it. However, if we try to find the positive cause by instead bounding ve to a specific range, or to a specific value such as the mean of ve the event e would be defined as [ve > E\u00bdve_x0004_ ]. Then we would have P\u00f0e\u00de \u00bc 0:5 (e occurs 4 times out of 8 time points) and P\u00f0ejc\u00de \u00bc 0:5 (2 out of 4), where c would not be considered a potential cause because it is not raising the probability of e. This shows the reduced sensitivity to change that comes with trying to be more specific. We can generalize this framework to a set of causes X of an effect e. We measure the influence of X towards e by cal_x0002_culating the probability change of e as P\u00f0ejX\u00de _x0007_ P\u00f0e\u00de or the change of expected value of ve as E\u00bdvejX_x0004_ _x0007_ E\u00bdve_x0004_ , depend_x0002_ing on the definition of e. Note that while the conditional probability is bounded within \u00bd0; 1_x0004_ , the expected value could be any amount, and either positive or negative. As mentioned, a causal relation is only potential; it may not be direct or it may be spurious, even if Eqs. (2) or (3) holds. This can be due to two possible situations: (1) c and e are actually independent but are commonly caused by another event x (the confounder) with c being caused earlier than e (Fig. 2a), or (2) c causes e indirectly via x (mediation, Fig. 2b). In either condition, we may observe that Eqs. (2) or (3) holds and erroneously mark c as directly causing e. One way to eliminate such error is to compare the distribution of e when c and x both occur, i.e., P\u00f0ejc ^ x\u00de, to that when only x is present, i.e., P\u00f0ej:c ^ x\u00de. Then the two will be found equal (or almost equal) if c is a spurious cause of e. Note that this requires the time window \u00bdr; s_x0004_ to be sufficiently wide such that both x and c could have caused e [24]. This idea can be generalized to multivariate time series. When considering multiple time series in a dataset, for a given effect, we usually can recognize a number of potential causes. To identify the real causes that can better explain the effect, Eells [8] proposed the average significance of a potential cause c, among all potential causes X toward the effect e, calculated as \"avg\u00f0c; e\u00de \u00bc X x2X=c P\u00f0ejc ^ x\u00de _x0007_ P\u00f0ej:c ^ x\u00de jX=cj : (5) Here X=c is the set of potential causes excluding c and jX=cj is the number of events in it. We need at least two potential causes to make the computation meaningful and all calcula_x0002_tions are associated with a preset time window. Then, by setting a certain threshold \", c is called an \"-significant cause of e if j\"avg\u00f0c; e\u00dej _x0005_ \". Further, if e stands for the increase or decrease of a continuous variable ve over the time window, the conditional probability in Eq. (5) can be replaced by the conditional expected value such that \"avg\u00f0c; e\u00de \u00bc X x2X= E\u00bdvejc ^ x_x0004_ _x0007_ E\u00bdvej:c ^ x_x0004_ jX=cj : (6) Although the \" threshold is decisive in testing if a cause is significant, its value can be difficult to determine auto_x0002_matically in practice. In presence of a large number of (say, thousands of) potential causes where significant causes are rare, all such \"avg values usually follow a Gaussian distribu_x0002_tion [23]. As a result, the problem can be solved by testing the significance of a null hypothesis where p values rejecting the null hypothesis deviate from the mean [9]. However, we find that this theoretical method cannot really be applied in most of our applications since we rarely encounter such a large number of time-series and causal events, especially when we just wish to explore the impact of some specific causes on the target. In such cases, the \" threshold can only be assigned empirically and interactively by the analyst. This requirement for user assistance, together with other analytical tasks that will be discussed later, motivated the visual analytics system that is at the heart of our work. Since a potential cause elevates the probability (Eq. (2)) oralters the expected value (Eq. (3)) of the effect, the processof searching for a cause c is the same as deciding an appro\\x02priate numerical constraint on the cause variable vc, onwhich c is made, so that Eqs. (2) or (3) can be satisfied. Thisis relatively easy and straightforward when vc has discretevalues, where we can simply scan through vc\u2019s domain andmake c take all the values satisfying the condition.The search becomes more complex when vc\u2019s domain iscontinuous. One solution could be to discretize vc and thenapply the same scanning process, but determining a gooddiscretization strategy is difficult. Our approach is toinstead only look at vc at time points t, noted as vc\u00f0t\u00de, wheree holds after the specified time delay (vc\u00f0t\u00de\u02c6 \\x05 r;\\x03 se). Record\\x02ing all such vc\u00f0t\u00de as Tc, we then discretize vc adaptively byclustering values in Tc. The idea is to consider values that vcfrequently takes and lead to e as possibly causing e.The clustering process follows Algorithm 3.3, whichtakes the same approach as the incremental clustering forhigh-dimensional data [41] but is applied in 1-D. The original algorithm has shown great performance in learning 'informative value segments. The adapted version here iteratively scans values in Tc until all clusters converge or themaximum number of iterations is reached. In each iteration,a value is assigned to a cluster center if the distance betweenthem is smaller than some threshold u. A new cluster isadded when a point is too far away from all clusters. Thethreshold u controls the size of the clusters, which decideshow vc will be discretized later. At last, we transform vc byconsidering the value range each cluster covers as a level,and test if it fulfills Eqs. (2) or (3). If multiple levels arereturned, we try to merge them if they overlap and take theone that best elevates e as the most possible cause. 'The algorithm can be easily parallelized [41] if we modify the incremental process such that it searches clusters inbatches instead of inspecting them one by one, enablingscalability. Also, the trade-off of taking different u valuesis that a larger u tends to produce a looser constraint (alarger value range of vc) in c, often resulting in a smallerP\u00f0ejc\u00de or an E\u00bdvejccloser to E\u00bdve\u2013 a smaller u does theopposite. This is similar to the problem of under-/over-fitting. In our experiments, we found when u equals 0.15 ofvc\u2019s value range 5 iterations were usually sufficient toreach a plausible result \u2013 one that would be close to manual adjustment.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Our design of the conditional distribution view was motivated by the definition of a potential cause mentioned in Section 3. With the conditional distributions displayed here, analysts can directly observe a time-lagged phenomenon and hence make causal hypotheses (T1,T4). Upon selecting and loading a tabular data file of a multif_x0002_variate time series (top menu) the analyst selects one of the variables as the effect variable e and specifies the effect of interest, such as Value Increase, Value Decrease, or Value Range(second menu row). Upon selecting e the system displays a histogram of its values ve over the entire time series duration(blue bars). This histogram can be brushed when the selected effect is Value Range (the Brushed box, top right, must be checked). Next, the analyst selects a cause variable in the menu on the bottom left, and a frequency histogram of the variable\u2019s levels (if discrete) or values (if continuous) will be shown above the menu. The analyst then indicates the potential cause c by selecting the corresponding level bar(s) and specifying a time delay via the slider on the right. Following, the system displays a histogram of e when c\u2019s event conditions are met (green bars). This histogram is necessarily lower in magnitude since the conditions are only met for some ve. The vertical bars for the histograms indicate their respective means, E?veand E?vejc_x0004_ . The wider apart these means the more pronounced c\u2019s potential effect.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Bar",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Bar"]
      },
      {
        "solution_text": "Our design of the conditional distribution view was motivated by the definition of a potential cause mentioned in Section 3. With the conditional distributions displayed here, analysts can directly observe a time-lagged phenomenon and hence make causal hypotheses (T1,T4). Upon selecting and loading a tabular data file of a multif_x0002_variate time series (top menu) the analyst selects one of the variables as the effect variable e and specifies the effect of interest, such as Value Increase, Value Decrease, or Value Range(second menu row). Upon selecting e the system displays a histogram of its values ve over the entire time series duration(blue bars). This histogram can be brushed when the selected effect is Value Range (the Brushed box, top right, must be checked). Next, the analyst selects a cause variable in the menu on the bottom left, and a frequency histogram of the variable\u2019s levels (if discrete) or values (if continuous) will be shown above the menu. The analyst then indicates the potential cause c by selecting the corresponding level bar(s) and specifying a time delay via the slider on the right. Following, the system displays a histogram of e when c\u2019s event conditions are met (green bars). This histogram is necessarily lower in magnitude since the conditions are only met for some ve. The vertical bars for the histograms indicate their respective means, E?veand E?vejc_x0004_ . The wider apart these means the more pronounced c\u2019s potential effect.",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Filtering+Abstract/Elaborate",
        "solution_compoent": "",
        "axial_code": ["Abstract/Elaborate", "Selecting", "Filtering"],
        "componenet_code": ["abstract/elaborate", "selecting", "filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 97,
    "paper_title": "DOMINO: Visual Causal Reasoning With Time-Dependent Phenomena",
    "pub_year": 2023,
    "domain": "Causality analysis",
    "requirement": {
      "requirement_text": "T1: Generating causal propositions and hypotheses is often the first step in causality analysis. Most current works on temporal causality achieve this either by manually grouping relevant values and then assigning them semantic meanings, or by conducting an exhaustive search after evenly partitioning the time series data into a large amount of sections each considered an event. Both of these approaches are limited in efficiency and flexibility. Since in logic-based causality a causal relation is defined over a time lagged conditional distribution, analysts should be given direct access to such information by allowing them to generate causal propositions and hypotheses with visual support. Also, since an effect can have multiple causes, an overview of the values and boolean labels of each time series in a synchronized fashion will help the understanding of the compound relations.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "the Air Quality dataset",
      "data_code": { "temporal": 1, "sequential": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "The testing of an event c being a cause of the effect e is based on the assumption that a true cause always increases the probability of the effect (we can view a preventative, which lowers the probability of e, as raising the probability of :e). Thus, we say c is a potential cause [25] of e if, taking into con_x0002_sideration the relative time delay window, it satisfies P\u00f0e\u00de < p and P\u00f0ejc\u00de _x0005_ p; (2) where P\u00f0ejc\u00de is calculated in the same fashion as Eq. (1). Additionally, if the effect e is defined on a continuous variable ve and we are looking for events that are potentially shifting the distribution of ve (as opposed to a value of ve falling into a specific range), the expected value of ve can be used instead for better sensitivity to change [23]. As such, c is considered a potential cause of e when E\u00bdve_x0004_ \u00bc E\u00bdvejc_x0004_ : (3) Here, the \u00bc sign can be replaced by either > or < to stipu_x0002_late only positive or negative causes. And the conditional expected value can be calculated as E\u00bdvejc_x0004_ \u00bc XyyQ\u00f0\u00bdve \u00bc y_x0004_ ^ c\u00deQ\u00f0c\u00de ; (4) where y are values in ve\u2019s domain and Q\u00f0x\u00de denotes the number of time points where x holds. To illustrate, Fig. 1 shows short sequences of a continu_x0002_ous variable ve and a causal event c. Averaging all values of ve, we have E\u00bdve_x0004_ =1.5. Then, when considering a time shift of exactly 1 unit, we have E\u00bdvejc_x0004_\u00bc\u00f00:9 \u00fe 3 \u00fe 2:3 \u00fe 1:3\u00de=4 \u00bc 1:875 (these are the values of ve exactly 1 unit after c is T). Since E\u00bdvejc_x0004_ > E\u00bde_x0004_ , according to Eq. (3), c increases the expected value of ve and thus is a potential cause of it. However, if we try to find the positive cause by instead bounding ve to a specific range, or to a specific value such as the mean of ve the event e would be defined as [ve > E\u00bdve_x0004_ ]. Then we would have P\u00f0e\u00de \u00bc 0:5 (e occurs 4 times out of 8 time points) and P\u00f0ejc\u00de \u00bc 0:5 (2 out of 4), where c would not be considered a potential cause because it is not raising the probability of e. This shows the reduced sensitivity to change that comes with trying to be more specific. We can generalize this framework to a set of causes X of an effect e. We measure the influence of X towards e by cal_x0002_culating the probability change of e as P\u00f0ejX\u00de _x0007_ P\u00f0e\u00de or the change of expected value of ve as E\u00bdvejX_x0004_ _x0007_ E\u00bdve_x0004_ , depend_x0002_ing on the definition of e. Note that while the conditional probability is bounded within \u00bd0; 1_x0004_ , the expected value could be any amount, and either positive or negative. As mentioned, a causal relation is only potential; it may not be direct or it may be spurious, even if Eqs. (2) or (3) holds. This can be due to two possible situations: (1) c and e are actually independent but are commonly caused by another event x (the confounder) with c being caused earlier than e (Fig. 2a), or (2) c causes e indirectly via x (mediation, Fig. 2b). In either condition, we may observe that Eqs. (2) or (3) holds and erroneously mark c as directly causing e. One way to eliminate such error is to compare the distribution of e when c and x both occur, i.e., P\u00f0ejc ^ x\u00de, to that when only x is present, i.e., P\u00f0ej:c ^ x\u00de. Then the two will be found equal (or almost equal) if c is a spurious cause of e. Note that this requires the time window \u00bdr; s_x0004_ to be sufficiently wide such that both x and c could have caused e [24]. This idea can be generalized to multivariate time series. When considering multiple time series in a dataset, for a given effect, we usually can recognize a number of potential causes. To identify the real causes that can better explain the effect, Eells [8] proposed the average significance of a potential cause c, among all potential causes X toward the effect e, calculated as \"avg\u00f0c; e\u00de \u00bc X x2X=c P\u00f0ejc ^ x\u00de _x0007_ P\u00f0ej:c ^ x\u00de jX=cj : (5) Here X=c is the set of potential causes excluding c and jX=cj is the number of events in it. We need at least two potential causes to make the computation meaningful and all calcula_x0002_tions are associated with a preset time window. Then, by setting a certain threshold \", c is called an \"-significant cause of e if j\"avg\u00f0c; e\u00dej _x0005_ \". Further, if e stands for the increase or decrease of a continuous variable ve over the time window, the conditional probability in Eq. (5) can be replaced by the conditional expected value such that \"avg\u00f0c; e\u00de \u00bc X x2X= E\u00bdvejc ^ x_x0004_ _x0007_ E\u00bdvej:c ^ x_x0004_ jX=cj : (6) Although the \" threshold is decisive in testing if a cause is significant, its value can be difficult to determine auto_x0002_matically in practice. In presence of a large number of (say, thousands of) potential causes where significant causes are rare, all such \"avg values usually follow a Gaussian distribu_x0002_tion [23]. As a result, the problem can be solved by testing the significance of a null hypothesis where p values rejecting the null hypothesis deviate from the mean [9]. However, we find that this theoretical method cannot really be applied in most of our applications since we rarely encounter such a large number of time-series and causal events, especially when we just wish to explore the impact of some specific causes on the target. In such cases, the \" threshold can only be assigned empirically and interactively by the analyst. This requirement for user assistance, together with other analytical tasks that will be discussed later, motivated the visual analytics system that is at the heart of our work. Since a potential cause elevates the probability (Eq. (2)) oralters the expected value (Eq. (3)) of the effect, the processof searching for a cause c is the same as deciding an appro\\x02priate numerical constraint on the cause variable vc, onwhich c is made, so that Eqs. (2) or (3) can be satisfied. Thisis relatively easy and straightforward when vc has discretevalues, where we can simply scan through vc\u2019s domain andmake c take all the values satisfying the condition.The search becomes more complex when vc\u2019s domain iscontinuous. One solution could be to discretize vc and thenapply the same scanning process, but determining a gooddiscretization strategy is difficult. Our approach is toinstead only look at vc at time points t, noted as vc\u00f0t\u00de, wheree holds after the specified time delay (vc\u00f0t\u00de\u02c6 \\x05 r;\\x03 se). Record\\x02ing all such vc\u00f0t\u00de as Tc, we then discretize vc adaptively byclustering values in Tc. The idea is to consider values that vcfrequently takes and lead to e as possibly causing e.The clustering process follows Algorithm 3.3, whichtakes the same approach as the incremental clustering forhigh-dimensional data [41] but is applied in 1-D. The original algorithm has shown great performance in learning 'informative value segments. The adapted version here iteratively scans values in Tc until all clusters converge or themaximum number of iterations is reached. In each iteration,a value is assigned to a cluster center if the distance betweenthem is smaller than some threshold u. A new cluster isadded when a point is too far away from all clusters. Thethreshold u controls the size of the clusters, which decideshow vc will be discretized later. At last, we transform vc byconsidering the value range each cluster covers as a level,and test if it fulfills Eqs. (2) or (3). If multiple levels arereturned, we try to merge them if they overlap and take theone that best elevates e as the most possible cause. 'The algorithm can be easily parallelized [41] if we modify the incremental process such that it searches clusters inbatches instead of inspecting them one by one, enablingscalability. Also, the trade-off of taking different u valuesis that a larger u tends to produce a looser constraint (alarger value range of vc) in c, often resulting in a smallerP\u00f0ejc\u00de or an E\u00bdvejccloser to E\u00bdve\u2013 a smaller u does theopposite. This is similar to the problem of under-/over-fitting. In our experiments, we found when u equals 0.15 ofvc\u2019s value range 5 iterations were usually sufficient toreach a plausible result \u2013 one that would be close to manual adjustment.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "The time sequence view presents an enhanced rendering for examining the time sequences directly (T1). The sequences are stacked, with the specified effect on the top. Each sequence can be rendered in two modes \u2013 label mode and value mode, which can be switched by clicking the check box on the left (unchecked is \u2019labels\u2019 and checked is \u2019values\u2019). The former visualizes the Boolean labels of an event at each time as a strip of colored bars (green for true, red for false). The value mode displays an area chart if the variable is continuous or a strip of bars colored by the level the discrete variable takes at each time with the legend on the right. The same colors are also used in the box chart in the causal inference panel. In both modes, missing values are left blank and long sequences are scrollable.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Sharing",
        "solution_compoent": "Bar+Area",
        "axial_code": ["Overlay-Coordinatesystemrelated-Sharing"],
        "componenet_code": ["Bar", "Area"]
      },
      {
        "solution_text": "The time sequence view presents an enhanced rendering for examining the time sequences directly (T1). The sequences are stacked, with the specified effect on the top. Each sequence can be rendered in two modes \u2013 label mode and value mode, which can be switched by clicking the check box on the left (unchecked is \u2019labels\u2019 and checked is \u2019values\u2019). The former visualizes the Boolean labels of an event at each time as a strip of colored bars (green for true, red for false). The value mode displays an area chart if the variable is continuous or a strip of bars colored by the level the discrete variable takes at each time with the legend on the right. The same colors are also used in the box chart in the causal inference panel. In both modes, missing values are left blank and long sequences are scrollable.",
        "solution_category": "interaction",
        "solution_axial": "Reconfigure",
        "solution_compoent": "",
        "axial_code": ["Reconfigure"],
        "componenet_code": ["reconfigure"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 98,
    "paper_title": "DOMINO: Visual Causal Reasoning With Time-Dependent Phenomena",
    "pub_year": 2023,
    "domain": "Causality analysis",
    "requirement": {
      "requirement_text": "T2: Identifying significant causes under a specified time delay is the most common task when investigating causality within time series. Examples are found in temporal causal analysis of the stock market, biomedical data, social activities, and terrorist activities. While the significance threshold determining the truthfulness of causes may often need to be decided empirically, a visual system should effectively externalize the identified causes and their levels of significance under the specified time delay, supporting the analyst\u2019s decision-making process.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "the Air Quality dataset",
      "data_code": { "temporal": 1, "sequential": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "The testing of an event c being a cause of the effect e is based on the assumption that a true cause always increases the probability of the effect (we can view a preventative, which lowers the probability of e, as raising the probability of :e). Thus, we say c is a potential cause [25] of e if, taking into con_x0002_sideration the relative time delay window, it satisfies P\u00f0e\u00de < p and P\u00f0ejc\u00de _x0005_ p; (2) where P\u00f0ejc\u00de is calculated in the same fashion as Eq. (1). Additionally, if the effect e is defined on a continuous variable ve and we are looking for events that are potentially shifting the distribution of ve (as opposed to a value of ve falling into a specific range), the expected value of ve can be used instead for better sensitivity to change [23]. As such, c is considered a potential cause of e when E\u00bdve_x0004_ \u00bc E\u00bdvejc_x0004_ : (3) Here, the \u00bc sign can be replaced by either > or < to stipu_x0002_late only positive or negative causes. And the conditional expected value can be calculated as E\u00bdvejc_x0004_ \u00bc XyyQ\u00f0\u00bdve \u00bc y_x0004_ ^ c\u00deQ\u00f0c\u00de ; (4) where y are values in ve\u2019s domain and Q\u00f0x\u00de denotes the number of time points where x holds. To illustrate, Fig. 1 shows short sequences of a continu_x0002_ous variable ve and a causal event c. Averaging all values of ve, we have E\u00bdve_x0004_ =1.5. Then, when considering a time shift of exactly 1 unit, we have E\u00bdvejc_x0004_\u00bc\u00f00:9 \u00fe 3 \u00fe 2:3 \u00fe 1:3\u00de=4 \u00bc 1:875 (these are the values of ve exactly 1 unit after c is T). Since E\u00bdvejc_x0004_ > E\u00bde_x0004_ , according to Eq. (3), c increases the expected value of ve and thus is a potential cause of it. However, if we try to find the positive cause by instead bounding ve to a specific range, or to a specific value such as the mean of ve the event e would be defined as [ve > E\u00bdve_x0004_ ]. Then we would have P\u00f0e\u00de \u00bc 0:5 (e occurs 4 times out of 8 time points) and P\u00f0ejc\u00de \u00bc 0:5 (2 out of 4), where c would not be considered a potential cause because it is not raising the probability of e. This shows the reduced sensitivity to change that comes with trying to be more specific. We can generalize this framework to a set of causes X of an effect e. We measure the influence of X towards e by cal_x0002_culating the probability change of e as P\u00f0ejX\u00de _x0007_ P\u00f0e\u00de or the change of expected value of ve as E\u00bdvejX_x0004_ _x0007_ E\u00bdve_x0004_ , depend_x0002_ing on the definition of e. Note that while the conditional probability is bounded within \u00bd0; 1_x0004_ , the expected value could be any amount, and either positive or negative. As mentioned, a causal relation is only potential; it may not be direct or it may be spurious, even if Eqs. (2) or (3) holds. This can be due to two possible situations: (1) c and e are actually independent but are commonly caused by another event x (the confounder) with c being caused earlier than e (Fig. 2a), or (2) c causes e indirectly via x (mediation, Fig. 2b). In either condition, we may observe that Eqs. (2) or (3) holds and erroneously mark c as directly causing e. One way to eliminate such error is to compare the distribution of e when c and x both occur, i.e., P\u00f0ejc ^ x\u00de, to that when only x is present, i.e., P\u00f0ej:c ^ x\u00de. Then the two will be found equal (or almost equal) if c is a spurious cause of e. Note that this requires the time window \u00bdr; s_x0004_ to be sufficiently wide such that both x and c could have caused e [24]. This idea can be generalized to multivariate time series. When considering multiple time series in a dataset, for a given effect, we usually can recognize a number of potential causes. To identify the real causes that can better explain the effect, Eells [8] proposed the average significance of a potential cause c, among all potential causes X toward the effect e, calculated as \"avg\u00f0c; e\u00de \u00bc X x2X=c P\u00f0ejc ^ x\u00de _x0007_ P\u00f0ej:c ^ x\u00de jX=cj : (5) Here X=c is the set of potential causes excluding c and jX=cj is the number of events in it. We need at least two potential causes to make the computation meaningful and all calcula_x0002_tions are associated with a preset time window. Then, by setting a certain threshold \", c is called an \"-significant cause of e if j\"avg\u00f0c; e\u00dej _x0005_ \". Further, if e stands for the increase or decrease of a continuous variable ve over the time window, the conditional probability in Eq. (5) can be replaced by the conditional expected value such that \"avg\u00f0c; e\u00de \u00bc X x2X= E\u00bdvejc ^ x_x0004_ _x0007_ E\u00bdvej:c ^ x_x0004_ jX=cj : (6) Although the \" threshold is decisive in testing if a cause is significant, its value can be difficult to determine auto_x0002_matically in practice. In presence of a large number of (say, thousands of) potential causes where significant causes are rare, all such \"avg values usually follow a Gaussian distribu_x0002_tion [23]. As a result, the problem can be solved by testing the significance of a null hypothesis where p values rejecting the null hypothesis deviate from the mean [9]. However, we find that this theoretical method cannot really be applied in most of our applications since we rarely encounter such a large number of time-series and causal events, especially when we just wish to explore the impact of some specific causes on the target. In such cases, the \" threshold can only be assigned empirically and interactively by the analyst. This requirement for user assistance, together with other analytical tasks that will be discussed later, motivated the visual analytics system that is at the heart of our work. Since a potential cause elevates the probability (Eq. (2)) oralters the expected value (Eq. (3)) of the effect, the processof searching for a cause c is the same as deciding an appro\\x02priate numerical constraint on the cause variable vc, onwhich c is made, so that Eqs. (2) or (3) can be satisfied. Thisis relatively easy and straightforward when vc has discretevalues, where we can simply scan through vc\u2019s domain andmake c take all the values satisfying the condition.The search becomes more complex when vc\u2019s domain iscontinuous. One solution could be to discretize vc and thenapply the same scanning process, but determining a gooddiscretization strategy is difficult. Our approach is toinstead only look at vc at time points t, noted as vc\u00f0t\u00de, wheree holds after the specified time delay (vc\u00f0t\u00de\u02c6 \\x05 r;\\x03 se). Record\\x02ing all such vc\u00f0t\u00de as Tc, we then discretize vc adaptively byclustering values in Tc. The idea is to consider values that vcfrequently takes and lead to e as possibly causing e.The clustering process follows Algorithm 3.3, whichtakes the same approach as the incremental clustering forhigh-dimensional data [41] but is applied in 1-D. The original algorithm has shown great performance in learning 'informative value segments. The adapted version here iteratively scans values in Tc until all clusters converge or themaximum number of iterations is reached. In each iteration,a value is assigned to a cluster center if the distance betweenthem is smaller than some threshold u. A new cluster isadded when a point is too far away from all clusters. Thethreshold u controls the size of the clusters, which decideshow vc will be discretized later. At last, we transform vc byconsidering the value range each cluster covers as a level,and test if it fulfills Eqs. (2) or (3). If multiple levels arereturned, we try to merge them if they overlap and take theone that best elevates e as the most possible cause. 'The algorithm can be easily parallelized [41] if we modify the incremental process such that it searches clusters inbatches instead of inspecting them one by one, enablingscalability. Also, the trade-off of taking different u valuesis that a larger u tends to produce a looser constraint (alarger value range of vc) in c, often resulting in a smallerP\u00f0ejc\u00de or an E\u00bdvejccloser to E\u00bdve\u2013 a smaller u does theopposite. This is similar to the problem of under-/over-fitting. In our experiments, we found when u equals 0.15 ofvc\u2019s value range 5 iterations were usually sufficient toreach a plausible result \u2013 one that would be close to manual adjustment.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Each time the Add button in panel A is pressed, the cause c specified there is added to the set of potential causes X. DOMINO then automatically tests its significance with regards to e and time delay (T2,T4), and positions it as a box in the box chart (bottom left). The boxes are identical in size and ordered by significance; the value is signified by the two small handles attached at the left/right box center. Users can move a vertical slider up/down to adjust the \"-threshold. If there are too many boxes, a horizontal scrollbar will appear for scalability.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Box",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Box"]
      },
      {
        "solution_text": "Each time the Add button in panel A is pressed, the cause c specified there is added to the set of potential causes X. DOMINO then automatically tests its significance with regards to e and time delay (T2,T4), and positions it as a box in the box chart (bottom left). The boxes are identical in size and ordered by significance; the value is signified by the two small handles attached at the left/right box center. Users can move a vertical slider up/down to adjust the \"-threshold. If there are too many boxes, a horizontal scrollbar will appear for scalability.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 99,
    "paper_title": "DOMINO: Visual Causal Reasoning With Time-Dependent Phenomena",
    "pub_year": 2023,
    "domain": "Causality analysis",
    "requirement": {
      "requirement_text": "T3: Analyzing the change of causal influences over time is another important task as the significance and influence of a cause toward the effect could differ over settings of time delay. Interactive operations can reveal the time span of a causal relation, as well as a proper window of time delay for identifying other causes. The latter, however, has been mostly assigned empirically with a limited set of values in the related work so far. When this knowledge is incomplete, a visual system should support analysts in these tasks by visualizing the causal influences toward the effect associated with all possible time delays under consideration.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "the Air Quality dataset",
      "data_code": { "clusters_and_sets_and_lists": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "The testing of an event c being a cause of the effect e is based on the assumption that a true cause always increases the probability of the effect (we can view a preventative, which lowers the probability of e, as raising the probability of :e). Thus, we say c is a potential cause [25] of e if, taking into con_x0002_sideration the relative time delay window, it satisfies P\u00f0e\u00de < p and P\u00f0ejc\u00de _x0005_ p; (2) where P\u00f0ejc\u00de is calculated in the same fashion as Eq. (1). Additionally, if the effect e is defined on a continuous variable ve and we are looking for events that are potentially shifting the distribution of ve (as opposed to a value of ve falling into a specific range), the expected value of ve can be used instead for better sensitivity to change [23]. As such, c is considered a potential cause of e when E\u00bdve_x0004_ \u00bc E\u00bdvejc_x0004_ : (3) Here, the \u00bc sign can be replaced by either > or < to stipu_x0002_late only positive or negative causes. And the conditional expected value can be calculated as E\u00bdvejc_x0004_ \u00bc XyyQ\u00f0\u00bdve \u00bc y_x0004_ ^ c\u00deQ\u00f0c\u00de ; (4) where y are values in ve\u2019s domain and Q\u00f0x\u00de denotes the number of time points where x holds. To illustrate, Fig. 1 shows short sequences of a continu_x0002_ous variable ve and a causal event c. Averaging all values of ve, we have E\u00bdve_x0004_ =1.5. Then, when considering a time shift of exactly 1 unit, we have E\u00bdvejc_x0004_\u00bc\u00f00:9 \u00fe 3 \u00fe 2:3 \u00fe 1:3\u00de=4 \u00bc 1:875 (these are the values of ve exactly 1 unit after c is T). Since E\u00bdvejc_x0004_ > E\u00bde_x0004_ , according to Eq. (3), c increases the expected value of ve and thus is a potential cause of it. However, if we try to find the positive cause by instead bounding ve to a specific range, or to a specific value such as the mean of ve the event e would be defined as [ve > E\u00bdve_x0004_ ]. Then we would have P\u00f0e\u00de \u00bc 0:5 (e occurs 4 times out of 8 time points) and P\u00f0ejc\u00de \u00bc 0:5 (2 out of 4), where c would not be considered a potential cause because it is not raising the probability of e. This shows the reduced sensitivity to change that comes with trying to be more specific. We can generalize this framework to a set of causes X of an effect e. We measure the influence of X towards e by cal_x0002_culating the probability change of e as P\u00f0ejX\u00de _x0007_ P\u00f0e\u00de or the change of expected value of ve as E\u00bdvejX_x0004_ _x0007_ E\u00bdve_x0004_ , depend_x0002_ing on the definition of e. Note that while the conditional probability is bounded within \u00bd0; 1_x0004_ , the expected value could be any amount, and either positive or negative. As mentioned, a causal relation is only potential; it may not be direct or it may be spurious, even if Eqs. (2) or (3) holds. This can be due to two possible situations: (1) c and e are actually independent but are commonly caused by another event x (the confounder) with c being caused earlier than e (Fig. 2a), or (2) c causes e indirectly via x (mediation, Fig. 2b). In either condition, we may observe that Eqs. (2) or (3) holds and erroneously mark c as directly causing e. One way to eliminate such error is to compare the distribution of e when c and x both occur, i.e., P\u00f0ejc ^ x\u00de, to that when only x is present, i.e., P\u00f0ej:c ^ x\u00de. Then the two will be found equal (or almost equal) if c is a spurious cause of e. Note that this requires the time window \u00bdr; s_x0004_ to be sufficiently wide such that both x and c could have caused e [24]. This idea can be generalized to multivariate time series. When considering multiple time series in a dataset, for a given effect, we usually can recognize a number of potential causes. To identify the real causes that can better explain the effect, Eells [8] proposed the average significance of a potential cause c, among all potential causes X toward the effect e, calculated as \"avg\u00f0c; e\u00de \u00bc X x2X=c P\u00f0ejc ^ x\u00de _x0007_ P\u00f0ej:c ^ x\u00de jX=cj : (5) Here X=c is the set of potential causes excluding c and jX=cj is the number of events in it. We need at least two potential causes to make the computation meaningful and all calcula_x0002_tions are associated with a preset time window. Then, by setting a certain threshold \", c is called an \"-significant cause of e if j\"avg\u00f0c; e\u00dej _x0005_ \". Further, if e stands for the increase or decrease of a continuous variable ve over the time window, the conditional probability in Eq. (5) can be replaced by the conditional expected value such that \"avg\u00f0c; e\u00de \u00bc X x2X= E\u00bdvejc ^ x_x0004_ _x0007_ E\u00bdvej:c ^ x_x0004_ jX=cj : (6) Although the \" threshold is decisive in testing if a cause is significant, its value can be difficult to determine auto_x0002_matically in practice. In presence of a large number of (say, thousands of) potential causes where significant causes are rare, all such \"avg values usually follow a Gaussian distribu_x0002_tion [23]. As a result, the problem can be solved by testing the significance of a null hypothesis where p values rejecting the null hypothesis deviate from the mean [9]. However, we find that this theoretical method cannot really be applied in most of our applications since we rarely encounter such a large number of time-series and causal events, especially when we just wish to explore the impact of some specific causes on the target. In such cases, the \" threshold can only be assigned empirically and interactively by the analyst. This requirement for user assistance, together with other analytical tasks that will be discussed later, motivated the visual analytics system that is at the heart of our work. Since a potential cause elevates the probability (Eq. (2)) oralters the expected value (Eq. (3)) of the effect, the processof searching for a cause c is the same as deciding an appro\\x02priate numerical constraint on the cause variable vc, onwhich c is made, so that Eqs. (2) or (3) can be satisfied. Thisis relatively easy and straightforward when vc has discretevalues, where we can simply scan through vc\u2019s domain andmake c take all the values satisfying the condition.The search becomes more complex when vc\u2019s domain iscontinuous. One solution could be to discretize vc and thenapply the same scanning process, but determining a gooddiscretization strategy is difficult. Our approach is toinstead only look at vc at time points t, noted as vc\u00f0t\u00de, wheree holds after the specified time delay (vc\u00f0t\u00de\u02c6 \\x05 r;\\x03 se). Record\\x02ing all such vc\u00f0t\u00de as Tc, we then discretize vc adaptively byclustering values in Tc. The idea is to consider values that vcfrequently takes and lead to e as possibly causing e.The clustering process follows Algorithm 3.3, whichtakes the same approach as the incremental clustering forhigh-dimensional data [41] but is applied in 1-D. The original algorithm has shown great performance in learning 'informative value segments. The adapted version here iteratively scans values in Tc until all clusters converge or themaximum number of iterations is reached. In each iteration,a value is assigned to a cluster center if the distance betweenthem is smaller than some threshold u. A new cluster isadded when a point is too far away from all clusters. Thethreshold u controls the size of the clusters, which decideshow vc will be discretized later. At last, we transform vc byconsidering the value range each cluster covers as a level,and test if it fulfills Eqs. (2) or (3). If multiple levels arereturned, we try to merge them if they overlap and take theone that best elevates e as the most possible cause. 'The algorithm can be easily parallelized [41] if we modify the incremental process such that it searches clusters inbatches instead of inspecting them one by one, enablingscalability. Also, the trade-off of taking different u valuesis that a larger u tends to produce a looser constraint (alarger value range of vc) in c, often resulting in a smallerP\u00f0ejc\u00de or an E\u00bdvejccloser to E\u00bdve\u2013 a smaller u does theopposite. This is similar to the problem of under-/over-fitting. In our experiments, we found when u equals 0.15 ofvc\u2019s value range 5 iterations were usually sufficient toreach a plausible result \u2013 one that would be close to manual adjustment.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "The area chart above the box chart visualizes the combined effect of all significant causes for a range of time delays (T3), starting at 0 all the way to the maximum delay selected in the value box on the left. The color encodes the type of effect \u2013 red for Decrease and green for Increase and Range. The slider on the chart\u2019s time-axis sets the time delay used in the significance tests. The default is a narrow window that just includes one sample point of the time series. A wider window can be chosen by checking the Select Range box on the left. This will expose two handles by which the window width [r, s] can be specified.",
        "solution_category": "visualization",
        "solution_axial": "Non-composite",
        "solution_compoent": "Area+Pie",
        "axial_code": ["Non-composite"],
        "componenet_code": ["Pie", "Area"]
      },
      {
        "solution_text": "The area chart above the box chart visualizes the combined effect of all significant causes for a range of time delays (T3), starting at 0 all the way to the maximum delay selected in the value box on the left. The color encodes the type of effect \u2013 red for Decrease and green for Increase and Range. The slider on the chart\u2019s time-axis sets the time delay used in the significance tests. The default is a narrow window that just includes one sample point of the time series. A wider window can be chosen by checking the Select Range box on the left. This will expose two handles by which the window width [r, s] can be specified.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 100,
    "paper_title": "DOMINO: Visual Causal Reasoning With Time-Dependent Phenomena",
    "pub_year": 2023,
    "domain": "Causality analysis",
    "requirement": {
      "requirement_text": "T4: Interactive analysis. As mentioned, causality analysis is often associated with a number of meta parameters to be determined by analysts empirically, e.g., the numerical constraints in the causal propositions and the threshold in the significance tests. Determining these parameters is an essential task in causality analysis and often requires interaction, as illustrated in other projects. To support interactive analysis, an effective system should provide visual feedback along with each of the user\u2019s parameter updates. Users should also be able to save the discoveries in an overview for later re-examination.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "the Air Quality dataset",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "The testing of an event c being a cause of the effect e is based on the assumption that a true cause always increases the probability of the effect (we can view a preventative, which lowers the probability of e, as raising the probability of :e). Thus, we say c is a potential cause [25] of e if, taking into con_x0002_sideration the relative time delay window, it satisfies P\u00f0e\u00de < p and P\u00f0ejc\u00de _x0005_ p; (2) where P\u00f0ejc\u00de is calculated in the same fashion as Eq. (1). Additionally, if the effect e is defined on a continuous variable ve and we are looking for events that are potentially shifting the distribution of ve (as opposed to a value of ve falling into a specific range), the expected value of ve can be used instead for better sensitivity to change [23]. As such, c is considered a potential cause of e when E\u00bdve_x0004_ \u00bc E\u00bdvejc_x0004_ : (3) Here, the \u00bc sign can be replaced by either > or < to stipu_x0002_late only positive or negative causes. And the conditional expected value can be calculated as E\u00bdvejc_x0004_ \u00bc XyyQ\u00f0\u00bdve \u00bc y_x0004_ ^ c\u00deQ\u00f0c\u00de ; (4) where y are values in ve\u2019s domain and Q\u00f0x\u00de denotes the number of time points where x holds. To illustrate, Fig. 1 shows short sequences of a continu_x0002_ous variable ve and a causal event c. Averaging all values of ve, we have E\u00bdve_x0004_ =1.5. Then, when considering a time shift of exactly 1 unit, we have E\u00bdvejc_x0004_\u00bc\u00f00:9 \u00fe 3 \u00fe 2:3 \u00fe 1:3\u00de=4 \u00bc 1:875 (these are the values of ve exactly 1 unit after c is T). Since E\u00bdvejc_x0004_ > E\u00bde_x0004_ , according to Eq. (3), c increases the expected value of ve and thus is a potential cause of it. However, if we try to find the positive cause by instead bounding ve to a specific range, or to a specific value such as the mean of ve the event e would be defined as [ve > E\u00bdve_x0004_ ]. Then we would have P\u00f0e\u00de \u00bc 0:5 (e occurs 4 times out of 8 time points) and P\u00f0ejc\u00de \u00bc 0:5 (2 out of 4), where c would not be considered a potential cause because it is not raising the probability of e. This shows the reduced sensitivity to change that comes with trying to be more specific. We can generalize this framework to a set of causes X of an effect e. We measure the influence of X towards e by cal_x0002_culating the probability change of e as P\u00f0ejX\u00de _x0007_ P\u00f0e\u00de or the change of expected value of ve as E\u00bdvejX_x0004_ _x0007_ E\u00bdve_x0004_ , depend_x0002_ing on the definition of e. Note that while the conditional probability is bounded within \u00bd0; 1_x0004_ , the expected value could be any amount, and either positive or negative. As mentioned, a causal relation is only potential; it may not be direct or it may be spurious, even if Eqs. (2) or (3) holds. This can be due to two possible situations: (1) c and e are actually independent but are commonly caused by another event x (the confounder) with c being caused earlier than e (Fig. 2a), or (2) c causes e indirectly via x (mediation, Fig. 2b). In either condition, we may observe that Eqs. (2) or (3) holds and erroneously mark c as directly causing e. One way to eliminate such error is to compare the distribution of e when c and x both occur, i.e., P\u00f0ejc ^ x\u00de, to that when only x is present, i.e., P\u00f0ej:c ^ x\u00de. Then the two will be found equal (or almost equal) if c is a spurious cause of e. Note that this requires the time window \u00bdr; s_x0004_ to be sufficiently wide such that both x and c could have caused e [24]. This idea can be generalized to multivariate time series. When considering multiple time series in a dataset, for a given effect, we usually can recognize a number of potential causes. To identify the real causes that can better explain the effect, Eells [8] proposed the average significance of a potential cause c, among all potential causes X toward the effect e, calculated as \"avg\u00f0c; e\u00de \u00bc X x2X=c P\u00f0ejc ^ x\u00de _x0007_ P\u00f0ej:c ^ x\u00de jX=cj : (5) Here X=c is the set of potential causes excluding c and jX=cj is the number of events in it. We need at least two potential causes to make the computation meaningful and all calcula_x0002_tions are associated with a preset time window. Then, by setting a certain threshold \", c is called an \"-significant cause of e if j\"avg\u00f0c; e\u00dej _x0005_ \". Further, if e stands for the increase or decrease of a continuous variable ve over the time window, the conditional probability in Eq. (5) can be replaced by the conditional expected value such that \"avg\u00f0c; e\u00de \u00bc X x2X= E\u00bdvejc ^ x_x0004_ _x0007_ E\u00bdvej:c ^ x_x0004_ jX=cj : (6) Although the \" threshold is decisive in testing if a cause is significant, its value can be difficult to determine auto_x0002_matically in practice. In presence of a large number of (say, thousands of) potential causes where significant causes are rare, all such \"avg values usually follow a Gaussian distribu_x0002_tion [23]. As a result, the problem can be solved by testing the significance of a null hypothesis where p values rejecting the null hypothesis deviate from the mean [9]. However, we find that this theoretical method cannot really be applied in most of our applications since we rarely encounter such a large number of time-series and causal events, especially when we just wish to explore the impact of some specific causes on the target. In such cases, the \" threshold can only be assigned empirically and interactively by the analyst. This requirement for user assistance, together with other analytical tasks that will be discussed later, motivated the visual analytics system that is at the heart of our work. Since a potential cause elevates the probability (Eq. (2)) oralters the expected value (Eq. (3)) of the effect, the processof searching for a cause c is the same as deciding an appro\\x02priate numerical constraint on the cause variable vc, onwhich c is made, so that Eqs. (2) or (3) can be satisfied. Thisis relatively easy and straightforward when vc has discretevalues, where we can simply scan through vc\u2019s domain andmake c take all the values satisfying the condition.The search becomes more complex when vc\u2019s domain iscontinuous. One solution could be to discretize vc and thenapply the same scanning process, but determining a gooddiscretization strategy is difficult. Our approach is toinstead only look at vc at time points t, noted as vc\u00f0t\u00de, wheree holds after the specified time delay (vc\u00f0t\u00de\u02c6 \\x05 r;\\x03 se). Record\\x02ing all such vc\u00f0t\u00de as Tc, we then discretize vc adaptively byclustering values in Tc. The idea is to consider values that vcfrequently takes and lead to e as possibly causing e.The clustering process follows Algorithm 3.3, whichtakes the same approach as the incremental clustering forhigh-dimensional data [41] but is applied in 1-D. The original algorithm has shown great performance in learning 'informative value segments. The adapted version here iteratively scans values in Tc until all clusters converge or themaximum number of iterations is reached. In each iteration,a value is assigned to a cluster center if the distance betweenthem is smaller than some threshold u. A new cluster isadded when a point is too far away from all clusters. Thethreshold u controls the size of the clusters, which decideshow vc will be discretized later. At last, we transform vc byconsidering the value range each cluster covers as a level,and test if it fulfills Eqs. (2) or (3). If multiple levels arereturned, we try to merge them if they overlap and take theone that best elevates e as the most possible cause. 'The algorithm can be easily parallelized [41] if we modify the incremental process such that it searches clusters inbatches instead of inspecting them one by one, enablingscalability. Also, the trade-off of taking different u valuesis that a larger u tends to produce a looser constraint (alarger value range of vc) in c, often resulting in a smallerP\u00f0ejc\u00de or an E\u00bdvejccloser to E\u00bdve\u2013 a smaller u does theopposite. This is similar to the problem of under-/over-fitting. In our experiments, we found when u equals 0.15 ofvc\u2019s value range 5 iterations were usually sufficient toreach a plausible result \u2013 one that would be close to manual adjustment.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Our design of the conditional distribution view was motivated by the definition of a potential cause mentioned in Section 3. With the conditional distributions displayed here, analysts can directly observe a time-lagged phenomenon and hence make causal hypotheses (T1,T4). Upon selecting and loading a tabular data file of a multi_x0002_variate time series (top menu) the analyst selects one of the variables as the effect variable e and specifies the effect of interest, such as Value Increase, Value Decrease, or Value Range(second menu row). Upon selecting e the system displays a histogram of its values ve over the entire time series duration(blue bars). This histogram can be brushed when the selected effect is Value Range (the Brushed box, top right, must be checked). Next, the analyst selects a cause variable in the menu on the bottom left, and a frequency histogram of the variable\u2019s levels (if discrete) or values (if continuous) will be shown above the menu. The analyst then indicates the potential cause c by selecting the corresponding level bar(s) and specifying a time delay via the slider on the right. Following, the system displays a histogram of e when c\u2019s event conditions are met (green bars). This histogram is necessarily lower in magnitude since the conditions are only met for some ve. The vertical bars for the histograms indicate their respective means, E?veand E?vejc_x0004_ . The wider apart these means the more pronounced c\u2019s potential effect.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Bar",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Bar"]
      },
      {
        "solution_text": "Our design of the conditional distribution view was motivated by the definition of a potential cause mentioned in Section 3. With the conditional distributions displayed here, analysts can directly observe a time-lagged phenomenon and hence make causal hypotheses (T1,T4). Upon selecting and loading a tabular data file of a multi_x0002_variate time series (top menu) the analyst selects one of the variables as the effect variable e and specifies the effect of interest, such as Value Increase, Value Decrease, or Value Range(second menu row). Upon selecting e the system displays a histogram of its values ve over the entire time series duration(blue bars). This histogram can be brushed when the selected effect is Value Range (the Brushed box, top right, must be checked). Next, the analyst selects a cause variable in the menu on the bottom left, and a frequency histogram of the variable\u2019s levels (if discrete) or values (if continuous) will be shown above the menu. The analyst then indicates the potential cause c by selecting the corresponding level bar(s) and specifying a time delay via the slider on the right. Following, the system displays a histogram of e when c\u2019s event conditions are met (green bars). This histogram is necessarily lower in magnitude since the conditions are only met for some ve. The vertical bars for the histograms indicate their respective means, E?veand E?vejc_x0004_ . The wider apart these means the more pronounced c\u2019s potential effect.",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Filtering+Abstract/Elaborate",
        "solution_compoent": "",
        "axial_code": ["Abstract/Elaborate", "Selecting", "Filtering"],
        "componenet_code": ["abstract/elaborate", "selecting", "filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 101,
    "paper_title": "DOMINO: Visual Causal Reasoning With Time-Dependent Phenomena",
    "pub_year": 2023,
    "domain": "Causality analysis",
    "requirement": {
      "requirement_text": "T4: Interactive analysis. As mentioned, causality analysis is often associated with a number of meta parameters to be determined by analysts empirically, e.g., the numerical constraints in the causal propositions and the threshold in the significance tests. Determining these parameters is an essential task in causality analysis and often requires interaction, as illustrated in other projects. To support interactive analysis, an effective system should provide visual feedback along with each of the user\u2019s parameter updates. Users should also be able to save the discoveries in an overview for later re-examination.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "the Air Quality dataset",
      "data_code": { "temporal": 1, "sequential": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "The testing of an event c being a cause of the effect e is based on the assumption that a true cause always increases the probability of the effect (we can view a preventative, which lowers the probability of e, as raising the probability of :e). Thus, we say c is a potential cause [25] of e if, taking into con_x0002_sideration the relative time delay window, it satisfies P\u00f0e\u00de < p and P\u00f0ejc\u00de _x0005_ p; (2) where P\u00f0ejc\u00de is calculated in the same fashion as Eq. (1). Additionally, if the effect e is defined on a continuous variable ve and we are looking for events that are potentially shifting the distribution of ve (as opposed to a value of ve falling into a specific range), the expected value of ve can be used instead for better sensitivity to change [23]. As such, c is considered a potential cause of e when E\u00bdve_x0004_ \u00bc E\u00bdvejc_x0004_ : (3) Here, the \u00bc sign can be replaced by either > or < to stipu_x0002_late only positive or negative causes. And the conditional expected value can be calculated as E\u00bdvejc_x0004_ \u00bc XyyQ\u00f0\u00bdve \u00bc y_x0004_ ^ c\u00deQ\u00f0c\u00de ; (4) where y are values in ve\u2019s domain and Q\u00f0x\u00de denotes the number of time points where x holds. To illustrate, Fig. 1 shows short sequences of a continu_x0002_ous variable ve and a causal event c. Averaging all values of ve, we have E\u00bdve_x0004_ =1.5. Then, when considering a time shift of exactly 1 unit, we have E\u00bdvejc_x0004_\u00bc\u00f00:9 \u00fe 3 \u00fe 2:3 \u00fe 1:3\u00de=4 \u00bc 1:875 (these are the values of ve exactly 1 unit after c is T). Since E\u00bdvejc_x0004_ > E\u00bde_x0004_ , according to Eq. (3), c increases the expected value of ve and thus is a potential cause of it. However, if we try to find the positive cause by instead bounding ve to a specific range, or to a specific value such as the mean of ve the event e would be defined as [ve > E\u00bdve_x0004_ ]. Then we would have P\u00f0e\u00de \u00bc 0:5 (e occurs 4 times out of 8 time points) and P\u00f0ejc\u00de \u00bc 0:5 (2 out of 4), where c would not be considered a potential cause because it is not raising the probability of e. This shows the reduced sensitivity to change that comes with trying to be more specific. We can generalize this framework to a set of causes X of an effect e. We measure the influence of X towards e by cal_x0002_culating the probability change of e as P\u00f0ejX\u00de _x0007_ P\u00f0e\u00de or the change of expected value of ve as E\u00bdvejX_x0004_ _x0007_ E\u00bdve_x0004_ , depend_x0002_ing on the definition of e. Note that while the conditional probability is bounded within \u00bd0; 1_x0004_ , the expected value could be any amount, and either positive or negative. As mentioned, a causal relation is only potential; it may not be direct or it may be spurious, even if Eqs. (2) or (3) holds. This can be due to two possible situations: (1) c and e are actually independent but are commonly caused by another event x (the confounder) with c being caused earlier than e (Fig. 2a), or (2) c causes e indirectly via x (mediation, Fig. 2b). In either condition, we may observe that Eqs. (2) or (3) holds and erroneously mark c as directly causing e. One way to eliminate such error is to compare the distribution of e when c and x both occur, i.e., P\u00f0ejc ^ x\u00de, to that when only x is present, i.e., P\u00f0ej:c ^ x\u00de. Then the two will be found equal (or almost equal) if c is a spurious cause of e. Note that this requires the time window \u00bdr; s_x0004_ to be sufficiently wide such that both x and c could have caused e [24]. This idea can be generalized to multivariate time series. When considering multiple time series in a dataset, for a given effect, we usually can recognize a number of potential causes. To identify the real causes that can better explain the effect, Eells [8] proposed the average significance of a potential cause c, among all potential causes X toward the effect e, calculated as \"avg\u00f0c; e\u00de \u00bc X x2X=c P\u00f0ejc ^ x\u00de _x0007_ P\u00f0ej:c ^ x\u00de jX=cj : (5) Here X=c is the set of potential causes excluding c and jX=cj is the number of events in it. We need at least two potential causes to make the computation meaningful and all calcula_x0002_tions are associated with a preset time window. Then, by setting a certain threshold \", c is called an \"-significant cause of e if j\"avg\u00f0c; e\u00dej _x0005_ \". Further, if e stands for the increase or decrease of a continuous variable ve over the time window, the conditional probability in Eq. (5) can be replaced by the conditional expected value such that \"avg\u00f0c; e\u00de \u00bc X x2X= E\u00bdvejc ^ x_x0004_ _x0007_ E\u00bdvej:c ^ x_x0004_ jX=cj : (6) Although the \" threshold is decisive in testing if a cause is significant, its value can be difficult to determine auto_x0002_matically in practice. In presence of a large number of (say, thousands of) potential causes where significant causes are rare, all such \"avg values usually follow a Gaussian distribu_x0002_tion [23]. As a result, the problem can be solved by testing the significance of a null hypothesis where p values rejecting the null hypothesis deviate from the mean [9]. However, we find that this theoretical method cannot really be applied in most of our applications since we rarely encounter such a large number of time-series and causal events, especially when we just wish to explore the impact of some specific causes on the target. In such cases, the \" threshold can only be assigned empirically and interactively by the analyst. This requirement for user assistance, together with other analytical tasks that will be discussed later, motivated the visual analytics system that is at the heart of our work. Since a potential cause elevates the probability (Eq. (2)) oralters the expected value (Eq. (3)) of the effect, the processof searching for a cause c is the same as deciding an appro\\x02priate numerical constraint on the cause variable vc, onwhich c is made, so that Eqs. (2) or (3) can be satisfied. Thisis relatively easy and straightforward when vc has discretevalues, where we can simply scan through vc\u2019s domain andmake c take all the values satisfying the condition.The search becomes more complex when vc\u2019s domain iscontinuous. One solution could be to discretize vc and thenapply the same scanning process, but determining a gooddiscretization strategy is difficult. Our approach is toinstead only look at vc at time points t, noted as vc\u00f0t\u00de, wheree holds after the specified time delay (vc\u00f0t\u00de\u02c6 \\x05 r;\\x03 se). Record\\x02ing all such vc\u00f0t\u00de as Tc, we then discretize vc adaptively byclustering values in Tc. The idea is to consider values that vcfrequently takes and lead to e as possibly causing e.The clustering process follows Algorithm 3.3, whichtakes the same approach as the incremental clustering forhigh-dimensional data [41] but is applied in 1-D. The original algorithm has shown great performance in learning 'informative value segments. The adapted version here iteratively scans values in Tc until all clusters converge or themaximum number of iterations is reached. In each iteration,a value is assigned to a cluster center if the distance betweenthem is smaller than some threshold u. A new cluster isadded when a point is too far away from all clusters. Thethreshold u controls the size of the clusters, which decideshow vc will be discretized later. At last, we transform vc byconsidering the value range each cluster covers as a level,and test if it fulfills Eqs. (2) or (3). If multiple levels arereturned, we try to merge them if they overlap and take theone that best elevates e as the most possible cause. 'The algorithm can be easily parallelized [41] if we modify the incremental process such that it searches clusters inbatches instead of inspecting them one by one, enablingscalability. Also, the trade-off of taking different u valuesis that a larger u tends to produce a looser constraint (alarger value range of vc) in c, often resulting in a smallerP\u00f0ejc\u00de or an E\u00bdvejccloser to E\u00bdve\u2013 a smaller u does theopposite. This is similar to the problem of under-/over-fitting. In our experiments, we found when u equals 0.15 ofvc\u2019s value range 5 iterations were usually sufficient toreach a plausible result \u2013 one that would be close to manual adjustment.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Each time the Add button in panel A is pressed, the cause c specified there is added to the set of potential causes X. DOMINO then automatically tests its significance with regards to e and time delay (T2,T4), and positions it as a box in the box chart (bottom left). The boxes are identical in size and ordered by significance; the value is signified by the two small handles attached at the left/right box center. Users can move a vertical slider up/down to adjust the \"-threshold. If there are too many boxes, a horizontal scrollbar will appear for scalability.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Box",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Box"]
      },
      {
        "solution_text": "Each time the Add button in panel A is pressed, the cause c specified there is added to the set of potential causes X. DOMINO then automatically tests its significance with regards to e and time delay (T2,T4), and positions it as a box in the box chart (bottom left). The boxes are identical in size and ordered by significance; the value is signified by the two small handles attached at the left/right box center. Users can move a vertical slider up/down to adjust the \"-threshold. If there are too many boxes, a horizontal scrollbar will appear for scalability.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 102,
    "paper_title": "DOMINO: Visual Causal Reasoning With Time-Dependent Phenomena",
    "pub_year": 2023,
    "domain": "Causality analysis",
    "requirement": {
      "requirement_text": "T4: Interactive analysis. As mentioned, causality analysis is often associated with a number of meta parameters to be determined by analysts empirically, e.g., the numerical constraints in the causal propositions and the threshold in the significance tests. Determining these parameters is an essential task in causality analysis and often requires interaction, as illustrated in other projects. To support interactive analysis, an effective system should provide visual feedback along with each of the user\u2019s parameter updates. Users should also be able to save the discoveries in an overview for later re-examination.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "the Air Quality dataset",
      "data_code": { "temporal": 1, "tables": 1, "geometry": 1 }
    },
    "solution": [
      {
        "solution_text": "The testing of an event c being a cause of the effect e is based on the assumption that a true cause always increases the probability of the effect (we can view a preventative, which lowers the probability of e, as raising the probability of :e). Thus, we say c is a potential cause [25] of e if, taking into con_x0002_sideration the relative time delay window, it satisfies P\u00f0e\u00de < p and P\u00f0ejc\u00de _x0005_ p; (2) where P\u00f0ejc\u00de is calculated in the same fashion as Eq. (1). Additionally, if the effect e is defined on a continuous variable ve and we are looking for events that are potentially shifting the distribution of ve (as opposed to a value of ve falling into a specific range), the expected value of ve can be used instead for better sensitivity to change [23]. As such, c is considered a potential cause of e when E\u00bdve_x0004_ \u00bc E\u00bdvejc_x0004_ : (3) Here, the \u00bc sign can be replaced by either > or < to stipu_x0002_late only positive or negative causes. And the conditional expected value can be calculated as E\u00bdvejc_x0004_ \u00bc XyyQ\u00f0\u00bdve \u00bc y_x0004_ ^ c\u00deQ\u00f0c\u00de ; (4) where y are values in ve\u2019s domain and Q\u00f0x\u00de denotes the number of time points where x holds. To illustrate, Fig. 1 shows short sequences of a continu_x0002_ous variable ve and a causal event c. Averaging all values of ve, we have E\u00bdve_x0004_ =1.5. Then, when considering a time shift of exactly 1 unit, we have E\u00bdvejc_x0004_\u00bc\u00f00:9 \u00fe 3 \u00fe 2:3 \u00fe 1:3\u00de=4 \u00bc 1:875 (these are the values of ve exactly 1 unit after c is T). Since E\u00bdvejc_x0004_ > E\u00bde_x0004_ , according to Eq. (3), c increases the expected value of ve and thus is a potential cause of it. However, if we try to find the positive cause by instead bounding ve to a specific range, or to a specific value such as the mean of ve the event e would be defined as [ve > E\u00bdve_x0004_ ]. Then we would have P\u00f0e\u00de \u00bc 0:5 (e occurs 4 times out of 8 time points) and P\u00f0ejc\u00de \u00bc 0:5 (2 out of 4), where c would not be considered a potential cause because it is not raising the probability of e. This shows the reduced sensitivity to change that comes with trying to be more specific. We can generalize this framework to a set of causes X of an effect e. We measure the influence of X towards e by cal_x0002_culating the probability change of e as P\u00f0ejX\u00de _x0007_ P\u00f0e\u00de or the change of expected value of ve as E\u00bdvejX_x0004_ _x0007_ E\u00bdve_x0004_ , depend_x0002_ing on the definition of e. Note that while the conditional probability is bounded within \u00bd0; 1_x0004_ , the expected value could be any amount, and either positive or negative. As mentioned, a causal relation is only potential; it may not be direct or it may be spurious, even if Eqs. (2) or (3) holds. This can be due to two possible situations: (1) c and e are actually independent but are commonly caused by another event x (the confounder) with c being caused earlier than e (Fig. 2a), or (2) c causes e indirectly via x (mediation, Fig. 2b). In either condition, we may observe that Eqs. (2) or (3) holds and erroneously mark c as directly causing e. One way to eliminate such error is to compare the distribution of e when c and x both occur, i.e., P\u00f0ejc ^ x\u00de, to that when only x is present, i.e., P\u00f0ej:c ^ x\u00de. Then the two will be found equal (or almost equal) if c is a spurious cause of e. Note that this requires the time window \u00bdr; s_x0004_ to be sufficiently wide such that both x and c could have caused e [24]. This idea can be generalized to multivariate time series. When considering multiple time series in a dataset, for a given effect, we usually can recognize a number of potential causes. To identify the real causes that can better explain the effect, Eells [8] proposed the average significance of a potential cause c, among all potential causes X toward the effect e, calculated as \"avg\u00f0c; e\u00de \u00bc X x2X=c P\u00f0ejc ^ x\u00de _x0007_ P\u00f0ej:c ^ x\u00de jX=cj : (5) Here X=c is the set of potential causes excluding c and jX=cj is the number of events in it. We need at least two potential causes to make the computation meaningful and all calcula_x0002_tions are associated with a preset time window. Then, by setting a certain threshold \", c is called an \"-significant cause of e if j\"avg\u00f0c; e\u00dej _x0005_ \". Further, if e stands for the increase or decrease of a continuous variable ve over the time window, the conditional probability in Eq. (5) can be replaced by the conditional expected value such that \"avg\u00f0c; e\u00de \u00bc X x2X= E\u00bdvejc ^ x_x0004_ _x0007_ E\u00bdvej:c ^ x_x0004_ jX=cj : (6) Although the \" threshold is decisive in testing if a cause is significant, its value can be difficult to determine auto_x0002_matically in practice. In presence of a large number of (say, thousands of) potential causes where significant causes are rare, all such \"avg values usually follow a Gaussian distribu_x0002_tion [23]. As a result, the problem can be solved by testing the significance of a null hypothesis where p values rejecting the null hypothesis deviate from the mean [9]. However, we find that this theoretical method cannot really be applied in most of our applications since we rarely encounter such a large number of time-series and causal events, especially when we just wish to explore the impact of some specific causes on the target. In such cases, the \" threshold can only be assigned empirically and interactively by the analyst. This requirement for user assistance, together with other analytical tasks that will be discussed later, motivated the visual analytics system that is at the heart of our work. Since a potential cause elevates the probability (Eq. (2)) oralters the expected value (Eq. (3)) of the effect, the processof searching for a cause c is the same as deciding an appro\\x02priate numerical constraint on the cause variable vc, onwhich c is made, so that Eqs. (2) or (3) can be satisfied. Thisis relatively easy and straightforward when vc has discretevalues, where we can simply scan through vc\u2019s domain andmake c take all the values satisfying the condition.The search becomes more complex when vc\u2019s domain iscontinuous. One solution could be to discretize vc and thenapply the same scanning process, but determining a gooddiscretization strategy is difficult. Our approach is toinstead only look at vc at time points t, noted as vc\u00f0t\u00de, wheree holds after the specified time delay (vc\u00f0t\u00de\u02c6 \\x05 r;\\x03 se). Record\\x02ing all such vc\u00f0t\u00de as Tc, we then discretize vc adaptively byclustering values in Tc. The idea is to consider values that vcfrequently takes and lead to e as possibly causing e.The clustering process follows Algorithm 3.3, whichtakes the same approach as the incremental clustering forhigh-dimensional data [41] but is applied in 1-D. The original algorithm has shown great performance in learning 'informative value segments. The adapted version here iteratively scans values in Tc until all clusters converge or themaximum number of iterations is reached. In each iteration,a value is assigned to a cluster center if the distance betweenthem is smaller than some threshold u. A new cluster isadded when a point is too far away from all clusters. Thethreshold u controls the size of the clusters, which decideshow vc will be discretized later. At last, we transform vc byconsidering the value range each cluster covers as a level,and test if it fulfills Eqs. (2) or (3). If multiple levels arereturned, we try to merge them if they overlap and take theone that best elevates e as the most possible cause. 'The algorithm can be easily parallelized [41] if we modify the incremental process such that it searches clusters inbatches instead of inspecting them one by one, enablingscalability. Also, the trade-off of taking different u valuesis that a larger u tends to produce a looser constraint (alarger value range of vc) in c, often resulting in a smallerP\u00f0ejc\u00de or an E\u00bdvejccloser to E\u00bdve\u2013 a smaller u does theopposite. This is similar to the problem of under-/over-fitting. In our experiments, we found when u equals 0.15 ofvc\u2019s value range 5 iterations were usually sufficient toreach a plausible result \u2013 one that would be close to manual adjustment.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "A user can click on the variable name of a sequence to revisit and adjust the event\u2019s value constraint in the conditional distribution view (T4). An event can be removed with the delete button on the right of the sequence. Two indicator lines will be rendered and move along with the mouse pointer. The longer line shows the value or label, depending on the visualization mode, of each cause at the time point hovered on. The other line shows the value of the effect ahead, with the time shift set in the causal inference panel area chart.",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Filtering+Participation/Collaboration+Reconfig",
        "solution_compoent": "",
        "axial_code": [
          "Participation/Collaboration",
          "Selecting",
          "Reconfig",
          "Filtering"
        ],
        "componenet_code": [
          "participation/collaboration",
          "selecting",
          "reconfigure",
          "filtering"
        ]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 103,
    "paper_title": "A Visual Analytics System for Improving Attention-based Traffic Forecasting Models",
    "pub_year": 2023,
    "domain": "Traffic Visualization",
    "requirement": {
      "requirement_text": "R1: a VA system should highlight the roads with low accuracy and provide information on when a model has low accuracy",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "In this work, we use the traffic data of two different road networks\u2013theurban and highway road networks to explore the model\u2019s inferenceprocess for speed prediction. For the urban road network, we use ded\\x02icated short range communication (DSRC) data [28] generated fromUlsan, South Korea (range: 9/1/2017\u223c12/28/2017), where more than1.1 million people live with more than 540,000 registered vehicles as of2017. A total of 116 DSRC sensors are used for data collection, whichare installed every 5.7km and cover a total of 68 main roads. For thehighway road network, we use the METR-LA data [24], which werecollected from 207 loop detectors (range: 3/1/2012\u223c6/27/2012) on thehighways of Los Angeles. Note that the highway network data we useare the standard benchmark data for traffic forecasting tasks [40, 77, 82].After discussing with domain experts and reviewing training results, wereplace the missing data and explicit errors with historical data. We alsouse 5-minute aggregated data to mitigate possible effects of outliers, asperformed in many previous studies (e.g., [40]).",
      "data_code": { "geometry": 1, "tables": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We describe how we utilize ST-GRAT to demonstrate our VA approach.We choose ST-GRAT because 1) it has demonstrated state-of-the-artperformance and because 2) we can produce attention matrices onspatio-temporal dependency (e.g., Eq. 1, Fig. 3D) [51]. We also consider that the domain experts have expressed that attention models areextensively used at work for not only speed prediction but also othertasks, such as travel time [16] and taxi demand prediction [74]. ST-GRAT (Fig. 3) is a variation of the transformer [65] that usesthe encoder-decoder architecture with self-attention (i.e., temporal attention). Additionally, ST-GRAT utilizes graph attention as spatialattention before temporal attention with a sentinel vector. The sentinel vector acts as weights for skip connection within the same road.ST-GRAT utilizes 12-length sequential historical speed with encodedfeatures for each road and predicts 12 sequential speed predictions.There are three types of layers in the encoder and decoder: embedding, spatial attention, and temporal attention layers (Fig. 3B). To allowthe model to extract the spatio-temporal dependencies, we provide aroad network, speed, and observed time as the input features for theembedding layer (Fig. 3B). The road network graph is directed graphG(V,E,A), where a road is represented as a node (i.e., V) and theconnection between roads is shown as a link (i.e., E). Note that roadnetwork G is directed graph, which allow model to distinguish the roaddirections. Note that we provide the order of a given sequence usingthe position embedding method [65].The model captures spatial dependencies among neighbor roads inthe spatial attention layer (Fig. 3B) by using a graph attention network [66, 82], a well-known graph modeling method. In short, thespatial attention layer integrates information among neighboring roadsby directed graph attention. This directed spatial attention improvesdependency modeling and helps developers interpret ST-GRAT.The temporal attention layer (Fig. 3B) models the temporal dependency and trends of given sequences. For modeling, temporal attentionperforms multi-head attention to compute temporal correlations. Theattention type is decided by attention axes; spatial attention aggregatesfeatures among the spatial axis (i.e., neighboring roads), while temporal attention aggregates input features within the temporal axis (i.e.,different time steps of the same roads (Fig. 3C).To sum up, there are two important attention layers\u2013spatial andtemporal attention\u2013in ST-GRAT. Vaswani et al. [65] describe an attention function as mapping a query and a set of key-value pairs to anoutput, where the query, key, values, and output are all vectors. Fromthis perspective, from each key-value pair, we can derive an attentionmatrix from each attention layer, called SA and TA, and create a spatio temporal matrix (ST matrix). Specifically, for each attention head,given X before spatial attention and H after temporal attention, theirrelationship can be written as follows:H = (TA\u2299SA)X, (1)where TA \u2299 SA, named spatio-temporal attention, is the attention thatusers mainly explore for understanding model behaviors.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "In this work, we incorporate two automated methods\u2013warping (DTW) with spectral clustering [6, 57, 73] anddynamic timeGrangercausality tests [14], to allow users to determine which referencesare appropriate for predictions and whether a model employs properreferences for the predictions (R2-3) [38]. We use daily trends of roads as input for DTW computation, whichshould be explored since they are encoded as temporal dependenciesamong roads, which a model learns during training [74]. However,it is difficult to analyze road trends for two reasons. First, there aretoo many trends (i.e., roads) in a traffic dataset. Second, there aredifferent time gaps between the trends of roads due to different levelsof dependencies, which are presented with either lagging or precedingtrends [39]. For example, when congestion occurs on a road, the roadslinked to the congested road also become congested, but there are noconcrete patterns when the neighboring roads are congested. As such,we incorporate dynamic time warping (DTW) [6] in this work, whichcalculates the similarity in two time series over time gaps after findingthe best matching alignment that minimizes the distance, as Le Guenand Thome do for their time-series forecasting model design [35]. Forefficiency, we use FastDTW [57], whose complexity is O(N). Then we perform spectral clustering [47] with the computed DTWscores to further allow cluster-based analysis, which has shown itseffectiveness in analyzing traffic data [5, 73]. For example, users caneasily confirm whether referred roads of a target road have similar dailytrends as the target road using clusters. If the target and referred roadsare in the same cluster, it is highly possible that the model learns thedependencies of the target and referred roads during training [39]. Notethat we use the ELBOW method [1] with visual inspection to choosethe number of clusters.As Wu et al. [71] show in their study, catching preceding patterns intime-series enables effective analysis of deep learning models. Therefore, to help users better explore the preceding patterns in the trafficdata, and to complement DTW that distorts the time during its computation, we incorporate the Granger causality test [14] in this work,a well-known temporal dependency investigation method. We first describe the definition of Granger causality based on two principles: (1) the cause happens prior to its effect and (2) has unique informationabout the future values of its effect. Given these two principles aboutcausality, Granger causality proposes testing the following hypothesisfor the identification of a causal effect.GrangerCausality = P[Y(t +1) \u2208 A|I (t)] = P[Y(t +1) \u2208 |I\u2212X (t)]Here, P is probability, A is an arbitrary non-empty set, and I (t) andI\u2212X (t) denote the information available as of time t in the entireuniverse, and in the modified universe where X is excluded, respectively.If the above hypothesis is accepted, we say X Granger-causes Y.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling;Explainability",
        "solution_compoent": "",
        "axial_code": ["Explainability", "Modeling"],
        "componenet_code": ["explainability", "modeling"]
      },
      {
        "solution_text": "Users can filter out the roads using Mean Absolute Error (A1: : scaled: min\u2013max values) and spatio-temporal attention (A2: scaled: 0\u20131) values. When roads\u2019 MAEs are higher than the threshold value, the center circle of the roads are colored in black (R1). If a road\u2019s MAE value is below the value at Q1 (i.e., top 25% in accuracy), the road is included in the low error group. If the MAE of a road is above the value at Q3 (i.e., bottom 25% in accuracy), it belongs to the high error group. When users hover over the filters, a tooltip pops up to show the MAE values at the Q1 and Q3 boundaries (Fig. 1, A1). The attention filter is applied to AttnArrows (e.g., Fig. 4) on the map, when a road is selected for investigation. There are two legends to represent clusters (Fig. 1 D1) and ratio values (D2).",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration", "Selecting"],
        "componenet_code": ["participation/collaboration", "selecting"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 105,
    "paper_title": "A Visual Analytics System for Improving Attention-based Traffic Forecasting Models",
    "pub_year": 2023,
    "domain": "Traffic Visualization",
    "requirement": {
      "requirement_text": "R2-1: Example information for supporting spatio-temporal dependency exploration includes (R2-1) historical traffic patterns of roads, speed distribution of data, standard deviation, daily speed trends (Q3, Q4), ",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "In this work, we use the traffic data of two different road networks\u2013theurban and highway road networks to explore the model\u2019s inferenceprocess for speed prediction. For the urban road network, we use ded\\x02icated short range communication (DSRC) data [28] generated fromUlsan, South Korea (range: 9/1/2017\u223c12/28/2017), where more than1.1 million people live with more than 540,000 registered vehicles as of2017. A total of 116 DSRC sensors are used for data collection, whichare installed every 5.7km and cover a total of 68 main roads. For thehighway road network, we use the METR-LA data [24], which werecollected from 207 loop detectors (range: 3/1/2012\u223c6/27/2012) on thehighways of Los Angeles. Note that the highway network data we useare the standard benchmark data for traffic forecasting tasks [40, 77, 82].After discussing with domain experts and reviewing training results, wereplace the missing data and explicit errors with historical data. We alsouse 5-minute aggregated data to mitigate possible effects of outliers, asperformed in many previous studies (e.g., [40]).",
      "data_code": { "geometry": 1, "tables": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We describe how we utilize ST-GRAT to demonstrate our VA approach.We choose ST-GRAT because 1) it has demonstrated state-of-the-artperformance and because 2) we can produce attention matrices onspatio-temporal dependency (e.g., Eq. 1, Fig. 3D) [51]. We also consider that the domain experts have expressed that attention models areextensively used at work for not only speed prediction but also othertasks, such as travel time [16] and taxi demand prediction [74]. ST-GRAT (Fig. 3) is a variation of the transformer [65] that usesthe encoder-decoder architecture with self-attention (i.e., temporal attention). Additionally, ST-GRAT utilizes graph attention as spatialattention before temporal attention with a sentinel vector. The sentinel vector acts as weights for skip connection within the same road.ST-GRAT utilizes 12-length sequential historical speed with encodedfeatures for each road and predicts 12 sequential speed predictions.There are three types of layers in the encoder and decoder: embedding, spatial attention, and temporal attention layers (Fig. 3B). To allowthe model to extract the spatio-temporal dependencies, we provide aroad network, speed, and observed time as the input features for theembedding layer (Fig. 3B). The road network graph is directed graphG(V,E,A), where a road is represented as a node (i.e., V) and theconnection between roads is shown as a link (i.e., E). Note that roadnetwork G is directed graph, which allow model to distinguish the roaddirections. Note that we provide the order of a given sequence usingthe position embedding method [65].The model captures spatial dependencies among neighbor roads inthe spatial attention layer (Fig. 3B) by using a graph attention network [66, 82], a well-known graph modeling method. In short, thespatial attention layer integrates information among neighboring roadsby directed graph attention. This directed spatial attention improvesdependency modeling and helps developers interpret ST-GRAT.The temporal attention layer (Fig. 3B) models the temporal dependency and trends of given sequences. For modeling, temporal attentionperforms multi-head attention to compute temporal correlations. Theattention type is decided by attention axes; spatial attention aggregatesfeatures among the spatial axis (i.e., neighboring roads), while temporal attention aggregates input features within the temporal axis (i.e.,different time steps of the same roads (Fig. 3C).To sum up, there are two important attention layers\u2013spatial andtemporal attention\u2013in ST-GRAT. Vaswani et al. [65] describe an attention function as mapping a query and a set of key-value pairs to anoutput, where the query, key, values, and output are all vectors. Fromthis perspective, from each key-value pair, we can derive an attentionmatrix from each attention layer, called SA and TA, and create a spatio temporal matrix (ST matrix). Specifically, for each attention head,given X before spatial attention and H after temporal attention, theirrelationship can be written as follows:H = (TA\u2299SA)X, (1)where TA \u2299 SA, named spatio-temporal attention, is the attention thatusers mainly explore for understanding model behaviors.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "In this work, we incorporate two automated methods\u2013warping (DTW) with spectral clustering [6, 57, 73] anddynamic timeGrangercausality tests [14], to allow users to determine which referencesare appropriate for predictions and whether a model employs properreferences for the predictions (R2-3) [38]. We use daily trends of roads as input for DTW computation, whichshould be explored since they are encoded as temporal dependenciesamong roads, which a model learns during training [74]. However,it is difficult to analyze road trends for two reasons. First, there aretoo many trends (i.e., roads) in a traffic dataset. Second, there aredifferent time gaps between the trends of roads due to different levelsof dependencies, which are presented with either lagging or precedingtrends [39]. For example, when congestion occurs on a road, the roadslinked to the congested road also become congested, but there are noconcrete patterns when the neighboring roads are congested. As such,we incorporate dynamic time warping (DTW) [6] in this work, whichcalculates the similarity in two time series over time gaps after findingthe best matching alignment that minimizes the distance, as Le Guenand Thome do for their time-series forecasting model design [35]. Forefficiency, we use FastDTW [57], whose complexity is O(N). Then we perform spectral clustering [47] with the computed DTWscores to further allow cluster-based analysis, which has shown itseffectiveness in analyzing traffic data [5, 73]. For example, users caneasily confirm whether referred roads of a target road have similar dailytrends as the target road using clusters. If the target and referred roadsare in the same cluster, it is highly possible that the model learns thedependencies of the target and referred roads during training [39]. Notethat we use the ELBOW method [1] with visual inspection to choosethe number of clusters.As Wu et al. [71] show in their study, catching preceding patterns intime-series enables effective analysis of deep learning models. Therefore, to help users better explore the preceding patterns in the trafficdata, and to complement DTW that distorts the time during its computation, we incorporate the Granger causality test [14] in this work,a well-known temporal dependency investigation method. We first describe the definition of Granger causality based on two principles: (1) the cause happens prior to its effect and (2) has unique informationabout the future values of its effect. Given these two principles aboutcausality, Granger causality proposes testing the following hypothesisfor the identification of a causal effect.GrangerCausality = P[Y(t +1) \u2208 A|I (t)] = P[Y(t +1) \u2208 |I\u2212X (t)]Here, P is probability, A is an arbitrary non-empty set, and I (t) andI\u2212X (t) denote the information available as of time t in the entireuniverse, and in the modified universe where X is excluded, respectively.If the above hypothesis is accepted, we say X Granger-causes Y.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling;Explainability",
        "solution_compoent": "",
        "axial_code": ["Explainability", "Modeling"],
        "componenet_code": ["explainability", "modeling"]
      },
      {
        "solution_text": "As models learn dependencies among roads and forecast based on the learned dependencies, it is important to explore what dependencies a model learns with road speed patterns and which roads a model refers to for performance investigation. To help users perform such exploration (R2-1, R2-2), we first present each road as a white dot. Each circle surrounding the dots shows the cluster that the road belongs to. We provide three visualizations in the map view: heatmap, attention arrows (AttnArrows), and cluster visualization. First, to help users overview the relationship between roads\u2019 congestion levels and model performance (Q1), we visualize roads\u2019 congestion levels using heatmaps (Fig. 1D). Here, the redder the heatmaps are, the slower the roads are (heatmap legend: Fig. 1 D2). We also have considered providing heatmaps with a time filter so that users can explore regions with high model error but decided against this because this approach requires many interactions for filtering and memorizing heatmaps that have changed due to filtering. Using Bezier curves, AttnArrows (Fig. 4) link a target road and the roads that the target road attends for prediction. Here, the color represents the amount of attention given to the roads. The darker the head color, the more attention the reference road is given.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Providing",
        "solution_compoent": "Map+Circle+Heatmap",
        "axial_code": ["Overlay-Coordinatesystemrelated-Providing"],
        "componenet_code": ["Heatmap", "Circle", "Map"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 106,
    "paper_title": "A Visual Analytics System for Improving Attention-based Traffic Forecasting Models",
    "pub_year": 2023,
    "domain": "Traffic Visualization",
    "requirement": {
      "requirement_text": "R2-2: data on model behaviors (Q2\u2013Q5), such as which roads a model refers to for forecasting (i.e., which roads influence the prediction for a target road [19]), and which input sequences are importantly used for the prediction.",
      "requirement_code": { "identify_main_cause_aggregate": 1 }
    },
    "data": {
      "data_text": "In this work, we use the traffic data of two different road networks\u2013theurban and highway road networks to explore the model\u2019s inferenceprocess for speed prediction. For the urban road network, we use ded\\x02icated short range communication (DSRC) data [28] generated fromUlsan, South Korea (range: 9/1/2017\u223c12/28/2017), where more than1.1 million people live with more than 540,000 registered vehicles as of2017. A total of 116 DSRC sensors are used for data collection, whichare installed every 5.7km and cover a total of 68 main roads. For thehighway road network, we use the METR-LA data [24], which werecollected from 207 loop detectors (range: 3/1/2012\u223c6/27/2012) on thehighways of Los Angeles. Note that the highway network data we useare the standard benchmark data for traffic forecasting tasks [40, 77, 82].After discussing with domain experts and reviewing training results, wereplace the missing data and explicit errors with historical data. We alsouse 5-minute aggregated data to mitigate possible effects of outliers, asperformed in many previous studies (e.g., [40]).",
      "data_code": { "geometry": 1, "tables": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We describe how we utilize ST-GRAT to demonstrate our VA approach.We choose ST-GRAT because 1) it has demonstrated state-of-the-artperformance and because 2) we can produce attention matrices onspatio-temporal dependency (e.g., Eq. 1, Fig. 3D) [51]. We also consider that the domain experts have expressed that attention models areextensively used at work for not only speed prediction but also othertasks, such as travel time [16] and taxi demand prediction [74]. ST-GRAT (Fig. 3) is a variation of the transformer [65] that usesthe encoder-decoder architecture with self-attention (i.e., temporal attention). Additionally, ST-GRAT utilizes graph attention as spatialattention before temporal attention with a sentinel vector. The sentinel vector acts as weights for skip connection within the same road.ST-GRAT utilizes 12-length sequential historical speed with encodedfeatures for each road and predicts 12 sequential speed predictions.There are three types of layers in the encoder and decoder: embedding, spatial attention, and temporal attention layers (Fig. 3B). To allowthe model to extract the spatio-temporal dependencies, we provide aroad network, speed, and observed time as the input features for theembedding layer (Fig. 3B). The road network graph is directed graphG(V,E,A), where a road is represented as a node (i.e., V) and theconnection between roads is shown as a link (i.e., E). Note that roadnetwork G is directed graph, which allow model to distinguish the roaddirections. Note that we provide the order of a given sequence usingthe position embedding method [65].The model captures spatial dependencies among neighbor roads inthe spatial attention layer (Fig. 3B) by using a graph attention network [66, 82], a well-known graph modeling method. In short, thespatial attention layer integrates information among neighboring roadsby directed graph attention. This directed spatial attention improvesdependency modeling and helps developers interpret ST-GRAT.The temporal attention layer (Fig. 3B) models the temporal dependency and trends of given sequences. For modeling, temporal attentionperforms multi-head attention to compute temporal correlations. Theattention type is decided by attention axes; spatial attention aggregatesfeatures among the spatial axis (i.e., neighboring roads), while temporal attention aggregates input features within the temporal axis (i.e.,different time steps of the same roads (Fig. 3C).To sum up, there are two important attention layers\u2013spatial andtemporal attention\u2013in ST-GRAT. Vaswani et al. [65] describe an attention function as mapping a query and a set of key-value pairs to anoutput, where the query, key, values, and output are all vectors. Fromthis perspective, from each key-value pair, we can derive an attentionmatrix from each attention layer, called SA and TA, and create a spatio temporal matrix (ST matrix). Specifically, for each attention head,given X before spatial attention and H after temporal attention, theirrelationship can be written as follows:H = (TA\u2299SA)X, (1)where TA \u2299 SA, named spatio-temporal attention, is the attention thatusers mainly explore for understanding model behaviors.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "In this work, we incorporate two automated methods\u2013warping (DTW) with spectral clustering [6, 57, 73] anddynamic timeGrangercausality tests [14], to allow users to determine which referencesare appropriate for predictions and whether a model employs properreferences for the predictions (R2-3) [38]. We use daily trends of roads as input for DTW computation, whichshould be explored since they are encoded as temporal dependenciesamong roads, which a model learns during training [74]. However,it is difficult to analyze road trends for two reasons. First, there aretoo many trends (i.e., roads) in a traffic dataset. Second, there aredifferent time gaps between the trends of roads due to different levelsof dependencies, which are presented with either lagging or precedingtrends [39]. For example, when congestion occurs on a road, the roadslinked to the congested road also become congested, but there are noconcrete patterns when the neighboring roads are congested. As such,we incorporate dynamic time warping (DTW) [6] in this work, whichcalculates the similarity in two time series over time gaps after findingthe best matching alignment that minimizes the distance, as Le Guenand Thome do for their time-series forecasting model design [35]. Forefficiency, we use FastDTW [57], whose complexity is O(N). Then we perform spectral clustering [47] with the computed DTWscores to further allow cluster-based analysis, which has shown itseffectiveness in analyzing traffic data [5, 73]. For example, users caneasily confirm whether referred roads of a target road have similar dailytrends as the target road using clusters. If the target and referred roadsare in the same cluster, it is highly possible that the model learns thedependencies of the target and referred roads during training [39]. Notethat we use the ELBOW method [1] with visual inspection to choosethe number of clusters.As Wu et al. [71] show in their study, catching preceding patterns intime-series enables effective analysis of deep learning models. Therefore, to help users better explore the preceding patterns in the trafficdata, and to complement DTW that distorts the time during its computation, we incorporate the Granger causality test [14] in this work,a well-known temporal dependency investigation method. We first describe the definition of Granger causality based on two principles: (1) the cause happens prior to its effect and (2) has unique informationabout the future values of its effect. Given these two principles aboutcausality, Granger causality proposes testing the following hypothesisfor the identification of a causal effect.GrangerCausality = P[Y(t +1) \u2208 A|I (t)] = P[Y(t +1) \u2208 |I\u2212X (t)]Here, P is probability, A is an arbitrary non-empty set, and I (t) andI\u2212X (t) denote the information available as of time t in the entireuniverse, and in the modified universe where X is excluded, respectively.If the above hypothesis is accepted, we say X Granger-causes Y.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling;Explainability",
        "solution_compoent": "",
        "axial_code": ["Explainability", "Modeling"],
        "componenet_code": ["explainability", "modeling"]
      },
      {
        "solution_text": "As models learn dependencies among roads and forecast based on the learned dependencies, it is important to explore what dependencies a model learns with road speed patterns and which roads a model refers to for performance investigation. To help users perform such exploration (R2-1, R2-2), we first present each road as a white dot. Each circle surrounding the dots shows the cluster that the road belongs to. We provide three visualizations in the map view: heatmap, attention arrows (AttnArrows), and cluster visualization. First, to help users overview the relationship between roads\u2019 congestion levels and model performance (Q1), we visualize roads\u2019 congestion levels using heatmaps (Fig. 1D). Here, the redder the heatmaps are, the slower the roads are (heatmap legend: Fig. 1 D2). We also have considered providing heatmaps with a time filter so that users can explore regions with high model error but decided against this because this approach requires many interactions for filtering and memorizing heatmaps that have changed due to filtering. Using Bezier curves, AttnArrows (Fig. 4) link a target road and the roads that the target road attends for prediction. Here, the color represents the amount of attention given to the roads. The darker the head color, the more attention the reference road is given.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Providing",
        "solution_compoent": "Map+Circle+Heatmap",
        "axial_code": ["Overlay-Coordinatesystemrelated-Providing"],
        "componenet_code": ["Heatmap", "Circle", "Map"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 107,
    "paper_title": "A Visual Analytics System for Improving Attention-based Traffic Forecasting Models",
    "pub_year": 2023,
    "domain": "Traffic Visualization",
    "requirement": {
      "requirement_text": "R2-2: data on model behaviors (Q2\u2013Q5), such as which roads a model refers to for forecasting (i.e., which roads influence the prediction for a target road [19]), and which input sequences are importantly used for the prediction.",
      "requirement_code": { "identify_main_cause_aggregate": 1 }
    },
    "data": {
      "data_text": "In this work, we use the traffic data of two different road networks\u2013theurban and highway road networks to explore the model\u2019s inferenceprocess for speed prediction. For the urban road network, we use ded\\x02icated short range communication (DSRC) data [28] generated fromUlsan, South Korea (range: 9/1/2017\u223c12/28/2017), where more than1.1 million people live with more than 540,000 registered vehicles as of2017. A total of 116 DSRC sensors are used for data collection, whichare installed every 5.7km and cover a total of 68 main roads. For thehighway road network, we use the METR-LA data [24], which werecollected from 207 loop detectors (range: 3/1/2012\u223c6/27/2012) on thehighways of Los Angeles. Note that the highway network data we useare the standard benchmark data for traffic forecasting tasks [40, 77, 82].After discussing with domain experts and reviewing training results, wereplace the missing data and explicit errors with historical data. We alsouse 5-minute aggregated data to mitigate possible effects of outliers, asperformed in many previous studies (e.g., [40]).",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "We describe how we utilize ST-GRAT to demonstrate our VA approach.We choose ST-GRAT because 1) it has demonstrated state-of-the-artperformance and because 2) we can produce attention matrices onspatio-temporal dependency (e.g., Eq. 1, Fig. 3D) [51]. We also consider that the domain experts have expressed that attention models areextensively used at work for not only speed prediction but also othertasks, such as travel time [16] and taxi demand prediction [74]. ST-GRAT (Fig. 3) is a variation of the transformer [65] that usesthe encoder-decoder architecture with self-attention (i.e., temporal attention). Additionally, ST-GRAT utilizes graph attention as spatialattention before temporal attention with a sentinel vector. The sentinel vector acts as weights for skip connection within the same road.ST-GRAT utilizes 12-length sequential historical speed with encodedfeatures for each road and predicts 12 sequential speed predictions.There are three types of layers in the encoder and decoder: embedding, spatial attention, and temporal attention layers (Fig. 3B). To allowthe model to extract the spatio-temporal dependencies, we provide aroad network, speed, and observed time as the input features for theembedding layer (Fig. 3B). The road network graph is directed graphG(V,E,A), where a road is represented as a node (i.e., V) and theconnection between roads is shown as a link (i.e., E). Note that roadnetwork G is directed graph, which allow model to distinguish the roaddirections. Note that we provide the order of a given sequence usingthe position embedding method [65].The model captures spatial dependencies among neighbor roads inthe spatial attention layer (Fig. 3B) by using a graph attention network [66, 82], a well-known graph modeling method. In short, thespatial attention layer integrates information among neighboring roadsby directed graph attention. This directed spatial attention improvesdependency modeling and helps developers interpret ST-GRAT.The temporal attention layer (Fig. 3B) models the temporal dependency and trends of given sequences. For modeling, temporal attentionperforms multi-head attention to compute temporal correlations. Theattention type is decided by attention axes; spatial attention aggregatesfeatures among the spatial axis (i.e., neighboring roads), while temporal attention aggregates input features within the temporal axis (i.e.,different time steps of the same roads (Fig. 3C).To sum up, there are two important attention layers\u2013spatial andtemporal attention\u2013in ST-GRAT. Vaswani et al. [65] describe an attention function as mapping a query and a set of key-value pairs to anoutput, where the query, key, values, and output are all vectors. Fromthis perspective, from each key-value pair, we can derive an attentionmatrix from each attention layer, called SA and TA, and create a spatio temporal matrix (ST matrix). Specifically, for each attention head,given X before spatial attention and H after temporal attention, theirrelationship can be written as follows:H = (TA\u2299SA)X, (1)where TA \u2299 SA, named spatio-temporal attention, is the attention thatusers mainly explore for understanding model behaviors.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "In the view, users can analyze which roads and time steps the model focuses on with attention intensity, represented by the color (legend: Fig. 1 D2). For example, when users click road 81 on the map, the spatio-temporal view (Fig. 1E) shows that road 81 attends itself (i.e., self-reference) and 73 more than other roads for its prediction.",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration", "Selecting"],
        "componenet_code": ["participation/collaboration", "selecting"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 108,
    "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics",
    "pub_year": 2023,
    "domain": "E-commerce",
    "requirement": {
      "requirement_text": "R.1: Enable interactive comparison of products. Marketers often compete with peers selling similar products and need information on other complementary products to make promotional decisions accordingly. Therefore, experts expected that our approach can provide an overview of available product data, including but not limited to product ID, category, and brand, to help them observe relationships with other items and quickly retrieve specific products for further analysis.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.",
      "data_code": { "temporal": 1, "sequential": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "We design a Promotion Overview to provide the analysis with the promotions of the selected product and their correlation with the sales volume in the past periods (R.1, R.3). As shown in Fig. 3, there are two rings, each representing a year. The internal one represents the previous year and the external one represents the next year. The internal line graph represents the sales volume of the corresponding promotion offered on that day, and the external bar graph generates information on \u201cwhat\u201d and \u201chow many\u201d promotion strategies were used, with different colors representing a different types of promotions. Also, the height of the bars of the same color indicates the number of promotions using that type.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Providing",
        "solution_compoent": "Pie+Bar+Line",
        "axial_code": ["Overlay-Coordinatesystemrelated-Providing"],
        "componenet_code": ["Bar", "Line", "Pie"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 109,
    "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics",
    "pub_year": 2023,
    "domain": "E-commerce",
    "requirement": {
      "requirement_text": "R.1: Enable interactive comparison of products. Marketers often compete with peers selling similar products and need information on other complementary products to make promotional decisions accordingly. Therefore, experts expected that our approach can provide an overview of available product data, including but not limited to product ID, category, and brand, to help them observe relationships with other items and quickly retrieve specific products for further analysis.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.",
      "data_code": { "clusters_and_sets_and_lists": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Here, we use t-SNE to project all products into two-dimensional space to see potential clusters and outlier points.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "The Product Overview (Fig. 2(A)) enables users to efficiently select a product of interest, observe its general information and examine its relationship with other products (R.1). Dimensionality reduction tech_x0002_niques like t-SNE, PCA, and MDS can generate low-dimensional representations and preserve local similarities to convey neighborhood struc_x0002_tures like potential anomalies and clusters, all of which can be applied to explore and illustrate patterns in higher-dimensional spaces [24, 42]. Here, we use t-SNE to project all products into two-dimensional space to see potential clusters and outlier points. After discussing with do_x0002_main experts and surveying existing e-commerce practices, we used the following six metrics: sales median, standard deviation, quartile range, and the correlation between price, promotion, season and sales. These metrics form an n-dimensional feature vector x1,...,xN. We place the corresponding product statistics glyph, each with six features, in the calculated positions (detailed product statistics glyphs will be presented in the Competitive Analysis View (Fig. 2(D)). To interact with this view, the user should first set the filters that control the data, type, and brand of the product. Then, the system will automatically highlight all prod_x0002_ucts that match the filter criteria for selection. The user can click on these points to select the target product. In addition, if the user knows the product ID, he/she can also enter that ID in the search box and press Search . When a specific product is selected, message tooltip such as product ID, product name and product category will be displayed._x0002_",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Scatter",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Scatter"]
      },
      {
        "solution_text": "To interact with this view, the user should first set the filters that control the data, type, and brand of the product. Then, the system will automatically highlight all prod_x0002_ucts that match the filter criteria for selection. The user can click on these points to select the target product. In addition, if the user knows the product ID, he/she can also enter that ID in the search box and press Search . When a specific product is selected, message tooltip such as product ID, product name and product category will be displayed._x0002_",
        "solution_category": "interaction",
        "solution_axial": "Filtering+Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration", "Filtering"],
        "componenet_code": ["participation/collaboration", "filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 110,
    "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics",
    "pub_year": 2023,
    "domain": "E-commerce",
    "requirement": {
      "requirement_text": "R.1: Enable interactive comparison of products. Marketers often compete with peers selling similar products and need information on other complementary products to make promotional decisions accordingly. Therefore, experts expected that our approach can provide an overview of available product data, including but not limited to product ID, category, and brand, to help them observe relationships with other items and quickly retrieve specific products for further analysis.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.",
      "data_code": { "temporal": 1, "tables": 1, "sequential": 1 }
    },
    "solution": [
      {
        "solution_text": "Designing appropriate product titles is one of the most fundamental ways to attract potential customers by hitting their searches. Theresulting titles are crucial for machine learning models to distinguishbetween various products. As mentioned earlier, product titles arecomposed of descriptive words that may not be logically connected toeach other and can therefore be treated as a set of words rather thanparsed from the perspective of a complete sentence. For this purpose,we utilize a bag-of-words (BOW) [58] method to obtain the informationhidden in the titles. Specifically, we first create a codebook of all thewords that appear in product titles. Then, we split each title into a setof terms and compare them to the codebook to determine which wordsappear in the title. In this way, each title can be encoded as a vectorwith either 0 or 1, where 0 indicates the corresponding keyword in thecodebook is missing from the title and 1 indicates that the keyword ispresent in the title. It should be noted that the dimensionality of theconstructed vector is quite high for a title with only ten words, whichis too sparse for the model to learn features from the title. To solvethe sparsity problem, we follow the idea of word2vec [15] and reducethe title dimension to 8 after several experiments, i.e., we calculate thenumber of semantically valid words in 100 titles. The results show thateight words are sufficient to describe a product. Although promotions can be diverse, they usually follow some basicrules that allow us to translate textual information into numerical values.Based on these rules and the recommendations of experts, we classifyall promotions into two categories, including six promotion types (Table 1), for precise quantification. For direct discounts (value discount,percentage discount, and flash sale), we extract two key features. (1) thediscount rate kd, which is the ratio of the discounted price to originalprice; (2) the trigger amount pt , which is the minimum amount toreceive the discount. For indirect discounts (i.e., loyalty points, freeshipping, and interest-free installment), where the reward is not a directprice reduction, the reward and the original price of the product aretaken out as features. In addition, since the effect of a promotion mayexist before the campaign starts or after it ends, we track the entirelifecycle of the promotion by setting a status flag.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "In order to select the most appropriate model(s), we conduct a series ofexperiments and derive Table 2. To take full advantage of the historicaltime series data, as suggested by E.1, we enter the historical averagesales into the model, including the average daily sales for the last month,quarter, half year and full year. Considering the performance and theinterpretability of the model, we select three representative multivariate time series forecasting models as candidate forecasting techniques(i.e., RandomForest [25], XGBoost [13], and MLP [23]). Our systemcan easily integrate other types of time series forecasting models, e.g.,Vector Auto-Regressive Model (VAR), RNN, and CNN. Feature importance measures the impact of each feature on the prediction results,and explains the model\u2019s decision by calculating the additivity of eachfeature. We apply SHapley Additive exPlanations (SHAP) [38], an interpretation method that calculates Shapley values at the instance level,to estimate the feature importance for the sales prediction results. TheShapley values of these features add up to five values (i.e., descriptions,price, temporal information, competitive information, and promotion),indicating the power from the five relevant aspects. It is worth notingthat the Shapley value can be positive or negative, indicating that thefeature contributes positively or negatively to the prediction.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling;Explainability",
        "solution_compoent": "",
        "axial_code": ["Explainability", "Modeling"],
        "componenet_code": ["explainability", "modeling"]
      },
      {
        "solution_text": "The Sales Prediction View in Fig. 2(C) helps users understand the de_x0002_tailed correlation between the five most important factors (i.e., price, competition, temporal variation, title and promotions) and product sell_x0002_ing amount. In particular, at the top of the view is a visualization of the time, which shows the forecast profiles for the three models (Fig. 4(a)), making it easy for the user to adjust the length by dragging the gray slider to determine the time horizon to examine in detail. The main view of the Sales Prediction View shows the results of the three forecast models (R.2) and their interpretation of sales changes. As shown in Fig. 4(b), the graph for each model contains four parts: (1) the black dashed line (Fig. 4(b).1) indicates the ground truth sales volume for the selected time horizon; (2) the pink solid line (Fig. 4(b).2) shows the actual prices; (3) the purple solid step lines (Fig. 4(b).3) represents the comparison of the predicted sales amount with the real amount; (4) five colored bars representing the importance of five characteristics are attached above and below the model prediction line (Fig. 4(b).4) (R.3). It is worth noting that, as shown in Fig. 4(b).3, the daily predicted sales amount is represented as a step line rather than a curve or line segment, so that we can use the step space as the x-axis of daily feature impor_x0002_tance. According to Fig. 4(c), inspired by the design of mTSeer [54], the five feature importance are calculated, normalized, and represented as a stack of bars (R.1, R.3). Bars above the step lines indicate the positive impact on the forecast and vice versa. Further than mTSeer, at the bottom of the Sales Prediction View is a dotted line graph (Fig. 4(d)), which implies the promotion time coded by the length of the dotted line. All the charts mentioned above have the same X-axis, i.e., the time axis. After selecting a specific time horizon, the user can compare the performance of different models and pick up the chart that most closely resembles the ground truth prediction in order to observe the importance of its features and the promotions on the selected days. The user can also check the newly plotted forecasts caused by the updated promotional activity settings in the Strategy Setting View (Fig. 2(E)).",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Line",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Line"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 111,
    "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics",
    "pub_year": 2023,
    "domain": "E-commerce",
    "requirement": {
      "requirement_text": "R.1: Enable interactive comparison of products. Marketers often compete with peers selling similar products and need information on other complementary products to make promotional decisions accordingly. Therefore, experts expected that our approach can provide an overview of available product data, including but not limited to product ID, category, and brand, to help them observe relationships with other items and quickly retrieve specific products for further analysis.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.",
      "data_code": { "temporal": 1, "tables": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "The Competitive Analysis View (Fig. 2(D)) provides a comparison between the selected product and several of the most similar products in the same category (R.1), which can be considered as potential competitors. In this work, experts recommend comparing at least five other products of the same type. With this view, the user can find similarities and differences between the target product and its competitors, and thus build a corresponding promotion strategy solidly and confidently.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Line",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Line"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 112,
    "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics",
    "pub_year": 2023,
    "domain": "E-commerce",
    "requirement": {
      "requirement_text": "R.2: Predict future trends for individual items. Traditionally, when estimating and forecasting sales, our domain experts obtain historical sales data from Enterprise Resource Planning (ERP) system and invite some univariate time series models to make forecasts. \u201cThe system doesn\u2019t perform as well as expected, especially during promotional seasons,\u201d said E3. \u201cWe can empirically add a linear factor determined by promotion discount rates, but the results are often unsatisfactory,\u201d said E2. E1 also said that multiple variables combine to determine the amount of sales, of which \u201csome are difficult to quantify.\u201d She also commented that \u201cpromotion is one of the biggest factors that motivate shoppers to press the \u2018buy button\u2019.\u201d Therefore, our approach should support multivariate time series forecasting, consider quantifiable factors, and add promotion features to adjust the prediction results.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.",
      "data_code": { "temporal": 1, "tables": 1, "sequential": 1 }
    },
    "solution": [
      {
        "solution_text": "Designing appropriate product titles is one of the most fundamental ways to attract potential customers by hitting their searches. Theresulting titles are crucial for machine learning models to distinguishbetween various products. As mentioned earlier, product titles arecomposed of descriptive words that may not be logically connected toeach other and can therefore be treated as a set of words rather thanparsed from the perspective of a complete sentence. For this purpose,we utilize a bag-of-words (BOW) [58] method to obtain the informationhidden in the titles. Specifically, we first create a codebook of all thewords that appear in product titles. Then, we split each title into a setof terms and compare them to the codebook to determine which wordsappear in the title. In this way, each title can be encoded as a vectorwith either 0 or 1, where 0 indicates the corresponding keyword in thecodebook is missing from the title and 1 indicates that the keyword ispresent in the title. It should be noted that the dimensionality of theconstructed vector is quite high for a title with only ten words, whichis too sparse for the model to learn features from the title. To solvethe sparsity problem, we follow the idea of word2vec [15] and reducethe title dimension to 8 after several experiments, i.e., we calculate thenumber of semantically valid words in 100 titles. The results show thateight words are sufficient to describe a product. Although promotions can be diverse, they usually follow some basicrules that allow us to translate textual information into numerical values.Based on these rules and the recommendations of experts, we classifyall promotions into two categories, including six promotion types (Table 1), for precise quantification. For direct discounts (value discount,percentage discount, and flash sale), we extract two key features. (1) thediscount rate kd, which is the ratio of the discounted price to originalprice; (2) the trigger amount pt , which is the minimum amount toreceive the discount. For indirect discounts (i.e., loyalty points, freeshipping, and interest-free installment), where the reward is not a directprice reduction, the reward and the original price of the product aretaken out as features. In addition, since the effect of a promotion mayexist before the campaign starts or after it ends, we track the entirelifecycle of the promotion by setting a status flag.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "In order to select the most appropriate model(s), we conduct a series ofexperiments and derive Table 2. To take full advantage of the historicaltime series data, as suggested by E.1, we enter the historical averagesales into the model, including the average daily sales for the last month,quarter, half year and full year. Considering the performance and theinterpretability of the model, we select three representative multivariate time series forecasting models as candidate forecasting techniques(i.e., RandomForest [25], XGBoost [13], and MLP [23]). Our systemcan easily integrate other types of time series forecasting models, e.g.,Vector Auto-Regressive Model (VAR), RNN, and CNN. Feature importance measures the impact of each feature on the prediction results,and explains the model\u2019s decision by calculating the additivity of eachfeature. We apply SHapley Additive exPlanations (SHAP) [38], an interpretation method that calculates Shapley values at the instance level,to estimate the feature importance for the sales prediction results. TheShapley values of these features add up to five values (i.e., descriptions,price, temporal information, competitive information, and promotion),indicating the power from the five relevant aspects. It is worth notingthat the Shapley value can be positive or negative, indicating that thefeature contributes positively or negatively to the prediction.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling;Explainability",
        "solution_compoent": "",
        "axial_code": ["Explainability", "Modeling"],
        "componenet_code": ["explainability", "modeling"]
      },
      {
        "solution_text": "The Sales Prediction View in Fig. 2(C) helps users understand the de_x0002_tailed correlation between the five most important factors (i.e., price, competition, temporal variation, title and promotions) and product sell_x0002_ing amount. In particular, at the top of the view is a visualization of the time, which shows the forecast profiles for the three models (Fig. 4(a)), making it easy for the user to adjust the length by dragging the gray slider to determine the time horizon to examine in detail. The main view of the Sales Prediction View shows the results of the three forecast models (R.2) and their interpretation of sales changes. As shown in Fig. 4(b), the graph for each model contains four parts: (1) the black dashed line (Fig. 4(b).1) indicates the ground truth sales volume for the selected time horizon; (2) the pink solid line (Fig. 4(b).2) shows the actual prices; (3) the purple solid step lines (Fig. 4(b).3) represents the comparison of the predicted sales amount with the real amount; (4) five colored bars representing the importance of five characteristics are attached above and below the model prediction line (Fig. 4(b).4) (R.3). It is worth noting that, as shown in Fig. 4(b).3, the daily predicted sales amount is represented as a step line rather than a curve or line segment, so that we can use the step space as the x-axis of daily feature impor_x0002_tance. According to Fig. 4(c), inspired by the design of mTSeer [54], the five feature importance are calculated, normalized, and represented as a stack of bars (R.1, R.3). Bars above the step lines indicate the positive impact on the forecast and vice versa. Further than mTSeer, at the bottom of the Sales Prediction View is a dotted line graph (Fig. 4(d)), which implies the promotion time coded by the length of the dotted line. All the charts mentioned above have the same X-axis, i.e., the time axis. After selecting a specific time horizon, the user can compare the performance of different models and pick up the chart that most closely resembles the ground truth prediction in order to observe the importance of its features and the promotions on the selected days. The user can also check the newly plotted forecasts caused by the updated promotional activity settings in the Strategy Setting View (Fig. 2(E)).",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Line",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Line"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 113,
    "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics",
    "pub_year": 2023,
    "domain": "E-commerce",
    "requirement": {
      "requirement_text": "R.3: Understand the factors that influence sales volume throughout the product lifecycle. According to the feedback from the domain experts, it is important not only to get the best promotion strategy, but also to find out what causes the diversity of results. E2 and E3 were particularly interested in some rare and unexpected situations in the timeline. Therefore, equipping the visualization system with factor analysis allows them to easily observe and speculate on the influencing factors. E1 also mentioned that advanced analytics should get feedback during the sales evolution of the merchandise and that \u201cretailer should be aware of influencing factors that may affect the sales volume for the whole sales period and not be limited to a certain promotion phase.\u201d",
      "requirement_code": {
        "identify_main_cause_aggregate": 1,
        "explain_differences": 1
      }
    },
    "data": {
      "data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.",
      "data_code": { "media": 1, "temporal": 1, "tables": 1, "sequential": 1 }
    },
    "solution": [
      {
        "solution_text": "We design a Promotion Overview to provide the analysis with the promotions of the selected product and their correlation with the sales volume in the past periods (R.1, R.3). As shown in Fig. 3, there are two rings, each representing a year. The internal one represents the previous year and the external one represents the next year. The internal line graph represents the sales volume of the corresponding promotion offered on that day, and the external bar graph generates information on \u201cwhat\u201d and \u201chow many\u201d promotion strategies were used, with different colors representing a different types of promotions. Also, the height of the bars of the same color indicates the number of promotions using that type.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Providing",
        "solution_compoent": "Pie+Bar+Line",
        "axial_code": ["Overlay-Coordinatesystemrelated-Providing"],
        "componenet_code": ["Bar", "Line", "Pie"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 114,
    "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics",
    "pub_year": 2023,
    "domain": "E-commerce",
    "requirement": {
      "requirement_text": "R.3: Understand the factors that influence sales volume throughout the product lifecycle. According to the feedback from the domain experts, it is important not only to get the best promotion strategy, but also to find out what causes the diversity of results. E2 and E3 were particularly interested in some rare and unexpected situations in the timeline. Therefore, equipping the visualization system with factor analysis allows them to easily observe and speculate on the influencing factors. E1 also mentioned that advanced analytics should get feedback during the sales evolution of the merchandise and that \u201cretailer should be aware of influencing factors that may affect the sales volume for the whole sales period and not be limited to a certain promotion phase.\u201d",
      "requirement_code": {
        "identify_main_cause_aggregate": 1,
        "explain_differences": 1
      }
    },
    "data": {
      "data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.",
      "data_code": { "temporal": 1, "media": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Designing appropriate product titles is one of the most fundamental ways to attract potential customers by hitting their searches. Theresulting titles are crucial for machine learning models to distinguishbetween various products. As mentioned earlier, product titles arecomposed of descriptive words that may not be logically connected toeach other and can therefore be treated as a set of words rather thanparsed from the perspective of a complete sentence. For this purpose,we utilize a bag-of-words (BOW) [58] method to obtain the informationhidden in the titles. Specifically, we first create a codebook of all thewords that appear in product titles. Then, we split each title into a setof terms and compare them to the codebook to determine which wordsappear in the title. In this way, each title can be encoded as a vectorwith either 0 or 1, where 0 indicates the corresponding keyword in thecodebook is missing from the title and 1 indicates that the keyword ispresent in the title. It should be noted that the dimensionality of theconstructed vector is quite high for a title with only ten words, whichis too sparse for the model to learn features from the title. To solvethe sparsity problem, we follow the idea of word2vec [15] and reducethe title dimension to 8 after several experiments, i.e., we calculate thenumber of semantically valid words in 100 titles. The results show thateight words are sufficient to describe a product. Although promotions can be diverse, they usually follow some basicrules that allow us to translate textual information into numerical values.Based on these rules and the recommendations of experts, we classifyall promotions into two categories, including six promotion types (Table 1), for precise quantification. For direct discounts (value discount,percentage discount, and flash sale), we extract two key features. (1) thediscount rate kd, which is the ratio of the discounted price to originalprice; (2) the trigger amount pt , which is the minimum amount toreceive the discount. For indirect discounts (i.e., loyalty points, freeshipping, and interest-free installment), where the reward is not a directprice reduction, the reward and the original price of the product aretaken out as features. In addition, since the effect of a promotion mayexist before the campaign starts or after it ends, we track the entirelifecycle of the promotion by setting a status flag.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "In order to select the most appropriate model(s), we conduct a series ofexperiments and derive Table 2. To take full advantage of the historicaltime series data, as suggested by E.1, we enter the historical averagesales into the model, including the average daily sales for the last month,quarter, half year and full year. Considering the performance and theinterpretability of the model, we select three representative multivariate time series forecasting models as candidate forecasting techniques(i.e., RandomForest [25], XGBoost [13], and MLP [23]). Our systemcan easily integrate other types of time series forecasting models, e.g.,Vector Auto-Regressive Model (VAR), RNN, and CNN. Feature importance measures the impact of each feature on the prediction results,and explains the model\u2019s decision by calculating the additivity of eachfeature. We apply SHapley Additive exPlanations (SHAP) [38], an interpretation method that calculates Shapley values at the instance level,to estimate the feature importance for the sales prediction results. TheShapley values of these features add up to five values (i.e., descriptions,price, temporal information, competitive information, and promotion),indicating the power from the five relevant aspects. It is worth notingthat the Shapley value can be positive or negative, indicating that thefeature contributes positively or negatively to the prediction.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling;Explainability",
        "solution_compoent": "",
        "axial_code": ["Explainability", "Modeling"],
        "componenet_code": ["explainability", "modeling"]
      },
      {
        "solution_text": "The Sales Prediction View in Fig. 2(C) helps users understand the de_x0002_tailed correlation between the five most important factors (i.e., price, competition, temporal variation, title and promotions) and product sell_x0002_ing amount. In particular, at the top of the view is a visualization of the time, which shows the forecast profiles for the three models (Fig. 4(a)), making it easy for the user to adjust the length by dragging the gray slider to determine the time horizon to examine in detail. The main view of the Sales Prediction View shows the results of the three forecast models (R.2) and their interpretation of sales changes. As shown in Fig. 4(b), the graph for each model contains four parts: (1) the black dashed line (Fig. 4(b).1) indicates the ground truth sales volume for the selected time horizon; (2) the pink solid line (Fig. 4(b).2) shows the actual prices; (3) the purple solid step lines (Fig. 4(b).3) represents the comparison of the predicted sales amount with the real amount; (4) five colored bars representing the importance of five characteristics are attached above and below the model prediction line (Fig. 4(b).4) (R.3). It is worth noting that, as shown in Fig. 4(b).3, the daily predicted sales amount is represented as a step line rather than a curve or line segment, so that we can use the step space as the x-axis of daily feature impor_x0002_tance. According to Fig. 4(c), inspired by the design of mTSeer [54], the five feature importance are calculated, normalized, and represented as a stack of bars (R.1, R.3). Bars above the step lines indicate the positive impact on the forecast and vice versa. Further than mTSeer, at the bottom of the Sales Prediction View is a dotted line graph (Fig. 4(d)), which implies the promotion time coded by the length of the dotted line. All the charts mentioned above have the same X-axis, i.e., the time axis. After selecting a specific time horizon, the user can compare the performance of different models and pick up the chart that most closely resembles the ground truth prediction in order to observe the importance of its features and the promotions on the selected days. The user can also check the newly plotted forecasts caused by the updated promotional activity settings in the Strategy Setting View (Fig. 2(E)).",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Line",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Line"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 115,
    "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics",
    "pub_year": 2023,
    "domain": "E-commerce",
    "requirement": {
      "requirement_text": "R.4: Identify the similarities and differences of existing promotion strategies. A promotion is a short-term incentive activity designed to encourage people to buy a product or service. Although there are many dazzling promotions such as \u2018\u2018coupons\u201d, \u2018\u2018gifts\u201d, and \u2018\u2018station-wide discounts\u201d, according to E4 and E5, there are many misconceptions in designing promotion strategies. In addition, many existing promotion strategies are relatively similar, and some retailers simply follow their competitors without thinking about why such promotion strategies work. \u201cSome merchants have only one promotional policy, for example, \u2018always discounts\u2019,\u201d said E4. Therefore, domain experts asked our system to support a detailed exploration of the similarities and differences of common promotion strategies available on e-commerce platforms.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.",
      "data_code": { "clusters_and_sets_and_lists": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Designing appropriate product titles is one of the most fundamental ways to attract potential customers by hitting their searches. Theresulting titles are crucial for machine learning models to distinguishbetween various products. As mentioned earlier, product titles arecomposed of descriptive words that may not be logically connected toeach other and can therefore be treated as a set of words rather thanparsed from the perspective of a complete sentence. For this purpose,we utilize a bag-of-words (BOW) [58] method to obtain the informationhidden in the titles. Specifically, we first create a codebook of all thewords that appear in product titles. Then, we split each title into a setof terms and compare them to the codebook to determine which wordsappear in the title. In this way, each title can be encoded as a vectorwith either 0 or 1, where 0 indicates the corresponding keyword in thecodebook is missing from the title and 1 indicates that the keyword ispresent in the title. It should be noted that the dimensionality of theconstructed vector is quite high for a title with only ten words, whichis too sparse for the model to learn features from the title. To solvethe sparsity problem, we follow the idea of word2vec [15] and reducethe title dimension to 8 after several experiments, i.e., we calculate thenumber of semantically valid words in 100 titles. The results show thateight words are sufficient to describe a product. Although promotions can be diverse, they usually follow some basicrules that allow us to translate textual information into numerical values.Based on these rules and the recommendations of experts, we classifyall promotions into two categories, including six promotion types (Table 1), for precise quantification. For direct discounts (value discount,percentage discount, and flash sale), we extract two key features. (1) thediscount rate kd, which is the ratio of the discounted price to originalprice; (2) the trigger amount pt , which is the minimum amount toreceive the discount. For indirect discounts (i.e., loyalty points, freeshipping, and interest-free installment), where the reward is not a directprice reduction, the reward and the original price of the product aretaken out as features. In addition, since the effect of a promotion mayexist before the campaign starts or after it ends, we track the entirelifecycle of the promotion by setting a status flag.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "In order to select the most appropriate model(s), we conduct a series ofexperiments and derive Table 2. To take full advantage of the historicaltime series data, as suggested by E.1, we enter the historical averagesales into the model, including the average daily sales for the last month,quarter, half year and full year. Considering the performance and theinterpretability of the model, we select three representative multivariate time series forecasting models as candidate forecasting techniques(i.e., RandomForest [25], XGBoost [13], and MLP [23]). Our systemcan easily integrate other types of time series forecasting models, e.g.,Vector Auto-Regressive Model (VAR), RNN, and CNN. Feature importance measures the impact of each feature on the prediction results,and explains the model\u2019s decision by calculating the additivity of eachfeature. We apply SHapley Additive exPlanations (SHAP) [38], an interpretation method that calculates Shapley values at the instance level,to estimate the feature importance for the sales prediction results. TheShapley values of these features add up to five values (i.e., descriptions,price, temporal information, competitive information, and promotion),indicating the power from the five relevant aspects. It is worth notingthat the Shapley value can be positive or negative, indicating that thefeature contributes positively or negatively to the prediction.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling;Explainability",
        "solution_compoent": "",
        "axial_code": ["Explainability", "Modeling"],
        "componenet_code": ["explainability", "modeling"]
      },
      {
        "solution_text": "the Strategy Setting View gives a detailed description of the promotion, allowing the domain experts to configure different promotion strategies and simulate their corresponding impact on sales volume (R.4, R.5). The Strategy Setting View is designed to meet the user\u2019s need for promotion strategy assumptions (R.5). When a product is selected in the Product Overview, all promotions within its time frame specified by the user in the sales prediction view are listed here. Users can use the category filters above to quickly locate specific promotions. For each campaign, users can press the Edit button to adjust the campaign status by changing the description in the box with the formatting in Table 1, or by enabling and disabling the checkboxes. Users can also modify the duration of each promotion by dragging the position and length of the slider below each term. Pressing the Delete button will delete an existing campaign, while pressing the Add button will add a campaign. When all promotions are set up, the user can re-run the model by pressing the Refresh button and the system will draw a new step line in purple in the Sales Prediction View and attach an updated feature importance bar to it.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Table+Text",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Table", "Text"]
      },
      {
        "solution_text": "the Strategy Setting View gives a detailed description of the promotion, allowing the domain experts to configure different promotion strategies and simulate their corresponding impact on sales volume (R.4, R.5). The Strategy Setting View is designed to meet the user\u2019s need for promotion strategy assumptions (R.5). When a product is selected in the Product Overview, all promotions within its time frame specified by the user in the sales prediction view are listed here. Users can use the category filters above to quickly locate specific promotions. For each campaign, users can press the Edit button to adjust the campaign status by changing the description in the box with the formatting in Table 1, or by enabling and disabling the checkboxes. Users can also modify the duration of each promotion by dragging the position and length of the slider below each term. Pressing the Delete button will delete an existing campaign, while pressing the Add button will add a campaign. When all promotions are set up, the user can re-run the model by pressing the Refresh button and the system will draw a new step line in purple in the Sales Prediction View and attach an updated feature importance bar to it.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration+Selecting+Connect/Relate",
        "solution_compoent": "",
        "axial_code": [
          "Selecting",
          "Connect/Relate",
          "Participation/Collaboration"
        ],
        "componenet_code": [
          "selecting",
          "connect/relate",
          "participation/collaboration"
        ]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 116,
    "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics",
    "pub_year": 2023,
    "domain": "E-commerce",
    "requirement": {
      "requirement_text": "R.4: Identify the similarities and differences of existing promotion strategies. A promotion is a short-term incentive activity designed to encourage people to buy a product or service. Although there are many dazzling promotions such as \u2018\u2018coupons\u201d, \u2018\u2018gifts\u201d, and \u2018\u2018station-wide discounts\u201d, according to E4 and E5, there are many misconceptions in designing promotion strategies. In addition, many existing promotion strategies are relatively similar, and some retailers simply follow their competitors without thinking about why such promotion strategies work. \u201cSome merchants have only one promotional policy, for example, \u2018always discounts\u2019,\u201d said E5. Therefore, domain experts asked our system to support a detailed exploration of the similarities and differences of common promotion strategies available on e-commerce platforms.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.",
      "data_code": { "temporal": 1, "tables": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Designing appropriate product titles is one of the most fundamental ways to attract potential customers by hitting their searches. Theresulting titles are crucial for machine learning models to distinguishbetween various products. As mentioned earlier, product titles arecomposed of descriptive words that may not be logically connected toeach other and can therefore be treated as a set of words rather thanparsed from the perspective of a complete sentence. For this purpose,we utilize a bag-of-words (BOW) [58] method to obtain the informationhidden in the titles. Specifically, we first create a codebook of all thewords that appear in product titles. Then, we split each title into a setof terms and compare them to the codebook to determine which wordsappear in the title. In this way, each title can be encoded as a vectorwith either 0 or 1, where 0 indicates the corresponding keyword in thecodebook is missing from the title and 1 indicates that the keyword ispresent in the title. It should be noted that the dimensionality of theconstructed vector is quite high for a title with only ten words, whichis too sparse for the model to learn features from the title. To solvethe sparsity problem, we follow the idea of word2vec [15] and reducethe title dimension to 8 after several experiments, i.e., we calculate thenumber of semantically valid words in 100 titles. The results show thateight words are sufficient to describe a product. Although promotions can be diverse, they usually follow some basicrules that allow us to translate textual information into numerical values.Based on these rules and the recommendations of experts, we classifyall promotions into two categories, including six promotion types (Table 1), for precise quantification. For direct discounts (value discount,percentage discount, and flash sale), we extract two key features. (1) thediscount rate kd, which is the ratio of the discounted price to originalprice; (2) the trigger amount pt , which is the minimum amount toreceive the discount. For indirect discounts (i.e., loyalty points, freeshipping, and interest-free installment), where the reward is not a directprice reduction, the reward and the original price of the product aretaken out as features. In addition, since the effect of a promotion mayexist before the campaign starts or after it ends, we track the entirelifecycle of the promotion by setting a status flag.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "To select the top five most similar products, we calculate the Euclidean distance between the target product and the other five products in the same category in the Product Overview (Fig. 2(A)).",
        "solution_category": "data_manipulation",
        "solution_axial": "SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["SimilarityCalculation"],
        "componenet_code": ["similarity_calculation"]
      },
      {
        "solution_text": "the Competitive Analysis View shows the characteristics of competing products, including their promotional information, and thus helps to compare promotion strategies and motivate domain experts to find better ones (R.1, R.4). The Competitive Analysis View (Fig. 2(D)) provides a comparison be_tween the selected product and several of the most similar products in the same category (R.1), which can be considered as potential competi_x0002_tors. In this work, experts recommend comparing at least five other products of the same type. With this view, the user can find similarities and differences between the target product and its competitors, and thus build a corresponding promotion strategy solidly and confidently. To select the top five most similar products, we calculate the Euclidean distance between the target product and the other five products in the same category in the Product Overview (Fig. 2(A)). To help users get a general idea of the products and compare the differences between them, we designed a novel glyph called product statistics glyph to encode the statistical attributes of the products (e.g., stability of sales amount, price elastics).",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Line+Text+Pie+Circle",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Circle", "Line", "Text", "Pie"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 117,
    "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics",
    "pub_year": 2023,
    "domain": "E-commerce",
    "requirement": {
      "requirement_text": "R.4: Identify the similarities and differences of existing promotion strategies. A promotion is a short-term incentive activity designed to encourage people to buy a product or service. Although there are many dazzling promotions such as \u2018\u2018coupons\u201d, \u2018\u2018gifts\u201d, and \u2018\u2018station-wide discounts\u201d, according to E4 and E5, there are many misconceptions in designing promotion strategies. In addition, many existing promotion strategies are relatively similar, and some retailers simply follow their competitors without thinking about why such promotion strategies work. \u201cSome merchants have only one promotional policy, for example, \u2018always discounts\u2019,\u201d said E4. Therefore, domain experts asked our system to support a detailed exploration of the similarities and differences of common promotion strategies available on e-commerce platforms.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.",
      "data_code": { "clusters_and_sets_and_lists": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Designing appropriate product titles is one of the most fundamental ways to attract potential customers by hitting their searches. Theresulting titles are crucial for machine learning models to distinguishbetween various products. As mentioned earlier, product titles arecomposed of descriptive words that may not be logically connected toeach other and can therefore be treated as a set of words rather thanparsed from the perspective of a complete sentence. For this purpose,we utilize a bag-of-words (BOW) [58] method to obtain the informationhidden in the titles. Specifically, we first create a codebook of all thewords that appear in product titles. Then, we split each title into a setof terms and compare them to the codebook to determine which wordsappear in the title. In this way, each title can be encoded as a vectorwith either 0 or 1, where 0 indicates the corresponding keyword in thecodebook is missing from the title and 1 indicates that the keyword ispresent in the title. It should be noted that the dimensionality of theconstructed vector is quite high for a title with only ten words, whichis too sparse for the model to learn features from the title. To solvethe sparsity problem, we follow the idea of word2vec [15] and reducethe title dimension to 8 after several experiments, i.e., we calculate thenumber of semantically valid words in 100 titles. The results show thateight words are sufficient to describe a product. Although promotions can be diverse, they usually follow some basicrules that allow us to translate textual information into numerical values.Based on these rules and the recommendations of experts, we classifyall promotions into two categories, including six promotion types (Table 1), for precise quantification. For direct discounts (value discount,percentage discount, and flash sale), we extract two key features. (1) thediscount rate kd, which is the ratio of the discounted price to originalprice; (2) the trigger amount pt , which is the minimum amount toreceive the discount. For indirect discounts (i.e., loyalty points, freeshipping, and interest-free installment), where the reward is not a directprice reduction, the reward and the original price of the product aretaken out as features. In addition, since the effect of a promotion mayexist before the campaign starts or after it ends, we track the entirelifecycle of the promotion by setting a status flag.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "In order to select the most appropriate model(s), we conduct a series ofexperiments and derive Table 2. To take full advantage of the historicaltime series data, as suggested by E.1, we enter the historical averagesales into the model, including the average daily sales for the last month,quarter, half year and full year. Considering the performance and theinterpretability of the model, we select three representative multivariate time series forecasting models as candidate forecasting techniques(i.e., RandomForest [25], XGBoost [13], and MLP [23]). Our systemcan easily integrate other types of time series forecasting models, e.g.,Vector Auto-Regressive Model (VAR), RNN, and CNN. Feature importance measures the impact of each feature on the prediction results,and explains the model\u2019s decision by calculating the additivity of eachfeature. We apply SHapley Additive exPlanations (SHAP) [38], an interpretation method that calculates Shapley values at the instance level,to estimate the feature importance for the sales prediction results. TheShapley values of these features add up to five values (i.e., descriptions,price, temporal information, competitive information, and promotion),indicating the power from the five relevant aspects. It is worth notingthat the Shapley value can be positive or negative, indicating that thefeature contributes positively or negatively to the prediction.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling;Explainability",
        "solution_compoent": "",
        "axial_code": ["Explainability", "Modeling"],
        "componenet_code": ["explainability", "modeling"]
      },
      {
        "solution_text": "the Strategy Setting View gives a detailed description of the promotion, allowing the domain experts to configure different promotion strategies and simulate their corresponding impact on sales volume (R.4, R.5). The Strategy Setting View is designed to meet the user\u2019s need for promotion strategy assumptions (R.5). When a product is selected in the Product Overview, all promotions within its time frame specified by the user in the sales prediction view are listed here. Users can use the category filters above to quickly locate specific promotions. For each campaign, users can press the Edit button to adjust the campaign status by changing the description in the box with the formatting in Table 1, or by enabling and disabling the checkboxes. Users can also modify the duration of each promotion by dragging the position and length of the slider below each term. Pressing the Delete button will delete an existing campaign, while pressing the Add button will add a campaign. When all promotions are set up, the user can re-run the model by pressing the Refresh button and the system will draw a new step line in purple in the Sales Prediction View and attach an updated feature importance bar to it.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Table+Text",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Table", "Text"]
      },
      {
        "solution_text": "the Strategy Setting View gives a detailed description of the promotion, allowing the domain experts to configure different promotion strategies and simulate their corresponding impact on sales volume (R.4, R.5). The Strategy Setting View is designed to meet the user\u2019s need for promotion strategy assumptions (R.5). When a product is selected in the Product Overview, all promotions within its time frame specified by the user in the sales prediction view are listed here. Users can use the category filters above to quickly locate specific promotions. For each campaign, users can press the Edit button to adjust the campaign status by changing the description in the box with the formatting in Table 1, or by enabling and disabling the checkboxes. Users can also modify the duration of each promotion by dragging the position and length of the slider below each term. Pressing the Delete button will delete an existing campaign, while pressing the Add button will add a campaign. When all promotions are set up, the user can re-run the model by pressing the Refresh button and the system will draw a new step line in purple in the Sales Prediction View and attach an updated feature importance bar to it.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration+Selecting+Connect/Relate",
        "solution_compoent": "",
        "axial_code": [
          "Selecting",
          "Connect/Relate",
          "Participation/Collaboration"
        ],
        "componenet_code": [
          "selecting",
          "connect/relate",
          "participation/collaboration"
        ]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 118,
    "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics",
    "pub_year": 2023,
    "domain": "E-commerce",
    "requirement": {
      "requirement_text": "R.4: Identify the similarities and differences of existing promotion strategies. A promotion is a short-term incentive activity designed to encourage people to buy a product or service. Although there are many dazzling promotions such as \u2018\u2018coupons\u201d, \u2018\u2018gifts\u201d, and \u2018\u2018station-wide discounts\u201d, according to E4 and E5, there are many misconceptions in designing promotion strategies. In addition, many existing promotion strategies are relatively similar, and some retailers simply follow their competitors without thinking about why such promotion strategies work. \u201cSome merchants have only one promotional policy, for example, \u2018always discounts\u2019,\u201d said E5. Therefore, domain experts asked our system to support a detailed exploration of the similarities and differences of common promotion strategies available on e-commerce platforms.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.",
      "data_code": { "temporal": 1, "tables": 1, "textual": 1 }
    },
    "solution": [
      {
        "solution_text": "Designing appropriate product titles is one of the most fundamental ways to attract potential customers by hitting their searches. Theresulting titles are crucial for machine learning models to distinguishbetween various products. As mentioned earlier, product titles arecomposed of descriptive words that may not be logically connected toeach other and can therefore be treated as a set of words rather thanparsed from the perspective of a complete sentence. For this purpose,we utilize a bag-of-words (BOW) [58] method to obtain the informationhidden in the titles. Specifically, we first create a codebook of all thewords that appear in product titles. Then, we split each title into a setof terms and compare them to the codebook to determine which wordsappear in the title. In this way, each title can be encoded as a vectorwith either 0 or 1, where 0 indicates the corresponding keyword in thecodebook is missing from the title and 1 indicates that the keyword ispresent in the title. It should be noted that the dimensionality of theconstructed vector is quite high for a title with only ten words, whichis too sparse for the model to learn features from the title. To solvethe sparsity problem, we follow the idea of word2vec [15] and reducethe title dimension to 8 after several experiments, i.e., we calculate thenumber of semantically valid words in 100 titles. The results show thateight words are sufficient to describe a product. Although promotions can be diverse, they usually follow some basicrules that allow us to translate textual information into numerical values.Based on these rules and the recommendations of experts, we classifyall promotions into two categories, including six promotion types (Table 1), for precise quantification. For direct discounts (value discount,percentage discount, and flash sale), we extract two key features. (1) thediscount rate kd, which is the ratio of the discounted price to originalprice; (2) the trigger amount pt , which is the minimum amount toreceive the discount. For indirect discounts (i.e., loyalty points, freeshipping, and interest-free installment), where the reward is not a directprice reduction, the reward and the original price of the product aretaken out as features. In addition, since the effect of a promotion mayexist before the campaign starts or after it ends, we track the entirelifecycle of the promotion by setting a status flag.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "To select the top five most similar products, we calculate the Euclidean distance between the target product and the other five products in the same category in the Product Overview (Fig. 2(A)).",
        "solution_category": "data_manipulation",
        "solution_axial": "SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["SimilarityCalculation"],
        "componenet_code": ["similarity_calculation"]
      },
      {
        "solution_text": "the Competitive Analysis View shows the characteristics of competing products, including their promotional information, and thus helps to compare promotion strategies and motivate domain experts to find better ones (R.1, R.4). The Competitive Analysis View (Fig. 2(D)) provides a comparison be_tween the selected product and several of the most similar products in the same category (R.1), which can be considered as potential competi_x0002_tors. In this work, experts recommend comparing at least five other products of the same type. With this view, the user can find similarities and differences between the target product and its competitors, and thus build a corresponding promotion strategy solidly and confidently. To select the top five most similar products, we calculate the Euclidean distance between the target product and the other five products in the same category in the Product Overview (Fig. 2(A)). To help users get a general idea of the products and compare the differences between them, we designed a novel glyph called product statistics glyph to encode the statistical attributes of the products (e.g., stability of sales amount, price elastics).",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Line+Text+Pie+Circle",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Circle", "Line", "Text", "Pie"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 119,
    "paper_title": "PromotionLens: Inspecting Promotion Strategies of Online E-commerce via Visual Analytics",
    "pub_year": 2023,
    "domain": "E-commerce",
    "requirement": {
      "requirement_text": "R.5: Support \u201cwhat-if\u201d promotion simulations for each commodity. According to E1, there is always a strong need and demand for promotion strategy development. Although previous studies have conducted theoretical and mathematical simulations of specific promotional behaviors, experts still wanted a system that can: enable interactive adjustment of promotional policies, demonstrate the effects of different promotion strategies on a specific commodity, and summarize the effects of promotional behaviors through the performance on various commodities. Thus, our system should support \u201cwhat-if\u201d promotion simulations to examine the model\u2019s response for each commodity.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "We used a dataset collected from a e-commerce platform with hundreds of millions of consumers, through a partnership with retail companies to cover the top 500 products in the sportswear market in terms ofsales volume over a two-and-a-half-year period from January 2019to July 2021. It is worth noting that there are three types of data: 1)Sales Volume. The daily sales volume and cumulative sales volumeof different products are recorded; (2) Product Description. Dailyinformation such as product name, price before discount, category,brand, and merchant to which the product belongs is recorded; (3)Promotions. All promotions experienced by each product are recorded,including a detailed description of the event (e.g., $10 Off Orders Over$69), the start time, and the end time.",
      "data_code": { "clusters_and_sets_and_lists": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Designing appropriate product titles is one of the most fundamental ways to attract potential customers by hitting their searches. Theresulting titles are crucial for machine learning models to distinguishbetween various products. As mentioned earlier, product titles arecomposed of descriptive words that may not be logically connected toeach other and can therefore be treated as a set of words rather thanparsed from the perspective of a complete sentence. For this purpose,we utilize a bag-of-words (BOW) [58] method to obtain the informationhidden in the titles. Specifically, we first create a codebook of all thewords that appear in product titles. Then, we split each title into a setof terms and compare them to the codebook to determine which wordsappear in the title. In this way, each title can be encoded as a vectorwith either 0 or 1, where 0 indicates the corresponding keyword in thecodebook is missing from the title and 1 indicates that the keyword ispresent in the title. It should be noted that the dimensionality of theconstructed vector is quite high for a title with only ten words, whichis too sparse for the model to learn features from the title. To solvethe sparsity problem, we follow the idea of word2vec [15] and reducethe title dimension to 8 after several experiments, i.e., we calculate thenumber of semantically valid words in 100 titles. The results show thateight words are sufficient to describe a product. Although promotions can be diverse, they usually follow some basicrules that allow us to translate textual information into numerical values.Based on these rules and the recommendations of experts, we classifyall promotions into two categories, including six promotion types (Table 1), for precise quantification. For direct discounts (value discount,percentage discount, and flash sale), we extract two key features. (1) thediscount rate kd, which is the ratio of the discounted price to originalprice; (2) the trigger amount pt , which is the minimum amount toreceive the discount. For indirect discounts (i.e., loyalty points, freeshipping, and interest-free installment), where the reward is not a directprice reduction, the reward and the original price of the product aretaken out as features. In addition, since the effect of a promotion mayexist before the campaign starts or after it ends, we track the entirelifecycle of the promotion by setting a status flag.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "To select the top five most similar products, we calculate the Euclidean distance between the target product and the other five products in the same category in the Product Overview (Fig. 2(A)).",
        "solution_category": "data_manipulation",
        "solution_axial": "SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["SimilarityCalculation"],
        "componenet_code": ["similarity_calculation"]
      },
      {
        "solution_text": "the Competitive Analysis View shows the characteristics of competing products, including their promotional information, and thus helps to compare promotion strategies and motivate domain experts to find better ones (R.1, R.4). The Competitive Analysis View (Fig. 2(D)) provides a comparison be_tween the selected product and several of the most similar products in the same category (R.1), which can be considered as potential competi_x0002_tors. In this work, experts recommend comparing at least five other products of the same type. With this view, the user can find similarities and differences between the target product and its competitors, and thus build a corresponding promotion strategy solidly and confidently. To select the top five most similar products, we calculate the Euclidean distance between the target product and the other five products in the same category in the Product Overview (Fig. 2(A)). To help users get a general idea of the products and compare the differences between them, we designed a novel glyph called product statistics glyph to encode the statistical attributes of the products (e.g., stability of sales amount, price elastics).",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Line+Text+Pie+Circle",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Circle", "Line", "Text", "Pie"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 120,
    "paper_title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics",
    "pub_year": 2023,
    "domain": "Spatiotemporal dynamics",
    "requirement": {
      "requirement_text": "R.1: Explain the temporal dynamics in the industrial and business sectors. The traditional approach is to transfer the change in information from the enterprise level to its corresponding sector level by calculating its contribution to the sector. While understanding the change in ratios for each sector provides an overview of the dynamics of all sectors over time, it only considers absolute numbers and ignores factors that may influence change in RIS dynamics. E1 is interested in explaining specific increases, outbreaks, or decreases on the time axis. Therefore, equipping the system with factor analysis allows to observe and speculate on the factors that influence the RIS temporal dynamics.",
      "requirement_code": { "identify_main_cause_aggregate": 1 }
    },
    "data": {
      "data_text": "The collaboration with the domain experts provided us with a publiclyavailable dataset of business registration records collected from theNational Enterprise Credit Information Publicity System2, coveringthe period from 1980 to 2015. Each record contains information suchas the enterprise\u2019s primary identification code, name, address, andregistered capital. Based on suggestions from domain experts, wedeveloped and studied the following attributes in this work: 1) address:it records detailed geographic information about a business; 2) startand end dates of operation: it allows us to track the the operations ofbusinesses that existed during a specific period for further analysis;and 3) basic information, including industry category, credit rating,registered capital, and enterprise status, among others. This informationcan be used to describe a specific business.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "To understand the evolutionary pattern of RIS, we transform theregistration records of existing enterprises into a time-series dataset.Then, we use a multivariant time series prediction machine learningmodel to obtain the basic features of the evolutionary patterns. Formingtime series data from raw business registration data is nontrivial due tothe large volume and complex attributes of raw business registration data. Moreover, the temporal characteristics, i.e., the start and end datesof the business, are hidden in each record independently of each other.Therefore, considering the computational efficiency and speeding upthe processing, we change the index of the data from the main identitycode of the enterprise to the date and construct a hash table to reorganizethe original information. Before modeling the time series data, severalnon-numerical dimensions, such as industry category and credit rating,must be quantified. Since these attributes have a limited and relativelyfixed range of values, it is acceptable to map them to numbers in orderto maintain the information encoded using one-hot encoding.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Dimensionality reduction techniques such as t-SNE, PCA, and MDS have been widely adopted to create lowdimensional representations that preserve local similarity to express neighborhood structure [24,54]. We follow the conventional practice of projecting all RIS snapshots for the entire period into two-dimensional space to see potential clusters and outliers. After discussions with experts, we use the following metrics, namely, enterprise classification code, registered capital, credit rating, enterprise property, and enterprise state, to evaluate the RIS snapshots at the overview level.  We use t-SNE as the dimensional projection because \u201cit reveals meaningful insights about the data and shows superiority in generating two-dimensional projection\u201d [40]. ",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "we design the RIS Projection Overview to obtain the overall distribution of RIS patterns over the entire periods (R.1). The RIS Projection View helps to identify potential anomalies and \u201cclusters\u201d in RIS snapshots. Dimensionality reduction techniques such as t-SNE, PCA, and MDS have been widely adopted to create lowdimensional representations that preserve local similarity to express neighborhood structure [24,54]. We follow the conventional practice of projecting all RIS snapshots for the entire period into two-dimensional space to see potential clusters and outliers. After discussions with experts, we use the following metrics, namely, enterprise classification code, registered capital, credit rating, enterprise property, and enterprise state, to evaluate the RIS snapshots at the overview level. After obtaining the above metrics, we can obtain the corresponding feature vectors for each RIS snapshot. We use t-SNE as the dimensional projection because \u201cit reveals meaningful insights about the data and shows superiority in generating two-dimensional projection\u201d [40]. Similar to [63], as shown in Fig. 3, each RIS snapshot in two-dimensional space is represented by a point, and the color indicates the snapshot. The first and last snapshots are highlighted (red for the first snapshot and blue for the last) and a black curve connects all snapshots in chronological order. The user can hover over any particular RIS snapshot and a tooltip will display a detailed timestamp. The user can lasso any entity on the projection space for interaction and for further \u201clink + view\u201d analysis.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Scatter",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Scatter"]
      },
      {
        "solution_text": "The user can hover over any particular RIS snapshot and a tooltip will display a detailed timestamp. The user can lasso any entity on the projection space for interaction and for further \u201clink + view\u201d analysis.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 121,
    "paper_title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics",
    "pub_year": 2023,
    "domain": "Spatiotemporal dynamics",
    "requirement": {
      "requirement_text": "R.2: Explore general patterns and potential outliers in the dynamics of sector change. Experts need to quickly browse through large amounts of enterprise data to identify areas of interest. For example, sectors with unprecedented growth are more likely to attract the expert\u2019s attention. Further investigation is needed to explain general patterns and potential outliers across historical periods. For example, E1 \u2013 2 would like to find out if there are standard characteristics and outliers between industrial sectors and the possible reasons behind them.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The collaboration with the domain experts provided us with a publiclyavailable dataset of business registration records collected from theNational Enterprise Credit Information Publicity System2, coveringthe period from 1980 to 2015. Each record contains information suchas the enterprise\u2019s primary identification code, name, address, andregistered capital. Based on suggestions from domain experts, wedeveloped and studied the following attributes in this work: 1) address:it records detailed geographic information about a business; 2) startand end dates of operation: it allows us to track the the operations ofbusinesses that existed during a specific period for further analysis;and 3) basic information, including industry category, credit rating,registered capital, and enterprise status, among others. This informationcan be used to describe a specific business.",
      "data_code": { "tables": 1, "temporal": 1, "sequential": 1 }
    },
    "solution": [
      {
        "solution_text": "To understand the evolutionary pattern of RIS, we transform theregistration records of existing enterprises into a time-series dataset.Then, we use a multivariant time series prediction machine learningmodel to obtain the basic features of the evolutionary patterns. Formingtime series data from raw business registration data is nontrivial due tothe large volume and complex attributes of raw business registration data. Moreover, the temporal characteristics, i.e., the start and end datesof the business, are hidden in each record independently of each other.Therefore, considering the computational efficiency and speeding upthe processing, we change the index of the data from the main identitycode of the enterprise to the date and construct a hash table to reorganizethe original information. Before modeling the time series data, severalnon-numerical dimensions, such as industry category and credit rating,must be quantified. Since these attributes have a limited and relativelyfixed range of values, it is acceptable to map them to numbers in orderto maintain the information encoded using one-hot encoding.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "We define the input to a time series prediction machine learning modelas a sequence of historical data Xn = xn\u2212L,xn\u2212(L\u22121),...,xn\u22121,xn, whereL is the parameter that is adapted for different datasets and xn is a multidimensional feature vector with timestamp n (i.e., the feature vectorconsists of 7 dimensions, including year, month, enterprise classification code, registered capital, credit rating, enterprise property, andenterprise state) denoted as x fn \u2208 R. The goal of a multivariate timeseries forecasting models is to predict specific values at some futuretime stamp. In our case, we provide only a single-step forecast with anoutput market of yn = xn+1. Considering that different countries dividethe industrial structure in different ways, but basically divided into threemain categories, we first divide the original dataset into three significantindustries based on industry category, namely, primary industry, secondary industry, and tertiary industry. Then, we calculate the numberof enterprises existing in each month from 1980 to 2015. We use thedata from 1980 to 1990 as the first training set to predict the trend in1991, followed by the data before 1991 as the training set to predictthe trend for 1992, and so on. The reason for performing stepwiseforecasting is based on the practical requirements of most real-worldtime series forecasting tasks. Thus, the final prediction curve is spelledout for each time unit. We use the classical MAPE metric to evaluatethe performance of the forecasting model. There are several representative time series forecasting models available for evaluation, such asArima [61], Vector Auto-Regression Model (VAR) [46], Random ForestModel (RF) or Random Decision Forests [26] and Long Short-TermMemory Recurrent Neural Networks (LSTM), covering linear, nonlinearregression methods, and machine learning models [64]. In this work,we considered both model performance and model interpretability, andfinally select two machine learning models for multivariate time seriesprediction, namely, RF and XGBoost [13], as they have sufficientlyhigh model accuracy and good interpretability.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "The Registration Prediction View captures the correlation between the predicted registrations and ground truth over time, and understands the latent features driving the changes from the perspective of the time series forecasting models (R.2 \u2013 R.3). The Registration Prediction View (Fig. 2) helps experts understand the detailed associations between RIS influencing factors and registration amounts based on several representative time-series forecasting models. It consists of three subplots, arranged from top to bottom, representing business activities in the primary, secondary and tertiary sectors, respectively. In each subplot, the dotted line represents the ground truth registration of enterprises. The solid black line represents the predicted business activity compared to the ground truth volume. Thus, users can observe the gap that represents the difference between the actual and the predicted value at different timestamps. In addition, the importance of the 7 features is calculated, normalized, and represented with different classification colors stacked on top of each other. In each snapshot, the glyphs of feature importance are placed along the Y-axis. Each bar in the stacked glyphs represents a feature. Bars stacked above the predicted values represent features with negative effects, pushing the predicted value down [64]. The features stacked below the predicted value are positive, pushing the predicted value higher. The user can select other models on the right side for comparison.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Bar",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Bar"]
      },
      {
        "solution_text": "The Registration Prediction View captures the correlation between the predicted registrations and ground truth over time, and understands the latent features driving the changes from the perspective of the time series forecasting models (R.2 \u2013 R.3). The Registration Prediction View (Fig. 2) helps experts understand the detailed associations between RIS influencing factors and registration amounts based on several representative time-series forecasting models. It consists of three subplots, arranged from top to bottom, representing business activities in the primary, secondary and tertiary sectors, respectively. In each subplot, the dotted line represents the ground truth registration of enterprises. The solid black line represents the predicted business activity compared to the ground truth volume. Thus, users can observe the gap that represents the difference between the actual and the predicted value at different timestamps. In addition, the importance of the 7 features is calculated, normalized, and represented with different classification colors stacked on top of each other. In each snapshot, the glyphs of feature importance are placed along the Y-axis. Each bar in the stacked glyphs represents a feature. Bars stacked above the predicted values represent features with negative effects, pushing the predicted value down [64]. The features stacked below the predicted value are positive, pushing the predicted value higher. The user can select other models on the right side for comparison.",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Connect",
        "solution_compoent": "",
        "axial_code": ["Selecting", "Connect"],
        "componenet_code": ["selecting", "connect/relate"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 122,
    "paper_title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics",
    "pub_year": 2023,
    "domain": "Spatiotemporal dynamics",
    "requirement": {
      "requirement_text": "R.3: Predict the future composition of the RIS and explore the influencing factors behind it. Another conventional approach to RIS analysis is to understand the composition of the industrial sector, for example, the ratio and the appropriate number of establishments. Based on the history and current status of RIS, experts want to know the future trends  and influencing factors of the quality of the composition of each industrial sector. Thus, they can make speculations and assumptions about the future before developing the corresponding strategies.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The collaboration with the domain experts provided us with a publiclyavailable dataset of business registration records collected from theNational Enterprise Credit Information Publicity System2, coveringthe period from 1980 to 2015. Each record contains information suchas the enterprise\u2019s primary identification code, name, address, andregistered capital. Based on suggestions from domain experts, wedeveloped and studied the following attributes in this work: 1) address:it records detailed geographic information about a business; 2) startand end dates of operation: it allows us to track the the operations ofbusinesses that existed during a specific period for further analysis;and 3) basic information, including industry category, credit rating,registered capital, and enterprise status, among others. This informationcan be used to describe a specific business.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "To understand the evolutionary pattern of RIS, we transform theregistration records of existing enterprises into a time-series dataset.Then, we use a multivariant time series prediction machine learningmodel to obtain the basic features of the evolutionary patterns. Formingtime series data from raw business registration data is nontrivial due tothe large volume and complex attributes of raw business registration data. Moreover, the temporal characteristics, i.e., the start and end datesof the business, are hidden in each record independently of each other.Therefore, considering the computational efficiency and speeding upthe processing, we change the index of the data from the main identitycode of the enterprise to the date and construct a hash table to reorganizethe original information. Before modeling the time series data, severalnon-numerical dimensions, such as industry category and credit rating,must be quantified. Since these attributes have a limited and relativelyfixed range of values, it is acceptable to map them to numbers in orderto maintain the information encoded using one-hot encoding.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "We define the input to a time series prediction machine learning modelas a sequence of historical data Xn = xn\u2212L,xn\u2212(L\u22121),...,xn\u22121,xn, whereL is the parameter that is adapted for different datasets and xn is a multidimensional feature vector with timestamp n (i.e., the feature vectorconsists of 7 dimensions, including year, month, enterprise classification code, registered capital, credit rating, enterprise property, andenterprise state) denoted as x fn \u2208 R. The goal of a multivariate timeseries forecasting models is to predict specific values at some futuretime stamp. In our case, we provide only a single-step forecast with anoutput market of yn = xn+1. Considering that different countries dividethe industrial structure in different ways, but basically divided into threemain categories, we first divide the original dataset into three significantindustries based on industry category, namely, primary industry, secondary industry, and tertiary industry. Then, we calculate the numberof enterprises existing in each month from 1980 to 2015. We use thedata from 1980 to 1990 as the first training set to predict the trend in1991, followed by the data before 1991 as the training set to predictthe trend for 1992, and so on. The reason for performing stepwiseforecasting is based on the practical requirements of most real-worldtime series forecasting tasks. Thus, the final prediction curve is spelledout for each time unit. We use the classical MAPE metric to evaluatethe performance of the forecasting model. There are several representative time series forecasting models available for evaluation, such asArima [61], Vector Auto-Regression Model (VAR) [46], Random ForestModel (RF) or Random Decision Forests [26] and Long Short-TermMemory Recurrent Neural Networks (LSTM), covering linear, nonlinearregression methods, and machine learning models [64]. In this work,we considered both model performance and model interpretability, andfinally select two machine learning models for multivariate time seriesprediction, namely, RF and XGBoost [13], as they have sufficientlyhigh model accuracy and good interpretability.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "The Registration Prediction View captures the correlation between the predicted registrations and ground truth over time, and understands the latent features driving the changes from the perspective of the time series forecasting models (R.2 \u2013 R.3). The Registration Prediction View (Fig. 2) helps experts understand the detailed associations between RIS influencing factors and registration amounts based on several representative time-series forecasting models. It consists of three subplots, arranged from top to bottom, representing business activities in the primary, secondary and tertiary sectors, respectively. In each subplot, the dotted line represents the ground truth registration of enterprises. The solid black line represents the predicted business activity compared to the ground truth volume. Thus, users can observe the gap that represents the difference between the actual and the predicted value at different timestamps. In addition, the importance of the 7 features is calculated, normalized, and represented with different classification colors stacked on top of each other. In each snapshot, the glyphs of feature importance are placed along the Y-axis. Each bar in the stacked glyphs represents a feature. Bars stacked above the predicted values represent features with negative effects, pushing the predicted value down [64]. The features stacked below the predicted value are positive, pushing the predicted value higher. The user can select other models on the right side for comparison.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Bar",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Bar"]
      },
      {
        "solution_text": "The Registration Prediction View captures the correlation between the predicted registrations and ground truth over time, and understands the latent features driving the changes from the perspective of the time series forecasting models (R.2 \u2013 R.3). The Registration Prediction View (Fig. 2) helps experts understand the detailed associations between RIS influencing factors and registration amounts based on several representative time-series forecasting models. It consists of three subplots, arranged from top to bottom, representing business activities in the primary, secondary and tertiary sectors, respectively. In each subplot, the dotted line represents the ground truth registration of enterprises. The solid black line represents the predicted business activity compared to the ground truth volume. Thus, users can observe the gap that represents the difference between the actual and the predicted value at different timestamps. In addition, the importance of the 7 features is calculated, normalized, and represented with different classification colors stacked on top of each other. In each snapshot, the glyphs of feature importance are placed along the Y-axis. Each bar in the stacked glyphs represents a feature. Bars stacked above the predicted values represent features with negative effects, pushing the predicted value down [64]. The features stacked below the predicted value are positive, pushing the predicted value higher. The user can select other models on the right side for comparison.",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Connect",
        "solution_compoent": "",
        "axial_code": ["Selecting", "Connect"],
        "componenet_code": ["selecting", "connect/relate"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 123,
    "paper_title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics",
    "pub_year": 2023,
    "domain": "Spatiotemporal dynamics",
    "requirement": {
      "requirement_text": "R.4: Summarize the dynamics and multivariance of enterprises. As mentioned earlier, directly mapping millions of enterprises on a geographic map without any abstraction or simplification is bound to create visual clutter, compromise effective understanding, and affect the performance of the system. In addition, as time goes on, more and more new businesses are being established and added to the existing environment. Integrating these factors requires careful design to summarize the dynamics and multiple attributes of large-scale enterprises.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The collaboration with the domain experts provided us with a publiclyavailable dataset of business registration records collected from theNational Enterprise Credit Information Publicity System2, coveringthe period from 1980 to 2015. Each record contains information suchas the enterprise\u2019s primary identification code, name, address, andregistered capital. Based on suggestions from domain experts, wedeveloped and studied the following attributes in this work: 1) address:it records detailed geographic information about a business; 2) startand end dates of operation: it allows us to track the the operations ofbusinesses that existed during a specific period for further analysis;and 3) basic information, including industry category, credit rating,registered capital, and enterprise status, among others. This informationcan be used to describe a specific business.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "To understand the evolutionary pattern of RIS, we transform theregistration records of existing enterprises into a time-series dataset.Then, we use a multivariant time series prediction machine learningmodel to obtain the basic features of the evolutionary patterns. Formingtime series data from raw business registration data is nontrivial due tothe large volume and complex attributes of raw business registration data. Moreover, the temporal characteristics, i.e., the start and end datesof the business, are hidden in each record independently of each other.Therefore, considering the computational efficiency and speeding upthe processing, we change the index of the data from the main identitycode of the enterprise to the date and construct a hash table to reorganizethe original information. Before modeling the time series data, severalnon-numerical dimensions, such as industry category and credit rating,must be quantified. Since these attributes have a limited and relativelyfixed range of values, it is acceptable to map them to numbers in orderto maintain the information encoded using one-hot encoding.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Two challenges exist in tracking the evolution of RIS over long periodsof time and large geographic scales. First, correctly delineating longtime periods is critical to identifying potential patterns in the RIS evolution as reflected by business registration activity. Simply retrievingrecords by year may lead to duplicate analyses if no significant changesoccur in the two subsequent periods. Second, even if we manage todivide the evolution of the RIS into different intervals, for each intervalwe are still confronted with a large amount of data that shows differentspatial locations by the records of the enterprises belonging to that interval. It is important to abstract the characteristics of the correspondingenterprises while maintaining dynamic and multi-attribute details. Inthe following subsections, we perform spatio-temporal clustering interms of time series segmentation and geospatial clustering.Partitioning of Time Series. To address the first challenge, weformulate the partitioning of long periods of RIS evolution as a segmentation problem, which partitions long time series into segments thatcan be formulated as follows: given a time series T, generate the bestrepresentation such that the combined errors (which can be obtainedby calculating the sum of the maximum error for all segments) is lessthan a user-specified threshold. Generating segments for such timeseries data is the key to an efficient and effective solution [36]. Notably,we take a top-down approach, considering every possible partition ofthe time series and segmenting it at the optimal location. The twosubsegments are then tested to see if their approximation error is belowa user-specified threshold. If not, the algorithm recursively splits thesubsequence until the approximation error of all segments is below thethreshold. Thus, the original time series registration is divided intoseveral piecewise linear representations after time series segmentation.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Geospatial Clustering. To address the second challenge of the various enterprise activities within each segment, we analyze the businessdata from a cluster perspective rather than an individual perspective.The goal of clustering is to make the similarity between objects withinthe same category as large as possible and, conversely, the similaritybetween different categories as small as possible. It is worth noting thatwe tried to cluster individual enterprises belonging to different time periods using the division-based KMeans algorithm and the density-basedclustering algorithm DBSCAN, respectively. However, they both exhibitspecific advantages and disadvantages. For example, although KMeanscan achieve clustering results, the determination of K is quite challenging because the location of enterprises is unpredictable. In addition,KMeans is developed based on the centroid location of the aggregatedclusters rather than the actual geographic locations of enterprises. Onthe other hand, DBSCAN requires manual input of two parameters,namely [E ps] and [minPts], so the accuracy of the clustering resultsdirectly depends on the users\u2019 parameter selection. Also, the obtainedclusters are spatially arbitrary shaped and are just sets of geographicalpoints rather than aggregated clusters. In addition, DBSCAN suffers from indistinguishable noisy data. For these reasons, we propose ahybrid algorithm based on DBSCAN and KMeans algorithms. First, thegeographic locations of enterprises are clustered into several clustersusing the density reachability of DBSCAN according to the specificsettings of the two parameters. The data in each cluster is taken asa new input. The centroid positions are then obtained using KMeansto minimize the Sum of Squared Error (SSE) between the data pointsin each cluster and the centroids of the clusters they are in using theiterative aggregation of KMeans and K is set to 1. In addition, we useKANN-DBSCAN [41], to automatically find a stable interval of clusternumber variation by generating candidate [E ps] and [MinPts] usingthe distribution characteristics of the dataset. We take the [E ps] and[MinPts] parameters corresponding to the minimum density thresholdof this interval as the optimal parameters. Specifically, first, the corresponding [E ps] candidate list can be obtained using KANN-DBSCAN.Then, for the given [E ps] candidate list, the number of neighboringobjects can be calculated sequentially and the expected values of these[E ps] are used as the [MinPts] candidate list: MinPts = 1n \u2211ni=1 Pi, Piis the number of [E ps] neighboring objects of the ith object, and n isthe total number of objects of the data. Finally, the list of these twoparameters is given as input to DBSCAN, and then the number of clusters generated with different parameter settings is obtained separatelyand quickly. The result can be considered stable when the number ofclusters is the same three times in a row. After deciding the clusters, we need to further define the centroids of each cluster. To summarize, we first apply KANN-DBSCAN to cluster the geographic locations of enter_x0002_prises into several clusters, and then use KMeans to find the centroid location of each cluster.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "We apply the classical map-centric visual exploration approach torepresent clusters of enterprise geographic locations. As shown inFig. 6, we design radar-like glyphs at the top of the map and placethem in the corresponding geographic regions of the regional clustersgenerated by the geospatial clustering described above. The indicatorsrepresented by the axes in the radar-like glyphs are the same as those inthe RIS evolution view, namely, aggregation index, average registeredcapital, total registered capital, credit rating, livability, and mortality.We did not choose a bar chart because we need to have the radius of theglyphs represent the number of enterprises in the cluster. In addition, inthe Regional Comparative View (see Section 6.5), we need to comparethe same indicators between two or three clusters, and radar-like glyphsare easier to perceive than bar charts for comparison.Note that we place all the regional clusters generated by DBSCANand KMeans on the map at different time intervals, inevitably causingvisual clutter and overlap problems. Therefore, we use a force-directedlayout to pack those overlapping clusters [15]. As shown in Fig. 6(a), aphysics-based simulator will be used to find the optimal circle positionsby 1) optimizing the distance between the circle and the force center,2) attracting each other slightly, and 3) avoiding overlap. We warp thepacked circles with an additional black ring (Fig. 6(a)), indicating thatthese groups of regions have some kind of unavoidable overlap. Toemphasize the representation of region groups from a particular timesnapshot and inspired by [3], we borrow the Bubble Sets technique [14]to provide continuous boundary contours that allow us to examine scenarios with semantically important spatial organization and importantset membership relationships for enterprises belonging to the snapshot.Notably, as shown in Fig. 6(c), the five colored bubble sets representthe geospatial locations in the five snapshots. We also provide regional evolution paths in the form of black curves that connect clusters ofregions in different snapshots in chronological order (Fig. 6(b)).To elaborate more detailed information about region-specific clusters,we provide a separate panel (Fig. 6(1 \u2013 5)) showing the geospatialmetrics (Fig. 6(1)), registration (Fig. 6(2)), livability (Fig. 6(3)), andbusiness category (Fig. 6(4)). The actual geographical coverage of theregional clusters is displayed as a heat map (Fig. 6(5)), where domainexperts can observe the most dense points and the scale of distribution.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Providing",
        "solution_compoent": "Map+Pie+Area",
        "axial_code": ["Overlay-Coordinatesystemrelated-Providing"],
        "componenet_code": ["Pie", "Area", "Map"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 124,
    "paper_title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics",
    "pub_year": 2023,
    "domain": "Spatiotemporal dynamics",
    "requirement": {
      "requirement_text": "R.4: Summarize the dynamics and multivariance of enterprises. As mentioned earlier, directly mapping millions of enterprises on a geographic map without any abstraction or simplification is bound to create visual clutter, compromise effective understanding, and affect the performance of the system. In addition, as time goes on, more and more new businesses are being established and added to the existing environment. Integrating these factors requires careful design to summarize the dynamics and multiple attributes of large-scale enterprises.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The collaboration with the domain experts provided us with a publiclyavailable dataset of business registration records collected from theNational Enterprise Credit Information Publicity System2, coveringthe period from 1980 to 2015. Each record contains information suchas the enterprise\u2019s primary identification code, name, address, andregistered capital. Based on suggestions from domain experts, wedeveloped and studied the following attributes in this work: 1) address:it records detailed geographic information about a business; 2) startand end dates of operation: it allows us to track the the operations ofbusinesses that existed during a specific period for further analysis;and 3) basic information, including industry category, credit rating,registered capital, and enterprise status, among others. This informationcan be used to describe a specific business.",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "To understand the evolutionary pattern of RIS, we transform theregistration records of existing enterprises into a time-series dataset.Then, we use a multivariant time series prediction machine learningmodel to obtain the basic features of the evolutionary patterns. Formingtime series data from raw business registration data is nontrivial due tothe large volume and complex attributes of raw business registration data. Moreover, the temporal characteristics, i.e., the start and end datesof the business, are hidden in each record independently of each other.Therefore, considering the computational efficiency and speeding upthe processing, we change the index of the data from the main identitycode of the enterprise to the date and construct a hash table to reorganizethe original information. Before modeling the time series data, severalnon-numerical dimensions, such as industry category and credit rating,must be quantified. Since these attributes have a limited and relativelyfixed range of values, it is acceptable to map them to numbers in orderto maintain the information encoded using one-hot encoding.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "The RIS Evolution View explores the regional features and their evolutionary paths over time (R.4). Although the above RIS Projection View conveys additional information about clusters and potential outliers in all RIS snapshots, it is still unclear how the RIS patterns evolve at the regional level. In addition, domain experts often need to explore the properties of each regional cluster in order to compare and evaluate them in one or more temporal snapshots, which can help them to assess the performance of RIS. Therefore, based on discussions with experts, we adopt and standardize the following indicators for a uniform comparison: 1) number of primary industries; 2) number of secondary industries; 3) number of tertiary industries; 4) aggregation index (AI(x)), which is calculated by the Coefficient of Variation [19], to measure the similarity of the distribution of indicators; 5) average registered capital; 6) total registered capital; 7) credit rating; 8) livability, i.e., the number of surviving enterprises divided by the total number of enterprises; and 9) mortality, complementary to livability.  we design a RIS Evolution View to help domain experts to compare metrics at the regional level. Notably, as shown in Fig. 5, we present the metric values for each region cluster as a combined bar, where the length of a single colored bar indicates the normalized metric value for the corresponding region cluster. We rank the region clusters generated in all consecutive snapshots. The ranking is based on the value of a specific metric. When a region cluster is clicked, all \u201cidentical\u201d region clusters across time snapshots are highlighted. We determine whether a regional cluster is identical between two subsequent snapshots based on the amount of overlaps, and we only depict the evolutionary path between two subsequent \u201cidentical\u201d region clusters with the largest number of overlaps. In the two subsequent snapshots, we place an axis with two different scales along the left and right sides of the axis. The left axis represents the scale of the number of enterprises that transition from one regional cluster to another in the next period. The right axis represents the scale of the distance between the two centroids of the regional clusters in the two subsequent snapshots. For example, as shown in in Fig. 5(1), these two figures indicate that 155,950 enterprises in one regional cluster between 1992 and 2006 stay in the other regional cluster between 2006 and 2012, while the distance between the centroids of the two regional clusters is 1.87km each. Intuitively, a small value indicates that the main distribution of enterprises in the early snapshot is largely consistent with the distribution of enterprises, including the existing enterprises in the early snapshot and newly established ones in the later snapshot.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Bar",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Bar"]
      },
      {
        "solution_text": "When a region cluster is clicked, all \u201cidentical\u201d region clusters across time snapshots are highlighted. ",
        "solution_category": "interaction",
        "solution_axial": "Filtering+Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting", "Filtering"],
        "componenet_code": ["selecting", "filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 125,
    "paper_title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics",
    "pub_year": 2023,
    "domain": "Spatiotemporal dynamics",
    "requirement": {
      "requirement_text": "R.5: Reveal the regional \u201cagglomeration-performance\u201d relationship resided in various groups of enterprises. According to E3 \u2013 4, enterprise-level performance may be strongly related to the geospatial locations of agglomerations. Although theoretical studies have shown predominantly on positive performance effects as an incentive for enterprise collocation \u201cto explain the emergence of agglomerations\u201d, experts have also focused on adverse performance effects, \u201csome enterprises may benefit from agglomeration, while others may be harmed by aggregation and relocate elsewhere,\u201d said E4. Thus, in the face of these conflicting empirical findings, the net performance effects of enterprises located in geographic agglomerations remain ambiguous, calling for clarification of, e.g., \u201cthe agglomeration-performance\u201d relationship hidden in the large-scale activities of enterprises.",
      "requirement_code": {
        "discover_observation": 1,
        "explain_differences": 1
      }
    },
    "data": {
      "data_text": "The collaboration with the domain experts provided us with a publiclyavailable dataset of business registration records collected from theNational Enterprise Credit Information Publicity System2, coveringthe period from 1980 to 2015. Each record contains information suchas the enterprise\u2019s primary identification code, name, address, andregistered capital. Based on suggestions from domain experts, wedeveloped and studied the following attributes in this work: 1) address:it records detailed geographic information about a business; 2) startand end dates of operation: it allows us to track the the operations ofbusinesses that existed during a specific period for further analysis;and 3) basic information, including industry category, credit rating,registered capital, and enterprise status, among others. This informationcan be used to describe a specific business.",
      "data_code": {
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "To understand the evolutionary pattern of RIS, we transform theregistration records of existing enterprises into a time-series dataset.Then, we use a multivariant time series prediction machine learningmodel to obtain the basic features of the evolutionary patterns. Formingtime series data from raw business registration data is nontrivial due tothe large volume and complex attributes of raw business registration data. Moreover, the temporal characteristics, i.e., the start and end datesof the business, are hidden in each record independently of each other.Therefore, considering the computational efficiency and speeding upthe processing, we change the index of the data from the main identitycode of the enterprise to the date and construct a hash table to reorganizethe original information. Before modeling the time series data, severalnon-numerical dimensions, such as industry category and credit rating,must be quantified. Since these attributes have a limited and relativelyfixed range of values, it is acceptable to map them to numbers in orderto maintain the information encoded using one-hot encoding.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "Two challenges exist in tracking the evolution of RIS over long periodsof time and large geographic scales. First, correctly delineating longtime periods is critical to identifying potential patterns in the RIS evolution as reflected by business registration activity. Simply retrievingrecords by year may lead to duplicate analyses if no significant changesoccur in the two subsequent periods. Second, even if we manage todivide the evolution of the RIS into different intervals, for each intervalwe are still confronted with a large amount of data that shows differentspatial locations by the records of the enterprises belonging to that interval. It is important to abstract the characteristics of the correspondingenterprises while maintaining dynamic and multi-attribute details. Inthe following subsections, we perform spatio-temporal clustering interms of time series segmentation and geospatial clustering.Partitioning of Time Series. To address the first challenge, weformulate the partitioning of long periods of RIS evolution as a segmentation problem, which partitions long time series into segments thatcan be formulated as follows: given a time series T, generate the bestrepresentation such that the combined errors (which can be obtainedby calculating the sum of the maximum error for all segments) is lessthan a user-specified threshold. Generating segments for such timeseries data is the key to an efficient and effective solution [36]. Notably,we take a top-down approach, considering every possible partition ofthe time series and segmenting it at the optimal location. The twosubsegments are then tested to see if their approximation error is belowa user-specified threshold. If not, the algorithm recursively splits thesubsequence until the approximation error of all segments is below thethreshold. Thus, the original time series registration is divided intoseveral piecewise linear representations after time series segmentation.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Geospatial Clustering. To address the second challenge of the various enterprise activities within each segment, we analyze the businessdata from a cluster perspective rather than an individual perspective.The goal of clustering is to make the similarity between objects withinthe same category as large as possible and, conversely, the similaritybetween different categories as small as possible. It is worth noting thatwe tried to cluster individual enterprises belonging to different time periods using the division-based KMeans algorithm and the density-basedclustering algorithm DBSCAN, respectively. However, they both exhibitspecific advantages and disadvantages. For example, although KMeanscan achieve clustering results, the determination of K is quite challenging because the location of enterprises is unpredictable. In addition,KMeans is developed based on the centroid location of the aggregatedclusters rather than the actual geographic locations of enterprises. Onthe other hand, DBSCAN requires manual input of two parameters,namely [E ps] and [minPts], so the accuracy of the clustering resultsdirectly depends on the users\u2019 parameter selection. Also, the obtainedclusters are spatially arbitrary shaped and are just sets of geographicalpoints rather than aggregated clusters. In addition, DBSCAN suffers from indistinguishable noisy data. For these reasons, we propose ahybrid algorithm based on DBSCAN and KMeans algorithms. First, thegeographic locations of enterprises are clustered into several clustersusing the density reachability of DBSCAN according to the specificsettings of the two parameters. The data in each cluster is taken asa new input. The centroid positions are then obtained using KMeansto minimize the Sum of Squared Error (SSE) between the data pointsin each cluster and the centroids of the clusters they are in using theiterative aggregation of KMeans and K is set to 1. In addition, we useKANN-DBSCAN [41], to automatically find a stable interval of clusternumber variation by generating candidate [E ps] and [MinPts] usingthe distribution characteristics of the dataset. We take the [E ps] and[MinPts] parameters corresponding to the minimum density thresholdof this interval as the optimal parameters. Specifically, first, the corresponding [E ps] candidate list can be obtained using KANN-DBSCAN.Then, for the given [E ps] candidate list, the number of neighboringobjects can be calculated sequentially and the expected values of these[E ps] are used as the [MinPts] candidate list: MinPts = 1n \u2211ni=1 Pi, Piis the number of [E ps] neighboring objects of the ith object, and n isthe total number of objects of the data. Finally, the list of these twoparameters is given as input to DBSCAN, and then the number of clusters generated with different parameter settings is obtained separatelyand quickly. The result can be considered stable when the number ofclusters is the same three times in a row. After deciding the clusters, we need to further define the centroids of each cluster. To summarize, we first apply KANN-DBSCAN to cluster the geographic locations of enter_x0002_prises into several clusters, and then use KMeans to find the centroid location of each cluster.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "We apply the classical map-centric visual exploration approach torepresent clusters of enterprise geographic locations. As shown inFig. 6, we design radar-like glyphs at the top of the map and placethem in the corresponding geographic regions of the regional clustersgenerated by the geospatial clustering described above. The indicatorsrepresented by the axes in the radar-like glyphs are the same as those inthe RIS evolution view, namely, aggregation index, average registeredcapital, total registered capital, credit rating, livability, and mortality.We did not choose a bar chart because we need to have the radius of theglyphs represent the number of enterprises in the cluster. In addition, inthe Regional Comparative View (see Section 6.5), we need to comparethe same indicators between two or three clusters, and radar-like glyphsare easier to perceive than bar charts for comparison.Note that we place all the regional clusters generated by DBSCANand KMeans on the map at different time intervals, inevitably causingvisual clutter and overlap problems. Therefore, we use a force-directedlayout to pack those overlapping clusters [15]. As shown in Fig. 6(a), aphysics-based simulator will be used to find the optimal circle positionsby 1) optimizing the distance between the circle and the force center,2) attracting each other slightly, and 3) avoiding overlap. We warp thepacked circles with an additional black ring (Fig. 6(a)), indicating thatthese groups of regions have some kind of unavoidable overlap. Toemphasize the representation of region groups from a particular timesnapshot and inspired by [3], we borrow the Bubble Sets technique [14]to provide continuous boundary contours that allow us to examine scenarios with semantically important spatial organization and importantset membership relationships for enterprises belonging to the snapshot.Notably, as shown in Fig. 6(c), the five colored bubble sets representthe geospatial locations in the five snapshots. We also provide regional evolution paths in the form of black curves that connect clusters ofregions in different snapshots in chronological order (Fig. 6(b)).To elaborate more detailed information about region-specific clusters,we provide a separate panel (Fig. 6(1 \u2013 5)) showing the geospatialmetrics (Fig. 6(1)), registration (Fig. 6(2)), livability (Fig. 6(3)), andbusiness category (Fig. 6(4)). The actual geographical coverage of theregional clusters is displayed as a heat map (Fig. 6(5)), where domainexperts can observe the most dense points and the scale of distribution.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Providing",
        "solution_compoent": "Map+Pie+Area",
        "axial_code": ["Overlay-Coordinatesystemrelated-Providing"],
        "componenet_code": ["Pie", "Area", "Map"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 126,
    "paper_title": "RISeer: Inspecting the Status and Dynamics of Regional Industrial Structure via Visual Analytics",
    "pub_year": 2023,
    "domain": "Spatiotemporal dynamics",
    "requirement": {
      "requirement_text": "R.6: Compare and track performance at the region level. Having summarized the dynamics and multiple attributes of enterprise at the regional level, experts want to conduct in-depth analyses of RIS, such as determining regional performance, comparing multiple regions at a specific point in time, and tracking the evolution of a particular region over a long period of time. For example, comparing two regions at a specific timestamp makes it possible to identify differences in their positioning, while tracking their performance over time can facilitate research into the reasons behind booms or busts.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The collaboration with the domain experts provided us with a publiclyavailable dataset of business registration records collected from theNational Enterprise Credit Information Publicity System2, coveringthe period from 1980 to 2015. Each record contains information suchas the enterprise\u2019s primary identification code, name, address, andregistered capital. Based on suggestions from domain experts, wedeveloped and studied the following attributes in this work: 1) address:it records detailed geographic information about a business; 2) startand end dates of operation: it allows us to track the the operations ofbusinesses that existed during a specific period for further analysis;and 3) basic information, including industry category, credit rating,registered capital, and enterprise status, among others. This informationcan be used to describe a specific business.",
      "data_code": { "tables": 1, "clusters_and_sets_and_lists": 1 }
    },
    "solution": [
      {
        "solution_text": "To understand the evolutionary pattern of RIS, we transform theregistration records of existing enterprises into a time-series dataset.Then, we use a multivariant time series prediction machine learningmodel to obtain the basic features of the evolutionary patterns. Formingtime series data from raw business registration data is nontrivial due tothe large volume and complex attributes of raw business registration data. Moreover, the temporal characteristics, i.e., the start and end datesof the business, are hidden in each record independently of each other.Therefore, considering the computational efficiency and speeding upthe processing, we change the index of the data from the main identitycode of the enterprise to the date and construct a hash table to reorganizethe original information. Before modeling the time series data, severalnon-numerical dimensions, such as industry category and credit rating,must be quantified. Since these attributes have a limited and relativelyfixed range of values, it is acceptable to map them to numbers in orderto maintain the information encoded using one-hot encoding.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification",
        "solution_compoent": "",
        "axial_code": ["Rectification"],
        "componenet_code": ["rectification"]
      },
      {
        "solution_text": "We define the input to a time series prediction machine learning modelas a sequence of historical data Xn = xn\u2212L,xn\u2212(L\u22121),...,xn\u22121,xn, whereL is the parameter that is adapted for different datasets and xn is a multidimensional feature vector with timestamp n (i.e., the feature vectorconsists of 7 dimensions, including year, month, enterprise classification code, registered capital, credit rating, enterprise property, andenterprise state) denoted as x fn \u2208 R. The goal of a multivariate timeseries forecasting models is to predict specific values at some futuretime stamp. In our case, we provide only a single-step forecast with anoutput market of yn = xn+1. Considering that different countries dividethe industrial structure in different ways, but basically divided into threemain categories, we first divide the original dataset into three significantindustries based on industry category, namely, primary industry, secondary industry, and tertiary industry. Then, we calculate the numberof enterprises existing in each month from 1980 to 2015. We use thedata from 1980 to 1990 as the first training set to predict the trend in1991, followed by the data before 1991 as the training set to predictthe trend for 1992, and so on. The reason for performing stepwiseforecasting is based on the practical requirements of most real-worldtime series forecasting tasks. Thus, the final prediction curve is spelledout for each time unit. We use the classical MAPE metric to evaluatethe performance of the forecasting model. There are several representative time series forecasting models available for evaluation, such asArima [61], Vector Auto-Regression Model (VAR) [46], Random ForestModel (RF) or Random Decision Forests [26] and Long Short-TermMemory Recurrent Neural Networks (LSTM), covering linear, nonlinearregression methods, and machine learning models [64]. In this work,we considered both model performance and model interpretability, andfinally select two machine learning models for multivariate time seriesprediction, namely, RF and XGBoost [13], as they have sufficientlyhigh model accuracy and good interpretability.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "The Registration Prediction View captures the correlation between the predicted registrations and ground truth over time, and understands the latent features driving the changes from the perspective of the time series forecasting models (R.2 \u2013 R.3). The Registration Prediction View (Fig. 2) helps experts understand the detailed associations between RIS influencing factors and registration amounts based on several representative time-series forecasting models. It consists of three subplots, arranged from top to bottom, representing business activities in the primary, secondary and tertiary sectors, respectively. In each subplot, the dotted line represents the ground truth registration of enterprises. The solid black line represents the predicted business activity compared to the ground truth volume. Thus, users can observe the gap that represents the difference between the actual and the predicted value at different timestamps. In addition, the importance of the 7 features is calculated, normalized, and represented with different classification colors stacked on top of each other. In each snapshot, the glyphs of feature importance are placed along the Y-axis. Each bar in the stacked glyphs represents a feature. Bars stacked above the predicted values represent features with negative effects, pushing the predicted value down [64]. The features stacked below the predicted value are positive, pushing the predicted value higher. The user can select other models on the right side for comparison.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Bar",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Bar"]
      },
      {
        "solution_text": "The Registration Prediction View captures the correlation between the predicted registrations and ground truth over time, and understands the latent features driving the changes from the perspective of the time series forecasting models (R.2 \u2013 R.3). The Registration Prediction View (Fig. 2) helps experts understand the detailed associations between RIS influencing factors and registration amounts based on several representative time-series forecasting models. It consists of three subplots, arranged from top to bottom, representing business activities in the primary, secondary and tertiary sectors, respectively. In each subplot, the dotted line represents the ground truth registration of enterprises. The solid black line represents the predicted business activity compared to the ground truth volume. Thus, users can observe the gap that represents the difference between the actual and the predicted value at different timestamps. In addition, the importance of the 7 features is calculated, normalized, and represented with different classification colors stacked on top of each other. In each snapshot, the glyphs of feature importance are placed along the Y-axis. Each bar in the stacked glyphs represents a feature. Bars stacked above the predicted values represent features with negative effects, pushing the predicted value down [64]. The features stacked below the predicted value are positive, pushing the predicted value higher. The user can select other models on the right side for comparison.",
        "solution_category": "interaction",
        "solution_axial": "Selecting+Connect",
        "solution_compoent": "",
        "axial_code": ["Selecting", "Connect"],
        "componenet_code": ["selecting", "connect/relate"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 127,
    "paper_title": "RankAxis: Towards a Systematic Combination of Projection and Ranking in Multi-Attribute Data Exploration",
    "pub_year": 2023,
    "domain": "Ranking",
    "requirement": {
      "requirement_text": "R.1: Connecting projections and rankings in a seamless context. When experts explore bank projection results, they can observe several clusters of banks, but mapping different clusters to ratings can be a challenge. They encounter similar problems when interpreting the ranking results, as they rely on their expertise to divide the rankings into segments and subjectively use these segments as ratings. Therefore, experts wanted to put dimension reduction and ranking in the same context so that they could explore and compare them more effectively.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "multi-attribute datasets",
      "data_code": { "geometry": 1, "clusters_and_sets_and_lists": 1 }
    },
    "solution": [
      {
        "solution_text": "The projection view uses classical dimensionality reduction techniques such as t-SNE to create low-dimensional projections and preserve local similarity to express neighborhood structure [16, 24]. Other techniques such as PCA, MDS and UMAP [30] can also be integrated. We use the same set of weights to normalize the values of the attributes used in the ranked table view to obtain a two-dimensional projection. Specifically, the four projection views are depicted in the Fig. 1. Fig. 1(B) shows the projection using the latest attribute weight vector. Fig. 1(D) shows the projection corresponding to the attribute weight vectors from the first three ranking schemes (R.1). We first introduce the interactive projection view and then the projection view for comparison. In the interactive projection view, there are two main components. First, as shown in Fig. 4, observations on the interactive projection view are coded by a coxcomb digram [20] that shows the distribution of attribute contributions. The color of the dot in the middle of the glyph encodes the ranking score: the higher the ranking score, the darker the color. The size of each pie encodes the corresponding attribute contribution. We did not choose the classic star glyph to encode attribute values because the lines in the star glyph are difficult to detect when the color saturation is low and the glyph is small [52]. A potential drawback of this design is visual clutter, which is a common problem for many reduced-dimensional based visualizations. To mitigate this problem, we first reduced the opacity of the glyphs so that individual glyphs could be observed. When hovering over a glyph, that glyph is zoomed in and displayed in the foreground. In addition, we support panning and semantic zooming to focus on specific areas of the glyph. Second, the interactive projection view can generate a ranking line that explicitly guides the analyst to explore the projection layout and orientation (R.3). We define a ranking line that connects certain sampling points according to their rankings. To reflect the ranking results in the ranking table view, we propose the following three methods to generate ranking lines in the interactive projection view. Other methods, such as clustering based on projection quality or pressure [5] can also be integrated to recommend initial clustering and avoid potential misinterpretations.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "The projection view uses classical dimensionality reduction techniques such as t-SNE to create low-dimensional projections and preserve local similarity to express neighborhood structure [16, 24]. Other techniques such as PCA, MDS and UMAP [30] can also be integrated. We use the same set of weights to normalize the values of the attributes used in the ranked table view to obtain a two-dimensional projection. Specifically, the four projection views are depicted in the Fig. 1. Fig. 1(B) shows the projection using the latest attribute weight vector. Fig. 1(D) shows the projection corresponding to the attribute weight vectors from the first three ranking schemes (R.1). We first introduce the interactive projection view and then the projection view for comparison. In the interactive projection view, there are two main components. First, as shown in Fig. 4, observations on the interactive projection view are coded by a coxcomb digram [20] that shows the distribution of attribute contributions. The color of the dot in the middle of the glyph encodes the ranking score: the higher the ranking score, the darker the color. The size of each pie encodes the corresponding attribute contribution. We did not choose the classic star glyph to encode attribute values because the lines in the star glyph are difficult to detect when the color saturation is low and the glyph is small [52]. A potential drawback of this design is visual clutter, which is a common problem for many reduced-dimensional based visualizations. To mitigate this problem, we first reduced the opacity of the glyphs so that individual glyphs could be observed. When hovering over a glyph, that glyph is zoomed in and displayed in the foreground. In addition, we support panning and semantic zooming to focus on specific areas of the glyph. Second, the interactive projection view can generate a ranking line that explicitly guides the analyst to explore the projection layout and orientation (R.3). We define a ranking line that connects certain sampling points according to their rankings. To reflect the ranking results in the ranking table view, we propose the following three methods to generate ranking lines in the interactive projection view. Other methods, such as clustering based on projection quality or pressure [5] can also be integrated to recommend initial clustering and avoid potential misinterpretations.",
        "solution_category": "visualization",
        "solution_axial": "Juxtaposition-Similar-Nonsymmetrical",
        "solution_compoent": "Scatter",
        "axial_code": ["Juxtaposition-Similar-Nonsymmetrical"],
        "componenet_code": ["Scatter"]
      },
      {
        "solution_text": "The projection view uses classical dimensionality reduction techniques such as t-SNE to create low-dimensional projections and preserve local similarity to express neighborhood structure [16, 24]. Other techniques such as PCA, MDS and UMAP [30] can also be integrated. We use the same set of weights to normalize the values of the attributes used in the ranked table view to obtain a two-dimensional projection. Specifically, the four projection views are depicted in the Fig. 1. Fig. 1(B) shows the projection using the latest attribute weight vector. Fig. 1(D) shows the projection corresponding to the attribute weight vectors from the first three ranking schemes (R.1). We first introduce the interactive projection view and then the projection view for comparison. In the interactive projection view, there are two main components. First, as shown in Fig. 4, observations on the interactive projection view are coded by a coxcomb digram [20] that shows the distribution of attribute contributions. The color of the dot in the middle of the glyph encodes the ranking score: the higher the ranking score, the darker the color. The size of each pie encodes the corresponding attribute contribution. We did not choose the classic star glyph to encode attribute values because the lines in the star glyph are difficult to detect when the color saturation is low and the glyph is small [52]. A potential drawback of this design is visual clutter, which is a common problem for many reduced-dimensional based visualizations. To mitigate this problem, we first reduced the opacity of the glyphs so that individual glyphs could be observed. When hovering over a glyph, that glyph is zoomed in and displayed in the foreground. In addition, we support panning and semantic zooming to focus on specific areas of the glyph. Second, the interactive projection view can generate a ranking line that explicitly guides the analyst to explore the projection layout and orientation (R.3). We define a ranking line that connects certain sampling points according to their rankings. To reflect the ranking results in the ranking table view, we propose the following three methods to generate ranking lines in the interactive projection view. Other methods, such as clustering based on projection quality or pressure [5] can also be integrated to recommend initial clustering and avoid potential misinterpretations.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore+Filtering+Reconfigure",
        "solution_compoent": "",
        "axial_code": ["Reconfigure", "Filtering", "OverviewandExplore"],
        "componenet_code": ["reconfigure", "filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 129,
    "paper_title": "RankAxis: Towards a Systematic Combination of Projection and Ranking in Multi-Attribute Data Exploration",
    "pub_year": 2023,
    "domain": "Ranking",
    "requirement": {
      "requirement_text": "R.3: Guiding semantic exploration in projections. Dimensional projections inevitably produce \u201cclusters\u201d and \u201coutliers\u201d, and experts want to understand the distribution of observations in the projection space because they sometimes cannot distinguish between the boundaries of clusters and whether an observation is an outlier. That is, when interpreting the layout of the projection results, they want more semantic help to guide their exploration in the projection.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "multi-attribute datasets",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "geometry": 1,
        "tables": 1
      }
    },
    "solution": [
      {
        "solution_text": "The interactive projection view can generate a ranking line that explicitly guides the analyst to explore the projection layout and orientation (R.3). We define a ranking line that connects certain sampling points according to their rankings. To reflect the ranking results in the ranking table view, we propose the following three methods to generate ranking lines in the interactive projection view. Other methods, such as clustering based on projection quality or pressure [5] can also be integrated to recommend initial clustering and avoid potential misinterpretations. Sequence Ranking Line. Based on the ranking score of each data item in the ranking tabular view, a line with arrows connects the corresponding observations in the interactive projection view (Fig. 5(A)). We observe that the connected layout may show a trending order, e.g., an ordered ranking line in one direction, or a \u201czigzag\u201d ranking line with backward and forward correspondence. Rating Line. Although a sequence ranking line may indicate an ordered layout, it strings all projection observations and may inevitably introduce visual clutter. For example, if \u201czigzags\u201d occur frequently, then ranking lines do not adequately reflect the sequential semantic information contained in the projection space. It may also be difficult for users to keep track of the order of ranked data items. To alleviate this problem, we first partition the ranking results to obtain a subset of sequences with sequential different ratings. Then, we generate the average of the data items for each rating as the center of the rating in the projected view. These newly generated observations are highlighted in red and linked according to the order of the ratings (Fig. 5(B)). That is, the generation of rating lines to link \u201caverage observations\u201d in the interactive projection view can be considered as a \u201cresampling\u201d of the observations in the sequential ranking line, better reflecting the sequential semantic information contained in the projection. Self-defined Rating Line. The first two methods draw lines based on rating results, but ignore users with extensive domain knowledge. For example, with respect to bank rating questions, joint-stock commercial banks generally outperform private banks. In the interactive projection view, analysts may be inclined to conclude that the regions where the joint-stock banks are located are likely to be the better performers overall because they use the visual metaphor of \u201cproximity \u2248 similarity\u201d. Therefore, analysts can perform customized interactive operations to generate ranking lines based on their judgment of the data. As shown in Fig. 5(C), analysts can lasso a region, and then the system automatically calculates the average of all observations in that region and joins all \u201caverage observations\u201d generated from the lassoed regions in the order of user interaction to form a user-defined rating line.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "The interactive projection view can generate a ranking line that explicitly guides the analyst to explore the projection layout and orientation (R.3). We define a ranking line that connects certain sampling points according to their rankings. To reflect the ranking results in the ranking table view, we propose the following three methods to generate ranking lines in the interactive projection view. Other methods, such as clustering based on projection quality or pressure [5] can also be integrated to recommend initial clustering and avoid potential misinterpretations. Sequence Ranking Line. Based on the ranking score of each data item in the ranking tabular view, a line with arrows connects the corresponding observations in the interactive projection view (Fig. 5(A)). We observe that the connected layout may show a trending order, e.g., an ordered ranking line in one direction, or a \u201czigzag\u201d ranking line with backward and forward correspondence. Rating Line. Although a sequence ranking line may indicate an ordered layout, it strings all projection observations and may inevitably introduce visual clutter. For example, if \u201czigzags\u201d occur frequently, then ranking lines do not adequately reflect the sequential semantic information contained in the projection space. It may also be difficult for users to keep track of the order of ranked data items. To alleviate this problem, we first partition the ranking results to obtain a subset of sequences with sequential different ratings. Then, we generate the average of the data items for each rating as the center of the rating in the projected view. These newly generated observations are highlighted in red and linked according to the order of the ratings (Fig. 5(B)). That is, the generation of rating lines to link \u201caverage observations\u201d in the interactive projection view can be considered as a \u201cresampling\u201d of the observations in the sequential ranking line, better reflecting the sequential semantic information contained in the projection. Self-defined Rating Line. The first two methods draw lines based on rating results, but ignore users with extensive domain knowledge. For example, with respect to bank rating questions, joint-stock commercial banks generally outperform private banks. In the interactive projection view, analysts may be inclined to conclude that the regions where the joint-stock banks are located are likely to be the better performers overall because they use the visual metaphor of \u201cproximity \u2248 similarity\u201d. Therefore, analysts can perform customized interactive operations to generate ranking lines based on their judgment of the data. As shown in Fig. 5(C), analysts can lasso a region, and then the system automatically calculates the average of all observations in that region and joins all \u201caverage observations\u201d generated from the lassoed regions in the order of user interaction to form a user-defined rating line.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "Line+Scatter",
        "axial_code": ["Nesting"],
        "componenet_code": ["Scatter", "Line"]
      },
      {
        "solution_text": "The interactive projection view can generate a ranking line that explicitly guides the analyst to explore the projection layout and orientation (R.3). We define a ranking line that connects certain sampling points according to their rankings. To reflect the ranking results in the ranking table view, we propose the following three methods to generate ranking lines in the interactive projection view. Other methods, such as clustering based on projection quality or pressure [5] can also be integrated to recommend initial clustering and avoid potential misinterpretations. Sequence Ranking Line. Based on the ranking score of each data item in the ranking tabular view, a line with arrows connects the corresponding observations in the interactive projection view (Fig. 5(A)). We observe that the connected layout may show a trending order, e.g., an ordered ranking line in one direction, or a \u201czigzag\u201d ranking line with backward and forward correspondence. Rating Line. Although a sequence ranking line may indicate an ordered layout, it strings all projection observations and may inevitably introduce visual clutter. For example, if \u201czigzags\u201d occur frequently, then ranking lines do not adequately reflect the sequential semantic information contained in the projection space. It may also be difficult for users to keep track of the order of ranked data items. To alleviate this problem, we first partition the ranking results to obtain a subset of sequences with sequential different ratings. Then, we generate the average of the data items for each rating as the center of the rating in the projected view. These newly generated observations are highlighted in red and linked according to the order of the ratings (Fig. 5(B)). That is, the generation of rating lines to link \u201caverage observations\u201d in the interactive projection view can be considered as a \u201cresampling\u201d of the observations in the sequential ranking line, better reflecting the sequential semantic information contained in the projection. Self-defined Rating Line. The first two methods draw lines based on rating results, but ignore users with extensive domain knowledge. For example, with respect to bank rating questions, joint-stock commercial banks generally outperform private banks. In the interactive projection view, analysts may be inclined to conclude that the regions where the joint-stock banks are located are likely to be the better performers overall because they use the visual metaphor of \u201cproximity \u2248 similarity\u201d. Therefore, analysts can perform customized interactive operations to generate ranking lines based on their judgment of the data. As shown in Fig. 5(C), analysts can lasso a region, and then the system automatically calculates the average of all observations in that region and joins all \u201caverage observations\u201d generated from the lassoed regions in the order of user interaction to form a user-defined rating line.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore+Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting", "OverviewandExplore"],
        "componenet_code": ["selecting", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 130,
    "paper_title": "RankAxis: Towards a Systematic Combination of Projection and Ranking in Multi-Attribute Data Exploration",
    "pub_year": 2023,
    "domain": "Ranking",
    "requirement": {
      "requirement_text": "R.4: Reveal any inconsistencies between projections and rankings. As mentioned earlier, items projected together are not necessarily close in the ranking list, a situation that arouses the curiosity of experts because they regard \u201cproximity\u201d as \u201csimilarity\u201d. Similarly, they often confirm ranking results by observing whether the nearby neighbors of a data item in a ranked list are semantically related. Thus, identifying potential inconsistencies can help them better interpret the meaning of \u201cneighbors\u201d in projections and ranking results, and ultimately identify the underlying data characteristics that lead to inconsistencies.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "multi-attribute datasets",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Step 1: Modeling Ranking SVM. Inspired by Podium [44], we use Ranking SVM to derive attribute weights. Ranking SVM applies the idea of optimizing the SVM hyperplane to the ranking problem with pairwise constraints. A finite set of data points di and dj and a label is used to derive whether di is better or not, instead of a complete set of data points with labels. The input to the Ranking SVM involves a difference vector of data point pairs, e.g. di \u2212dj. Specifically, we transfer a pair (di,dj) and their relative ranks to a tuple based on the following statement: If di is preferred, di \u2212 dj = 1; otherwise, di \u2212dj = \u22121. The generated model can be used to predict which of the given pair of points is better. Nevertheless, the constraints derived from user interactions may be unsatisfying [19]. Therefore, we model all constraints as soft constraints rather than hard constraints to avoid vacuous results. Thus, user interactions can always produce a set of attribute weights that maximize the simulation of user constraints [26].",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "Step 2: Deriving constraints. We transfer ranking to a binary classification problem by using the linear separator of SVM. That is, we generate labeled data for Ranking SVM by using the data items that the user has interacted with and dragging these items to a new location (Fig. 3(B)). These items are the k marked rows. Without loss of generality, the k points {dl1 ,...,dlk} are indexed by [l1,...,lk]. Then we create a combination of all pairs of difference vectors as training instances [19], i.e., for i, j \u2208 {1...k}, where i = j, we derive a training tuple based on the above formula, i.e., each training instance is a pair of differences between rows di and dj, classified as y = 1 if di is ranked higher than dj, and y = \u22121 if di is ranked lower than dj. Similar to Podium, we set k = 6 to ensure that the minimum training data amount for the attribute weight vector is derived after the experimental analysis",
        "solution_category": "data_manipulation",
        "solution_axial": "SimilarityCalculation",
        "solution_compoent": "",
        "axial_code": ["SimilarityCalculation"],
        "componenet_code": ["similarity_calculation"]
      },
      {
        "solution_text": "Step 3: Calculating the ranking score. After transforming the user interaction and learning the model, a weight vector w is obtained for us to rank the data items. We compute the individual dot products of w with each data item to generate a rank score as r(di) = w\u00b7 di = \u2211m j=1wjdi j. with the highest one corresponding to the top rank.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Step 4: Transfer ranking to rating. We adapt an entropy discretiza\\x02tion method to transfer rankings to ratings [10]. We first sort ranking scores and consider each score as a split point, and then calculate the entropy of the left and right parts of each point. We consider the split point with the lowest entropy value to be the first split point. We repeat the above procedure until we have n split points (we determine the value of n for each dataset after the experimental results). We round the frac\\x02tion of each data item to multiples of n. We denote the random variable of scores by X and sort the scores of the data items as (x1, x2,...,xn). The P(xi) denotes the probability of the fraction xi. The entropy of X is H(X) = E[\u2212logP(xi)] = \u2212\u2211N i=1 P(xi)logP(xi). Suppose there are k distinct scores among the ranking scores of all data items and k < n. We order k scores as (u1,u2,...uk), and these scores can be considered as x1,x2,...,xn of consecutive values of breakpoints. Then, we select a point with the lowest entropy value from the candidate points. We repeat this process until we have n\u22121 breakpoints, forming n ratings.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "The ranking projection axis view allows to compare ranking results from the ranking tabular view with the projection axis generated in the interactive projection view (R.4), which consists of a projection axis, a score axis, a contribution axis and an attribute comparison subviews.",
        "solution_category": "visualization",
        "solution_axial": "Overlay-Coordinatesystemrelated-Sharing",
        "solution_compoent": "Flow+Circle+Bar",
        "axial_code": ["Overlay-Coordinatesystemrelated-Sharing"],
        "componenet_code": ["Circle", "Bar", "Flow"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 133,
    "paper_title": "DPVisCreator: Incorporating Pattern Constraints to Privacy-preserving Visualizations via Differential Privacy",
    "pub_year": 2023,
    "domain": "Privacy-preserving visualization",
    "requirement": {
      "requirement_text": "R3: Explore data patterns of interest. Multidimensional tabular data contains rich information under different attributes. Data custodians need to explore the publishing dataset from different aspects to identify sensitive attributes and representative data patterns. They evaluate the intrinsic values of data patterns and seek ways to sustain the important ones in the privacy protection process. We should support visual exploration because the final output is in a visual form.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "health records with patients\u2019 private information",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "A data custodian first loads a dataset and looks for sensitive attributes in the Data View (Fig. 3-A), which shows the general data characteristics. He selects attributes of interest and specifies the desired chart settings in the Pattern View (Fig. 3-B). He can also highlight the data patterns of interest with the data selection tools (R3). Then, these preferences are processed by PriVis to propose a privacy protection scheme (R1)",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration+Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting", "Participation/Collaboration"],
        "componenet_code": ["selecting", "participation/collaboration"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 134,
    "paper_title": "DPVisCreator: Incorporating Pattern Constraints to Privacy-preserving Visualizations via Differential Privacy",
    "pub_year": 2023,
    "domain": "Privacy-preserving visualization",
    "requirement": {
      "requirement_text": "R3: Explore data patterns of interest. Multidimensional tabular data contains rich information under different attributes. Data custodians need to explore the publishing dataset from different aspects to identify sensitive attributes and representative data patterns. They evaluate the intrinsic values of data patterns and seek ways to sustain the important ones in the privacy protection process. We should support visual exploration because the final output is in a visual form.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "health records with patients\u2019 private information",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "We focus on the widely used tabular data, which stores data records with a set of attributes as rows in a multidimensional table. Privacy preservation for tabular data is mainly achieved by syntactic anonymity and DP. Syntactic anonymity (e.g., k-anonymity [64], l-diversity [44], and t-closeness [42]) constructs equivalence groups to prevent attackers from distinguishing individuals. Unfortunately, equivalence groups could be cracked by background knowledge [17]. Compared with syntactic anonymity approaches, DP is gradually being applied to more real-world scenarios because of its robust mathematical guarantees [26]. DP is defined based on two neighboring datasets D1 and D2, which differ by adding or removing a record. Given a user-defined privacy budget \u03b5 (\u03b5 > 0), a randomized algorithm G satisfies \u03b5-differential privacy (\u03b5-DP), if and only if the following equation holds for any possible output O: Pr(G(D1) = O) \u2264 e\u03b5 \u00b7Pr(G(D2) = O). (1) Existing studies have proved that the Laplace and exponential mechanisms are feasible randomized algorithms. For a function f with numerical output, the Laplace mechanism [26] constructs a corresponding Gf by adding noise sampled from a Laplace distribution Lap( \u2206 \u03b5 f ), where \u2206f denotes the l1-sensitivity of f , that is, the maximum difference between the output of two neighboring datasets. The exponential mechanism [48] applies to function f with categorical output, which allows privately selecting the \u201cbest\u201d element from a set. Assume that q(D) is a designed score function that gives each discrete value a probability; the algorithm Gf provides \u03b5-DP if it approximately maximizes the score by returning values from the discrete domain with the probability proportional to exp( \u03b5q(D) 2\u2206q ), where \u2206q is the l1-sensitivity of q.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "PriVis generates privacy-preserving visualizations in four steps. Step 1: Understand pattern constraints. Users may select multiple patterns from different charts, represented as P = {P1,P2,\u00b7\u00b7\u00b7 ,Ps}. Each pattern Pi corresponds to a subset of data records in the whole data D. For example, as mentioned in Sec. 2.1, the data custodian needs data records with both age and illness attributes from the hospital data to generate a bar chart. To emphasize the illness risk in people over 60, the data custodian can specify the pattern by selecting bars corresponding to patients over 60 and setting a weight. User preferences for different patterns can be recorded as different subsets of data records with corresponding weight W = {w1,w2,\u00b7\u00b7\u00b7 ,ws}. For a data record r, its final weight needs to consider its occurrence under each pattern. To this end, we give each record an initial weight of 1 and then add additional weights of wk for each record r in pattern Pk. The mixture weight MW(r) of data record r is defined as: MW(r) = s \u2211 k=1 wk Pk (r) +1, (4) where Pk is indicator function which maps record in pattern Pk to 1, and all others to 0. Therefore, we translate the different pattern constraints into a mixture weight assigned to each record. Step 2: Construct a Bayesian network with pattern constraints. To preserve user-specified patterns, PriVis emphasizes the correlations between the corresponding records when constructing the Bayesian network. Specifically, when selecting AP pairs, PriVis not only employs the exponential mechanism but also replaces the mutual information in the PrivBayes network with the weighted mutual information: Iw(X,\u03a0,MW) = \u2211x\u2208dom(X) \u2211 \u03c0\u2208dom(\u03a0) \u2211r\u2208x\u2229\u03c0 MW(r) |x\u2229\u03c0| Pr(X = x,\u03a0 = \u03c0)log Pr(X = x,\u03a0 = \u03c0) Pr(X = x)Pr(\u03a0 = \u03c0) . (5) With mixture weights, the constructed Bayesian network can better preserve dependence among user-selected data. Therefore, the patterns consisting of these records can be better maintained in visualizations. Step 3: Inject noise to the Bayesian network. The con\\x02structed Bayesian network provides conditional probability distribu\\x02tions Pr(Xi|\u03a0i)(i \u2208 [1,d]) to approximate the distribution of the high\\x02dimensional dataset. To inject noise, PriVis first calculates noisy dis\\x02tribution Pr\u2217(Xi,\u03a0i) for any i \u2208 [k +1,d] by adding Laplace noise to joint probability distribution Pr(Xi,\u03a0i). Then, the conditional distribu\\x02tion Pr\u2217(Xi|\u03a0i) can be derived. The Pr\u2217(Xi|\u03a0i)(i \u2208 [1,k]) can thus be directly obtained from Pr\u2217(Xk+1|\u03a0k+1). Step 4: Sample records to generate private datasets. According to the topological order of the Bayesian network, Xi(i \u2208 [1,d]) can be sampled in increasing order of i based on the noisy conditional distribution. For example, when all parent attributes of Xj(j \u2208 [2,d]) are sampled, Xj can be sampled by conditional probability Pr\u2217(Xj The sampled data can then be used for publishing visualizations. |\u03a0j). Privacy Analysis. In our approach, the data custodian is located in a trust zone to elicit pattern preferences before the model performs privacy learning. The model is guided towards the right direction without additional privacy budgets. In the PriVis model, access to the original data is required for steps 2 and 3. In step 2, the model uses the exponential mechanism to privately select AP pairs, which converts selection from multiple candidate pairs into probabilistic sampling, which consumes an \u03b51 privacy budget. In step 3, Laplace noise is added to the conditional probability distributions, which consumes an \u03b52 privacy budget. According to the composition property [27] of DP, the resulting visualizations satisfy the \u03b5-DP, where \u03b5 = \u03b51 +\u03b52.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "The recommended scheme is visualized in the Model View (Fig. 3-C), with its structure and impact explained (R2). The Model View contextualizes the privacy-preserving process to foster users\u2019 understanding and supports users in making trade-offs be_x0002_tween data pattern maintenance and privacy protection. The Model View (Fig. 3-C) contains four parts: (1) The weight configuration panel lets users adjust importance weights to each pattern (R4); (2) The pattern constraint relationship represents the overall impact between different pattern constraints (R4); (3) The pattern constraint flow shows the detailed pattern constraint distribution on each attribute and highlight them for comparisons (R4); (4) The Bayesian network view visu_x0002_alizes the structure in the underlying PriVis model (R2). The latter two views can be switched with a button. The weight configuration panel (Fig. 3-C1) uses a stacked bar chart to provide an overview of the importance weight distribution. Colors encode the pattern constraint\u2019s type, having green for the cluster, brown for correlation, and purple for order. This color encoding is applied throughout the system. The corresponding pattern ID involving data amount and importance weights are displayed on the data pattern list and hovers on the corresponding bar. Users can adjust the correspond_x0002_ing pattern constraint weights in the data pattern list. The pattern constraint relationship (Fig. 3-C2) encodes pattern con_x0002_straints as circles. The position of a circle is determined by multidimen_x0002_sional scaling (MDS) [18], where the Wasserstein distance [68] is used to measure the similarity between patterns. The projection reflects the differences in distributions. Edges between circles encode the impact of different pattern constraints by affecting the network structure. Their colors encode the type of influence of changing weights. Green edges imply positive correlations, where increasing one pattern\u2019s weight will promote the other. In contrast, red represents negative correlations that the other pattern will be weakened. The magnitude of influence is encoded by the thickness of the edge, which is calculated by the sum of the difference between the intersection and the symmetric difference of AP pairs. Clicking a pattern constraint will highlight it and its adjacent edges, together with the corresponding entities in other views. The pattern constraint flow (Fig. 3-C3) improves the basic Sankey diagram with sub-flows representing constraints in each attribute. The x-axis lists all attributes, and the y-axis indicates the discrete interval of each attribute. Continuous attributes are discretized by k-means with the elbow method [46], while categorical attributes use their discrete do_x0002_main as the interval. The length along the y-axis represents the amount of data belonging to this interval. The specific interval information can be viewed by hovering. Different flows represent different data distributions, and the width of a flow is proportional to its data volume. By default, all data flows are grayed out to provide a clear background, while the selected data pattern will be highlighted correspondingly for a more intuitive comparison. The Bayesian network view (Fig. 3-C4) shows the structure of the PriVis model. Since the network is a DAG, we use a hierarchical layout [4] to show the dependencies of attributes. Specifically, the depth of each node is obtained by topological sorting. We draw each layer in turn along the x-axis, and nodes in the same layer are evenly distributed along the y-axis. Nodes represent attributes, and edges reflect conditional dependencies. The probability density distributions before and after noise addition are displayed by superposition when hovering over a node, making it easy to compare and understand the exact degree of privacy protection.",
        "solution_category": "visualization",
        "solution_axial": "Non-composite",
        "solution_compoent": "Sankey+Bar+Circle+Tree",
        "axial_code": ["Non-composite"],
        "componenet_code": ["Sankey", "Bar", "Tree", "Circle"]
      },
      {
        "solution_text": "The recommended scheme is visualized in the Model View (Fig. 3-C), with its structure and impact explained (R2). The Model View contextualizes the privacy-preserving process to foster users\u2019 understanding and supports users in making trade-offs be_x0002_tween data pattern maintenance and privacy protection. The Model View (Fig. 3-C) contains four parts: (1) The weight configuration panel lets users adjust importance weights to each pattern (R4); (2) The pattern constraint relationship represents the overall impact between different pattern constraints (R4); (3) The pattern constraint flow shows the detailed pattern constraint distribution on each attribute and highlight them for comparisons (R4); (4) The Bayesian network view visu_x0002_alizes the structure in the underlying PriVis model (R2). The latter two views can be switched with a button. The weight configuration panel (Fig. 3-C1) uses a stacked bar chart to provide an overview of the importance weight distribution. Colors encode the pattern constraint\u2019s type, having green for the cluster, brown for correlation, and purple for order. This color encoding is applied throughout the system. The corresponding pattern ID involving data amount and importance weights are displayed on the data pattern list and hovers on the corresponding bar. Users can adjust the correspond_x0002_ing pattern constraint weights in the data pattern list. The pattern constraint relationship (Fig. 3-C2) encodes pattern con_x0002_straints as circles. The position of a circle is determined by multidimen_x0002_sional scaling (MDS) [18], where the Wasserstein distance [68] is used to measure the similarity between patterns. The projection reflects the differences in distributions. Edges between circles encode the impact of different pattern constraints by affecting the network structure. Their colors encode the type of influence of changing weights. Green edges imply positive correlations, where increasing one pattern\u2019s weight will promote the other. In contrast, red represents negative correlations that the other pattern will be weakened. The magnitude of influence is encoded by the thickness of the edge, which is calculated by the sum of the difference between the intersection and the symmetric difference of AP pairs. Clicking a pattern constraint will highlight it and its adjacent edges, together with the corresponding entities in other views. The pattern constraint flow (Fig. 3-C3) improves the basic Sankey diagram with sub-flows representing constraints in each attribute. The x-axis lists all attributes, and the y-axis indicates the discrete interval of each attribute. Continuous attributes are discretized by k-means with the elbow method [46], while categorical attributes use their discrete do_x0002_main as the interval. The length along the y-axis represents the amount of data belonging to this interval. The specific interval information can be viewed by hovering. Different flows represent different data distributions, and the width of a flow is proportional to its data volume. By default, all data flows are grayed out to provide a clear background, while the selected data pattern will be highlighted correspondingly for a more intuitive comparison. The Bayesian network view (Fig. 3-C4) shows the structure of the PriVis model. Since the network is a DAG, we use a hierarchical layout [4] to show the dependencies of attributes. Specifically, the depth of each node is obtained by topological sorting. We draw each layer in turn along the x-axis, and nodes in the same layer are evenly distributed along the y-axis. Nodes represent attributes, and edges reflect conditional dependencies. The probability density distributions before and after noise addition are displayed by superposition when hovering over a node, making it easy to compare and understand the exact degree of privacy protection.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 135,
    "paper_title": "DPVisCreator: Incorporating Pattern Constraints to Privacy-preserving Visualizations via Differential Privacy",
    "pub_year": 2023,
    "domain": "Privacy-preserving visualization",
    "requirement": {
      "requirement_text": "R3: Explore data patterns of interest. Multidimensional tabular data contains rich information under different attributes. Data custodians need to explore the publishing dataset from different aspects to identify sensitive attributes and representative data patterns. They evaluate the intrinsic values of data patterns and seek ways to sustain the important ones in the privacy protection process. We should support visual exploration because the final output is in a visual form.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "health records with patients\u2019 private information",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "We focus on the widely used tabular data, which stores data records with a set of attributes as rows in a multidimensional table. Privacy preservation for tabular data is mainly achieved by syntactic anonymity and DP. Syntactic anonymity (e.g., k-anonymity [64], l-diversity [44], and t-closeness [42]) constructs equivalence groups to prevent attackers from distinguishing individuals. Unfortunately, equivalence groups could be cracked by background knowledge [17]. Compared with syntactic anonymity approaches, DP is gradually being applied to more real-world scenarios because of its robust mathematical guarantees [26]. DP is defined based on two neighboring datasets D1 and D2, which differ by adding or removing a record. Given a user-defined privacy budget \u03b5 (\u03b5 > 0), a randomized algorithm G satisfies \u03b5-differential privacy (\u03b5-DP), if and only if the following equation holds for any possible output O: Pr(G(D1) = O) \u2264 e\u03b5 \u00b7Pr(G(D2) = O). (1) Existing studies have proved that the Laplace and exponential mechanisms are feasible randomized algorithms. For a function f with numerical output, the Laplace mechanism [26] constructs a corresponding Gf by adding noise sampled from a Laplace distribution Lap( \u2206 \u03b5 f ), where \u2206f denotes the l1-sensitivity of f , that is, the maximum difference between the output of two neighboring datasets. The exponential mechanism [48] applies to function f with categorical output, which allows privately selecting the \u201cbest\u201d element from a set. Assume that q(D) is a designed score function that gives each discrete value a probability; the algorithm Gf provides \u03b5-DP if it approximately maximizes the score by returning values from the discrete domain with the probability proportional to exp( \u03b5q(D) 2\u2206q ), where \u2206q is the l1-sensitivity of q.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "PriVis generates privacy-preserving visualizations in four steps. Step 1: Understand pattern constraints. Users may select multiple patterns from different charts, represented as P = {P1,P2,\u00b7\u00b7\u00b7 ,Ps}. Each pattern Pi corresponds to a subset of data records in the whole data D. For example, as mentioned in Sec. 2.1, the data custodian needs data records with both age and illness attributes from the hospital data to generate a bar chart. To emphasize the illness risk in people over 60, the data custodian can specify the pattern by selecting bars corresponding to patients over 60 and setting a weight. User preferences for different patterns can be recorded as different subsets of data records with corresponding weight W = {w1,w2,\u00b7\u00b7\u00b7 ,ws}. For a data record r, its final weight needs to consider its occurrence under each pattern. To this end, we give each record an initial weight of 1 and then add additional weights of wk for each record r in pattern Pk. The mixture weight MW(r) of data record r is defined as: MW(r) = s \u2211 k=1 wk Pk (r) +1, (4) where Pk is indicator function which maps record in pattern Pk to 1, and all others to 0. Therefore, we translate the different pattern constraints into a mixture weight assigned to each record. Step 2: Construct a Bayesian network with pattern constraints. To preserve user-specified patterns, PriVis emphasizes the correlations between the corresponding records when constructing the Bayesian network. Specifically, when selecting AP pairs, PriVis not only employs the exponential mechanism but also replaces the mutual information in the PrivBayes network with the weighted mutual information: Iw(X,\u03a0,MW) = \u2211x\u2208dom(X) \u2211 \u03c0\u2208dom(\u03a0) \u2211r\u2208x\u2229\u03c0 MW(r) |x\u2229\u03c0| Pr(X = x,\u03a0 = \u03c0)log Pr(X = x,\u03a0 = \u03c0) Pr(X = x)Pr(\u03a0 = \u03c0) . (5) With mixture weights, the constructed Bayesian network can better preserve dependence among user-selected data. Therefore, the patterns consisting of these records can be better maintained in visualizations. Step 3: Inject noise to the Bayesian network. The con\\x02structed Bayesian network provides conditional probability distribu\\x02tions Pr(Xi|\u03a0i)(i \u2208 [1,d]) to approximate the distribution of the high\\x02dimensional dataset. To inject noise, PriVis first calculates noisy dis\\x02tribution Pr\u2217(Xi,\u03a0i) for any i \u2208 [k +1,d] by adding Laplace noise to joint probability distribution Pr(Xi,\u03a0i). Then, the conditional distribu\\x02tion Pr\u2217(Xi|\u03a0i) can be derived. The Pr\u2217(Xi|\u03a0i)(i \u2208 [1,k]) can thus be directly obtained from Pr\u2217(Xk+1|\u03a0k+1). Step 4: Sample records to generate private datasets. According to the topological order of the Bayesian network, Xi(i \u2208 [1,d]) can be sampled in increasing order of i based on the noisy conditional distribution. For example, when all parent attributes of Xj(j \u2208 [2,d]) are sampled, Xj can be sampled by conditional probability Pr\u2217(Xj The sampled data can then be used for publishing visualizations. |\u03a0j). Privacy Analysis. In our approach, the data custodian is located in a trust zone to elicit pattern preferences before the model performs privacy learning. The model is guided towards the right direction without additional privacy budgets. In the PriVis model, access to the original data is required for steps 2 and 3. In step 2, the model uses the exponential mechanism to privately select AP pairs, which converts selection from multiple candidate pairs into probabilistic sampling, which consumes an \u03b51 privacy budget. In step 3, Laplace noise is added to the conditional probability distributions, which consumes an \u03b52 privacy budget. According to the composition property [27] of DP, the resulting visualizations satisfy the \u03b5-DP, where \u03b5 = \u03b51 +\u03b52.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "The Pattern View (Fig. 3-B) consists of two parts: (1) The chart settings panel lets users configure visualization charts; (2) The pattern selection view supports the interactive selection of data patterns (R3). The chart settings panel (Fig. 3-B1) lets users select the chart type, color, and attributes for the xand y-axis of the charts. Three common charts (i.e., scatter, line, and bar charts) are supported. Users can quickly specify chart types by mapping selected attributes to appropriate visual encodings via drop-down boxes. In order to reduce the visual clutter for large amounts of data, the view supports step adjustment for the x-axis and aggregation calculation for the y-axis. The pattern selection view (Fig. 3-B2) contains an information card, a visualization chart, and a pattern list. Users can select the data patterns of interest from the visualization chart, which is rendered according to the above configuration. Due to the different shapes of the visualized data patterns, we provide a set of interactive selection methods. For cluster patterns in scatter plots, use the box and lasso to select regions of interest. For correlation patterns in line charts, horizontal selection indicates intervals that match a specific trend. For order patterns in bar charts, the bar being clicked is chosen. Gray backgrounds are overlaid on all selected areas. The information card shows the statistics of current and all patterns, and the pattern list records all selected patterns to track analytic provenance. Users can add the current pattern to the list and revisit previously saved patterns by clicking it.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 136,
    "paper_title": "DPVisCreator: Incorporating Pattern Constraints to Privacy-preserving Visualizations via Differential Privacy",
    "pub_year": 2023,
    "domain": "Privacy-preserving visualization",
    "requirement": {
      "requirement_text": "R4: Examine different configurations to balance the privacyutility trade-off. While the automatic generation process can yield privacy-preserving visualizations, the data pattern preference elicited by data custodians might not be sustained. Data custodians should review the protection results to check whether the current privacy budget constraints and pattern preferences produce satisfactory results. Then, they should adjust their preferences and optimize configurations to obtain acceptable results.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "health records with patients\u2019 private information",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "We focus on the widely used tabular data, which stores data records with a set of attributes as rows in a multidimensional table. Privacy preservation for tabular data is mainly achieved by syntactic anonymity and DP. Syntactic anonymity (e.g., k-anonymity [64], l-diversity [44], and t-closeness [42]) constructs equivalence groups to prevent attackers from distinguishing individuals. Unfortunately, equivalence groups could be cracked by background knowledge [17]. Compared with syntactic anonymity approaches, DP is gradually being applied to more real-world scenarios because of its robust mathematical guarantees [26]. DP is defined based on two neighboring datasets D1 and D2, which differ by adding or removing a record. Given a user-defined privacy budget \u03b5 (\u03b5 > 0), a randomized algorithm G satisfies \u03b5-differential privacy (\u03b5-DP), if and only if the following equation holds for any possible output O: Pr(G(D1) = O) \u2264 e\u03b5 \u00b7Pr(G(D2) = O). (1) Existing studies have proved that the Laplace and exponential mechanisms are feasible randomized algorithms. For a function f with numerical output, the Laplace mechanism [26] constructs a corresponding Gf by adding noise sampled from a Laplace distribution Lap( \u2206 \u03b5 f ), where \u2206f denotes the l1-sensitivity of f , that is, the maximum difference between the output of two neighboring datasets. The exponential mechanism [48] applies to function f with categorical output, which allows privately selecting the \u201cbest\u201d element from a set. Assume that q(D) is a designed score function that gives each discrete value a probability; the algorithm Gf provides \u03b5-DP if it approximately maximizes the score by returning values from the discrete domain with the probability proportional to exp( \u03b5q(D) 2\u2206q ), where \u2206q is the l1-sensitivity of q.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "PriVis generates privacy-preserving visualizations in four steps. Step 1: Understand pattern constraints. Users may select multiple patterns from different charts, represented as P = {P1,P2,\u00b7\u00b7\u00b7 ,Ps}. Each pattern Pi corresponds to a subset of data records in the whole data D. For example, as mentioned in Sec. 2.1, the data custodian needs data records with both age and illness attributes from the hospital data to generate a bar chart. To emphasize the illness risk in people over 60, the data custodian can specify the pattern by selecting bars corresponding to patients over 60 and setting a weight. User preferences for different patterns can be recorded as different subsets of data records with corresponding weight W = {w1,w2,\u00b7\u00b7\u00b7 ,ws}. For a data record r, its final weight needs to consider its occurrence under each pattern. To this end, we give each record an initial weight of 1 and then add additional weights of wk for each record r in pattern Pk. The mixture weight MW(r) of data record r is defined as: MW(r) = s \u2211 k=1 wk Pk (r) +1, (4) where Pk is indicator function which maps record in pattern Pk to 1, and all others to 0. Therefore, we translate the different pattern constraints into a mixture weight assigned to each record. Step 2: Construct a Bayesian network with pattern constraints. To preserve user-specified patterns, PriVis emphasizes the correlations between the corresponding records when constructing the Bayesian network. Specifically, when selecting AP pairs, PriVis not only employs the exponential mechanism but also replaces the mutual information in the PrivBayes network with the weighted mutual information: Iw(X,\u03a0,MW) = \u2211x\u2208dom(X) \u2211 \u03c0\u2208dom(\u03a0) \u2211r\u2208x\u2229\u03c0 MW(r) |x\u2229\u03c0| Pr(X = x,\u03a0 = \u03c0)log Pr(X = x,\u03a0 = \u03c0) Pr(X = x)Pr(\u03a0 = \u03c0) . (5) With mixture weights, the constructed Bayesian network can better preserve dependence among user-selected data. Therefore, the patterns consisting of these records can be better maintained in visualizations. Step 3: Inject noise to the Bayesian network. The con\\x02structed Bayesian network provides conditional probability distribu\\x02tions Pr(Xi|\u03a0i)(i \u2208 [1,d]) to approximate the distribution of the high\\x02dimensional dataset. To inject noise, PriVis first calculates noisy dis\\x02tribution Pr\u2217(Xi,\u03a0i) for any i \u2208 [k +1,d] by adding Laplace noise to joint probability distribution Pr(Xi,\u03a0i). Then, the conditional distribu\\x02tion Pr\u2217(Xi|\u03a0i) can be derived. The Pr\u2217(Xi|\u03a0i)(i \u2208 [1,k]) can thus be directly obtained from Pr\u2217(Xk+1|\u03a0k+1). Step 4: Sample records to generate private datasets. According to the topological order of the Bayesian network, Xi(i \u2208 [1,d]) can be sampled in increasing order of i based on the noisy conditional distribution. For example, when all parent attributes of Xj(j \u2208 [2,d]) are sampled, Xj can be sampled by conditional probability Pr\u2217(Xj The sampled data can then be used for publishing visualizations. |\u03a0j). Privacy Analysis. In our approach, the data custodian is located in a trust zone to elicit pattern preferences before the model performs privacy learning. The model is guided towards the right direction without additional privacy budgets. In the PriVis model, access to the original data is required for steps 2 and 3. In step 2, the model uses the exponential mechanism to privately select AP pairs, which converts selection from multiple candidate pairs into probabilistic sampling, which consumes an \u03b51 privacy budget. In step 3, Laplace noise is added to the conditional probability distributions, which consumes an \u03b52 privacy budget. According to the composition property [27] of DP, the resulting visualizations satisfy the \u03b5-DP, where \u03b5 = \u03b51 +\u03b52.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "The recommended scheme is visualized in the Model View (Fig. 3-C), with its structure and impact explained (R2). The Model View contextualizes the privacy-preserving process to foster users\u2019 understanding and supports users in making trade-offs be_x0002_tween data pattern maintenance and privacy protection. The Model View (Fig. 3-C) contains four parts: (1) The weight configuration panel lets users adjust importance weights to each pattern (R4); (2) The pattern constraint relationship represents the overall impact between different pattern constraints (R4); (3) The pattern constraint flow shows the detailed pattern constraint distribution on each attribute and highlight them for comparisons (R4); (4) The Bayesian network view visu_x0002_alizes the structure in the underlying PriVis model (R2). The latter two views can be switched with a button. The weight configuration panel (Fig. 3-C1) uses a stacked bar chart to provide an overview of the importance weight distribution. Colors encode the pattern constraint\u2019s type, having green for the cluster, brown for correlation, and purple for order. This color encoding is applied throughout the system. The corresponding pattern ID involving data amount and importance weights are displayed on the data pattern list and hovers on the corresponding bar. Users can adjust the correspond_x0002_ing pattern constraint weights in the data pattern list. The pattern constraint relationship (Fig. 3-C2) encodes pattern con_x0002_straints as circles. The position of a circle is determined by multidimen_x0002_sional scaling (MDS) [18], where the Wasserstein distance [68] is used to measure the similarity between patterns. The projection reflects the differences in distributions. Edges between circles encode the impact of different pattern constraints by affecting the network structure. Their colors encode the type of influence of changing weights. Green edges imply positive correlations, where increasing one pattern\u2019s weight will promote the other. In contrast, red represents negative correlations that the other pattern will be weakened. The magnitude of influence is encoded by the thickness of the edge, which is calculated by the sum of the difference between the intersection and the symmetric difference of AP pairs. Clicking a pattern constraint will highlight it and its adjacent edges, together with the corresponding entities in other views. The pattern constraint flow (Fig. 3-C3) improves the basic Sankey diagram with sub-flows representing constraints in each attribute. The x-axis lists all attributes, and the y-axis indicates the discrete interval of each attribute. Continuous attributes are discretized by k-means with the elbow method [46], while categorical attributes use their discrete do_x0002_main as the interval. The length along the y-axis represents the amount of data belonging to this interval. The specific interval information can be viewed by hovering. Different flows represent different data distributions, and the width of a flow is proportional to its data volume. By default, all data flows are grayed out to provide a clear background, while the selected data pattern will be highlighted correspondingly for a more intuitive comparison. The Bayesian network view (Fig. 3-C4) shows the structure of the PriVis model. Since the network is a DAG, we use a hierarchical layout [4] to show the dependencies of attributes. Specifically, the depth of each node is obtained by topological sorting. We draw each layer in turn along the x-axis, and nodes in the same layer are evenly distributed along the y-axis. Nodes represent attributes, and edges reflect conditional dependencies. The probability density distributions before and after noise addition are displayed by superposition when hovering over a node, making it easy to compare and understand the exact degree of privacy protection.",
        "solution_category": "visualization",
        "solution_axial": "Non-composite",
        "solution_compoent": "Sankey+Bar+Circle+Tree",
        "axial_code": ["Non-composite"],
        "componenet_code": ["Sankey", "Bar", "Tree", "Circle"]
      },
      {
        "solution_text": "The recommended scheme is visualized in the Model View (Fig. 3-C), with its structure and impact explained (R2). The Model View contextualizes the privacy-preserving process to foster users\u2019 understanding and supports users in making trade-offs be_x0002_tween data pattern maintenance and privacy protection. The Model View (Fig. 3-C) contains four parts: (1) The weight configuration panel lets users adjust importance weights to each pattern (R4); (2) The pattern constraint relationship represents the overall impact between different pattern constraints (R4); (3) The pattern constraint flow shows the detailed pattern constraint distribution on each attribute and highlight them for comparisons (R4); (4) The Bayesian network view visu_x0002_alizes the structure in the underlying PriVis model (R2). The latter two views can be switched with a button. The weight configuration panel (Fig. 3-C1) uses a stacked bar chart to provide an overview of the importance weight distribution. Colors encode the pattern constraint\u2019s type, having green for the cluster, brown for correlation, and purple for order. This color encoding is applied throughout the system. The corresponding pattern ID involving data amount and importance weights are displayed on the data pattern list and hovers on the corresponding bar. Users can adjust the correspond_x0002_ing pattern constraint weights in the data pattern list. The pattern constraint relationship (Fig. 3-C2) encodes pattern con_x0002_straints as circles. The position of a circle is determined by multidimen_x0002_sional scaling (MDS) [18], where the Wasserstein distance [68] is used to measure the similarity between patterns. The projection reflects the differences in distributions. Edges between circles encode the impact of different pattern constraints by affecting the network structure. Their colors encode the type of influence of changing weights. Green edges imply positive correlations, where increasing one pattern\u2019s weight will promote the other. In contrast, red represents negative correlations that the other pattern will be weakened. The magnitude of influence is encoded by the thickness of the edge, which is calculated by the sum of the difference between the intersection and the symmetric difference of AP pairs. Clicking a pattern constraint will highlight it and its adjacent edges, together with the corresponding entities in other views. The pattern constraint flow (Fig. 3-C3) improves the basic Sankey diagram with sub-flows representing constraints in each attribute. The x-axis lists all attributes, and the y-axis indicates the discrete interval of each attribute. Continuous attributes are discretized by k-means with the elbow method [46], while categorical attributes use their discrete do_x0002_main as the interval. The length along the y-axis represents the amount of data belonging to this interval. The specific interval information can be viewed by hovering. Different flows represent different data distributions, and the width of a flow is proportional to its data volume. By default, all data flows are grayed out to provide a clear background, while the selected data pattern will be highlighted correspondingly for a more intuitive comparison. The Bayesian network view (Fig. 3-C4) shows the structure of the PriVis model. Since the network is a DAG, we use a hierarchical layout [4] to show the dependencies of attributes. Specifically, the depth of each node is obtained by topological sorting. We draw each layer in turn along the x-axis, and nodes in the same layer are evenly distributed along the y-axis. Nodes represent attributes, and edges reflect conditional dependencies. The probability density distributions before and after noise addition are displayed by superposition when hovering over a node, making it easy to compare and understand the exact degree of privacy protection.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "dxf",
    "index_original": 137,
    "paper_title": "DPVisCreator: Incorporating Pattern Constraints to Privacy-preserving Visualizations via Differential Privacy",
    "pub_year": 2023,
    "domain": "Privacy-preserving visualization",
    "requirement": {
      "requirement_text": "R5: Compare different privacy protection schemes. For different analysis purposes, data custodians may indicate different data pattern preferences. They want to compare different privacy protection schemes for an optimal solution to their goal. We should provide quantitative metrics to help them evaluate these schemes. Moreover, multiple charts might be produced under a single privacy protection scheme, and the desired data patterns might not be easily perceived among these charts. Therefore, we should also provide support for qualitative assessments of patterns.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "health records with patients\u2019 private information",
      "data_code": { "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "We focus on the widely used tabular data, which stores data records with a set of attributes as rows in a multidimensional table. Privacy preservation for tabular data is mainly achieved by syntactic anonymity and DP. Syntactic anonymity (e.g., k-anonymity [64], l-diversity [44], and t-closeness [42]) constructs equivalence groups to prevent attackers from distinguishing individuals. Unfortunately, equivalence groups could be cracked by background knowledge [17]. Compared with syntactic anonymity approaches, DP is gradually being applied to more real-world scenarios because of its robust mathematical guarantees [26]. DP is defined based on two neighboring datasets D1 and D2, which differ by adding or removing a record. Given a user-defined privacy budget \u03b5 (\u03b5 > 0), a randomized algorithm G satisfies \u03b5-differential privacy (\u03b5-DP), if and only if the following equation holds for any possible output O: Pr(G(D1) = O) \u2264 e\u03b5 \u00b7Pr(G(D2) = O). (1) Existing studies have proved that the Laplace and exponential mechanisms are feasible randomized algorithms. For a function f with numerical output, the Laplace mechanism [26] constructs a corresponding Gf by adding noise sampled from a Laplace distribution Lap( \u2206 \u03b5 f ), where \u2206f denotes the l1-sensitivity of f , that is, the maximum difference between the output of two neighboring datasets. The exponential mechanism [48] applies to function f with categorical output, which allows privately selecting the \u201cbest\u201d element from a set. Assume that q(D) is a designed score function that gives each discrete value a probability; the algorithm Gf provides \u03b5-DP if it approximately maximizes the score by returning values from the discrete domain with the probability proportional to exp( \u03b5q(D) 2\u2206q ), where \u2206q is the l1-sensitivity of q.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmicCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmicCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "PriVis generates privacy-preserving visualizations in four steps. Step 1: Understand pattern constraints. Users may select multiple patterns from different charts, represented as P = {P1,P2,\u00b7\u00b7\u00b7 ,Ps}. Each pattern Pi corresponds to a subset of data records in the whole data D. For example, as mentioned in Sec. 2.1, the data custodian needs data records with both age and illness attributes from the hospital data to generate a bar chart. To emphasize the illness risk in people over 60, the data custodian can specify the pattern by selecting bars corresponding to patients over 60 and setting a weight. User preferences for different patterns can be recorded as different subsets of data records with corresponding weight W = {w1,w2,\u00b7\u00b7\u00b7 ,ws}. For a data record r, its final weight needs to consider its occurrence under each pattern. To this end, we give each record an initial weight of 1 and then add additional weights of wk for each record r in pattern Pk. The mixture weight MW(r) of data record r is defined as: MW(r) = s \u2211 k=1 wk Pk (r) +1, (4) where Pk is indicator function which maps record in pattern Pk to 1, and all others to 0. Therefore, we translate the different pattern constraints into a mixture weight assigned to each record. Step 2: Construct a Bayesian network with pattern constraints. To preserve user-specified patterns, PriVis emphasizes the correlations between the corresponding records when constructing the Bayesian network. Specifically, when selecting AP pairs, PriVis not only employs the exponential mechanism but also replaces the mutual information in the PrivBayes network with the weighted mutual information: Iw(X,\u03a0,MW) = \u2211x\u2208dom(X) \u2211 \u03c0\u2208dom(\u03a0) \u2211r\u2208x\u2229\u03c0 MW(r) |x\u2229\u03c0| Pr(X = x,\u03a0 = \u03c0)log Pr(X = x,\u03a0 = \u03c0) Pr(X = x)Pr(\u03a0 = \u03c0) . (5) With mixture weights, the constructed Bayesian network can better preserve dependence among user-selected data. Therefore, the patterns consisting of these records can be better maintained in visualizations. Step 3: Inject noise to the Bayesian network. The con\\x02structed Bayesian network provides conditional probability distribu\\x02tions Pr(Xi|\u03a0i)(i \u2208 [1,d]) to approximate the distribution of the high\\x02dimensional dataset. To inject noise, PriVis first calculates noisy dis\\x02tribution Pr\u2217(Xi,\u03a0i) for any i \u2208 [k +1,d] by adding Laplace noise to joint probability distribution Pr(Xi,\u03a0i). Then, the conditional distribu\\x02tion Pr\u2217(Xi|\u03a0i) can be derived. The Pr\u2217(Xi|\u03a0i)(i \u2208 [1,k]) can thus be directly obtained from Pr\u2217(Xk+1|\u03a0k+1). Step 4: Sample records to generate private datasets. According to the topological order of the Bayesian network, Xi(i \u2208 [1,d]) can be sampled in increasing order of i based on the noisy conditional distribution. For example, when all parent attributes of Xj(j \u2208 [2,d]) are sampled, Xj can be sampled by conditional probability Pr\u2217(Xj The sampled data can then be used for publishing visualizations. |\u03a0j). Privacy Analysis. In our approach, the data custodian is located in a trust zone to elicit pattern preferences before the model performs privacy learning. The model is guided towards the right direction without additional privacy budgets. In the PriVis model, access to the original data is required for steps 2 and 3. In step 2, the model uses the exponential mechanism to privately select AP pairs, which converts selection from multiple candidate pairs into probabilistic sampling, which consumes an \u03b51 privacy budget. In step 3, Laplace noise is added to the conditional probability distributions, which consumes an \u03b52 privacy budget. According to the composition property [27] of DP, the resulting visualizations satisfy the \u03b5-DP, where \u03b5 = \u03b51 +\u03b52.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "The Solution View (Fig. 3-D) measures privacy-preserving schemes from different perspectives (R4) and shows comparisons for selecting an appropriate scheme (R5). It contains two parts: (1) The schemes ranking list displays all privacy-preserving schemes. Inspired by lineup [31], the schemes\u2019 metrics are organized into a multidimensional table. (2) The pattern comparison view uses the superposition technique of visual comparisons [30] to highlight the scheme\u2019s effect. The schemes ranking list (Fig. 3-D1) shows evaluation metrics for each privacy protection scheme in detail, including privacy budgets, statistical indicators, and pattern retention metrics. KSTest [7] and CSTest [32] analyze the differences in statistical properties of data. As to utility, suitable metrics are adopted for different patterns, such as Wasserstein distance for the cluster, dynamic time wrapping (DTW) for correlation, and Euclidean distance for order. These metrics reflect the degree of pattern retention. Users can choose different metrics to measure the scheme\u2019s utility according to their needs. The length of the horizontal bar encodes the metrics\u2019 value. We explicitly encode the differences in the metrics before and after privacy preservation for comparing pattern-related metrics. The increased proportion is represented by green bars, while the decreased proportion is represented by red striped bars. Users can rank schemes based on metrics with sorting and filtering functions. The pattern comparison view (Fig. 3-D2) shows the visualization chart after privacy protection. The gray border represents the previous selection area, and the gray visual elements represent the original data. This visual comparison helps users qualitatively perceive the difference in visualization utility before and after. Users can also switch off the pattern constraints, i.e., to compare the chart with the baseline.",
        "solution_category": "visualization",
        "solution_axial": "Non-composite",
        "solution_compoent": "Table+Bar",
        "axial_code": ["Non-composite"],
        "componenet_code": ["Bar", "Table"]
      },
      {
        "solution_text": "Users can choose different metrics to measure the scheme\u2019s utility according to their needs. The length of the horizontal bar encodes the metrics\u2019 value. We explicitly encode the differences in the metrics before and after privacy preservation for comparing pattern-related metrics. The increased proportion is represented by green bars, while the decreased proportion is represented by red striped bars. Users can rank schemes based on metrics with sorting and filtering functions. The pattern comparison view (Fig. 3-D2) shows the visualization chart after privacy protection. The gray border represents the previous selection area, and the gray visual elements represent the original data. This visual comparison helps users qualitatively perceive the difference in visualization utility before and after. Users can also switch off the pattern constraints, i.e., to compare the chart with the baseline. ",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration+Selecting+Connect/Relate",
        "solution_compoent": "",
        "axial_code": [
          "Selecting",
          "Connect/Relate",
          "Participation/Collaboration"
        ],
        "componenet_code": [
          "selecting",
          "connect/relate",
          "participation/collaboration"
        ]
      }
    ]
  }
]
