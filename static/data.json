[
  {
    "author": "zsz",
    "index_original": 2,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T1: Cell Type Discovery and Calling. The task most frequently mentioned by all experts is identifying and analyzing speci\ufb01c types and states of cells based on the intensity and pattern of staining with speci\ufb01c antibodies (O1, O2, P1, P2, CB1-6). Challenges: Challenges lie in processing, displaying, and faceting the large and high-dimensional data, as well as in mutual support of manual and automated analysis. A lack of adequate tools makes this task very time-consuming at present.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "tables": 1,
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "Classification and clustering can be triggered in different views to support the hierarchical discovery of cell types.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 3,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T1: Cell Type Discovery and Calling. The task most frequently mentioned by all experts is identifying and analyzing speci\ufb01c types and states of cells based on the intensity and pattern of staining with speci\ufb01c antibodies (O1, O2, P1, P2, CB1-6). Challenges: Challenges lie in processing, displaying, and faceting the large and high-dimensional data, as well as in mutual support of manual and automated analysis. A lack of adequate tools makes this task very time-consuming at present.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "tables": 1,
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 4,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T2: Overview-Detail Exploration of Multi-Channel Image Data. A crucial task for oncologists and pathologists is rapid navigation and visualization of multi-channel images (O1, O2, P1, P2). Pathologists are accustomed to moving slides back and forth physically on a micro-scope stage and switching between high and low power views. They rely on a seamless visual experience to make a diagnosis. Challenges: Image analysis must not only support seamless pan and zoom, but also switching between groups of channels. Current tools do not scale beyond 4-5 channels and lack on-demand rendering, blending of channels, and means to emphasize (and recall) regions or individual cells of interest.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "tables": 1,
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "Facetto\u2019s image viewer allows users to navigate and explore large multi- channel image data using a scalable multi-resolution visualization approach. The visualization supports interactive and seamless zooming and panning, on-demand rendering of multi-channel information, man- ual selection of cells, and highlighting of classification and clustering results as well as faceting operations. The image viewer also provides details on demand when hovering over individual cells.",
        "solution_category": "interaction",
        "solution_axial": "Filtering,OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["Filtering", "OverviewandExplore"],
        "componenet_code": ["filtering", "overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 5,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T2: Overview-Detail Exploration of Multi-Channel Image Data. A crucial task for oncologists and pathologists is rapid navigation and visualization of multi-channel images (O1, O2, P1, P2). Pathologists are accustomed to moving slides back and forth physically on a micro-scope stage and switching between high and low power views. They rely on a seamless visual experience to make a diagnosis. Challenges: Image analysis must not only support seamless pan and zoom, but also switching between groups of channels. Current tools do not scale beyond 4-5 channels and lack on-demand rendering, blending of channels, and means to emphasize (and recall) regions or individual cells of interest.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "tables": 1,
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Support dynamic selection of cells and their visual display in image space and dynamically adjust the render mode based on the cur-rent pixel\u2019s cell ID. ",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "image+area",
        "axial_code": ["Coordinate"],
        "componenet_code": ["area", "image"]
      },
      {
        "solution_text": "Facetto supports different render modes, depending on whether the user is currently focusing on the entire dataset, a subset(i.e., a node in the hierarchical phenotype tree), or a user selection (i.e.,based either on manual selection or a clustering/classification result).Facetto can then display individual cells in their original grayscale in-tensity, apply a color and opacity transfer function, or show color overlays for cluster/class membership.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 6,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T3: Data Filtering and (Sub-)Structuring. Another task frequently performed by pathologists is gating, which refers to manual \ufb01ltering of selected image channels based on the channel\u2019s intensity value range (often visualized as a frequency-intensity plot), or speci\ufb01c spatial features or regions of interest (P2, O2, CB2). Challenges: Analysis steps such as gating are often applied in an iterative manner in which the data is hierarchically faceted into subsets. These subsets can then be further analyzed, used in benchmarks, or exported for presentation or reuse with other samples. Thus, tracking the evolution and provenance of gates and gated data is important.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": { "tables": 1, "clusters_and_sets_and_lists": 1, "media": 1 }
    },
    "solution": [
      {
        "solution_text": "Experts usually start with a region of interest (ROI) at high resolution (so thatindividuals cells are visible) that represents a subset of the completemulti-channel image stack and then define spatial and image featuresto create data subsets of interest. The results obtained on this ROI are then applied to the entire specimen",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "tree+circle",
        "axial_code": ["Nesting"],
        "componenet_code": ["tree", "circle"]
      },
      {
        "solution_text": "Experts usually start with a region of interest (ROI) at high resolution (so thatindividuals cells are visible) that represents a subset of the completemulti-channel image stack and then define spatial and image featuresto create data subsets of interest. The results obtained on this ROI are then applied to the entire specimen",
        "solution_category": "interaction",
        "solution_axial": "Extractionoffeatures",
        "solution_compoent": "",
        "axial_code": ["Extractionoffeatures"],
        "componenet_code": ["extraction_of_features"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 7,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T3: Data Filtering and (Sub-)Structuring. Another task frequently performed by pathologists is gating, which refers to manual \ufb01ltering of selected image channels based on the channel\u2019s intensity value range (often visualized as a frequency-intensity plot), or speci\ufb01c spatial features or regions of interest (P2, O2, CB2). Challenges: Analysis steps such as gating are often applied in an iterative manner in which the data is hierarchically faceted into subsets. These subsets can then be further analyzed, used in benchmarks, or exported for presentation or reuse with other samples. Thus, tracking the evolution and provenance of gates and gated data is important.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "tables": 1,
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Visually encode features of interest and subsets of the data. To make spatial distributions and correlations among multiple channels pre-attentively visible, we allow the user to blend the data from different channels into a single image.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "image+area",
        "axial_code": ["Coordinate"],
        "componenet_code": ["area", "image"]
      },
      {
        "solution_text": "Users select the appropriate channels for their current tasks based on domain knowledge. For each channel, users can set and modify a linear color and opacity transfer function by specifying the respective intensity range (i.e., lower and upper bound) as well as the colors of the transfer function.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 8,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T3: Data Filtering and (Sub-)Structuring. Another task frequently performed by pathologists is gating, which refers to manual \ufb01ltering of selected image channels based on the channel\u2019s intensity value range (often visualized as a frequency-intensity plot), or speci\ufb01c spatial features or regions of interest (P2, O2, CB2). Challenges: Analysis steps such as gating are often applied in an iterative manner in which the data is hierarchically faceted into subsets. These subsets can then be further analyzed, used in benchmarks, or exported for presentation or reuse with other samples. Thus, tracking the evolution and provenance of gates and gated data is important.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "tables": 1,
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Support dynamic selection of cells and their visual display in image space and dynamically adjust the render mode based on the cur-rent pixel\u2019s cell ID. ",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "image+area",
        "axial_code": ["Coordinate"],
        "componenet_code": ["area", "image"]
      },
      {
        "solution_text": "Facetto supports different render modes, depending on whether the user is currently focusing on the entire dataset, a subset(i.e., a node in the hierarchical phenotype tree), or a user selection (i.e.,based either on manual selection or a clustering/classification result).Facetto can then display individual cells in their original grayscale in-tensity, apply a color and opacity transfer function, or show color overlays for cluster/class membership.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 9,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T3: Data Filtering and (Sub-)Structuring. Another task frequently performed by pathologists is gating, which refers to manual \ufb01ltering of selected image channels based on the channel\u2019s intensity value range (often visualized as a frequency-intensity plot), or speci\ufb01c spatial features or regions of interest (P2, O2, CB2). Challenges: Analysis steps such as gating are often applied in an iterative manner in which the data is hierarchically faceted into subsets. These subsets can then be further analyzed, used in benchmarks, or exported for presentation or reuse with other samples. Thus, tracking the evolution and provenance of gates and gated data is important.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "tables": 1,
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Each ridge is equipped with range sliders, allowing to filter theunderlying data with visual feedback at the level of the distribution.The selection range is used directly for specifying the color transferfunction applied in the image viewer. To allow the exploration of fea-ture distributions for subsets of the image or individually selected cells,we overlay the parallel x-axes in the ridgeplot with vertical polylines(see orange paths), encoding each cell\u2019s features as a connectedpath, similar to parallel coordinate plots [36]. To reduce clutter, wedecrease opacity as the number of selected cells grows, an approachthat provides an indication of the correlations and distribution rangesof a selection, while focusing less on individual cells. Alternatively, asingle polyline can be used to represent the mean values of a cluster.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "image+parallelcoordinates",
        "axial_code": ["Stack"],
        "componenet_code": ["image", "parallelcoordinates"]
      },
      {
        "solution_text": "Each ridge is equipped with range sliders, allowing to filter theunderlying data with visual feedback at the level of the distribution.The selection range is used directly for specifying the color transferfunction applied in the image viewer. To allow the exploration of fea-ture distributions for subsets of the image or individually selected cells,we overlay the parallel x-axes in the ridgeplot with vertical polylines(see orange paths), encoding each cell\u2019s features as a connectedpath, similar to parallel coordinate plots [36]. To reduce clutter, wedecrease opacity as the number of selected cells grows, an approachthat provides an indication of the correlations and distribution rangesof a selection, while focusing less on individual cells. Alternatively, asingle polyline can be used to represent the mean values of a cluster.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 10,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T4: Proofreading and Analyzing Results in Spatial Context. Many algorithms operate on features computed from images following segmentation; these include mean intensity value per cell and channel. Feature extraction and segmentation from tissues, in which cells of different sizes and shapes are crowded together, are challenging tasks for which software tools are still being developed. As a result, it is essential that the results of feature extraction are checked and corrected prior to downstream data processing (CB1, CB3). This requires effective means to link feature and image space (P1, O1). Challenges: Currently, such linking is only supported by HistoCat [60], and generally requires domain experts to continuously switch between tools (CB2).",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "tables": 1,
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "Visually encode features of interest and subsets of the data. To make spatial distributions and correlations among multiple channels pre-attentively visible, we allow the user to blend the data from different channels into a single image.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "image+area",
        "axial_code": ["Coordinate"],
        "componenet_code": ["area", "image"]
      },
      {
        "solution_text": "Users select the appropriate channels for their current tasks based on domain knowledge. For each channel, users can set and modify a linear color and opacity transfer function by specifying the respective intensity range (i.e., lower and upper bound) as well as the colors of the transfer function.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 11,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T4: Proofreading and Analyzing Results in Spatial Context. Many algorithms operate on features computed from images following segmentation; these include mean intensity value per cell and channel. Feature extraction and segmentation from tissues, in which cells of different sizes and shapes are crowded together, are challenging tasks for which software tools are still being developed. As a result, it is essential that the results of feature extraction are checked and corrected prior to downstream data processing (CB1, CB3). This requires effective means to link feature and image space (P1, O1). Challenges: Currently, such linking is only supported by HistoCat [60], and generally requires domain experts to continuously switch between tools (CB2).",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": { "tables": 1, "clusters_and_sets_and_lists": 1, "media": 1 }
    },
    "solution": [
      {
        "solution_text": "Experts can sort, inspect, select,and manipulate individual values for each feature of a cell using theinteractive visual tabular display. The main goal of thetabular view is a) detailed analysis and direct manipulation of individualcells, and b) allowing users access to the original data table. We encodethe extracted intensity values in the tabular view as numbers, as wellas by using small multiples of bars. The tabular view is also colorencoded, with each color indicating a distinct phenotypic class. Allviews in Facetto are connected via brushing and linking so that userscan analyze a selection from different perspectives. In this way, userscan mark (and edit) individual cells with certain features in the tabularview and inspect spatial context in the image view or vice versa.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "bar"]
      },
      {
        "solution_text": "Experts can sort, inspect, select,and manipulate individual values for each feature of a cell using theinteractive visual tabular display. The main goal of thetabular view is a) detailed analysis and direct manipulation of individualcells, and b) allowing users access to the original data table. We encodethe extracted intensity values in the tabular view as numbers, as wellas by using small multiples of bars. The tabular view is also colorencoded, with each color indicating a distinct phenotypic class. Allviews in Facetto are connected via brushing and linking so that userscan analyze a selection from different perspectives. In this way, userscan mark (and edit) individual cells with certain features in the tabularview and inspect spatial context in the image view or vice versa.",
        "solution_category": "interaction",
        "solution_axial": "Selecting,Reconfigure,Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Reconfigure", "Participation/Collaboration", "Selecting"],
        "componenet_code": [
          "reconfigure",
          "participation/collaboration",
          "selecting"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 12,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T5: Deriving Pro\ufb01les for (Sub)regions and Classes. Once a type/region is detected, it is important to identify, annotate, and extract a pro\ufb01le of typical marker distributions within an area of interest (O2, P2). The pro\ufb01le includes statistical measures and distributions of cell features and can be used to present the outcome of an analysis session, diagnosis, or as a starting point for further analysis. Challenges: The variables used to construct pro\ufb01les, and the ways in which these variables are displayed, are not standardized and can only be developed by human-machine interaction.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": { "tables": 1, "clusters_and_sets_and_lists": 1, "media": 1 }
    },
    "solution": [
      {
        "solution_text": "Users can build up a hierarchy of different data subsets, and we automatically display this ongoing analysis in a hierarchical phenotype tree view. This allows users to track their progress, and to maintain an overview of their data faceting and analysis steps they have performed.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "tree+circle",
        "axial_code": ["Nesting"],
        "componenet_code": ["tree", "circle"]
      },
      {
        "solution_text": "Users can build up a hierarchy of different data subsets, and we automatically display this ongoing analysis in a hierarchical phenotype tree view. This allows users to track their progress, and to maintain an overview of their data faceting and analysis steps they have performed.",
        "solution_category": "interaction",
        "solution_axial": "Selecting,Abstract/Elaborate",
        "solution_compoent": "",
        "axial_code": ["Abstract/Elaborate", "Selecting"],
        "componenet_code": ["abstract/elaborate", "selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 13,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T5: Deriving Pro\ufb01les for (Sub)regions and Classes. Once a type/region is detected, it is important to identify, annotate, and extract a pro\ufb01le of typical marker distributions within an area of interest (O2, P2). The pro\ufb01le includes statistical measures and distributions of cell features and can be used to present the outcome of an analysis session, diagnosis, or as a starting point for further analysis. Challenges: The variables used to construct pro\ufb01les, and the ways in which these variables are displayed, are not standardized and can only be developed by human-machine interaction.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "tables": 1,
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "To reveal contextual feature information for anindividual cell or for a selection, users can click on a cell in the im-age viewer and show a visual profile card in which data statistics aresummarized. The card shows a boxplot of the feature space, thephenotype labels, and a short summary that includes any previous userannotation, making it possible for information to be acquired sequen-tially over a number of sessions involving multiple users.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "scatter",
        "axial_code": ["Repetition"],
        "componenet_code": ["scatter"]
      },
      {
        "solution_text": "To reveal contextual feature information for anindividual cell or for a selection, users can click on a cell in the im-age viewer and show a visual profile card in which data statistics aresummarized. The card shows a boxplot of the feature space, thephenotype labels, and a short summary that includes any previous userannotation, making it possible for information to be acquired sequen-tially over a number of sessions involving multiple users.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 14,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T5: Deriving Pro\ufb01les for (Sub)regions and Classes. Once a type/region is detected, it is important to identify, annotate, and extract a pro\ufb01le of typical marker distributions within an area of interest (O2, P2). The pro\ufb01le includes statistical measures and distributions of cell features and can be used to present the outcome of an analysis session, diagnosis, or as a starting point for further analysis. Challenges: The variables used to construct pro\ufb01les, and the ways in which these variables are displayed, are not standardized and can only be developed by human-machine interaction.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "tables": 1,
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "Specifically, we leverage clustering as a means to discover novel cell subtypes (T1) and acquire new knowledge, and classification as a means to propagate the learned cell types/states across CyCIF images (T1).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "For the exploration of each chan-nel\u2019s distribution, we integrated a ridgeplot that comprisesmultiple areas alongside relevant information about the variablebeing examined. Each chart represents a feature\u2019s value distribu-tion (a ridge). Typically, the distribution of features that are derivedfrom a channel\u2019s intensity values is skewed, having a few distinct peaksand some outliers.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "image+parallelcoordinates",
        "axial_code": ["Stack"],
        "componenet_code": ["image", "parallelcoordinates"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 15,
    "paper_title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data",
    "pub_year": 2020,
    "domain": "Healthcare",
    "requirement": {
      "requirement_text": "T5: Deriving Pro\ufb01les for (Sub)regions and Classes. Once a type/region is detected, it is important to identify, annotate, and extract a pro\ufb01le of typical marker distributions within an area of interest (O2, P2). The pro\ufb01le includes statistical measures and distributions of cell features and can be used to present the outcome of an analysis session, diagnosis, or as a starting point for further analysis. Challenges: The variables used to construct pro\ufb01les, and the ways in which these variables are displayed, are not standardized and can only be developed by human-machine interaction.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each image tile in CyCIF is a 16 bit grayscale image,typically comprising 4 \u00d7106 pixels (the dimensions of a scientific gradeCMOS camera). Each channel is recorded in a separate grayscale imagethat is registered to other channels and pseudocolored for visualization.Segmentation assigns an ID (cell ID) to each cell in the stitched image.",
      "data_code": {
        "tables": 1,
        "geometry": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "We visualize higher-order similarities and differences between data subsets using dimensionality reduction techniques and subsequent displayin a 2D scatterplot. We use UMAP (uniform manifold approximationand projection for dimension reduction), a recently developed ma-chine learning technique, to display features of the current data subset.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Discovercellsubtypes,propagatelearnedtypesacrossimages",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      },
      {
        "solution_text": "We visualize higher-order similarities and differences between data subsets using dimensionality reduction techniques and subsequent displayin a 2D scatterplot. We use UMAP (uniform manifold approximationand projection for dimension reduction), a recently developed ma-chine learning technique, to display features of the current data subset.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "scatter",
        "axial_code": ["Repetition"],
        "componenet_code": ["scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 18,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Obtain the emotion status of all the people in a video. Given a specific video, users have a great interest in gaining a quick overview of the video content. For example, what is the overall emotion trend as the video progresses? What kind of emotion dominates the video? Compared with checking the original video back and forth, a visual overview would greatly reduce the browsing burden.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "media": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "It is important to provide users with an overview of the emotion evolution of individuals (R1). Thus, we design a summary view to provide users with both a static and a dynamic summary of the emotions: the emotion archives (Fig. 3a) to visualize the emotion distribution of individuals (static summary), and the emotion \ufb02ow (Fig. 3b) to show the dynamic evolution of these emotions (dynamic summary).",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling,Modeling",
        "solution_compoent": "Videosampling,categoricalmodels",
        "axial_code": ["Modeling", "Sampling"],
        "componenet_code": ["modeling", "sampling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 19,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Obtain the emotion status of all the people in a video. Given a specific video, users have a great interest in gaining a quick overview of the video content. For example, what is the overall emotion trend as the video progresses? What kind of emotion dominates the video? Compared with checking the original video back and forth, a visual overview would greatly reduce the browsing burden.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "media": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "It is important to provide users with an overview of the emotion evolution of individuals. Thus, we design a summary view to provide users with both a static and a dynamic summary of the emotions: the emotion flow to show the dynamic evolution of these emotions (dynamic summary).",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling,Modeling",
        "solution_compoent": "Videosampling,categoricalmodels",
        "axial_code": ["Modeling", "Sampling"],
        "componenet_code": ["modeling", "sampling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 20,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Uncover emotion patterns of an individual in a video. After gaining an overview of the given video, users concentrate on an individual of interest. For example, most parents are concerned about their own children, and they are likely to explore individuals in a video. What is the emotion pattern of a selected person in this video? How do his/her emotions evolve over time?",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "media": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We use lines to connect the emotion archives to the corresponding flows. Each line represents a person, starting from a person selected in the emotion archives, and then connects to his/her emotion flow. Therefore, users can easily track personal emotion evolution.",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling,Modeling",
        "solution_compoent": "Videosampling,categoricalmodels",
        "axial_code": ["Modeling", "Sampling"],
        "componenet_code": ["modeling", "sampling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 21,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Uncover emotion patterns of an individual in a video. After gaining an overview of the given video, users concentrate on an individual of interest. For example, most parents are concerned about their own children, and they are likely to explore individuals in a video. What is the emotion pattern of a selected person in this video? How do his/her emotions evolve over time?",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "media": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The character view visualizes the emotion status of a selected person with a portrait glyph. We adopt a tailored donut chart in this design. Each annular sector on the outer part represents an emotion. The area of each annular sector illustrates the amount of the corresponding emotion which appears in the video.",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling,Modeling",
        "solution_compoent": "Videosampling,categoricalmodels",
        "axial_code": ["Modeling", "Sampling"],
        "componenet_code": ["modeling", "sampling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 22,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Compare emotion portraits of different people. Users would like to explore a person of interest, especially to obtain his/her relative status in a video. Further comparisons between different people empower users to identify abnormal patterns. For example, teachers may worry about a special student in the class, and parents are curious about whether their children behave differently compared to others. Therefore, comparing different people\u2019s emotion patterns and measuring their similarity and difference are very valuable for users.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "media": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We use lines to connect the emotion archives to the corresponding flows. Each line represents a person, starting from a person selected in the emotion archives, and then connects to his/her emotion flow. Therefore, users can easily compare the emotion evolution of different people.",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling,Modeling",
        "solution_compoent": "Videosampling,categoricalmodels",
        "axial_code": ["Modeling", "Sampling"],
        "componenet_code": ["modeling", "sampling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 23,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Compare emotion portraits of different people. Users would like to explore a person of interest, especially to obtain his/her relative status in a video. Further comparisons between different people empower users to identify abnormal patterns. For example, teachers may worry about a special student in the class, and parents are curious about whether their children behave differently compared to others. Therefore, comparing different people\u2019s emotion patterns and measuring their similarity and difference are very valuable for users.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "media": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Comparison between different emotion portraits enables users to identify and compare the characteristics of different people. We adopt a tailored donut chart in this design. Each annular sector on the outer part represents an emotion. The area of each annular sector illustrates the amount of the corresponding emotion which appears in the video.",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling,Modeling",
        "solution_compoent": "Videosampling,categoricalmodels",
        "axial_code": ["Modeling", "Sampling"],
        "componenet_code": ["modeling", "sampling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 24,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Compare emotion portraits of different people. Users would like to explore a person of interest, especially to obtain his/her relative status in a video. Further comparisons between different people empower users to identify abnormal patterns. For example, teachers may worry about a special student in the class, and parents are curious about whether their children behave differently compared to others. Therefore, comparing different people\u2019s emotion patterns and measuring their similarity and difference are very valuable for users.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "media": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The comparison of the basic emotion information between different people become easy with the screen snapshot function. If users want to explore details, they can click the snapshot of interest for further exploration. Snapshot examples are demonstrated on the left-hand side of the character view.",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling,Modeling",
        "solution_compoent": "Videosampling,categoricalmodels",
        "axial_code": ["Modeling", "Sampling"],
        "componenet_code": ["modeling", "sampling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 25,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Reveal model uncertainty with influencing factors. Emotion recognition algorithms are not perfect and the accuracy is influenced by multiple factors. Leveraging these factors properly can provide useful cues for inferring underlying patterns. For example, the accuracy of emotion recognition probably decreases, when the algorithm processes a child face image with a small face size in the video or occluded by others. It would also be better to allow users to investigate model accuracy and correct corresponding errors if needed.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "media": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The influencing factor bar chart mainly shows the aggregation information of face size and occlusion. We use a bar to encode different influencing factors foreach time span. The height of the bar indicates the face size detected in the video (the higher, the larger face size), whereas the black shading area of the bar represents the occlusion degree.",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling",
        "solution_compoent": "Videosampling",
        "axial_code": ["Sampling"],
        "componenet_code": ["sampling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 26,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Reveal model uncertainty with influencing factors. Emotion recognition algorithms are not perfect and the accuracy is influenced by multiple factors. Leveraging these factors properly can provide useful cues for inferring underlying patterns. For example, the accuracy of emotion recognition probably decreases, when the algorithm processes a child face image with a small face size in the video or occluded by others. It would also be better to allow users to investigate model accuracy and correct corresponding errors if needed.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "media": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We can easily observe detailed emotion information and influencing factors for the person of interest with such a tailored donut chart design. ",
        "solution_category": "data_manipulation",
        "solution_axial": "Sampling,Modeling",
        "solution_compoent": "Videosampling,categoricalmodels",
        "axial_code": ["Modeling", "Sampling"],
        "componenet_code": ["modeling", "sampling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 27,
    "paper_title": "EmotionCues: Emotion-Oriented Visual Summarization of Classroom Videos",
    "pub_year": 2021,
    "domain": "Emotion",
    "requirement": {
      "requirement_text": "Provide context for video analysis. The visual analytics system is based on complex recognition models and abstract data representation. Users also want to know the original video context, which helps them understand the analytical results and validate assumptions. For example, what kind of scenario leads to a change of emotions? Do their assumptions about these findings make sense?",
      "requirement_code": {
        "evaluate_hypothesis": 1,
        "describe_observation_item": 1
      }
    },
    "data": {
      "data_text": "The video data we use are mainly collected from our collabo_x0002_rating kindergartens. Teachers in the kindergartens use differ_x0002_ent cameras to shoot videos of children in class. Each video is about 10 minutes long (1.26 G) with a resolution of 1920 _x0003_ 1080 and 30 frames per second (FPS). That is, each video consists of nearly 18,000 high-resolution frames with a wealth of details.",
      "data_code": { "media": 1, "sequential": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We provide the original video for users to explore in the video view. Users can play the video at slow, normal, or fast speeds. When users pause the video, the corresponding faces in each frame are highlighted. Users can also pick out the parts of interest for further exploration based on their observation from the emotion flow. This view is mainly used for providing evidence for users. When users explore other views and find something interesting, they can link to corresponding frames in the video view. Accurately extracting information from a video is a challenge. Therefore, we provide an interactive way for users to correct this inaccuracy. When users identify a wrongly labeled person, they can click on the person and select the correct label. The face label will be automatically updated in the database. Similarly, users can correct the emotion information. With this method, users are allowed to interactively correct any inaccurate information caused by models.",
        "solution_category": "visualization",
        "solution_axial": "Basic",
        "solution_compoent": "Video",
        "axial_code": ["Basic"],
        "componenet_code": ["Video"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 31,
    "paper_title": "Interactive Correction of Mislabeled Training Data",
    "pub_year": 2019,
    "domain": "label",
    "requirement": {
      "requirement_text": "Examining unusual distribution for identifying labelingerrors. A cleaning pipeline oftenstarts by identifying local regions with unusual patterns of data or la-bel distribution, upon which mislabeled training items can be largelyidentified and inspected. Thus, the experts wanted to quickly locatesuch suspicious regions. Moreover, in each region, the mislabeledtraining items should always be displayed with priority no matterwhich data filter or sampling strategy is applied.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).",
      "data_code": { "clusters_and_sets_and_lists": 1, "ordinal": 1, "text": 1 }
    },
    "solution": [
      {
        "solution_text": "An item view that utilizes a tSNE-based hierarchical visualization to display the distribution of training items, discloses patterns of clusters and outliers, and supports interactive exploration of the details.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification,Sampling,DimensionalityReduction",
        "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE",
        "axial_code": ["DimensionalityReduction", "Rectification", "Sampling"],
        "componenet_code": [
          "dimensionality_reduction",
          "rectification",
          "sampling"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 32,
    "paper_title": "Interactive Correction of Mislabeled Training Data",
    "pub_year": 2019,
    "domain": "label",
    "requirement": {
      "requirement_text": "Examining unusual distribution for identifying labelingerrors. A cleaning pipeline oftenstarts by identifying local regions with unusual patterns of data or la-bel distribution, upon which mislabeled training items can be largelyidentified and inspected. Thus, the experts wanted to quickly locatesuch suspicious regions. Moreover, in each region, the mislabeledtraining items should always be displayed with priority no matterwhich data filter or sampling strategy is applied.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).",
      "data_code": { "clusters_and_sets_and_lists": 1, "ordinal": 1, "text": 1 }
    },
    "solution": [
      {
        "solution_text": "The labeling noises in the training data often result in the mixed color distribution in some regions. These suspicious regions indicate potential mislabeled items and deserve further examination, e.g., zooming into the regions to explore more items.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification,Sampling,DimensionalityReduction",
        "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE",
        "axial_code": ["DimensionalityReduction", "Rectification", "Sampling"],
        "componenet_code": [
          "dimensionality_reduction",
          "rectification",
          "sampling"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 33,
    "paper_title": "Interactive Correction of Mislabeled Training Data",
    "pub_year": 2019,
    "domain": "label",
    "requirement": {
      "requirement_text": "Recommending and verifying trusted items. While manual inspection and correction is a routine for debugging mislabeled items, the experts need automated approaches to improve efficiency. Methods such as propagation of trusted items are the most discussed and recognized. However, without efficient algorithms and interactive tools, selecting and validating a set of trusted items from the cluttered visualization can still become laborious. Two requirements were identified by our collaborators based on their experience. First, automatic recommendation algorithms are desired to locate trusted items quickly. Second, flexible ways to examine and compare trusted items are required for further verification",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "Identifying and selecting trusted items (R3). The item view and other three views cooperate to support the identi\ufb01cation, selection, and correction of trusted items. During the exploration, the user can select a set of items and add them into the selected item view. If s/he wants to reduce the number of selected items, s/he can use the \u201cRecommend trusted items\u201d operation to select a representative subset. Images of the selected items are shown in the selected items view Fig. 1 (c), where the user can look at the images, relate to the distribution in the item view, re\ufb01ne the selection accordingly, and correct the labels if labeling errors are noticed. After the re\ufb01nement, the selected items can be added into trusted items. The trusted item view (Fig. 1 (d)) displays the images and labels of all the trusted items. Further editing can be performed in the trusted item view. Usually, the identi\ufb01cation and selection of trusted items in different regions require different strategies. In a region without labeling outliers, the user usually selects trusted items using system \u201crecommendation\u201d, and verifies the labels by only a glance at the images in the selected item view. In a region with labeling outliers, more careful examinations and operations are required from the user, including investigating the distribution at different levels, selecting class-balanced trusted items, and correcting the labels step-by-step for each class.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "image+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["image", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 34,
    "paper_title": "Interactive Correction of Mislabeled Training Data",
    "pub_year": 2019,
    "domain": "label",
    "requirement": {
      "requirement_text": "Recommending and verifying trusted items. While manual inspection and correction is a routine for debugging mislabeled items, the experts need automated approaches to improve efficiency. Methods such as propagation of trusted items are the most discussed and recognized. However, without efficient algorithms and interactive tools, selecting and validating a set of trusted items from the cluttered visualization can still become laborious. Two requirements were identified by our collaborators based on their experience. First, automatic recommendation algorithms are desired to locate trusted items quickly. Second, flexible ways to examine and compare trusted items are required for further verification",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "The trusted item view displays the images and labels of all the trusted items. Further editing can be performed in the trusted item view. Usually, the identification and selection of trusted items in different regions require different strategies. In a region without labeling outliers, the user usually selects trusted items using system \u201crecommendation\u201d, and verifies the labels by only a glance at the images in the selected item view. In a region with labeling outliers, more careful examinations and operations are required from the user, including investigating the distribution at different levels, selecting class-balanced trusted items, and correcting the labels step-by-step for each class.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "image+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["image", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 35,
    "paper_title": "Interactive Correction of Mislabeled Training Data",
    "pub_year": 2019,
    "domain": "label",
    "requirement": {
      "requirement_text": "Recommending and verifying trusted items. While manual inspection and correction is a routine for debugging mislabeled items, the experts need automated approaches to improve efficiency. Methods such as propagation of trusted items are the most discussed and recognized. However, without efficient algorithms and interactive tools, selecting and validating a set of trusted items from the cluttered visualization can still become laborious. Two requirements were identified by our collaborators based on their experience. First, automatic recommendation algorithms are desired to locate trusted items quickly. Second, flexible ways to examine and compare trusted items are required for further verification",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).",
      "data_code": {
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "Propagating trusted items to improve the quality of the training set (R3). The propagation of trusted items is performed iteratively. In each iteration, newly selected trusted items are added into the trusted item set of the last iteration. They are fed into the correction module, which propagates the trusted items to the entire dataset. After the propagation, users can verify the quality improvements from the updated item distribution in the item view and the action trail. The action trail (Fig. 1 (e)) represents the historical record of correction iterations as a tree. Each iteration is represented by a node containing two bar charts: the chart on the top displays the number of newly added trusted items while the other counts the corrected items by the data correction module. In the case of undesired correction resulting, rolling back to previous iterations allows the user to reselect trusted items to re\ufb01ne the propagation results. If there is a lack of obvious visible quality improvements in sequences of iterations, the user can stop the iteration to \ufb01nish the correction process.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "image+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["image", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 36,
    "paper_title": "Interactive Correction of Mislabeled Training Data",
    "pub_year": 2019,
    "domain": "label",
    "requirement": {
      "requirement_text": "Exploring the details. Exploring the details of training items was considered an essential step to verify the labeling correctness. Specifically, the experts would like to be able to explore the details in the context of a distribution visualization so that they can quickly compare groups of items in local regions.",
      "requirement_code": { "evaluate_hypothesis": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).",
      "data_code": { "clusters_and_sets_and_lists": 1, "ordinal": 1, "text": 1 }
    },
    "solution": [
      {
        "solution_text": "An item view that utilizes a tSNE-based hierarchical visualization to display the distribution of training items, discloses patterns of clusters and outliers, and supports interactive exploration of the details.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification,Sampling,DimensionalityReduction",
        "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE",
        "axial_code": ["DimensionalityReduction", "Rectification", "Sampling"],
        "componenet_code": [
          "dimensionality_reduction",
          "rectification",
          "sampling"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 37,
    "paper_title": "Interactive Correction of Mislabeled Training Data",
    "pub_year": 2019,
    "domain": "label",
    "requirement": {
      "requirement_text": "Exploring the details. Exploring the details of training items was considered an essential step to verify the labeling correctness. Specifically, the experts would like to be able to explore the details in the context of a distribution visualization so that they can quickly compare groups of items in local regions.",
      "requirement_code": { "evaluate_hypothesis": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The MNIST dataset [57] contains 10,000 training items with correct labels of the 10 digits (0...9). For our experiments, label errors are introduced into the dataset following the contamination mechanism in [54]. The Clothing dataset is a subset of the Clothing 1M dataset [52] in which images were crawled from several online shopping websites. The images are of 14 classes (T-shirt, Shirt, Knitwear, etc.) with some confusing ones (e.g. Knitwear and Sweater).",
      "data_code": { "clusters_and_sets_and_lists": 1, "ordinal": 1, "text": 1 }
    },
    "solution": [
      {
        "solution_text": "The labeling noises in the training data often result in the mixed color distribution in some regions. These suspicious regions indicate potential mislabeled items and deserve further examination, e.g., zooming into the regions to explore more items.",
        "solution_category": "data_manipulation",
        "solution_axial": "Rectification,Sampling,DimensionalityReduction",
        "solution_compoent": "correctpossiblelabelerrors,sample,incrementaltSNE",
        "axial_code": ["DimensionalityReduction", "Rectification", "Sampling"],
        "componenet_code": [
          "dimensionality_reduction",
          "rectification",
          "sampling"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 45,
    "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
    "pub_year": 2020,
    "domain": "XAI",
    "requirement": {
      "requirement_text": "Provide the means to investigate quality further, differentiating between the trustworthiness of different regions of the projection.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "To avoid this clutter problem and increase the readability of the Shepard Diagram for large data sets, we propose the Shepard Heatmap, which is an aggregated version of the Shepard Diagram, with the information of the number of points in each cell mapped to a single-hue colormap. The main goal of the Shepard Heatmap is to offer a broad, simplified overview of the accuracy of the projection in terms of distance preservation: cells close to the main diagonal of the heatmap indicate that the respective pairs of instances have been represented in the 2-D space with distances that are comparable to their original N-D distances. ",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 46,
    "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
    "pub_year": 2020,
    "domain": "XAI",
    "requirement": {
      "requirement_text": "Provide the means to investigate quality further, differentiating between the trustworthiness of different regions of the projection.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Visual Mapping. The Visual Mapping panel includes controls for mapping Density and Remaining Cost of each point to either color or size in the main view. These correspond to information extracted from the t-SNE algorithm itself, which would otherwise be hidden from the analyst. Their inspection, however, may prove fruitful.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 47,
    "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
    "pub_year": 2020,
    "domain": "XAI",
    "requirement": {
      "requirement_text": "Provide the means to investigate quality further, differentiating between the trustworthiness of different regions of the projection.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "The ability to investigatethe extent to which such neighborhoods are preserved is one important piece of the puzzle that forms a full assessment of the accuracy of a t-SNE projection.We present a Neighborhood Preservation plot that shows an overview of the preservation of neighborhoods of different sizes (k) in both the entire projection and the current selection, based on the Jaccard distance between sets. For each value of k, NPk yields the average preservation of neighborhoods of up to k points, centered at the n selected points (or for the entire projection, if nothing is selected). The default visualization for the Neighborhood Preservation is a bar chart, but users have two more options to visualize the same information using line plots. The black bars are always fixed, showing the average preservation for all points of the projection. The ability to investigatethe extent to which such neighborhoods are preserved is one important piece of the puzzle that forms a full assessment of the accuracy of a t-SNE projection.We present a Neighborhood Preservation plot that shows an overview of the preservation of neighborhoods of different sizes (k) in both the entire projection and the current selection, based on the Jaccard distance between sets. For each value of k, NPk yields the average preservation of neighborhoods of up to k points, centered at the n selected points (or for the entire projection, if nothing is selected). The default visualization for the Neighborhood Preservation is a bar chart, but users have two more options to visualize the same information using line plots. The black bars are always fixed, showing the average preservation for all points of the projection.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 48,
    "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
    "pub_year": 2020,
    "domain": "XAI",
    "requirement": {
      "requirement_text": "Allow the interpretation of different visible patterns of the projection in terms of the original data set\u2019s dimensions.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Adaptive Parallel Coordinates Plot. Our first proposal to support the task of interpreting patterns in a t-SNE projection is an Adaptive PCP. It highlights the dimensions of the points selected with the lasso tool, using a maximum of 8 axes at any time, to avoid clutter. The shown axes (and their order) are, however, not fixed, as is the usual case. Instead, they are adapted to the selection in the following way. First, a Principal Component Analysis is performed using only the selected points, but with all dimensions. That yields two results: (1) a set of eigenvectors that represent a new base that best explains the variance of the selected points, and (2) a set of eigenvalues that represent how much variance is explained by each eigenvector. Simulating a reduction of the dimensions of the selected points to 1-Dimensional space using PCA, we pick the eigenvector with the largest eigenvalue, i.e., the most representative one. This N-D vector can be seen as sequence w of N weights, one per original dimen-sion, where the value of wj indicates the importance of dimension j in explaining the variance of the user-selected subset of the data. Finally, we sort w in descending order, then pick the dimensions that correspond to the first (up to) 8 values of the sorted w. These are the (up to) 8 dimensions shown in the PCP axes, in the same descending order (from left to right).",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 49,
    "paper_title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections",
    "pub_year": 2020,
    "domain": "XAI",
    "requirement": {
      "requirement_text": "Allow the interpretation of different visible patterns of the projection in terms of the original data set\u2019s dimensions.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The Pima Indian Diabetes dataset, the data set includes 768 female patients of Pima Indian heritage, aged between 21 to 81. The main task in this example is to classify the patients into positive (which have diabetes; 268 data points) or negative to diabetes (i.e., healthy; 500 data points). Every data instance contains eight dimensions: the number of times each patient/person was pregnant and their age, plasma glucose concentration level, diastolic blood pressure, skin thickness, insulin level, body mass index (BMI), and diabetes pedigree function (DPF), which is a function measuring the hereditary or genetic risk of having diabetes.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "The results (i.e., relevances of each dimension) are finally shown in an interactive horizontal bar chart, where the dimensions are sorted from top to bottom according to relevance (with the most relevant on the top). While the relevance is computed using the absolute value of the correlation, we decided to show the original value in the bars (including negative correlations to the left of the central axis) to avoid possibly misleading the analyst. The final component of the Dimension Correlation tool is the ability to explore the different dimensions by clicking on the bars, which will change the colormap of the main view to reflect the values of the points for that specific dimension.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 50,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "T1: Hand-Craft, Merge, and Split Clusters. Biologists apply their domain knowledge to create customized clusters to better understand which factor(s) is causing the ascertainment bias on the dataset that are being used popularly. For example, one of the biologist stated: \u201cGiven the identified SNPs [single-nucleotide polymorphisms] that are associated with common disease and traits, it\u2019s interesting to create a cluster of SNPs.\u201d In addition, biologists apply their domain expertise to merge or split two or more clusters depending on how related they think the clusters are based on given feature(s). For example, one of the biologists mentioned: \u201cDepending on the evolutionary history of the genes, two or more clusters can be really related to each other. If ascertained they are related, we will merge them as one cluster.\u201d Another biologist reported that \u201cIn my new project, we are comparing Africans to non-Africans. In this case I merge Americans, East Asians, and Europeans as one cluster, and compare that to Africans data.\u201d",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Merging and Splitting Clusters (T1). To merge two or more  clusters, users first click on a cluster. They then demonstrate their interest in merging two clusters by drag-anddropping the cluster on top of another cluster. Users can drag point(s) out of the cluster and drop into either i) another cluster or ii) a blank space (on the Cluster View). Drag-and-drop items into blank space is translated as forming a new cluster of the selected items outside the current cluster (see Fig. 2). Demonstration-based cluster customization enables users to interact with the data directly and removes any mid-level instruments such as control panels or menus",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 51,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "T2: Divide Each Cluster to Sub-Clusters. Biologists often investigate sub-clusters within a specific cluster to: 1) understand which other factors can affect the cluster, 2) compare two clusters based on the member data items in each, and 3) see trends and patterns in the sub-clusters, with respect to chosen features, We noticed that the biologists found existing solutions challenging because they had to write lines of scripts to compute and visualize sub-clusters in a given cluster. Furthermore, the existing methods prohibit rapid iteration and visualization of results, which inevitably prolongs the exploratory clustering process to understand their data better.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1, "text": 1 }
    },
    "solution": [
      {
        "solution_text": "Sub Clustering (T2). Hovering over a cluster reveals a plus button. Users can click on it to open a subcluster panel on the Cluster View, which shows subgroups of the data items within the selected cluster. In addition, a bar chart shows the distribution of a chosen attribute. Alongside, text description highlights the attributes that were used to compute the subclusters. Given that the users are not experts in data science, we do not present the quality metrics (e.g., silhouette scores, homogeneity score, etc.) Instead, we describe cluster models by showing thumbnail previews of clustering results with text descriptions as Fig. 4 shows.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Clustering&Grouping"],
        "componenet_code": [
          "dimensionality_reduction",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 52,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "T2: Divide Each Cluster to Sub-Clusters. Biologists often investigate sub-clusters within a specific cluster to: 1) understand which other factors can affect the cluster, 2) compare two clusters based on the member data items in each, and 3) see trends and patterns in the sub-clusters, with respect to chosen features, We noticed that the biologists found existing solutions challenging because they had to write lines of scripts to compute and visualize sub-clusters in a given cluster. Furthermore, the existing methods prohibit rapid iteration and visualization of results, which inevitably prolongs the exploratory clustering process to understand their data better.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Sub Clustering. When triggered by users, the system builds a sub-cluster model Msi, for data instances E, member of a selected cluster Ci. Unlike the set of main cluster models M, only a single sub-cluster model is generated per cluster (T2). For sub-clustering we relied on the parameterization of the best-recommended cluster model for the entire data i.e., best-found parameteriza tion of the K-Means cluster model. To avoid further compute times that may impact real-time interactions, we did not construct and test multiple cluster models for sub-clustering. However, clicking on the \u201cadd subcluster\u201d button again for the same selected cluster Ci, the system recomputes the sub-cluster model Msi, by  randomly choosing a new set of a learning algorithm v and hyperparameters f; e.g., it picks a new \u201ck\u201d on the \u201cK-Means\u201d cluster model. This technique allows users to rapidly browse a large set of sub-cluster models.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 53,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "T3: Adjust Feature Contributions. Biologists need to easily see by how much different attributes/features contribute to computing a cluster. Moreover, they often need to adjust the importance of different features used for computing a cluster. Biologists currently have to programmatically adjust the importance of features, execute the code, and visualize the outcome. They often repeat this process multiple times until they achieve a satisfactory result. They need interactive methods to view and refine feature contributions.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Delete Data Items or Clusters (T3). Our discussion with biologists revealed that they sometimes need to \u2018exclude\u2019 data items or clusters from their analysis while testing a hypothesis. Thus, we initially implemented the \u2018delete\u2019 feature by enabling users to select a subset of items or clusters from the main view and click on the delete icon. However, when we showed it to the biologists, they had trouble due to inconsistencies between the button-based interaction and other demonstration-based interaction. Currently in Geono-Cluster users cam drag-drop a selected cluster on the delete icon shown on the top-left of the interface to show their interest in moving the selected cluster out of the layout. Similarly, they can drag-drop individual data items to demonstrate their interests in removing them from the cluster assignment.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Clustering&Grouping"],
        "componenet_code": [
          "dimensionality_reduction",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 54,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "T3: Adjust Feature Contributions. Biologists need to easily see by how much different attributes/features contribute to computing a cluster. Moreover, they often need to adjust the importance of different features used for computing a cluster. Biologists currently have to programmatically adjust the importance of features, execute the code, and visualize the outcome. They often repeat this process multiple times until they achieve a satisfactory result. They need interactive methods to view and refine feature contributions.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "User Driven Feature Selection. A cluster model Mi is driven by a set of features F = fi1, fi2; f i3; f i4 . . . . . . :fik as input to compute the distance function which assigns a set of data items D to individual clusters C. In Geono-Cluster, the set of features F is either computed using feature selection methods e.g., \u201cselect K Best\u201d [43], \u201cPCA\u201d [44] or can be retrieved from users if they specify a set of features and their relative weights (from the Attribute Panel supporting the task T3). When users specify a set of k features Fu = fi1, fi2; f i3; f i4 . . . fik with respective weights for each feature (Wu = wi1, wi2; w i3; w i4 . . . wik, the system updates the distance function in the clustering algorithm.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 55,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "T3: Adjust Feature Contributions. Biologists need to easily see by how much different attributes/features contribute to computing a cluster. Moreover, they often need to adjust the importance of different features used for computing a cluster. Biologists currently have to programmatically adjust the importance of features, execute the code, and visualize the outcome. They often repeat this process multiple times until they achieve a satisfactory result. They need interactive methods to view and refine feature contributions.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Similar Item Selection. Users click on a cell (qj) of a quantitative attribute on the Table View to select a value vj of the data item di. Geono-Cluster finds a set of r data instances, U = da; db; dc . . .dr, each of whose value vj falls within a threshold range, say [+eps, -_x0003_eps]. The parameter eps is set for each quantitative attribute Q by heuristics and can be adjusted. This technique allows users to pick data instances which are similar, based on the selected quantitative attribute qj. Further, users can select another quantitative attribute cell qk. Next, from the set of selected data instances U, the system finds all instances V which fall within a threshold range of the value selected for attribute qk. Here the size of V is less than that of U. This technique allow users to filter and select a subset of data instances V from the Table View. For categorical features X, Geono-Cluster performs exact feature value matching instead of matching data items based on a predefined range. Users can drag-drop these V data items to the Cluster View as a single cluster (C = C1). They can continue selecting another set of data items, then add them to the cluster view as a new cluster (C = C1, C 2). Users complete the data exploration or they can request the system to find a model Mi iteratively (T3).",
        "solution_category": "interaction",
        "solution_axial": "Selecting.Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering", "Selecting"],
        "componenet_code": ["filtering", "selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 57,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "Enable User Interaction to Drive Recommendations. As analysts explore their data, their interests will evolve. Our initial observations and interviews also showed that biologists need to explore various clustering models rapidly during their data analysis process. One potential approach to support such a rapid data analysis is to recommend potential cluster models that biologists should consider during their data analysis process. Furthermore, the clustering recommendations should be adapted for biologists\u2019 analytic goals. The recommendation engine should steer multiple clustering models based on biologist-specified expected visual outcomes. In addition, biologists can also directly adjust feature contributions to update the clustering results. In aggregate, these interactions create demonstrations which serve as the primary units by which biologists communicate their expected changes to the system.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Cluster View is an environment similar to a spatial workspace in which users can move data items to structure their information and provide visual demonstrations. For example, a biologist might notice a set of data items should not be in a specific cluster. Thus, she can demonstrate that those points belong to a different cluster by dragging them from one cluster to another. The system uses the visual demonstrations provided by the users to steer the underlying recommendation engine.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Clustering&Grouping"],
        "componenet_code": [
          "dimensionality_reduction",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 58,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "Enable User Interaction to Drive Recommendations. As analysts explore their data, their interests will evolve. Our initial observations and interviews also showed that biologists need to explore various clustering models rapidly during their data analysis process. One potential approach to support such a rapid data analysis is to recommend potential cluster models that biologists should consider during their data analysis process. Furthermore, the clustering recommendations should be adapted for biologists\u2019 analytic goals. The recommendation engine should steer multiple clustering models based on biologist-specified expected visual outcomes. In addition, biologists can also directly adjust feature contributions to update the clustering results. In aggregate, these interactions create demonstrations which serve as the primary units by which biologists communicate their expected changes to the system.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "Attribute Panel lists the attributes of the loaded data set. Users can turn on and off a set of attributes which directly affects the clustering algorithm. Furthermore, users can also adjust attribute contributions, specify-ing relative importance of the selected attributes to define cluster memberships.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 59,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "Enable User Interaction to Drive Recommendations. As analysts explore their data, their interests will evolve. Our initial observations and interviews also showed that biologists need to explore various clustering models rapidly during their data analysis process. One potential approach to support such a rapid data analysis is to recommend potential cluster models that biologists should consider during their data analysis process. Furthermore, the clustering recommendations should be adapted for biologists\u2019 analytic goals. The recommendation engine should steer multiple clustering models based on biologist-specified expected visual outcomes. In addition, biologists can also directly adjust feature contributions to update the clustering results. In aggregate, these interactions create demonstrations which serve as the primary units by which biologists communicate their expected changes to the system.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1, "text": 1 }
    },
    "solution": [
      {
        "solution_text": "Recommendation Technique. Geono-Cluster ranks the models in M by their scores S explained below, and visualizes the best clustering layout in the Cluster View. Further, the system allows the user to inspect top f best cluster models from the ranked models M, through the Recommendation Panel. If a user makes any customization to the shown cluster model Mc (e.g., merge or split clusters), the system automatically updates the recommendations by computing a new set of M cluster models, except the model Mc, which is currently shown in the Cluster View. Per iteration, the system updates S and the ranking of the models M based on user interactions with the data. Next it visualizes the best model in M in the Cluster View and shows thumbnail previews of the top f models in the Recommendation Panel.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Clustering&Grouping"],
        "componenet_code": [
          "dimensionality_reduction",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 60,
    "paper_title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists",
    "pub_year": 2021,
    "domain": "Biology",
    "requirement": {
      "requirement_text": "Enhance Interpretability of Recommendations. Biologists reported their interest in seeing more details about different clustering results while skimming through different recommendations. However, not all biologists might be familiar with technical terms used to describe a cluster such as silhouette value. Therefore, recommended clustering results should be presented in a transparent manner so that biologists can extract the most important and understandable information (e.g., contributing features) used for clustering results. One powerful approach to enhance transparency of the recommended clustering options is to use natural language to explain them. This way biologists can learn about the recommended clustering outcomes without having to know about more technical terms describing each clustering outcome.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The GWAS Catalog dataset, this dataset includes detailed information regarding the identified single-nucleotide polymorphisms (SNPs) associated with common diseases and traits (e.g., position on the genome, risk allele frequencies, p-value, effect sizes, etc.). SNP is a region on the gene where more than one allele (A, C, G, T) is observed and each row on the dataset is a SNP.",
      "data_code": { "tables": 1, "categorical": 1, "text": 1 }
    },
    "solution": [
      {
        "solution_text": "Recommendation Technique. Geono-Cluster ranks the models in M by their scores S explained below, and visualizes the best clustering layout in the Cluster View. Further, the system allows the user to inspect top f best cluster models from the ranked models M, through the Recommendation Panel. If a user makes any customization to the shown cluster model Mc (e.g., merge or split clusters), the system automatically updates the recommendations by computing a new set of M cluster models, except the model Mc, which is currently shown in the Cluster View. Per iteration, the system updates S and the ranking of the models M based on user interactions with the data. Next it visualizes the best model in M in the Cluster View and shows thumbnail previews of the top f models in the Recommendation Panel.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction,Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction", "Clustering&Grouping"],
        "componenet_code": [
          "dimensionality_reduction",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 62,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.1 Extract Bias Patterns. As indicated in the previous section, bias patterns are a kind of statistical abstraction of the historical data in a certain spatiotemporal interval. However, methods that aim to extract the potential bias patterns from the reanalysis data are limited.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": {
        "temporal": 1,
        "geometry": 1,
        "sequential": 1,
        "fields": 1
      }
    },
    "solution": [
      {
        "solution_text": "In the bias pattern extraction module, given a forecast result in a spatiotemporal interval and the corresponding observation data, the forecast-observation probability density function at each grid is initially generated via a kernel density estimation (KDE) (R.1). The distribution \ufb01eld can represent the bias pattern at the grid in a given time range because the probability density distribution of the observation-forecast values can re\ufb02ect the distribution characteristics of the forecast and the observation data in the current time window. Furthermore, the probability density distribution can derive the distribution characteristics of the bias, which is equal to the difference between the observation and the forecast data. We used a bottom-up hierarchical clustering algorithm to aggregate a single grid point into a connected area on the basis of the similarity of the PDF among the grids. The result of the hierarchical clustering can be considered a clustering tree of the connected areas with similar bias patterns, which are subsequently fed into the following visual analysis module for further exploration.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 63,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.2 Visualize the Spatiotemporal Distribution of Similar Bias Patterns. E.2 hoped to have a visualization that can well demonstrate the bias patterns in the spatial and temporal dimensions for an overview, which can help him immediately identify the basic properties and distribution of the bias patterns present in the data.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": {
        "temporal": 1,
        "geometry": 1,
        "sequential": 1,
        "fields": 1
      }
    },
    "solution": [
      {
        "solution_text": "In the visual analysis module, several coordinated views and intuitive interactions are provided to analyze the bias patterns. Specifically, we proposed a relative position and similarity-encoded scatter plot to visualize the spatiotemporal distribution of the bias patterns.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 64,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.2 Visualize the Spatiotemporal Distribution of Similar Bias Patterns. E.2 hoped to have a visualization that can well demonstrate the bias patterns in the spatial and temporal dimensions for an overview, which can help him immediately identify the basic properties and distribution of the bias patterns present in the data.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": {
        "temporal": 1,
        "geometry": 1,
        "sequential": 1,
        "fields": 1
      }
    },
    "solution": [
      {
        "solution_text": "The spatiotemporal bias pattern view (Fig. 4a) provides an overview of the areas with similar bias patterns extracted from the reanalysis data in each time window (R.2). ",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 65,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.2 Visualize the Spatiotemporal Distribution of Similar Bias Patterns. E.2 hoped to have a visualization that can well demonstrate the bias patterns in the spatial and temporal dimensions for an overview, which can help him immediately identify the basic properties and distribution of the bias patterns present in the data.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": { "geometry": 1, "fields": 1 }
    },
    "solution": [
      {
        "solution_text": "The map view was designed to visualize the spatial distribution of the similar bias pattern areas and grid points. It also corresponds to the interaction conducted in the spatiotemporal bias pattern view.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 66,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.2 Visualize the Spatiotemporal Distribution of Similar Bias Patterns. E.2 hoped to have a visualization that can well demonstrate the bias patterns in the spatial and temporal dimensions for an overview, which can help him immediately identify the basic properties and distribution of the bias patterns present in the data.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": { "geometry": 1, "fields": 1 }
    },
    "solution": [
      {
        "solution_text": "The map view was designed to visualize the spatial distribution of the similar bias pattern areas and grid points. It also corresponds to the interaction conducted in the spatiotemporal bias pattern view.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 67,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.3 Reveal the Differences and Correlations Among Different Spatiotemporal Bias Patterns. E.2 commented that the current bias analysis methods only focus on a single location, i.e., calibrating precipitation at a single station instead of analyzing its correlation among its neighbors. Understanding the stability and uniqueness of the bias patterns can help explore and make informed decisions. Thus, E.2 wanted to observe the difference and correlations among different spatiotemporal bias patterns.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": {
        "temporal": 1,
        "geometry": 1,
        "sequential": 1,
        "fields": 1
      }
    },
    "solution": [
      {
        "solution_text": "The spatiotemporal bias pattern view also visualizes the transition of grid points between areas with similar bias patterns in the adjacent time windows.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 68,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.3 Reveal the Differences and Correlations Among Different Spatiotemporal Bias Patterns. E.2 commented that the current bias analysis methods only focus on a single location, i.e., calibrating precipitation at a single station instead of analyzing its correlation among its neighbors. Understanding the stability and uniqueness of the bias patterns can help explore and make informed decisions. Thus, E.2 wanted to observe the difference and correlations among different spatiotemporal bias patterns.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": { "geometry": 1, "fields": 1 }
    },
    "solution": [
      {
        "solution_text": "A contour map is used in the map view to convey the stability of each grid point in the space.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 69,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.4 Select Bias Patterns of Interest. E.1 needed to \ufb01lter out the bias patterns of interest from the data overview rapidly for a detailed analysis. Thus, a visual interactive mechanism and visual cues should be provided to assist him in selecting the bias patterns of interest.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": {
        "temporal": 1,
        "geometry": 1,
        "sequential": 1,
        "fields": 1
      }
    },
    "solution": [
      {
        "solution_text": "The users can further filter similar bias pattern areas to explore the bias patterns through interaction further, thereby observing the spatiotemporal characteristics of the bias patterns in conjunction with their domain expertise and interests. For instance, we can click on a similar bias pattern area, and the corresponding areas in each time window would be highlighted automatically. We also support similarity-based filtering interaction. The users can directly input a similar threshold, and the areas with a similarity higher than the threshold would be filtered.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 70,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.5 Facilitate Parameter Comparison. To understand the similarities and differences among bias patterns, comparing the atmospheric parameters when generating the corresponding bias patterns is essential to understand the underlying causes of the bias patterns and to further support making informed decisions.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": {
        "temporal": 1,
        "geometry": 1,
        "sequential": 1,
        "fields": 1
      }
    },
    "solution": [
      {
        "solution_text": "The forecast-observation PDF view provides detailed information on the bias pattern and assists analysts in drafting the calibration curve on the basis of the bias patterns.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 71,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.5 Facilitate Parameter Comparison. To understand the similarities and differences among bias patterns, comparing the atmospheric parameters when generating the corresponding bias patterns is essential to understand the underlying causes of the bias patterns and to further support making informed decisions.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": {
        "temporal": 1,
        "geometry": 1,
        "sequential": 1,
        "fields": 1
      }
    },
    "solution": [
      {
        "solution_text": "The design of the parameter comparison view focused on the correlation between the distribution of the data dimensions and the bias, thus enabling the analysis and comparison of atmospheric parameters generated by the forecast ensemble. That is, it visualizes the data distribution in the five dimensions in a bias pattern and the correlation between the data distribution and the bias of the accumulated precipitation instead of visualizing the correlation among these dimensions.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 72,
    "paper_title": "A Probability Density-Based Visual Analytics Approach to Forecast Bias Calibration",
    "pub_year": 2022,
    "domain": "Weather forecast",
    "requirement": {
      "requirement_text": "R.6 Support Detailed Calibrations. According to E.1, the accuracy of weather forecasts largely depends on the subjective judgment of the forecaster in terms of his/her quali\ufb01cation, status, and working conditions, thereby increasing the unreliability of the calibrated results. Therefore, an interactive calibration mechanism is desired to support the combination of domain experts\u2019 experience and bias patterns extracted from historical data; in this manner, the accuracy of forecast calibration is improved.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The data used in the cases comprise two types of datasets, namely, the forecast and observation data. The forecast data are produced by the Global Ensemble Forecast System (GEFS) global ensemble from the National Centers for Environmental Prediction in the United States. The dataset consists of 11 ensemble members at different convection parameters. Each member has approximately 8000 grids. The ensemble runs and generates forecast atmospheric para-meters daily from 1985, covering an area of [25:0436N, 53:1299N][66:094W, 125:625W] (latitude-longitude) with are solution of 0.5. The data have approximately 150 million forecast records (forecast average value of four output parameters during a 10-year period). Additional details of the GEFS forecast dataset can be found in [5]. The observation data are produced by the climatology-calibrated precipitation analysis (CCPA), which records the observed precipitation in the same area throughout the US every six hours from 2002 with a resolution of 0.125. These data are considered the ground-truth state of the actual weather state. The size of the data used is approximately 30 million observation records(observed precipitation at each interpolated grid point during a 10-years period). ",
      "data_code": {
        "temporal": 1,
        "geometry": 1,
        "sequential": 1,
        "fields": 1
      }
    },
    "solution": [
      {
        "solution_text": "An interactive semi-automatic calibration curve based on the detailed visualization of a specific bias pattern was designed to assist the forecast calibration.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation,Clustering&Grouping",
        "solution_compoent": "kerneldensityestimation",
        "axial_code": ["AlgorithmCalculation", "Clustering&Grouping"],
        "componenet_code": [
          "algorithmic_calculation",
          "clustering_and_grouping"
        ]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 73,
    "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics",
    "pub_year": 2021,
    "domain": "Graph mining",
    "requirement": {
      "requirement_text": "D1 Visualize the Instance-level Sensitivity. The system should visualize ranking and auditing results for all instances (T1). The view for summarizing the instance-level sensitivity should include the sensitivity index (T1.1) for all nodes with respect to the node attributes (T1.3).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.",
      "data_code": { "network_and_trees": 1 }
    },
    "solution": [
      {
        "solution_text": "The system visualizes the output in the sortable sensitivity index list, which shows each node\u2019s current ranking and sensitivity indices with respect to the node\u2019s class label(s).",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding",
        "solution_compoent": "",
        "axial_code": ["Excluding"],
        "componenet_code": ["excluding"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 74,
    "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics",
    "pub_year": 2021,
    "domain": "Graph mining",
    "requirement": {
      "requirement_text": "D2 Visualize the Effect of Perturbation. The system should be able to guide analysts to explore the perturbation effect of certain node\u2019s removal and support interactions such as sorting, searching and \ufb01ltering to inspect the auditing results and corresponding perturbation effects (T2, T3). This view should include:D2.1 In\ufb02uence Overview, which summarizes the perturbation\u2019s in\ufb02uence, the degree of ranking changes, and the proportion of nodes whose rankings are increased/decreased, etc. (T1.2, T2.1, T3).",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.",
      "data_code": { "network_and_trees": 1 }
    },
    "solution": [
      {
        "solution_text": "The influence overview provides basic information on changes caused by removing a specific node (D2.1). These changes include 1) the number of influenced nodes which have ranking changes after the perturbation; 2, 3) the number of influenced nodes whose ranking increased/decreased after the perturbation; 4, 5) the max/min of increased/decreased ranking changes; 6, 7) the median of increased/decreased ranking changes; and 8) the degrees of the node. These 8 metrics provide the analyst with a statistical overview of the ranking influence of nodes due to perturbations. The radar is used to provide an overview of the sensitivity metrics with respect to the effects of a perturbation. The radar allows for the further addition of new metrics and can also preserve the overall information of the perturbation when the analyst switches between multiple perturbation diagnoses through the tabs on the top of the view.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding,AlgorithmCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmCalculation", "Excluding"],
        "componenet_code": ["algorithmic_calculation", "excluding"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 75,
    "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics",
    "pub_year": 2021,
    "domain": "Graph mining",
    "requirement": {
      "requirement_text": "D2 Visualize the Effect of Perturbation. The system should be able to guide analysts to explore the perturbation effect of certain node\u2019s removal and support interactions such as sorting, searching and \ufb01ltering to inspect the auditing results and corresponding perturbation effects (T2, T3). This view should include: D2.2 Distribution View, which shows how the ranking position changes caused by the perturbation are distributed for each instance and the ranking distribution for each group of nodes. (T2.2, T1.3)",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.",
      "data_code": { "network_and_trees": 1 }
    },
    "solution": [
      {
        "solution_text": "Ranking Change Distribution View: A bar chart is used to show the ranking change distribution. Each bar is a node. The position of the bar on the x axis denotes the original ranking position for the node. The height of the bar on the y axis denotes the ranking change for the node. Colors represent the node labels. We scale the axes of the bar chart such that a 90-degree clockwise rotation of the bar also allows the analyst to infer the future rank of the node.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding",
        "solution_compoent": "",
        "axial_code": ["Excluding"],
        "componenet_code": ["excluding"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 76,
    "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics",
    "pub_year": 2021,
    "domain": "Graph mining",
    "requirement": {
      "requirement_text": "D2 Visualize the Effect of Perturbation. The system should be able to guide analysts to explore the perturbation effect of certain node\u2019s removal and support interactions such as sorting, searching and \ufb01ltering to inspect the auditing results and corresponding perturbation effects (T2, T3). This view should include: D2.2 Distribution View, which shows how the ranking position changes caused by the perturbation are distributed for each instance and the ranking distribution for each group of nodes. (T2.2, T1.3)",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.",
      "data_code": { "network_and_trees": 1 }
    },
    "solution": [
      {
        "solution_text": "Top-k Proportional Distribution View: When applying graph-based ranking algorithms for search engines, there could be an argument that ranking changes of webpages that are not part of the top-k ranking are less important than those in the top-k. This argument can be extended to any general graph ranking problem, where the analyst can choose a k for which elements below this ranking will not be considered. For example, a hiring manager may not be interested in resumes ranked outside of the top-25, but it important to understand whether certain node attributes are underrepresented in the top-k. In the Top-k Proportional Distribution View, we use two donut charts to represent the proportions of nodes of different categories belonging to ranking 1 to ranking k before and after the perturbation, and k is interactively specified.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding",
        "solution_compoent": "",
        "axial_code": ["Excluding"],
        "componenet_code": ["excluding"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 77,
    "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics",
    "pub_year": 2021,
    "domain": "Graph mining",
    "requirement": {
      "requirement_text": "D2 Visualize the Effect of Perturbation. The system should be able to guide analysts to explore the perturbation effect of certain node\u2019s removal and support interactions such as sorting, searching and \ufb01ltering to inspect the auditing results and corresponding perturbation effects (T2, T3). This view should include: D2.3 Ranking Change Detail View, which lists the in\ufb02uenced nodes for this perturbation. The view should support basic query operations, e.g., sorting, \ufb01ltering and searching, etc.(T2.1, T3)",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.",
      "data_code": { "network_and_trees": 1 }
    },
    "solution": [
      {
        "solution_text": "The view should support basic query operations, e.g., sorting, filtering and searching, etc.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding",
        "solution_compoent": "",
        "axial_code": ["Excluding"],
        "componenet_code": ["excluding"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 78,
    "paper_title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics",
    "pub_year": 2021,
    "domain": "Graph mining",
    "requirement": {
      "requirement_text": "D2 Visualize the Effect of Perturbation. The system should be able to guide analysts to explore the perturbation effect of certain node\u2019s removal and support interactions such as sorting, searching and \ufb01ltering to inspect the auditing results and corresponding perturbation effects (T2, T3). This view should include: D2.4 Local In\ufb02uence Graph View, which illustrates the relationship between the ranking changes of nodes and the topological changes caused by the perturbation. (T2.3, T3)",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "A Facebook social circle, Political blogs, and a Reddit interaction network, The political blog dataset [24]. The dataset includes a topic citation graph between liberal and conservative blogs prior to the 2004 U.S. Presidential Election.",
      "data_code": { "network_and_trees": 1 }
    },
    "solution": [
      {
        "solution_text": "We visualize the influence caused by removing/perturbing a node as a customized radial graph layout. In this customized layout, the removed node is set as the center of the force, and the strength of the charge force for each type of the node (hop-1 node, hop-2 node, etc.) is increased gradually based on the number n of hop-n.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding",
        "solution_compoent": "",
        "axial_code": ["Excluding"],
        "componenet_code": ["excluding"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 81,
    "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
    "pub_year": 2021,
    "domain": "Tax evasion",
    "requirement": {
      "requirement_text": "R1 Enable interactive con\ufb01gurations for suspicious RPTTE groups detection. The automated algorithms signi\ufb01cantly improve the ef\ufb01ciency of the tax inspection procedure when they can successfully identify the most relevant suspicious RPTTE groups. As mentioned by E2, different users may want to explore RPTTE groups that satisfy speci\ufb01c conditions and would like to con\ufb01gure different parameters to extend or narrow down the scope of suspicious groups by \ufb01ltering taxpayers not tightly connected to the related party transactions. For example, different users may have an interest in detecting tax evasion groups from different periods, and exploring tax evasion groups with a speci\ufb01c relevance or complexity",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.",
      "data_code": {
        "tables": 1,
        "categorical": 1,
        "text": 1,
        "network_and_trees": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "Network Construction. Similar to prior research [35], we also fuse multiple data sources and construct a taxpayer network to facilitate further analysis. We \ufb01rst formulate the taxpayer network with taxpayers and investors, where the nodes can be both taxpayers and investors, and edges represent investment relationships. Then we provide more information about the nodes by fusing their pro\ufb01le information and audit records to the network. A connected component in the resulting directed graph re\ufb02ects that of a related party. We further adjust the taxpayer network according to our design requirements (R1) and domain knowledge.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 82,
    "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
    "pub_year": 2021,
    "domain": "Tax evasion",
    "requirement": {
      "requirement_text": "R1 Enable interactive con\ufb01gurations for suspicious RPTTE groups detection. The automated algorithms signi\ufb01cantly improve the ef\ufb01ciency of the tax inspection procedure when they can successfully identify the most relevant suspicious RPTTE groups. As mentioned by E2, different users may want to explore RPTTE groups that satisfy speci\ufb01c conditions and would like to con\ufb01gure different parameters to extend or narrow down the scope of suspicious groups by \ufb01ltering taxpayers not tightly connected to the related party transactions. For example, different users may have an interest in detecting tax evasion groups from different periods, and exploring tax evasion groups with a speci\ufb01c relevance or complexity",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.",
      "data_code": {
        "tables": 1,
        "categorical": 1,
        "text": 1,
        "network_and_trees": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The Control Panel consists of a bar chart and a parameter selector to support network fusion through interactively setting the preferred period or relevant thresholds. The bar chart on the right-hand side of the Control Panel offers a temporal summary of the daily related party transaction amount, which reveals a cyclic pattern where most of the peaks are near the end of the month. Since the profit manipulation should happen after tax evaders know how much taxable income they have, we speculate that this information can work as a visual cue to guide the tax administration officers in conducting their exploration of the suspicious RPTTE groups. Users can select their period of concern by brushing or clicking a bar to automatically select the quarter, which is a typical tax period.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 83,
    "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
    "pub_year": 2021,
    "domain": "Tax evasion",
    "requirement": {
      "requirement_text": "R1 Enable interactive con\ufb01gurations for suspicious RPTTE groups detection. The automated algorithms signi\ufb01cantly improve the ef\ufb01ciency of the tax inspection procedure when they can successfully identify the most relevant suspicious RPTTE groups. As mentioned by E2, different users may want to explore RPTTE groups that satisfy speci\ufb01c conditions and would like to con\ufb01gure different parameters to extend or narrow down the scope of suspicious groups by \ufb01ltering taxpayers not tightly connected to the related party transactions. For example, different users may have an interest in detecting tax evasion groups from different periods, and exploring tax evasion groups with a speci\ufb01c relevance or complexity",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.",
      "data_code": {
        "tables": 1,
        "categorical": 1,
        "text": 1,
        "network_and_trees": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The parameter selector allows users to configure the maximum transaction chain length and maximum control chain length for the algorithm to determine the extent of the suspicious RPTTE groups.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 84,
    "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
    "pub_year": 2021,
    "domain": "Tax evasion",
    "requirement": {
      "requirement_text": "R1 Enable interactive con\ufb01gurations for suspicious RPTTE groups detection. The automated algorithms signi\ufb01cantly improve the ef\ufb01ciency of the tax inspection procedure when they can successfully identify the most relevant suspicious RPTTE groups. As mentioned by E2, different users may want to explore RPTTE groups that satisfy speci\ufb01c conditions and would like to con\ufb01gure different parameters to extend or narrow down the scope of suspicious groups by \ufb01ltering taxpayers not tightly connected to the related party transactions. For example, different users may have an interest in detecting tax evasion groups from different periods, and exploring tax evasion groups with a speci\ufb01c relevance or complexity",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.",
      "data_code": {
        "tables": 1,
        "categorical": 1,
        "text": 1,
        "network_and_trees": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The nodes are labeled with their IDs for users to track the entity down in the Control Panel. Through the Graph View, users can gain an impression about whether the relevant thresholds have over-pruned or under-pruned the group and require a rerun of the network fusion algorithm with a different parameter setting. By hovering over the related party transactions, the common beneficial owners and the entire ownership chain is highlighted to reveal deceptive cases where tax evaders use a complex ownership structure to hide their identities. Clicking on the related party transaction link propagates the details to the detail view for further analysis.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 85,
    "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
    "pub_year": 2021,
    "domain": "Tax evasion",
    "requirement": {
      "requirement_text": "R2 Rank suspicious tax evasion groups with multiple criteria. Given the vast number of suspicious groups, it is critical to help users quickly locate groups with the highest risk level. The system should support the sorting of the groups based on multiple criteria which re\ufb02ects the suspicion in different dimensions. For example, according to E1\u2019s audit experience, one of the major suspicion criteria is the existence of historical tax evasion records because those groups are likely to commit tax evasion again.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.",
      "data_code": {
        "tables": 1,
        "categorical": 1,
        "text": 1,
        "network_and_trees": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The Group Overview shows a list of suspicious RPTTE groups, in which each row represents a suspicious RPTTE group, and consists of an arc diagram based glyph and a bar chart to help users focus on the most suspicious groups. We extend the arc diagram as a glyph to visualize the topology of the related party transactions, which allows users to estimate the complexity of the suspicious RPTTE group quickly.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 86,
    "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
    "pub_year": 2021,
    "domain": "Tax evasion",
    "requirement": {
      "requirement_text": "R2 Rank suspicious tax evasion groups with multiple criteria. Given the vast number of suspicious groups, it is critical to help users quickly locate groups with the highest risk level. The system should support the sorting of the groups based on multiple criteria which re\ufb02ects the suspicion in different dimensions. For example, according to E1\u2019s audit experience, one of the major suspicion criteria is the existence of historical tax evasion records because those groups are likely to commit tax evasion again.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.",
      "data_code": {
        "tables": 1,
        "categorical": 1,
        "text": 1,
        "network_and_trees": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The bar chart shows three group features, including the number of taxpayers with tax evasion records, the related party transaction amount, and the number of effective related party transactions. By default, the number of effective related party transactions is selected to rank the groups because the feature is engineered to represent one of the standard settings for transfer pricing. Users can click the sort icons located at the top of the list to sort the groups. Hovering over the nodes in the glyph highlights the corresponding node in the graph view. In addition, clicking the row propagates the details of any suspicious RPTTE group to the graph view for further analysis.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 87,
    "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
    "pub_year": 2021,
    "domain": "Tax evasion",
    "requirement": {
      "requirement_text": "R3 Support the interactive exploration of the common bene\ufb01cial owners of taxpayers who conduct the related party transactions and their attributes. As at least one common bene\ufb01cial owner will bene\ufb01t from RPTTE behaviors, exploring the investment and trading relationships helps users to understand how the tax evasion scheme works among taxpayers. In addition, the attributes of taxpayers such as historical tax evasion records provide the tax administration of\ufb01cers a context for suspicion and risk evaluation. Therefore, the experts require that all taxpayer with a common bene\ufb01cial owner should also be clearly visualized, facilitating the deep exploration and inspection of any highly suspicious related party transactions.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.",
      "data_code": {
        "tables": 1,
        "categorical": 1,
        "text": 1,
        "network_and_trees": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The Graph View shows the hierarchical investment relationship and related party transactions within the selected suspicious group. We employ the method proposed by Jnger and Mutzel to conduct the graph layout, as it is more efficient in a layered graph drawing and can effectively reduce the crossed links. Also, we propose several encoding schemes to offer context for group assessment. The color of the borders encodes the entity type of node (investor or taxpayer), and the corresponding period-end profit status for the node is encoded by the fill colors where the diverging color scheme is the same as the node color of the detailed view. For the links, the type of relations uses color encoding. To stay consistent with the color encoding scheme of the node type, blue represents the related party transaction, while orange represents an investment. We emphasize the taxpayers with tax evasion records by drawing an exclamation mark in the circle.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 88,
    "paper_title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups",
    "pub_year": 2021,
    "domain": "Tax evasion",
    "requirement": {
      "requirement_text": "R4 Provide convenient pro\ufb01t analysis of taxpayers that conducted related party transactions. To facilitate the tax inspection process, the users need to know how the taxpayers redistribute their pro\ufb01ts through RPTTE behaviors. It is also important to present the related party transaction as evidence for users to quickly make audit decisions. Both E1 and E2 agree that the evaluation of suspicious related party transactions is challenging because tax evaders try their best to disguise their transactions as legal ones. However, the intent of such transactions must be re\ufb02ected in reported pro\ufb01ts, which leads to a lower tax burden. Therefore, pro\ufb01t analysis can act as the critical context to help users in decision-making. The visualization should display the pro\ufb01t status of the taxpayers, as the analysis of pro\ufb01t variations can reveal whether the related party transaction behaviors affect the overall tax burden.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "We provide a brief introduction to the three types of tax-related data used for tax evasion detection and anal_x0002_ysis. Taxpayer and Investor Profiles: A taxpayer is a person or organi_x0002_zation (such as a company) who needs to pay taxes to the government. An investor is a person or organization that buys shares in taxpayers and holds voting rights. Our collaborator offered us the profile infor_x0002_mation of each taxpayer and the corresponding investors. The taxpayer profile describes the business nature of taxpayers, such as industry, major merchandise, ownership type, and so on. The corresponding investor information includes investor entity type, investment amount, and share ratio. There are over 4 million taxpayers and 0.9 million investors in the entire dataset provided by our collaborator. This infor_x0002_mation helps us understand the topology of investment relationships of all taxpayers. Invoice Information: The invoice information is collected to record the details of each transaction between taxpayers. We obtained 14 million Value-Added-Tax (VAT) invoices in Shaanxi Province and the time range of the invoices is from Jan 1, 2014 to Dec 31, 2015. Each invoice record consists of five attributes: date, seller, buyer, VAT tax amount, and the transaction amount. The buyer and seller in an invoice record are two taxpayers, of whom, at least one of them, are registered in Shaanxi Province. The invoices explain the trading rela_x0002_tionships among the taxpayers and show the cash flow of the taxpayers regarding transactions. Audit Records: Audit records refer to the results after a tax admin_x0002_istration officer conducts an official examination on a taxpayer\u2019s finan_x0002_cial account. Our collaborators offered us historical audit records to help analyze each case and develop our system. Each auditing record consists of audit date, violation type, case description, action taken, and tax payable. Together with the taxpayer profile, we can trace the tax evasion history of taxpayers.",
      "data_code": {
        "tables": 1,
        "categorical": 1,
        "text": 1,
        "network_and_trees": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "We proposed using two calendar heatmaps to visualize the profit status of two traders who conducted the selected related party transaction. As shown in figure, we encode the cumulative daily profit with the background color of each visual mark in the calendar heatmap. We proposed two diverging color schemes with the help of ColorBrewer, namely, the brown-blue-green and the red-yellow-green. The former is colorblind-friendly, and the latter is consistent with conventional color usages in the Chinese stock market (i.e., red indicates profit while green represents loss).",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 89,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The top straight line in the DAG of a Git repository generally represents the master branch. However, an overwhelming number of branches and the connected links between them could hinder tracking down the origin of changes even for commits in the master branch. To alleviate this problem, Githru removes the connected links between the branches in a DAG to form a group of stems. A stem is a list of ancestor nodes for a specific commit that includes only one of the parents when there are multiple preceding nodes. It is similar to the first-parent option of the git log command, which removes other parent nodes from a branch.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "network+vector",
        "axial_code": ["Nesting"],
        "componenet_code": ["vector", "network"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 90,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The CSM can drastically reduce the number of stems and commits on the screen by removing implicit stems (Fig. 4d), stems corresponding to merged branches, or merged PRs (R1). LinVis [72] proposed a similar approach that grouped parents into a hierarchical structure and presented the structure in Merge-Tree. However, this prior work focused mainly on analyzing the details of CSM-sources (i.e., parent commits) in the master branch and presenting a visual representation of the hierarchy. In contrast, Githru provides an overview of the entire repository by applying the CSM to every stem. Furthermore, users can decide whether to apply a CSM or not, depending on their task. Users can also, if necessary, explicitly visualize the edges between the CSM-base and CSM-sources.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 91,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In another effort to improve scalability, we adopt a clustering technique to group neighboring commits in each stem. The scope of the grouping is confined to similar commits in each stem to preserve the temporal sequence and topology. We exploit the Simple AdditiveWeighting (SAW) model in calculating similarity since this model is intuitive for users to understand and is known to serve exploration well.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Clusterneighboringcommitsinstemstopreservetemporalsequence,topology.SAWmodelforsimilarity.",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 92,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "If there is still an overwhelming number of nodes even after the above techniques are applied, users can additionally apply Non-Con\ufb02ict Commits Clustering, which can group non-neighbor commits (R1). For instance, suppose that a cluster A is not adjacent to a cluster C, but their similarity is above the threshold (being suf\ufb01ciently similar). If the cluster A has no commonly modi\ufb01ed \ufb01les with cluster B (an inbetween cluster of A and C), changing the order of B and C could further simplify the underlying structure by grouping the clusters A and C as illustrated in Fig. 5. However, we make this process optional because it considers only the con\ufb02ict coming from modi\ufb01ed \ufb01les and not the contextual con\ufb02ict",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "Clusternon-neighborcommits,simplifystructureoptionally.",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 93,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We emphasize each block using a black border line when it has a CSM-base and the CSM is enabled; the line turns dashed gray if the CSM is disabled. Regarding visual clutter of borders, adjacent clusters with identical pale colors were hardly distinguishable without any additional visual cues. This issue was raised during the interviews with domain experts, and we eventually included borders. The visibility of the edges between the CSM-base and CSM-sources also changes accordingly. This allows us to reduce the number of visualelements in the horizontal dimension without losing the temporal order of commits across stems.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 94,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Grouped Summary View shows a brief overview of the selected clusters as shown in Fig. 1d (R1). The columns in the view are mapped to individual clusters and the width of each column is proportional to the number of commits. This view enables a visual comparison of the relative size among selected clusters, which was frequently cited as a needed task in the requirement analysis (R4). Each column has a group of horizontal bars that brie\ufb02y show the top two or three values from the clustering criteria (i.e., author, commit types, modi\ufb01ed \ufb01les, and keywords). In addition, there are bars for the list of modi\ufb01ed directories and \ufb01les to offer more context. Furthermore, the length of each bar is proportional to the number of relevant commits that users could visually compare. For instance, users could \ufb01nd the author who has contributed the most to the cluster by \ufb01nding the longest bar. Enabling the Summary by CLOC option changes the width of each column and the length of the \ufb01le criteria bar proportionally to the number of CLOCs (changed LOCs, added LOCs + deleted LOCs).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 95,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "When users select a cluster in Grouped Summary View, Cluster Detail View appears at the bottom. This view provides commit-level details along with a visual summary of the affected \ufb01les and directories on the left (R1). A list of raw commit metadata is presented in a tabular form by date in ascending order (Fig. 1f). In the case of a CSM commit, it shows only the CSM-base at \ufb01rst, but users can expand the row to also see the relevant CSM-source commits. On the left of the table, we prepared a \ufb01le icicle tree [39] (Fig. 1e). Since \ufb01les and directories are organized in a hierarchy, we consider a number of space-\ufb01lling approaches to maximize space utilization [42]. Among Tree-Map [42], SunBurst [64], and the icicle tree, we \ufb01nally choose the last to comply with the task requirements. Tree-Map shows limitations in the structural interpretation task [18] and SunBurst is inadequate to embed long \ufb01le names because of the radial coordinate. On the other hand, the icicle tree explicitly shows a structural hierarchy in a Cartesian coordinate system well suited for displaying a string (e.g., \ufb01le-name) horizontally [39]. Also, as the depth of the modi\ufb01ed \ufb01le structure can vary, we enable users to zoom in or out with a mouse click on the icicle tree (R3).",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 96,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R1: Provide an overview. The system should present an overview of development history where (a) the commits are grouped according to speci\ufb01c criteria to avoid examining each commit individually; (b) the visualization of a group encodes its size and topological position compared to others; and (c) the summary of the selected group(s) is presented interactively (T1).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Selection Cards represent corresponding selections. The stem information, such as a branch name or PR number, is prominently presented on each card, as they directly represent the characteristics of the cluster. For the same reason, the color of the card is also derived from the color of the stem where the selected cluster(s) is located.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 97,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R2: Visualize a graph while preserving topology. The graph representing the abstracted data should be visualized in an interpretable form. The graph should contain abstracted topological data that include (a) the temporal sequence of each node (i.e., commit) and (b) branch information and merge relation; and (c) the graph should be navigable with minimal interactions (T2).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "The DAG representation of a Git repository suffers not only from a large number of nodes (i.e., commits) but also from diverging and converging links at implicit and explicit branches. As the number of commits and branches inevitably increases over time in an ongoing project, scalability is crucial for DAG-based visual analysis. As a remedy, we introduce graph reorganizing techniques tailored to the Git metadata, which could interactively reduce the number of nodes and links during analysis.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 98,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R2: Visualize a graph while preserving topology. The graph representing the abstracted data should be visualized in an interpretable form. The graph should contain abstracted topological data that include (a) the temporal sequence of each node (i.e., commit) and (b) branch information and merge relation; and (c) the graph should be navigable with minimal interactions (T2).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "As a resolution, we propose a Context-preserving Squash Merge (CSM). CSM fuses relevant commits (i.e., second parent commits from the merged branch [22]) into a single node for simplicity (Fig. 4c) and fetches messages from the stems to preserve the merge context (R2). For each merge commit on the main stem (i.e., CSM-base), CSM traverses every parent commit (i.e., the CSM-source) on the other stems. When a commit is a parent of multiple CSM-bases, we select the leftmost commit as a base to avoid redundant merges. CSM gathers contextual information from every CSM-source (e.g., author, commit type, and log message) and appends it to the end of the corresponding \ufb01eld in the CSM-base. For instance, the authors of CSM-sources become coauthors of the corresponding CSM-base. However, the list of changed \ufb01les remains the same since CSM-base encompasses the changes from CSM-sources.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 99,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R2: Visualize a graph while preserving topology. The graph representing the abstracted data should be visualized in an interpretable form. The graph should contain abstracted topological data that include (a) the temporal sequence of each node (i.e., commit) and (b) branch information and merge relation; and (c) the graph should be navigable with minimal interactions (T2).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "We emphasize each block using a black border line when it has a CSM-base and the CSM is enabled; the line turns dashed gray if the CSM is disabled. Regarding visual clutter of borders, adjacent clusters with identical pale colors were hardly distinguishable without any additional visual cues. This issue was raised during the interviews with domain experts, and we eventually included borders. The visibility of the edges between the CSM-base and CSM-sources also changes accordingly. This allows us to reduce the number of visualelements in the horizontal dimension without losing the temporal order of commits across stems.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 100,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R2: Visualize a graph while preserving topology. The graph representing the abstracted data should be visualized in an interpretable form. The graph should contain abstracted topological data that include (a) the temporal sequence of each node (i.e., commit) and (b) branch information and merge relation; and (c) the graph should be navigable with minimal interactions (T2).",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "One can control the granularity of clustering by setting a ClusteringStep encoded as a vertical slider. The desired level of abstraction can be set by adjusting the maximum difference value (threshold)to be clustered. For instance, if one moves up the slider, the clustering becomes granular. Thus, one can analyze fine-grained clusters by moving up the slider. For the same reason, we also provided a way to set Preference Weights for each similarity criterion. For instance, if one wants to cluster only commits with similar commit types, one can simply set the weight of the commit type to 1 and the rest to 0. Such a capability reveals the underlying policy of clustering, helping users to understand the context.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 101,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "On the left of the table, we prepared a file icicle tree. Since files and directories are organized in a hierarchy, we finally choose the icicle tree to comply with the task requirements. Because the icicle tree explicitly shows a structural hierarchy in a Cartesian coordinate system well suited for displaying a string (e.g., file-name) horizontally]. Also, as the depth of the modified file structure can vary, we enable users to zoom in or out with a mouse click on the icicle tree.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 102,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "One can control the granularity of clustering by setting a ClusteringStep encoded as a vertical slider. The desired level of abstraction can be set by adjusting the maximum difference value (threshold)to be clustered. For instance, if one moves up the slider, the clustering becomes granular. Thus, one can analyze fine-grained clusters by moving up the slider. For the same reason, we also provided a way to set Preference Weights for each similarity criterion. For instance, if one wants to cluster only commits with similar commit types, one can simply set the weight of the commit type to 1 and the rest to 0. Such a capability reveals the underlying policy of clustering, helping users to find the information they want by allowing them to set appropriate clustering schemes for their task.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 103,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "In many tasks, understanding what happened over a period of time is important. Therefore, we provided a Global Temporal Filter with ways to filter for a certain time period: Brushing and a Select Box. Githru provides two horizontal bars that can be brushed. The bar at the top includes two areas aligned vertically, which represent the number of commits and LOCs by date respectively. The bar at the bottom is a horizontal list of boxes that encodes each commit ordered by date. Both brushes allow for filtering in a specific range and they are synchronized. This method allows users to effectively select a specific period of dates or commits. Users can also select a specific date or release tag using the Select Box. This solves the problem that occurs in brushing when the user has to choose an exact position, which is difficult to select.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 104,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Stem Type Filter Each stem type in Githru has various characteristics, such as the existence of a name, its relation to PRs, and its PR status. Users may want to focus on a particular stem type depending on their task. For instance, there is no need to see merged or closed branches when looking for recently opened PRs. Thus, we offered options to show or hide each stem type (R3).",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 105,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "Search and Highlight If one searches for a certain keyword, Githru scans branch names, tags, commit messages, authors, commit IDs, and modi\ufb01ed \ufb01les. Then, it highlights every block that matches. Multiple keyword highlighting is also allowed (R3).",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 106,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R3: Support \ufb01ltering by and searching for details. Depending on the user query, which can be a keyword or a temporal range, the corresponding commits should be (a) \ufb01ltered in or out and (b) searched and highlighted to reduce the exploration scope. Moreover, users should be able to (c) browse the details of each commit (T3).",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": {
        "network_and_trees": 1,
        "temporal": 1,
        "sequential": 1,
        "text": 1
      }
    },
    "solution": [
      {
        "solution_text": "Diff View shows a two-way comparison between selections for authors, commit types, files, and keywords. Since comparison becomes difficult as the number of objects increases and Grouped Summary View already provides a rough overview of a multi-way comparison, a two-way comparison fits the details-on-demand strategy.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 107,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R4: Support comparison. The system should facilitate comparisons (a) based on the number of commits and LOC. The magnitude can be compared according to (b) overall trends, or (c) within/between user-selections. (d) In particular, the information in the changed \ufb01les should be compared while being organized according to the directory that contains the structure of the source code (T3).",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": { "sequential": 1, "network_and_trees": 1, "temporal": 1 }
    },
    "solution": [
      {
        "solution_text": "This view enables a visual comparison of the relative size among selected clusters, which was frequently cited as a needed task in the requirement analysis.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 108,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R4: Support comparison. The system should facilitate comparisons (a) based on the number of commits and LOC. The magnitude can be compared according to (b) overall trends, or (c) within/between user-selections. (d) In particular, the information in the changed \ufb01les should be compared while being organized according to the directory that contains the structure of the source code (T3).",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": {
        "network_and_trees": 1,
        "temporal": 1,
        "sequential": 1,
        "text": 1
      }
    },
    "solution": [
      {
        "solution_text": "Comparison View provides a detailed comparison of clusters or cluster sets (R4) based on similarity criteria and stem topology. It is designed following the details-on-detail strategy: a rough comparison in Grouped Summary View, and a detailed comparison in Comparison View. We used keywords instead of raw messages for comparison since it was more dif\ufb01cult to use unstructured strings than keywords when visualizing the differences and commonalities between clusters. We used Selection Cards to compare stem information and date and used a Diff View for the other criteria.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 109,
    "paper_title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis",
    "pub_year": 2021,
    "domain": "git",
    "requirement": {
      "requirement_text": "R4: Support comparison. The system should facilitate comparisons (a) based on the number of commits and LOC. The magnitude can be compared according to (b) overall trends, or (c) within/between user-selections. (d) In particular, the information in the changed \ufb01les should be compared while being organized according to the directory that contains the structure of the source code (T3).",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "an in-house repository dataset, the public GitHub repository realm-java",
      "data_code": {
        "network_and_trees": 1,
        "temporal": 1,
        "sequential": 1,
        "text": 1
      }
    },
    "solution": [
      {
        "solution_text": "Diff View shows a two-way comparison between selections for authors, commit types, files, and keywords. Since comparison becomes difficult as the number of objects increases and Grouped Summary View already provides a rough overview of a multi-way comparison, a two-way comparison fits the details-on-demand strategy.",
        "solution_category": "data_manipulation",
        "solution_axial": "Clustering&Grouping",
        "solution_compoent": "",
        "axial_code": ["Clustering&Grouping"],
        "componenet_code": ["clustering_and_grouping"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 110,
    "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles",
    "pub_year": 2021,
    "domain": "Functional Safety",
    "requirement": {
      "requirement_text": "DG1: FACILITATE E XPLORATION OF A P ROJECT User tasks such as \u201cFind missing links\u201d, \u201cLookup and analyze a node\u2019s end-to-end traceability\u201d are network exploration tasks. Hence, we modeled Functional Safety data as a network and derived this design goal to support tasks like fetching node details on hover, \ufb01nding adjacent nodes, and \ufb01nding paths from one node to another based on the taxonomies by Lee et al. [34] and Pretorius et al. [40].",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "Since the visualization canvas and the tab layout are vertically stacked within a project panel, we positioned each panel side by side. This way, SafetyLens would facilitate exploratory analyses within and comparative analyses across projects ",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "scatter+table",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 114,
    "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles",
    "pub_year": 2021,
    "domain": "Functional Safety",
    "requirement": {
      "requirement_text": "DG3: D ISCOVER PATTERNS AND A NOMALIES The domain experts we spoke to make use of existing commercial and open source tools. These tools support basic exploration and comparison of Functional Safety projects but may fall short when the scale and complexity of data increases. Thus, SafetyLens should support discovering interesting patterns within and across projects.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "S UMMARY V IEW, shown in Figure 5, provides a summary of the projects selected in the Dashboard View (DG6). There are juxtaposed heatmap visualizations for Node Type, Link Relation, and ASIL respectively positioned next to each other. For each attribute table, projects are along the column axis and the corresponding attribute values are along the row axis. An additional column titled \u201cS\u201d is added to show the number of entities (nodes and links) that are shared among these projects. The cells show the per-project entity counts for the corresponding attribute, colored using a continuous color scale (white-to-gray) to help the user discover patterns within as well as across projects (DG3)",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "heatmap",
        "axial_code": ["Repetition"],
        "componenet_code": ["heatmap"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 115,
    "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles",
    "pub_year": 2021,
    "domain": "Functional Safety",
    "requirement": {
      "requirement_text": "DG3: D ISCOVER PATTERNS AND A NOMALIES The domain experts we spoke to make use of existing commercial and open source tools. These tools support basic exploration and comparison of Functional Safety projects but may fall short when the scale and complexity of data increases. Thus, SafetyLens should support discovering interesting patterns within and across projects.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "VISUALIZATION CANVAS shows a node-link group visualiza- tion. Each node (small circle) is an Element of a functional safety project. These nodes are clustered together based on attributes (e.g., Type). The largest circles represent the group nodes and are labeled with the Type and the number of nodes that are part of it. The boundary marking the extent of the groups (convex hull) is highlighted. The nodes are positioned in each others\u2019 vicinity using an implementation of the circle packing algorithm. The node size and color can be mapped to attributes such as {ASIL, Type, Degree (number of edges to a node)}which in the presence of multiple nodes across multiple projects will create visual clusters leading to discoveries of patterns and anomalies. ",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "scatter",
        "axial_code": ["Repetition"],
        "componenet_code": ["scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 116,
    "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles",
    "pub_year": 2021,
    "domain": "Functional Safety",
    "requirement": {
      "requirement_text": "DG4: FACILITATE T RACEABILITY AND D ECOMPOSITION OF ASIL S An important task for domain experts is to trace the ASIL from one node to another (e.g., {MB \u2192 HzE \u2192 SG \u2192 FSR \u2192 TSR}). Since ASILs determine the extent of safety mechanisms for elements, any discrepancy such as an element assigned an ASIL=A instead of ASIL=D is an important concern. To diagnose the problem, users should be able to decompose the ASIL into its Severity (S), Exposure (E), and Controllability (C). Thus, it is an important design goal for SafetyLens to unify tasks allowing users to ef\ufb01ciently detect, diagnose, and \ufb01x ASIL assignment issues",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "SafetyLens first checks if a path exists between the two nodes and overlays it onto the node-link group visualization. It also returns a linearized node-link diagram showing the entire route from source to destination. Below the node link diagram is a heatmap showing the S-E-C (Severity Exposure-Controllability) break up of the ASILs.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "scatter+table+heatmap",
        "axial_code": ["Stack"],
        "componenet_code": ["heatmap", "table", "scatter"]
      },
      {
        "solution_text": "TRACE TAB allows the user to find and visualize if a path exists between two nodes as well as trace their ASILs (e.g., tracing the ASIL from a System Behavior to a Technical Software Requirement). The user can set a node as Source and another as Destination.",
        "solution_category": "interaction",
        "solution_axial": "Selecting,",
        "solution_compoent": "",
        "axial_code": ["", "Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 118,
    "paper_title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles",
    "pub_year": 2021,
    "domain": "Functional Safety",
    "requirement": {
      "requirement_text": "DG6: P ROVIDE A S UMMARY OF K EY M ETRICS The user tasks suggested that key metrics should be readily available (e.g., total number of nodes, total number of nodes with ASIL=D, etc).",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "Each Project constitutes an instance of a Functional Safety dataset. This can be modeled as a network with nodes and links. Each node represents a functional safety Element with seven attributes: {ID, Name, Type, ASIL, Severity, Exposure, Controllability}. A link between two nodes represents the Relation between two Elements and has three attributes: {Source, Target, Relation}",
      "data_code": { "network_and_trees": 1, "ordinal": 1 }
    },
    "solution": [
      {
        "solution_text": "S UMMARY V IEW, shown in Figure 5, provides a summary of the projects selected in the Dashboard View (DG6). There are juxtaposed heatmap visualizations for Node Type, Link Relation, and ASIL respectively positioned next to each other. For each attribute table, projects are along the column axis and the corresponding attribute values are along the row axis. An additional column titled \u201cS\u201d is added to show the number of entities (nodes and links) that are shared among these projects. The cells show the per-project entity counts for the corresponding attribute, colored using a continuous color scale (white-to-gray) to help the user discover patterns within as well as across projects (DG3)",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "heatmap",
        "axial_code": ["Repetition"],
        "componenet_code": ["heatmap"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 120,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "P1: Obtain the spatial overview of the bus network and its performance. The experts requested to see a map-based overview of the bus network similar to other GIS software [20, 53]. Such an overview should help users to rapidly orient themselves in the spatial context and grasp the spatial distribution of routes. The overview should also include the visualization of the network performance to guide users in performing a drill-down analysis on the potentially ineffective routes.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "tables": 1, "geometry": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "For the network-level analysis, a spatial aggregation view is designed to provide a spatial overview of the entire network and help the users filter routes with spatial constraints",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "map+radar+area",
        "axial_code": ["Nesting"],
        "componenet_code": ["area", "map", "radar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 121,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "P1: Obtain the spatial overview of the bus network and its performance. The experts requested to see a map-based overview of the bus network similar to other GIS software [20, 53]. Such an overview should help users to rapidly orient themselves in the spatial context and grasp the spatial distribution of routes. The overview should also include the visualization of the network performance to guide users in performing a drill-down analysis on the potentially ineffective routes.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "tables": 1, "geometry": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "For the route-level analysis, a route ranking view is implemented to depict the performance of the routes, allowing the users to find inefficient routes based on performance criteria.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "table+area",
        "axial_code": ["Stack"],
        "componenet_code": ["area", "table"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 122,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "P1: Obtain the spatial overview of the bus network and its performance. The experts requested to see a map-based overview of the bus network similar to other GIS software [20, 53]. Such an overview should help users to rapidly orient themselves in the spatial context and grasp the spatial distribution of routes. The overview should also include the visualization of the network performance to guide users in performing a drill-down analysis on the potentially ineffective routes.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "tables": 1, "geometry": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "6.1.1 Network-Level Analysis An overview of the bus network is essential in locating the areas where the inef\ufb01cient routes most likely exist. The spatial aggregation view (Fig. 1B), designed for the network-level analysis, comprises three linked layers, namely, the map, route, and aggregation layers, to help users analyze the network on a broad scale. The map layer comprises a base map. The route layer draws all routes on the map in blue with opacity. However, the route layer cannot depict the network topology because of the overlapping routes. Therefore, we designed the aggregation layer to visualize the topology with an aggregation graph. Each node in the aggregation graph corresponds to a group of bus stops aggregated spatially with hierarchical clustering [33] by balancing the number of stops in each group. These groups divide the city into transportation zones. The boundaries of these zones are computed with a Voronoi diagram [23] by unifying the polygons that enclose the bus stops inside the zones. In addition, the numbers of the routes between the zones are encoded with the link widths. A zone glyph (Fig. 1D) is placed at the centroid of each transportation zone to summarize the key statistics of this zone. The radar at the glyph center encodes six averaged criteria, namely, route length (RL), number of stops (NS), passenger volume (PV), average load (AL), route directness (DR), and service cost (SC).",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "map+radar+area",
        "axial_code": ["Nesting"],
        "componenet_code": ["area", "map", "radar"]
      },
      {
        "solution_text": "Users can con\ufb01gure the visibility and order of the dimensions \ufb02exibly in the context menu. Two diverging circular distributions around the glyph visualize the amount of passenger \ufb02ows by the geographical directions in which the passengers in this zone leave (green) or enter (orange). Double clicks magnify the glyphs, allowing users to obtain a clearer view of the radars inside. The design of this glyph is kept simple yet informative, such that the users can naturally obtain and compare the performance of different zones with a number of glyphs.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 123,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "P1: Obtain the spatial overview of the bus network and its performance. The experts requested to see a map-based overview of the bus network similar to other GIS software [20, 53]. Such an overview should help users to rapidly orient themselves in the spatial context and grasp the spatial distribution of routes. The overview should also include the visualization of the network performance to guide users in performing a drill-down analysis on the potentially ineffective routes.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "tables": 1, "geometry": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "6.1.2 Route-Level Analysis In the route-level analysis, we focus on assisting the users in \ufb01nding inef\ufb01cient routes based on performance criteria in complementary to the spatial information. Inspired by LineUp [27], a table-based ranking visualization is included in the route ranking view to facilitate the multicriteria analysis of the routes.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "table+area",
        "axial_code": ["Stack"],
        "componenet_code": ["area", "table"]
      },
      {
        "solution_text": "As illustrated in Fig. 1F, the columns represent six criteria similar to the zone glyphs, and each row is a route that can be ranked by selecting any column. The criteria can also be grouped and sorted with different weights. In addition, the criterion distributions are shown in the column headers, providing an overview and range \ufb01lters for the routes in the table.",
        "solution_category": "interaction",
        "solution_axial": "Reconfigure",
        "solution_compoent": "",
        "axial_code": ["Reconfigure"],
        "componenet_code": ["reconfigure"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 124,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "P2: Analyze the passenger \ufb02ows of bus routes to \ufb01nd weaknesses. The movement of passengers through the bus network provides key insights for the experts to evaluate the performance of the routes. For example, some parts of a route might be non-functional if few passengers were getting on or off in these parts. Therefore, intuitive visualization of passenger \ufb02ows is highly demanded to facilitate the identi\ufb01cation of de\ufb01cient routes. Moreover, such a visualization should also enable the experts to analyze the transfers among multiple bus routes, which may reveal patterns to improve route design.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "tables": 1, "geometry": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "For the network-level analysis, a spatial aggregation view is designed to provide a spatial overview of the entire network and help the users filter routes with spatial constraints.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "map+radar+area",
        "axial_code": ["Nesting"],
        "componenet_code": ["area", "map", "radar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 125,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "P2: Analyze the passenger \ufb02ows of bus routes to \ufb01nd weaknesses. The movement of passengers through the bus network provides key insights for the experts to evaluate the performance of the routes. For example, some parts of a route might be non-functional if few passengers were getting on or off in these parts. Therefore, intuitive visualization of passenger \ufb02ows is highly demanded to facilitate the identi\ufb01cation of de\ufb01cient routes. Moreover, such a visualization should also enable the experts to analyze the transfers among multiple bus routes, which may reveal patterns to improve route design.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "tables": 1, "geometry": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "For the stop-level analysis, a route matrix view is proposed to visualize the passenger flows and transfers among the stops in a selected route with matrices, establishing fine-grained inspection of route performance.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "matrix+bar",
        "axial_code": ["Coordinate"],
        "componenet_code": ["matrix", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 126,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "P2: Analyze the passenger \ufb02ows of bus routes to \ufb01nd weaknesses. The movement of passengers through the bus network provides key insights for the experts to evaluate the performance of the routes. For example, some parts of a route might be non-functional if few passengers were getting on or off in these parts. Therefore, intuitive visualization of passenger \ufb02ows is highly demanded to facilitate the identi\ufb01cation of de\ufb01cient routes. Moreover, such a visualization should also enable the experts to analyze the transfers among multiple bus routes, which may reveal patterns to improve route design.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "tables": 1, "geometry": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "6.1.1 Network-Level Analysis An overview of the bus network is essential in locating the areas where the inef\ufb01cient routes most likely exist. The spatial aggregation view (Fig. 1B), designed for the network-level analysis, comprises three linked layers, namely, the map, route, and aggregation layers, to help users analyze the network on a broad scale. The map layer comprises a base map. The route layer draws all routes on the map in blue with opacity. However, the route layer cannot depict the network topology because of the overlapping routes. Therefore, we designed the aggregation layer to visualize the topology with an aggregation graph. Each node in the aggregation graph corresponds to a group of bus stops aggregated spatially with hierarchical clustering [33] by balancing the number of stops in each group. These groups divide the city into transportation zones. The boundaries of these zones are computed with a Voronoi diagram [23] by unifying the polygons that enclose the bus stops inside the zones. In addition, the numbers of the routes between the zones are encoded with the link widths. A zone glyph (Fig. 1D) is placed at the centroid of each transportation zone to summarize the key statistics of this zone. The radar at the glyph center encodes six averaged criteria, namely, route length (RL), number of stops (NS), passenger volume (PV), average load (AL), route directness (DR), and service cost (SC).",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "map+radar+area",
        "axial_code": ["Nesting"],
        "componenet_code": ["area", "map", "radar"]
      },
      {
        "solution_text": "Users can con\ufb01gure the visibility and order of the dimensions \ufb02exibly in the context menu. Two diverging circular distributions around the glyph visualize the amount of passenger \ufb02ows by the geographical directions in which the passengers in this zone leave (green) or enter (orange). Double clicks magnify the glyphs, allowing users to obtain a clearer view of the radars inside. The design of this glyph is kept simple yet informative, such that the users can naturally obtain and compare the performance of different zones with a number of glyphs.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 127,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "P2: Analyze the passenger \ufb02ows of bus routes to \ufb01nd weaknesses. The movement of passengers through the bus network provides key insights for the experts to evaluate the performance of the routes. For example, some parts of a route might be non-functional if few passengers were getting on or off in these parts. Therefore, intuitive visualization of passenger \ufb02ows is highly demanded to facilitate the identi\ufb01cation of de\ufb01cient routes. Moreover, such a visualization should also enable the experts to analyze the transfers among multiple bus routes, which may reveal patterns to improve route design.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": {
        "tables": 1,
        "geometry": 1,
        "categorical": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "6.1.3 Stop-Level Analysis The stop-level analysis enables the users to explore and evaluate the passenger \ufb02ows and transfers among the stops in a selected route. A \ufb02ow matrix (Fig. 4A) is designed to visualize the passenger \ufb02ows and transfers. The columns and rows of the matrix correspond to the bus stops, and the color intensity of each cell in the matrix encodes the number of passengers traveling between the stops. The color intensity is computed by normalizing the number of passengers into [0, 1] against a global passenger \ufb02ow threshold, which can be changed by users. The horizon charts (Fig. 4B) visualize the number of passengers, aggregated by time, checking in or out at the corresponding stops. The visibility of the horizon charts can be toggled in the context menu to simplify the view. Three-band horizon charts are chosen over bar charts because the estimation accuracy of horizon charts is better than that of bar charts when chart height is limited [30]. In addition, the bars (Fig. 4C) encoding the number of passengers per stop are positioned to the bottom and right of the matrix. In case of long bus routes, users can right click on the matrix to select stops they wish to keep in the view. Massive transfers may indicate that the routes are not well planned and discourage the use of bus transportation [75]. To visualize transfer information, we encode the number of passengers transferred to or from other routes with the opacity of the circles (Fig. 4D) next to the station names. The numbers on the circles indicate how many routes passengers have transferred to or from. The minimum opacity of the circles has been tuned to maintain the visibility of the numbers inside them. Clicking on a circle expands a list of the associated routes (Fig. 4E), each preceded by a small pie chart indicating the percentage of passenger \ufb02ows transferred to or from the route. Selecting a route in the list reveals another \ufb02ow matrix visualizing this route, which is aligned and linked to the original matrix (Fig. 4F). Inspired by MatrixWave [79], we rotate the matrices by 45 \u00b0 clockwise to accommodate them linearly for enhanced scalability. An overview of the matrices (Fig. 4G) is provided at the bottom-left of the view, where each matrix is represented with a square. The currently focused matrix is outlined in the overview, and the numbers of transferred routes are enclosed in the dashed squares.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "matrix+bar",
        "axial_code": ["Coordinate"],
        "componenet_code": ["matrix", "bar"]
      },
      {
        "solution_text": "6.1.3 Stop-Level Analysis The stop-level analysis enables the users to explore and evaluate the passenger \ufb02ows and transfers among the stops in a selected route. A \ufb02ow matrix (Fig. 4A) is designed to visualize the passenger \ufb02ows and transfers. The columns and rows of the matrix correspond to the bus stops, and the color intensity of each cell in the matrix encodes the number of passengers traveling between the stops. The color intensity is computed by normalizing the number of passengers into [0, 1] against a global passenger \ufb02ow threshold, which can be changed by users. The horizon charts (Fig. 4B) visualize the number of passengers, aggregated by time, checking in or out at the corresponding stops. The visibility of the horizon charts can be toggled in the context menu to simplify the view. Three-band horizon charts are chosen over bar charts because the estimation accuracy of horizon charts is better than that of bar charts when chart height is limited [30]. In addition, the bars (Fig. 4C) encoding the number of passengers per stop are positioned to the bottom and right of the matrix. In case of long bus routes, users can right click on the matrix to select stops they wish to keep in the view. Massive transfers may indicate that the routes are not well planned and discourage the use of bus transportation [75]. To visualize transfer information, we encode the number of passengers transferred to or from other routes with the opacity of the circles (Fig. 4D) next to the station names. The numbers on the circles indicate how many routes passengers have transferred to or from. The minimum opacity of the circles has been tuned to maintain the visibility of the numbers inside them. Clicking on a circle expands a list of the associated routes (Fig. 4E), each preceded by a small pie chart indicating the percentage of passenger \ufb02ows transferred to or from the route. Selecting a route in the list reveals another \ufb02ow matrix visualizing this route, which is aligned and linked to the original matrix (Fig. 4F). Inspired by MatrixWave [79], we rotate the matrices by 45 \u00b0 clockwise to accommodate them linearly for enhanced scalability. An overview of the matrices (Fig. 4G) is provided at the bottom-left of the view, where each matrix is represented with a square. The currently focused matrix is outlined in the overview, and the numbers of transferred routes are enclosed in the dashed squares.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 128,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "M1: Generate a set of alternative routes based on the constraints. The experts prefer to interact with the model and generate the Paretooptimal routes as the potential replacements of the selected ineffective route. To minimize the disruption caused by route changes, the experts may preserve several primary stops from the original route. Moreover, certain constraints, such as the construction cost and maximum route length, may be imposed on route generation. An easy-to-use interface is required for translating these constraints into the complex parameters required by the model.",
      "requirement_code": { "evaluate_hypothesis": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": {
        "tables": 1,
        "geometry": 1,
        "categorical": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "This subsection summarizes the core idea of Weng et al.\u2019s method [63] that searches Pareto-optimal transit routes based on the Monte-Carlo search tree. The Monte-Carlo search tree [14] is studied to search the best next move in a game. Starting from a given game state, the search repeats four stages, namely, selection, expansion, simulation, and backpropagation. First, the most promising state is selected. Then, a new state is created based on the estimated best next move of the selected state. Next, the game result is obtained via simulation. Finally, the estimated value of the states in the tree is updated accordingly.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "The model results will be streamed and visualized in real-time to help the users determine the quality of the generated routes.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "map+area",
        "axial_code": ["Stack"],
        "componenet_code": ["area", "map"]
      },
      {
        "solution_text": "After a deficient route has been determined with the exploration inter- face, the users can obtain replacement routes from the manipulation interface. This interface allows the users to control the model by speci- fying model parameters, criterion filters, and anchored stops.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 129,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "M2: Inspect the quality of the generated routes in real time. Considering that the model is progressive and does not stop automatically, the experts need to know when the results are suf\ufb01ciently good to stop the route generation process. Hence, tailored visualizations are required to depict the current status of the generation process in real-time and provide the early quality preview of the alternative routes as the process continues. Moreover, such visualizations should allow some undesired routes to be removed from the search space to interactively guide and accelerate the search process.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": {
        "tables": 1,
        "geometry": 1,
        "categorical": 1,
        "temporal": 1,
        "sequential": 1
      }
    },
    "solution": [
      {
        "solution_text": "The model results will be streamed and visualized in real-time to help the users determine the quality of the generated routes.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "map+area",
        "axial_code": ["Stack"],
        "componenet_code": ["area", "map"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 130,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "L1: Compare the generated routes based on topologies. To help the experts identify the most promising ones from the generated routes, the proposed system must reveal the topological similarities and differences among these routes. The experts may want to know: What stops do these two routes share? Which pair of consecutive stops is the most frequently selected? How much does a route deviate from another one by taking a detour? Integrating topological information can help experts estimate the performance of these routes and eliminate the undesired ones that share similar characteristics",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "tables": 1, "geometry": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "To aid the users in such progressive decision-making processes, the routes and their topologies are depicted with conflict markers on the map.",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "matrix+bar",
        "axial_code": ["Coordinate"],
        "componenet_code": ["matrix", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 131,
    "paper_title": "Towards Better Bus Networks: A Visual Analytics Approach",
    "pub_year": 2021,
    "domain": "Bus route planning",
    "requirement": {
      "requirement_text": "L2: Compare the generated routes based on multiple criteria. The most promising alternative route should also be determined in terms of the performance criteria. However, experts may treat each criterion differently under different circumstances. For example, the distances between the stops in a suburban bus route will be considerably longer than those between the stops in a city bus route. To facilitate a judicious decision-making process, the system should enable the experts to inspect the criteria of the generated routes and identify the most optimal one ef\ufb01ciently with tailored ranking models.",
      "requirement_code": { "explain_differences": 1 }
    },
    "data": {
      "data_text": "The proposed system is based on three types of data, namely, bus stop, route, and trip data, collected from bus networks. Bus stop data comprise the bus stops in a city. Each stop is defined by its ID, name, and coordinates. Bus route data comprise the bus routes, where each of them is identified by its ID and a stop sequence. Bus trip data contain a series of bus fare card records, where each of them comprises a card ID, a tap-on timestamp, and the route and stops where the fare card was tapped on and off. However, the tap-off timestamps are not present in the dataset because of sensor errors. This timestamp was inferred either by using the tap-on timestamps at the destination stops if such transfer records exist, or based on driving time along the bus route at 20 km/h plus 2 min spent at every stop, as suggested by our domain experts.",
      "data_code": { "tables": 1, "geometry": 1, "categorical": 1 }
    },
    "solution": [
      {
        "solution_text": "The criteria of the available choices are visualized in the ranking view to facilitate the analysis of route performance",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "table+area",
        "axial_code": ["Stack"],
        "componenet_code": ["area", "table"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 132,
    "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
    "pub_year": 2021,
    "domain": "cheminformatics",
    "requirement": {
      "requirement_text": "R1: Overview and detailed analysis of a molecular ensemble in the low-dimensional space. For large datasets, scatter plots, which are commonly used to represent the DR output, may suffer from occlusion problems for large datasets. Therefore, the tool should provide visual support for the analysis of data on different levels of abstraction, from the overall distribution of the compounds within the 2D space to the detailed view of individual compounds for a selected region of interest.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1
      }
    },
    "solution": [
      {
        "solution_text": "User can get an overview of the distribution of compounds in a selected DR projection using a given molecular representation. ",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "The core of ChemVA consists of 2D plots, which give the user an overview of the distribution of compounds in a selected DR projection using a given molecular representation. This overview is supported by the Hexagonal view, a well-adopted and commonly used approach to visualize the outcome of DR techniques [57]. The Hexagonal view aims to overcome the overplotting problem, in which the projected data items overlap and cause visual clutter , thus limiting the interpretability, especially for datasets evincing high similarity between data items. The Hexagonal view of ChemVA seeks to solve this problem by aggregating individual data items into individual hexagons. The user can interactively select a subset of hexagons of interest and explore the distribution of individual data items within the Detail view. The combination of the Hexagonal view and the Detail view aims to ful\ufb01ll requirement R1. Finally, since ChemVA is tailored to support the visual comparison of different projections, it also offers the Difference view. This view was speci\ufb01cally designed to address that task, which is stated in requirements R2 and R3.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "hexagonarea+scatter",
        "axial_code": ["Repetition"],
        "componenet_code": ["hexagonarea", "scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 133,
    "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
    "pub_year": 2021,
    "domain": "cheminformatics",
    "requirement": {
      "requirement_text": "R2: Visual inspection of multiple projections. A set of compounds can be expressed by different vector-based molecular representations, each yielding a different DR projection. The tool should enable the user to intuitively combine information encoded in the individual projections to allow studying at once the similarity between compounds based on different molecular representations. More speci\ufb01cally, this includes exploring similarities and differences between chemical compounds, expressed by the different projections. Additionally, the visual representations and interactions should help the domain experts evaluate the suitability of the selected DR model.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1
      }
    },
    "solution": [
      {
        "solution_text": "User can get an overview of the distribution of compounds in a selected DR projection using a given molecular representation. ",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "In order to support the task of comparing the outputs of different 2D projections, as stated in our requirement R2, we propose a novel view called Difference view, that combines and contrasts two selected 2D Hexagonal views, A and B. Initially it displays a hexagonal layout similar to that presented in the Hexagonal view, where the opacity of each hexagon encodes the computed correlation score of the trustworthiness of the projections A and B under study (requirement R3)",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "hexagonarea",
        "axial_code": ["Coordinate"],
        "componenet_code": ["hexagonarea"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 134,
    "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
    "pub_year": 2021,
    "domain": "cheminformatics",
    "requirement": {
      "requirement_text": "R3: Evaluation of the trustworthiness of projections. Users need proper visual support for assessing the trustworthiness of a lowdimensional projection based on the distortion with regard to pairwise distances between compounds in the original space. When such trustworthiness can be compared on different DR projections, users can focus the exploration on a subset of molecular representations.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1
      }
    },
    "solution": [
      {
        "solution_text": "User can get an overview of the distribution of compounds in a selected DR projection using a given molecular representation. ",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "The color of a hexagon encodes the prevailing trend among its compounds for a selected feature, which is by default their bioactivity but can be switched to other molecular properties, including the trustworthiness of the projection.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "hexagonarea",
        "axial_code": ["Repetition"],
        "componenet_code": ["hexagonarea"]
      },
      {
        "solution_text": "The color of a hexagon encodes the prevailing trend among its compounds for a selected feature, which is by default their bioactivity but can be switched to other molecular properties, including the trustworthiness of the projection.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 135,
    "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
    "pub_year": 2021,
    "domain": "cheminformatics",
    "requirement": {
      "requirement_text": "R3: Evaluation of the trustworthiness of projections. Users need proper visual support for assessing the trustworthiness of a lowdimensional projection based on the distortion with regard to pairwise distances between compounds in the original space. When such trustworthiness can be compared on different DR projections, users can focus the exploration on a subset of molecular representations.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1
      }
    },
    "solution": [
      {
        "solution_text": "In addition, all properties and features described in Section 3 can be color-encoded on points representing each compound. These features are selected from a drop-down menu, and their color encodings are chosen according to their type, i.e., quantitative, such as molecular weight, or categorical, such as bioactivity towards a target protein. Another quantitative property that can be used for color encoding corresponds to the correlation scores, which encode the trustworthiness of the DR projection of the compound (requirement R3). These scores were computed using Pearson and Kendall correlation, whose calculation is explained in the Supplementary Material.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "Upon selecting a subset of compounds in the Hexagonal view, the user can explore the selected data in the Detail view, depicted in Fig_x0002_ure 3. In this view, the compounds are represented using a standard scatter plot, enhanced by a subtle overlay of the same hexagonal grid as in the Hexagonal view, which helps users keep the correspondence between the zoom level in these views. ",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "hexagonarea+scatter",
        "axial_code": ["Nesting"],
        "componenet_code": ["hexagonarea", "scatter"]
      },
      {
        "solution_text": "The Detail view displays only the selected hexagons zoomed in after the selection. To further enhance the link between the Hexagonal and Detail views, the corresponding hexagons are highlighted when the user hovers over them in any of these views. In order to perform a selection of individual compounds in this view, a lasso-shaped selector is supported. The selected compounds are then displayed in the 3D view and also highlighted in the Table view (Section 5.1.2).",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 136,
    "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
    "pub_year": 2021,
    "domain": "cheminformatics",
    "requirement": {
      "requirement_text": "R3: Evaluation of the trustworthiness of projections. Users need proper visual support for assessing the trustworthiness of a lowdimensional projection based on the distortion with regard to pairwise distances between compounds in the original space. When such trustworthiness can be compared on different DR projections, users can focus the exploration on a subset of molecular representations.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1
      }
    },
    "solution": [
      {
        "solution_text": "User can get an overview of the distribution of compounds in a selected DR projection using a given molecular representation. ",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "In order to support the task of comparing the outputs of different 2D projections, as stated in our requirement R2, we propose a novel view called Difference view, that combines and contrasts two selected 2D Hexagonal views, A and B. Initially it displays a hexagonal layout similar to that presented in the Hexagonal view, where the opacity of each hexagon encodes the computed correlation score of the trustworthiness of the projections A and B under study (requirement R3)",
        "solution_category": "visualization",
        "solution_axial": "Coordinate",
        "solution_compoent": "hexagonarea",
        "axial_code": ["Coordinate"],
        "componenet_code": ["hexagonarea"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 137,
    "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
    "pub_year": 2021,
    "domain": "cheminformatics",
    "requirement": {
      "requirement_text": "R4: Comparison of compounds according to features related to drug-likeness. Chemical compounds have many features and descriptors related to drug-likeness that can complement other molecular representations in the task of virtual screening. Therefore, it is desirable to provide users with an option to visualize these additional features along with the projections.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1
      }
    },
    "solution": [
      {
        "solution_text": "Besides from the vector-based molecular representations used in the DR projections and displayed in our 2D plot views, there are several other molecular features related to drug-likeness that should be taken into consideration when analyzing the compounds, as stated in requirement R4. ChemVA enables the users to explore such features, listed in Section 3.2, by means of a Table view which offers advanced interaction options. We adopted a well-established tool, published by Gratzl et al. [13], and its extension [11]. As these tools perfectly \ufb01t to our needs, we incorporate them to ChemVA. Further details about the broad range of interaction possibilities can be found in the original papers. In addition to the list of compounds, the Table view provides the users with graphical elements in the form of juxtaposed bar charts and box plots in the right side panel. By default, the compounds in the table are logically grouped by their membership to hexagons in the Hexagonal view. These groups can be either expanded or compressed. When compressed, the table displays the box plots of the distribution of the values for each feature in the hexagon.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "bar+box",
        "axial_code": ["Stack"],
        "componenet_code": ["box", "bar"]
      },
      {
        "solution_text": "As shown in Figure 5. This view is interactively linked with the other visual components of ChemVA. When performing a selection in the 2D plot views, the corresponding compounds are automatically highlighted in the Table view. Conversely, when the user selects compounds in the Table view, the corresponding compounds are highlighted in the Detail view, displayed in the 3D view, and the corresponding hexagons are highlighted in the Hexagonal view",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 138,
    "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
    "pub_year": 2021,
    "domain": "cheminformatics",
    "requirement": {
      "requirement_text": "R5: Comprehensible viewing of 3D structural similarity. The tool should support the inspection of individual compounds in terms of their 3D geometry, as well as the visual comparison of common 3D substructures in a selected set of compounds. For multiple compounds, such a view should convey the information about similarities and differences in their 3D structure.",
      "requirement_code": {
        "describe_observation_item": 1,
        "compare_entities": 1
      }
    },
    "data": {
      "data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1
      }
    },
    "solution": [
      {
        "solution_text": "Compound similarity can be better perceived when the compounds are structurally aligned in the view. To serve this purpose, we use a structural alignment functionality provided by the OpenBabel tool [43]. Once molecules are aligned, the user should be able to easily identify their common parts, i.e., the subsets of atoms and bonds that are present in most of the selected compounds.",
        "solution_category": "visualization",
        "solution_axial": "Co-axis",
        "solution_compoent": "3Dstructure",
        "axial_code": ["Co-axis"],
        "componenet_code": ["3Dstructure"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 139,
    "paper_title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening",
    "pub_year": 2021,
    "domain": "cheminformatics",
    "requirement": {
      "requirement_text": "R6: Possibility to add new compounds and comparison with the existing data. The tool should support the process of exploration of different features and bioactivity for newly added compounds. The new compound should be projected using the DR model and integrated into the remaining views, so that the user can compare its features to those of the compounds in the existing dataset.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "The first dataset was composed by merging ligands binding to the Serotonin 1a receptor1 and Dopamine D2 receptor2, whereas the second dataset comprised ligands to the P-glycoprotein 13. We assigned a categorical label to each compound according to its experimentally measured IC50 bioac_x0002_tivity value towards the target(s) under study. Compounds showing IC50 values below 10 nM were labeled as Active; compounds between 10 and 1000 nM were labeled as Moderately Active, and those over 1000 nM were labeled as Inactive. The serotonin-dopamine dataset comprises 118 compounds, whereas the P-glycoprotein dataset contains 893 compounds.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "ordinal": 1
      }
    },
    "solution": [
      {
        "solution_text": "ChemVA provides an option to add new compounds to the dataset being studied in order to explore their features and compare them with those of other compounds. By means of this functionality, the expert can assess the potential of this newly added compound prior to extensive wet lab testing.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 156,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.1 Match source code with binary cod",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "The source code view displays a single source code file. By default, it displays the one with the most data, but the file can be changed in the interface.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["text", "table"]
      },
      {
        "solution_text": "The source code view displays a single source code file. By default, it displays the one with the most data, but the file can be changed in the interface.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 157,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.1 Match source code with binary cod",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "Multiple lines can be selected and will be highlighted across other views, supporting the task of matching the source code and disassembly.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 158,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.1 Match source code with binary cod",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "The disassembly represents the ground truth of the compiled program. One strategy commonly employed by users was to use linked navigation to get close to an area of interest not otherwise selectable with information from our automated analysis and then search by scrolling from there, so we include it in its entirety.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["text", "table"]
      },
      {
        "solution_text": "The disassembly represents the ground truth of the compiled program. One strategy commonly employed by users was to use linked navigation to get close to an area of interest not otherwise selectable with information from our automated analysis and then search by scrolling from there, so we include it in its entirety.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 159,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.1 Match source code with binary cod",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "Basic blocks (nodes) in the CFG can be selected individually or by brush and will update all views. The CFG view also supports panning and zooming.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 160,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.2 Identify/Relate structures with code",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding",
        "solution_compoent": "",
        "axial_code": ["Excluding"],
        "componenet_code": ["excluding"]
      },
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["text", "table"]
      },
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 161,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.2 Identify/Relate structures with code",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "The CFG view shows a subgraph of the full binary CFG, based on the current selection. Prior work on CFGs by the visualization experts led to this project. However, early meetings indicated matching of source and disassembly was the main workflow. Thus, our initial prototypes did not include a CFG. In subsequent meetings, we observed our domain experts had difficulty understanding structures such as loops with only matching or nesting. We thus chose to provide such context with a CFG view.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "network+contour+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["text", "network", "contour"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 162,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.2 Identify/Relate structures with code",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      },
      {
        "solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "text",
        "axial_code": ["Repetition"],
        "componenet_code": ["text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 163,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.2 Identify/Relate structures with code",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "Identifying or navigating to a particular loop is a common operation, so we chose to directly support it by creating a loop-centric view.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "tree+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["text", "tree"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 164,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.2 Identify/Relate structures with code",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "Consistent with the function inlining view, selections in other views will filter this one, providing loop context to those other views.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 165,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.2 Identify/Relate structures with code",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "The call graph view shows a subgraph of the full call graph, with all functions reported by our analysis regardless of whether they were inlined. This view provides a way to relate selected disassembly to the functions and call stack.  Inlined calls are shown with a dashed red line to help identify them.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "network+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["text", "network"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 166,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.3 Annotate relations",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding",
        "solution_compoent": "",
        "axial_code": ["Excluding"],
        "componenet_code": ["excluding"]
      },
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["text", "table"]
      },
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 167,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.3 Annotate relations",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "Annotating the disassembly with source code variable names is a common task. While our automated analysis provides a best-effort annotation, it is incomplete. We allow the user to manually add annotations with this view. The view further summarizes all active renamings.",
        "solution_category": "interaction",
        "solution_axial": "Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration"],
        "componenet_code": ["participation/collaboration"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 168,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.4 Trace variable",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "data_manipulation",
        "solution_axial": "Excluding",
        "solution_compoent": "",
        "axial_code": ["Excluding"],
        "componenet_code": ["excluding"]
      },
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+text",
        "axial_code": ["Repetition"],
        "componenet_code": ["text", "table"]
      },
      {
        "solution_text": "When available, we modify the instruction text to include the associ- ated source code variable name. We denote this by striking through the register name and presenting the source code with a pink background. This feature supports our annotation, structure identification, and variable tracing tasks.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 169,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T1.4 Trace variable",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "The call graph view shows a subgraph of the full call graph, with all functions reported by our analysis regardless of whether they were inlined. This view provides a way to relate selected disassembly to the functions and call stack.  Inlined calls are shown with a dashed red line to help identify them.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "network+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["text", "network"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 170,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T2.1 Find areas of interest",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "Another change from CFGExplorer is filtering the graph to a k-hop region of interest around selected basic blocks. Our data creates CFGs that are too large for Sugiyama-style layouts. To support the winnowing of data to find areas of interest, k is configurable via the interface, with a default of k=3 determined through our users\u2019 experience.",
        "solution_category": "interaction",
        "solution_axial": "Filtering,Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration", "Filtering"],
        "componenet_code": ["participation/collaboration", "filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 171,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T2.1 Find areas of interest",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "Function inlining is one of the most common optimizations performed by compiler and is of great interest to our collaborators. Thus, we design a separate panel for inlining information to help identify and navigate to them.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "tree+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["text", "tree"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 172,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T2.1 Find areas of interest",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "Identifying or navigating to a particular loop is a common operation, so we chose to directly support it by creating a loop-centric view.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "tree+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["text", "tree"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 173,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T2.1 Find areas of interest",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "Consistent with the function inlining view, selections in other views will filter this one, providing loop context to those other views.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 174,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T2.2 Identify optimizations",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      },
      {
        "solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "text",
        "axial_code": ["Repetition"],
        "componenet_code": ["text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 175,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T2.2 Identify optimizations",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "Function inlining is one of the most common optimizations performed by compiler and is of great interest to our collaborators. Thus, we design a separate panel for inlining information to help identify and navigate to them.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "tree+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["text", "tree"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 176,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T2.2 Identify optimizations",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "The call graph view shows a subgraph of the full call graph, with all functions reported by our analysis regardless of whether they were inlined. This view provides a way to relate selected disassembly to the functions and call stack.  Inlined calls are shown with a dashed red line to help identify them.",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "network+text",
        "axial_code": ["Nesting"],
        "componenet_code": ["text", "network"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 177,
    "paper_title": "CcNav: Understanding Compiler Optimizations in Binary Code",
    "pub_year": 2021,
    "domain": "compilation",
    "requirement": {
      "requirement_text": "T2.3 Assess optimizations",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The input data for CcNavis a compiled executable and its source code, the former of which can be disassembled into disassembly code. Both source and disassembly code are text data. There may be multiple source files associated with a single executable file.",
      "data_code": { "text": 1 }
    },
    "solution": [
      {
        "solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      },
      {
        "solution_text": "The highlighted items view lists the highlighted source lines, disassembly lines, and basic blocks without context. As highlighted items are often dispersed across large ranges of source lines, this view provides a way to examine them together when the content is more important than the context, e.g., when assessing the use of instruction types or the presence of variables.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "text",
        "axial_code": ["Repetition"],
        "componenet_code": ["text"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 179,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R1. Identify relevant dimensions that exhibit high levels of bias.Users should be able to see which dimensions exhibit high lev-els of selection bias and understand which of those high-biasdimensions, and groups of dimensions, are most relevant for theiranalytical question.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "The split icicle plot was developed to visualize shifts in distribution for dimensions in large hierarchies, indicating potential selection bias [6]. Although effective for communicating this information, it exhibits some limitations when used for DR. We therefore developed the icicle table to address these limitations and add functionality useful for DR.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "icicle",
        "axial_code": ["Repetition"],
        "componenet_code": ["icicle"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 180,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R1. Identify relevant dimensions that exhibit high levels of bias.Users should be able to see which dimensions exhibit high lev-els of selection bias and understand which of those high-biasdimensions, and groups of dimensions, are most relevant for theiranalytical question.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "The split icicle plot modifies the strict hierarchical icicle plot layout by splitting certain nodes, enabling more effective sorting of the plot by the maximum distance along each path from a leaf node to the root. Thus areas with large shifts can be sorted toward the top of the plot to help the user prioritize dimensions to investigate for selection bias.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "icicle",
        "axial_code": ["Repetition"],
        "componenet_code": ["icicle"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 181,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R1. Identify relevant dimensions that exhibit high levels of bias.Users should be able to see which dimensions exhibit high lev-els of selection bias and understand which of those high-biasdimensions, and groups of dimensions, are most relevant for theiranalytical question.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Integrating a table with the split icicle plot enables the inclusion of multi-attribute information to help the user select appropriate dimen- sions for reweighting.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "icicle+table",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "icicle"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 182,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R1. Identify relevant dimensions that exhibit high levels of bias.Users should be able to see which dimensions exhibit high lev-els of selection bias and understand which of those high-biasdimensions, and groups of dimensions, are most relevant for theiranalytical question.",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Three additional visualizations show the effect of reweighting on per- dimension distances and outcome correlations for the baseline and focus cohorts and enable the selection of reweighting dimensions: a scatter plot, contour plot, and vector plot. Each view shows correlation with outcome along the x-axis and focus-to-baseline distance along the y-axis. The user can switch views on demand, with linked selection between the three views and the icicle table.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "scatter+contour+vector",
        "axial_code": ["Stack"],
        "componenet_code": ["contour", "vector", "scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 183,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R2. Apply bias correction based on user-selected dimensions.Users should be able to specify one or more dimensions for biascorrection and have the system automatically determine the re-quired sample weighting to perform the correction.",
      "requirement_code": { "evaluate_hypothesis": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "The split icicle plot was developed to visualize shifts in distribution for dimensions in large hierarchies, indicating potential selection bias [6]. Although effective for communicating this information, it exhibits some limitations when used for DR. We therefore developed the icicle table to address these limitations and add functionality useful for DR.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "icicle",
        "axial_code": ["Repetition"],
        "componenet_code": ["icicle"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 184,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R2. Apply bias correction based on user-selected dimensions.Users should be able to specify one or more dimensions for biascorrection and have the system automatically determine the re-quired sample weighting to perform the correction.",
      "requirement_code": { "evaluate_hypothesis": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "To further support the DR workflow, we have added a replace reweight mode. When fine-tuning the reweighting configuration, the user may wish to adjust a reweight dimension by moving up in the hierarchy to a more general type or down to a more specific type. To do so, the user can enter replace reweight mode, causing the icicle plot to show only the reweight dimension, its ancestors, and its descendants. All ancestors, the reweight dimension, and two levels of children are marked as salient, providing a detailed view of the local neighborhood around the reweight dimension. The user can then select a dimension to take its place in the list of reweight dimensions.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "icicle+table",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "icicle"]
      },
      {
        "solution_text": "To further support the DR workflow, we have added a replace reweight mode. When fine-tuning the reweighting configuration, the user may wish to adjust a reweight dimension by moving up in the hierarchy to a more general type or down to a more specific type. To do so, the user can enter replace reweight mode, causing the icicle plot to show only the reweight dimension, its ancestors, and its descendants. All ancestors, the reweight dimension, and two levels of children are marked as salient, providing a detailed view of the local neighborhood around the reweight dimension. The user can then select a dimension to take its place in the list of reweight dimensions.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 185,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R2. Apply bias correction based on user-selected dimensions.Users should be able to specify one or more dimensions for biascorrection and have the system automatically determine the re-quired sample weighting to perform the correction.",
      "requirement_code": { "evaluate_hypothesis": 1, "collect_evidence": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "The balance panel includes visualizations and controls for the DR process. A reweight list shows all dimensions selected for reweighting. The user can remove dimensions from the list, initiate the reweighting process, and use a slider to control the amount of reweighting. A detailed view of the per-cohort subgroups used for reweighting is shown in the reweight set visualization.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 186,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R3. Understand the effect of bias correction. This includes two keyaspects. First (R3.1), as correcting for a small set of speci\ufb01eddimensions will affect a larger set of dimensions, the user shouldbe made aware of how widespread the effects of reweighting are,where bias was reduced, and where and how much bias remains.Second (R3.2), users must understand the effect of bias correctionon the visualizations driving their primary analysis.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "An improved aggregate distance measure is included to better sum- marize the effect of reweighting upon a cohort (R3.1). In a typical cohort exhibiting selection bias, the vast majority of dimensions un- dergo small to moderate shifts in distribution, whereas a smaller number of dimensions that are highly correlated with the dimensions used for selection will undergo larger shifts. It is typically these high-bias di- mensions that the user wishes to correct.",
        "solution_category": "data_manipulation",
        "solution_axial": "AlgorithmCalculation",
        "solution_compoent": "",
        "axial_code": ["AlgorithmCalculation"],
        "componenet_code": ["algorithmic_calculation"]
      },
      {
        "solution_text": "As the event sequence analysis capabilities of Cadence are used to filter existing cohorts to create new cohorts, representations of each cohort and their provenance are shown in the cohort provenance tree (Figure 1-a). Each cohort is represented by a glyph encoding cohort size and aggregate distance across all data dimensions, along with icons indicating the baseline and focus cohorts (Figure 3-a and b).",
        "solution_category": "visualization",
        "solution_axial": "Nesting",
        "solution_compoent": "tree+glyph",
        "axial_code": ["Nesting"],
        "componenet_code": ["glyph", "tree"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 187,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R3. Understand the effect of bias correction. This includes two keyaspects. First (R3.1), as correcting for a small set of speci\ufb01eddimensions will affect a larger set of dimensions, the user shouldbe made aware of how widespread the effects of reweighting are,where bias was reduced, and where and how much bias remains.Second (R3.2), users must understand the effect of bias correctionon the visualizations driving their primary analysis.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1 }
    },
    "solution": [
      {
        "solution_text": "The split icicle plot was developed to visualize shifts in distribution for dimensions in large hierarchies, indicating potential selection bias [6]. Although effective for communicating this information, it exhibits some limitations when used for DR. We therefore developed the icicle table to address these limitations and add functionality useful for DR.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "icicle",
        "axial_code": ["Repetition"],
        "componenet_code": ["icicle"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 188,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R3. Understand the effect of bias correction. This includes two keyaspects. First (R3.1), as correcting for a small set of speci\ufb01eddimensions will affect a larger set of dimensions, the user shouldbe made aware of how widespread the effects of reweighting are,where bias was reduced, and where and how much bias remains.Second (R3.2), users must understand the effect of bias correctionon the visualizations driving their primary analysis.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Three additional visualizations show the effect of reweighting on per- dimension distances and outcome correlations for the baseline and focus cohorts and enable the selection of reweighting dimensions: a scatter plot, contour plot, and vector plot. Each view shows correlation with outcome along the x-axis and focus-to-baseline distance along the y-axis. The user can switch views on demand, with linked selection between the three views and the icicle table.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "scatter+contour+vector",
        "axial_code": ["Stack"],
        "componenet_code": ["contour", "vector", "scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 189,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R3. Understand the effect of bias correction. This includes two keyaspects. First (R3.1), as correcting for a small set of speci\ufb01eddimensions will affect a larger set of dimensions, the user shouldbe made aware of how widespread the effects of reweighting are,where bias was reduced, and where and how much bias remains.Second (R3.2), users must understand the effect of bias correctionon the visualizations driving their primary analysis.",
      "requirement_code": { "describe_observation_aggregate": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "Data type-dependent visualizations supporting R3 show the distribution of any dimension selected via the icicle table or distance vs. correlation plots, enabling the user to view information such as which cohort has a higher percentage of heart disease or what the gender breakdown is for each cohort. Figure shows the distributions of nicotine dependence in the baseline (top) and focus (bottom) using the designs from Figure 3-d. In this example, the weighted focus cohort\u2019s distribution has shifted to match that of the baseline cohort. This design is also incorporated into a histogram visualization for numeric dimensions (e.g., age) and a dumbbell plot for categorical dimensions (e.g., gender or race).",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "distancevs.correlationplots",
        "axial_code": ["Repetition"],
        "componenet_code": ["correlationplots", "distancevs"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 190,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R4. Prevent over\ufb01tting for poorly represented subgroups. Insome cases the weighted samples used for bias correction canexcessively amplify poorly sampled subgroups, similar to theproblem of model over\ufb01tting. Users must be able to understandwhen a proposed bias correction poses a risk of over\ufb01tting dueto limitations in the underlying data, and be able to revise thereweighting con\ufb01guration appropriately.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "The danger score alerts the user to potential problems with the current reweighting configuration. A score is computed for each cohort to identify those with poorly sampled subgroups. An indicator is shown next to any cohort with a score approaching a system-defined threshold, indicating that adjustments to the current reweighting configuration may be warranted.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 191,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R4. Prevent over\ufb01tting for poorly represented subgroups. Insome cases the weighted samples used for bias correction canexcessively amplify poorly sampled subgroups, similar to theproblem of model over\ufb01tting. Users must be able to understandwhen a proposed bias correction poses a risk of over\ufb01tting dueto limitations in the underlying data, and be able to revise thereweighting con\ufb01guration appropriately.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "To further support the DR workflow, we have added a replace reweight mode. When fine-tuning the reweighting configuration, the user may wish to adjust a reweight dimension by moving up in the hierarchy to a more general type or down to a more specific type. To do so, the user can enter replace reweight mode, causing the icicle plot to show only the reweight dimension, its ancestors, and its descendants. All ancestors, the reweight dimension, and two levels of children are marked as salient, providing a detailed view of the local neighborhood around the reweight dimension. The user can then select a dimension to take its place in the list of reweight dimensions.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "icicle+table",
        "axial_code": ["Stack"],
        "componenet_code": ["table", "icicle"]
      },
      {
        "solution_text": "To further support the DR workflow, we have added a replace reweight mode. When fine-tuning the reweighting configuration, the user may wish to adjust a reweight dimension by moving up in the hierarchy to a more general type or down to a more specific type. To do so, the user can enter replace reweight mode, causing the icicle plot to show only the reweight dimension, its ancestors, and its descendants. All ancestors, the reweight dimension, and two levels of children are marked as salient, providing a detailed view of the local neighborhood around the reweight dimension. The user can then select a dimension to take its place in the list of reweight dimensions.",
        "solution_category": "interaction",
        "solution_axial": "OverviewandExplore",
        "solution_compoent": "",
        "axial_code": ["OverviewandExplore"],
        "componenet_code": ["overview_and_explore"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 192,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R4. Prevent over\ufb01tting for poorly represented subgroups. Insome cases the weighted samples used for bias correction canexcessively amplify poorly sampled subgroups, similar to theproblem of model over\ufb01tting. Users must be able to understandwhen a proposed bias correction poses a risk of over\ufb01tting dueto limitations in the underlying data, and be able to revise thereweighting con\ufb01guration appropriately.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "The balance panel includes visualizations and controls for the DR process. A reweight list shows all dimensions selected for reweighting. The user can remove dimensions from the list, initiate the reweighting process, and use a slider to control the amount of reweighting. A detailed view of the per-cohort subgroups used for reweighting is shown in the reweight set visualization.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 193,
    "paper_title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting",
    "pub_year": 2021,
    "domain": "bias ",
    "requirement": {
      "requirement_text": "R4. Prevent over\ufb01tting for poorly represented subgroups. Insome cases the weighted samples used for bias correction canexcessively amplify poorly sampled subgroups, similar to theproblem of model over\ufb01tting. Users must be able to understandwhen a proposed bias correction poses a risk of over\ufb01tting dueto limitations in the underlying data, and be able to revise thereweighting con\ufb01guration appropriately.",
      "requirement_code": { "identify_main_cause_item": 1 }
    },
    "data": {
      "data_text": "The prototype implementation of the techniques described in this paper are applied to medical data which contain both non-temporal attributes (e.g., gender and race) and time-dependent events (e.g., diagnoses and procedures). Medical events are represented using widely-used coding systems such as ICD-10-CM [31] and SNOMED-CT [37] for diagnoses and procedures, respectively. These coding systems include over 300,000 distinct codes organized within hierarchical structures, and the electronic health record data can contain events coded at various levels of details. For instance, a single patient might be diagnosed with a generic ICD-10-CM code of I50: Heart Failure at one time and the more specific I50.32: Chronic diastolic (congestive) heart failure at another time. The hierarchical nature of the coding systems means that a patient with the specific I50.32 diagnosis would also be considered to have the more generic I50. This property highlights the importance of understanding selection bias and how DR corrects for it at different levels of specificity in the event type hierarchies.",
      "data_code": { "network_and_trees": 1, "quantitative": 1, "tables": 1 }
    },
    "solution": [
      {
        "solution_text": "The reweight set visualization is based on Upset [26]. It shows how the reweight dimensions combine to form subgroups of each cohort to be reweighted (Section 6), and enables identification and cor- rection of potential reweighting issues that may result in unreasonably high weights for small subgroups of individuals.",
        "solution_category": "visualization",
        "solution_axial": "Stack",
        "solution_compoent": "matrix+bar",
        "axial_code": ["Stack"],
        "componenet_code": ["matrix", "bar"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 194,
    "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection",
    "pub_year": 2021,
    "domain": "autonomous driving",
    "requirement": {
      "requirement_text": "x-ray image classi\ufb01cation",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "text": 1
      }
    },
    "solution": [
      {
        "solution_text": "In the second module, two core learning components, disentangled representation learning and semantic adversarial learning, are introduced to augment our analysis and meet the requirements of RD1 and RD2. Disentangled representation learning extracts intrinsic and interpretable attributes of traffic lights, such as colors, brightness, and background (Fig. 4b1 ). This component first provides a human-friendly data presentation, and also offers a data space where an adversarial generation can efficiently search. Semantic adversarial learning learns prediction behaviors of a detector, and generates meaningful adversarial examples (Fig. 4b2 ) on top of disentangled representation learning. After this, both acquired data (Fig. 4b3 ) and unseen data (Fig. 4b4 ) are passed to the detector to obtain detection results.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "Disentangledrepresentationandsemanticadversariallearning.",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 195,
    "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection",
    "pub_year": 2021,
    "domain": "autonomous driving",
    "requirement": {
      "requirement_text": "RD1: Human-friendly data representation and summarization. This issue arises from the nature of high dimensionality and sheer volume of images. We need a representation to capture the intrinsic attributes of images in a lower dimension space and then summarize them in a human-friendly way [27,28].",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "text": 1
      }
    },
    "solution": [
      {
        "solution_text": "Disentangled Representation Learning (DRL) is introduced to extract semantic latent representation of traffic lights (e.g. colors, background, rotation, etc.), shown in Fig.5-a and generate more data with controllable semantics for data augmentation. The semantic latent representation also serves as a cornerstone for both human-friendly data summarization (RD1) and semantic adversarial learning (RD2).",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "Semanticlatentrepresentationextractionfortrafficlights.",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 196,
    "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection",
    "pub_year": 2021,
    "domain": "autonomous driving",
    "requirement": {
      "requirement_text": "RD2: Efficient generation of unseen test cases. We seek for a method generating edge cases to probe model robustness. These test cases should be different from the imperceivable noises that learned from traditional adversarial approaches, and have semantic meanings to guide human to improve the robustness [10,23].",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "text": 1
      }
    },
    "solution": [
      {
        "solution_text": "In the second module, two core learning components, disentangled representation learning and semantic adversarial learning, are introduced to augment our analysis and meet the requirements of RD1 and RD2. Disentangled representation learning extracts intrinsic and interpretable attributes of traffic lights, such as colors, brightness, and background (Fig. 4b1 ). This component first provides a human-friendly data presentation, and also offers a data space where an adversarial generation can efficiently search. Semantic adversarial learning learns prediction behaviors of a detector, and generates meaningful adversarial examples (Fig. 4b2 ) on top of disentangled representation learning. After this, both acquired data (Fig. 4b3 ) and unseen data (Fig. 4b4 ) are passed to the detector to obtain detection results.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "Disentangledrepresentationandsemanticadversariallearning.",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 197,
    "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection",
    "pub_year": 2021,
    "domain": "autonomous driving",
    "requirement": {
      "requirement_text": "RD2: Efficient generation of unseen test cases. We seek for a method generating edge cases to probe model robustness. These test cases should be different from the imperceivable noises that learned from traditional adversarial approaches, and have semantic meanings to guide human to improve the robustness [10,23].",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "text": 1
      }
    },
    "solution": [
      {
        "solution_text": "Disentangled Representation Learning (DRL) is introduced to extract semantic latent representation of traffic lights (e.g. colors, background, rotation, etc.), shown in Fig.5-a and generate more data with controllable semantics for data augmentation. The semantic latent representation also serves as a cornerstone for both human-friendly data summarization (RD1) and semantic adversarial learning (RD2).",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "Semanticlatentrepresentationextractionfortrafficlights.",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 198,
    "paper_title": "VATLD: A Visual Analytics System to Assess, Understand and Improve Traffic Light Detection",
    "pub_year": 2021,
    "domain": "autonomous driving",
    "requirement": {
      "requirement_text": "RP1: A contextualized understanding for model performance. In_x0002_stead of using an aggregated metric to evaluate models [18], we would like to put a single score into the contexts of various sizes, IoU thresholds and confident score ranges.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "Bosch Small Traffic Lights Dataset: The dataset includes 5093 training images (10756 annotated traffic lights) and 8334 test images (13486 annotated traffic lights). The baseline traffic light detector was provided by domain experts for case study pur- pose.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "media": 1,
        "text": 1
      }
    },
    "solution": [
      {
        "solution_text": "In the second module, two core learning components, disentangled representation learning and semantic adversarial learning, are introduced to augment our analysis and meet the requirements of RD1 and RD2. Disentangled representation learning extracts intrinsic and interpretable attributes of traffic lights, such as colors, brightness, and background (Fig. 4b1 ). This component first provides a human-friendly data presentation, and also offers a data space where an adversarial generation can efficiently search. Semantic adversarial learning learns prediction behaviors of a detector, and generates meaningful adversarial examples (Fig. 4b2 ) on top of disentangled representation learning. After this, both acquired data (Fig. 4b3 ) and unseen data (Fig. 4b4 ) are passed to the detector to obtain detection results.",
        "solution_category": "data_manipulation",
        "solution_axial": "Modeling",
        "solution_compoent": "Disentangledrepresentationandsemanticadversariallearning.",
        "axial_code": ["Modeling"],
        "componenet_code": ["modeling"]
      },
      {
        "solution_text": "The metric charts are coordinated with each other to filter data and support multi-faceted performance analysis for accuracy and robustness in other views. This is designed to support contextualized understanding of performance.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table+bar",
        "axial_code": ["Repetition"],
        "componenet_code": ["table", "bar"]
      },
      {
        "solution_text": "The metric charts are coordinated with each other to filter data and support multi-faceted performance analysis for accuracy and robustness in other views. This is designed to support contextualized understanding of performance.",
        "solution_category": "interaction",
        "solution_axial": "Filtering",
        "solution_compoent": "",
        "axial_code": ["Filtering"],
        "componenet_code": ["filtering"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 202,
    "paper_title": "Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images",
    "pub_year": 2021,
    "domain": "x-ray image classi\ufb01cation",
    "requirement": {
      "requirement_text": "T1. Analysis in Model Spaces: Investigate scienti\ufb01c images withinthe spaces of ACT, PRD, and FEA, for users to understand how theimages are modeled by the ResNet in the feature space and then classi-\ufb01ed by fully connected layers in the prediction space, with respect tothe real labels. Users can study ResNet model performance by com-paring the distributions of images after feature extraction (FEA), afterclassi\ufb01cation (PRD), and with actual labels (ACT). This study needs tobe performed in an exploratory process. Therefore, it is important tovisualize the images in the three spaces at the same time",
      "requirement_code": { "discover_observation": 1 }
    },
    "data": {
      "data_text": "We utilize an open x-ray scattering dataset [36], and an updated ResNet model [33] that was designed for multiple attributes classification of the dataset. About 1,000 x-ray scattering images were employed in our visualization system. They include different types of images including semiconductors, nano-particles, polymer, lithographic gratings, and so on. The attributes in these images are either labeled by domain experts or synthetically generated by a simulation algorithm [33]. Each image thus has an actual attribute vector (ACT vector) consisting of 17 Boolean (0 or 1) values to show if the image has a number of the 17 attributes.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "categorical": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "The goal is to allow users to interactively select, compare, and study images of interest. Therefore, the 2048-dimensional FEA space and 17-dimensional ACT and PRD spaces are projected to the embedded 2D spaces to fulfill the goal. In our system, we have included two commonly used dimension reduction (DR) algorithms, t-SNE and PCA, for deep learning visualization. Other DR methods may further be added.",
        "solution_category": "data_manipulation",
        "solution_axial": "DimensionalityReduction",
        "solution_compoent": "",
        "axial_code": ["DimensionalityReduction"],
        "componenet_code": ["dimensionality_reduction"]
      },
      {
        "solution_text": "Coordinated Visualization in Embedded Spaces: Images are visualized in the 2D canvases of ACT, PRD and FEA spaces, respectively. The goal is to allow users to easily observe many x-ray images and their relationships in these spaces simultaneously.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "scatter",
        "axial_code": ["Repetition"],
        "componenet_code": ["scatter"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 203,
    "paper_title": "Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images",
    "pub_year": 2021,
    "domain": "x-ray image classi\ufb01cation",
    "requirement": {
      "requirement_text": "T2. Analysis with Group Behaviors: Select and examine speci\ufb01cgroups of images in the ACT, PRD, and FEA spaces, in order to \ufb01ndimportant clusters and outliers with respect to the learning model.",
      "requirement_code": { "collect_evidence": 1 }
    },
    "data": {
      "data_text": "We utilize an open x-ray scattering dataset [36], and an updated ResNet model [33] that was designed for multiple attributes classification of the dataset. About 1,000 x-ray scattering images were employed in our visualization system. They include different types of images including semiconductors, nano-particles, polymer, lithographic gratings, and so on. The attributes in these images are either labeled by domain experts or synthetically generated by a simulation algorithm [33]. Each image thus has an actual attribute vector (ACT vector) consisting of 17 Boolean (0 or 1) values to show if the image has a number of the 17 attributes.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "categorical": 1
      }
    },
    "solution": [
      {
        "solution_text": "Image Group Selection and Visualization: Within the embedded spaces, users are enabled to flexibly select images into groups at each embedded space by lasso tools. Then the selected images in each group are visually highlighted in other spaces. This function is very important for users to freely explore images of interest and conduct comparative analysis among the three spaces. The grouped images are also visualized by a statistic view of image metrics and an image gallery view. They can be further clustered for drill-down study and comparison.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "radialbarchart",
        "axial_code": ["Repetition"],
        "componenet_code": ["radialbarchart"]
      },
      {
        "solution_text": "Image Group Selection and Visualization: Within the embedded spaces, users are enabled to flexibly select images into groups at each embedded space by lasso tools. Then the selected images in each group are visually highlighted in other spaces. This function is very important for users to freely explore images of interest and conduct comparative analysis among the three spaces. The grouped images are also visualized by a statistic view of image metrics and an image gallery view. They can be further clustered for drill-down study and comparison.",
        "solution_category": "interaction",
        "solution_axial": "Selecting,Participation/Collaboration",
        "solution_compoent": "",
        "axial_code": ["Participation/Collaboration", "Selecting"],
        "componenet_code": ["participation/collaboration", "selecting"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 204,
    "paper_title": "Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images",
    "pub_year": 2021,
    "domain": "x-ray image classi\ufb01cation",
    "requirement": {
      "requirement_text": "T3. Analysis with Image Attributes: Identify important image in-stances with the performance metrics of individual attributes and co-existent attributes to perform the \ufb01rst two tasks.",
      "requirement_code": { "describe_observation_item": 1 }
    },
    "data": {
      "data_text": "We utilize an open x-ray scattering dataset [36], and an updated ResNet model [33] that was designed for multiple attributes classification of the dataset. About 1,000 x-ray scattering images were employed in our visualization system. They include different types of images including semiconductors, nano-particles, polymer, lithographic gratings, and so on. The attributes in these images are either labeled by domain experts or synthetically generated by a simulation algorithm [33]. Each image thus has an actual attribute vector (ACT vector) consisting of 17 Boolean (0 or 1) values to show if the image has a number of the 17 attributes.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "categorical": 1
      }
    },
    "solution": [
      {
        "solution_text": "Attribute Co-existence Visualization: The model perfor- mance with the relations of co-existing attributes is visualized in an interactive view. Users can then define image groups based on the visual cues of model performance.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "table",
        "axial_code": ["Repetition"],
        "componenet_code": ["table"]
      }
    ]
  },
  {
    "author": "zsz",
    "index_original": 205,
    "paper_title": "Interactive Visual Study of Multiple Attributes Learning Model of X-Ray Scattering Images",
    "pub_year": 2021,
    "domain": "x-ray image classi\ufb01cation",
    "requirement": {
      "requirement_text": "T4. Analysis with Comparisons: Compare individual images andimage clusters for the model prediction performance.",
      "requirement_code": { "compare_entities": 1 }
    },
    "data": {
      "data_text": "We utilize an open x-ray scattering dataset [36], and an updated ResNet model [33] that was designed for multiple attributes classification of the dataset. About 1,000 x-ray scattering images were employed in our visualization system. They include different types of images including semiconductors, nano-particles, polymer, lithographic gratings, and so on. The attributes in these images are either labeled by domain experts or synthetically generated by a simulation algorithm [33]. Each image thus has an actual attribute vector (ACT vector) consisting of 17 Boolean (0 or 1) values to show if the image has a number of the 17 attributes.",
      "data_code": {
        "tables": 1,
        "clusters_and_sets_and_lists": 1,
        "categorical": 1,
        "media": 1
      }
    },
    "solution": [
      {
        "solution_text": "Group Comparative View and Image Comparison: The selected groups can be easily investigated and compared with group panels for detailed views. Through interactive selection over all the above visualizations, users can also open multiple images to compare their details of raw data and model predictions.",
        "solution_category": "visualization",
        "solution_axial": "Repetition",
        "solution_compoent": "image+matrix",
        "axial_code": ["Repetition"],
        "componenet_code": ["matrix", "image"]
      },
      {
        "solution_text": "Group Comparative View and Image Comparison: The selected groups can be easily investigated and compared with group panels for detailed views. Through interactive selection over all the above visualizations, users can also open multiple images to compare their details of raw data and model predictions.",
        "solution_category": "interaction",
        "solution_axial": "Selecting",
        "solution_compoent": "",
        "axial_code": ["Selecting"],
        "componenet_code": ["selecting"]
      }
    ]
  }
]
